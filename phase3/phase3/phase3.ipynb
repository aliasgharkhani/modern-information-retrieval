{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from itertools import groupby\n",
    "from joblib import dump, load\n",
    "from sklearn import svm\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = []\n",
    "paper_authors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(start_urls, limit):\n",
    "    process = CrawlerProcess(get_project_settings())\n",
    "    process.crawl('first', start_urls=start_urls, limit=limit)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# crawler(start_urls = [\n",
    "#             \"https://www.semanticscholar.org/paper/The-Lottery-Ticket-Hypothesis%3A-Training-Pruned-Frankle-Carbin/f90720ed12e045ac84beb94c27271d6fb8ad48cf\",\n",
    "#             \"https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\n",
    "#             \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\"\n",
    "#         ], limit=2000)\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"name\" : \"DESKTOP-ROLDIVD\",\\n  \"cluster_name\" : \"elasticsearch\",\\n  \"cluster_uuid\" : \"5oe-5bcqR7-aftBs4zCoOQ\",\\n  \"version\" : {\\n    \"number\" : \"7.7.1\",\\n    \"build_flavor\" : \"unknown\",\\n    \"build_type\" : \"unknown\",\\n    \"build_hash\" : \"ad56dce891c901a492bb1ee393f12dfff473a423\",\\n    \"build_date\" : \"2020-05-28T16:30:01.040088Z\",\\n    \"build_snapshot\" : false,\\n    \"lucene_version\" : \"8.5.1\",\\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('http://localhost:9200')\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def es_iterate_all_documents(es, index, pagesize=350, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Helper to iterate ALL values from\n",
    "#     Yields all the documents.\n",
    "#     \"\"\"\n",
    "#     offset = 0\n",
    "#     while True:\n",
    "#         result = es.search(index=index, **kwargs, body={\n",
    "#             \"size\": pagesize,\n",
    "#             \"from\": offset\n",
    "#         })\n",
    "#         hits = result[\"hits\"][\"hits\"]\n",
    "#         # Stop after no more docs\n",
    "#         if not hits:\n",
    "#             break\n",
    "#         # Yield each entry\n",
    "#         yield from (hit['_source'] for hit in hits)\n",
    "#         # Continue from there\n",
    "#         offset += pagesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, host, port, data_dir):\n",
    "        self.es = Elasticsearch([{'host': host, 'port': port}])\n",
    "        with open(data_dir, encoding=\"utf8\") as json_file:\n",
    "            self.papers = json.load(json_file)\n",
    "    def delete(self, index_name='paper_index'):\n",
    "        self.es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    \n",
    "    def save_data(self, index_name='paper_index'):\n",
    "        global paper_ids\n",
    "        for paper in self.papers:\n",
    "            paper_ids.append(paper['id'])\n",
    "            paper_authors[paper['id']] = paper['authors']\n",
    "            self.es.index(index=index_name, id=paper['id'], body=json.dumps({\"paper\":paper}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alpha(host, port, alpha, index_name='paper_index'):\n",
    "    es = Elasticsearch([{'host': host, 'port': port}])\n",
    "    global paper_ids\n",
    "    p = np.zeros((len(paper_ids), len(paper_ids)))\n",
    "    aux = np.ones((len(paper_ids), len(paper_ids))) * (1/len(paper_ids))\n",
    "    for paper_idx, paper_id in enumerate(paper_ids):\n",
    "        entry = es.get(index_name, id=paper_id)['_source']['paper']\n",
    "        references = entry.get('references', False)\n",
    "        if references:\n",
    "            for reference_id in entry['references']:\n",
    "                try:\n",
    "                    reference_idx = paper_ids.index(reference_id)\n",
    "                    p[reference_idx][paper_idx] = 1\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    sums = np.sum(p, axis=1, keepdims=True)\n",
    "    p = ((sums > 0) * 1) * (((1 - alpha) * p) / (sums + (sums == 0) * 1) + (alpha * aux))  + ((sums == 0) * 1) * aux\n",
    "    x = np.ones((len(paper_ids))) * (1/(len(paper_ids)))\n",
    "    while True:\n",
    "        aux = x @ p\n",
    "        if np.all(np.abs(aux - x) < 0.00001):\n",
    "            break\n",
    "        x = aux\n",
    "    p = 0\n",
    "    aux = 0\n",
    "    del p\n",
    "    del aux\n",
    "    for idx, paper_id in enumerate(paper_ids):\n",
    "        body = es.get(index_name, id=paper_id)['_source']\n",
    "        body['paper']['page_rank'] = x[idx]\n",
    "        response = es.index(index=index_name, id=paper_id, body=body)\n",
    "    print(x)\n",
    "    print(np.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(host, port, title, title_weight, abstract, abstract_weight, date, date_weight, use_page_rank, page_rank_weight=0):\n",
    "    es = Elasticsearch([{'host': host, 'port': port}])\n",
    "    if use_page_rank:\n",
    "        search_param = {\n",
    "            \"query\": {\n",
    "\n",
    "                \"function_score\": {\n",
    "#                   \"query\": {\n",
    "#                       \"match_all\" : {}\n",
    "#                   },\n",
    "                  \"functions\": [\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"match_phrase\": {\n",
    "                          \"paper.title\": title\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": title_weight\n",
    "                    },\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"match_phrase\": {\n",
    "                          \"paper.abstract\": abstract\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": abstract_weight\n",
    "                    },\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"range\": {\n",
    "                          \"paper.date\": {\"gte\": date}\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": date_weight\n",
    "                    },\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"_score * saturation(doc['paper.page_rank'].value, 0.0001)\"\n",
    "                                \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                  ],\n",
    "                  \"score_mode\": \"sum\", \n",
    "                  \"boost\": \"5\",\n",
    "                  \"boost_mode\": \"multiply\",\n",
    "\n",
    "                }\n",
    "\n",
    "            }  \n",
    "        }\n",
    "        response = es.search(index=\"paper_index\", body=search_param, size=10)\n",
    "        for idx, i in enumerate(response['hits']['hits']):\n",
    "            paper = i['_source']['paper']\n",
    "            print(idx, paper['title'], '\\n', paper['abstract'], '\\n', paper['authors'], '\\n', paper['date'])\n",
    "            print('-' * 60)\n",
    "        print('*^'*60)\n",
    "        search_param = {\n",
    "            \"query\": {\n",
    "\n",
    "                \"function_score\": {\n",
    "#                   \"query\": {\n",
    "#                       \"match_all\" : {}\n",
    "#                   },\n",
    "                  \"functions\": [\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"match_phrase\": {\n",
    "                          \"paper.title\": title\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": title_weight\n",
    "                    },\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"match_phrase\": {\n",
    "                          \"paper.abstract\": abstract\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": abstract_weight\n",
    "                    },\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"range\": {\n",
    "                          \"paper.date\": {\"gte\": date}\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": date_weight\n",
    "                    }\n",
    "                  ],\n",
    "                  \"score_mode\": \"sum\", \n",
    "                  \"boost\": \"5\",\n",
    "                  \"boost_mode\": \"multiply\",\n",
    "\n",
    "                }\n",
    "\n",
    "            },\n",
    "            \"sort\": [{ \"_score\": { \"order\": \"desc\" }}],\n",
    "        }\n",
    "        response = es.search(index=\"paper_index\", body=search_param, size=10)\n",
    "        for idx, i in enumerate(response['hits']['hits']):\n",
    "            paper = i['_source']['paper']\n",
    "            print(idx, paper['title'], '\\n', paper['abstract'], '\\n', paper['authors'], '\\n', paper['date'])\n",
    "            print('-' * 60)\n",
    "    else:\n",
    "        search_param = {\n",
    "#             \"from\" : 0, \"size\" : 10,\n",
    "\n",
    "            \"query\": {\n",
    "\n",
    "                \"function_score\": {\n",
    "#                   \"query\": {\n",
    "#                       \"match_all\" : {}\n",
    "#                   },\n",
    "                  \"functions\": [\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"match_phrase\": {\n",
    "                          \"paper.title\": title\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": title_weight\n",
    "                    },\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"match_phrase\": {\n",
    "                          \"paper.abstract\": abstract\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": abstract_weight\n",
    "                    },\n",
    "                    {\n",
    "                      \"filter\": {\n",
    "                        \"range\": {\n",
    "                          \"paper.date\": {\"gte\": date}\n",
    "                        }\n",
    "                      },\n",
    "                      \"weight\": date_weight\n",
    "                    }\n",
    "                  ],\n",
    "                  \"score_mode\": \"sum\", \n",
    "                  \"boost\": \"5\",\n",
    "                  \"boost_mode\": \"multiply\",\n",
    "\n",
    "                }\n",
    "\n",
    "            },\n",
    "            \"sort\": [{ \"_score\": { \"order\": \"desc\" }}],   \n",
    "        }\n",
    "        response = es.search(index=\"paper_index\", body=search_param, size=10)\n",
    "        for i in response['hits']['hits']:\n",
    "            paper = i['_source']['paper']\n",
    "            print(paper['title'], '\\n', paper['abstract'], '\\n', paper['authors'], '\\n', paper['date'])\n",
    "            print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = Index('localhost', 9200, 'papers.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_alpha('localhost', 9200, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "# for id in paper_ids:\n",
    "#     print(es.get(index='paper_index', id=id)['_source']['paper']['page_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search('localhost', 9200, title='Attention is All you Need', title_weight=10, abstract='We propose a novel neural attention', abstract_weight=15, date=2017, date_weight=5, use_page_rank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# search('localhost', 9200, title='Attention is All you Need', title_weight=10, abstract='We propose a novel neural attention', abstract_weight=15, date=2017, date_weight=5, use_page_rank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_link = {}\n",
    "authority = {}\n",
    "hub = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_HITS(host, port, authors_number):\n",
    "    global paper_authors\n",
    "    es = Elasticsearch([{'host': host, 'port': port}])\n",
    "    for paper_id in paper_ids:\n",
    "        paper = es.get('paper_index', paper_id)['_source']['paper']\n",
    "        references = paper.get('references', False)\n",
    "        if references:\n",
    "            for reference_id in references:\n",
    "                reference_authors = paper_authors.get(reference_id, False)\n",
    "                if reference_authors:\n",
    "                    for reference_author in reference_authors:\n",
    "                        author = authors_link.get(reference_author, False)\n",
    "                        if author:\n",
    "                            authors_link[reference_author] = author.union(paper.get('authors', None))\n",
    "                        else:\n",
    "                            authors_link[reference_author] = set(paper.get('authors', None))\n",
    "                            \n",
    "    for i in range(5):\n",
    "        for author, r_authors in authors_link.items():\n",
    "            aux = authority.get(author, False)\n",
    "            if not aux:\n",
    "                authority[author] = 1\n",
    "            for r_author in r_authors:\n",
    "                authority[author] += hub.get(r_author, 1)\n",
    "        for author, r_authors in authors_link.items():\n",
    "            for r_author in r_authors:\n",
    "                aux = hub.get(r_author, False)\n",
    "                if not aux:\n",
    "                    hub[r_author] = 1\n",
    "                hub[r_author] += authority.get(author, 1)\n",
    "    print('asf')\n",
    "    for k, v in sorted(authority.items(), key=lambda item: item[1], reverse=True)[:authors_number]:\n",
    "        print(k, v)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asf\n"
     ]
    }
   ],
   "source": [
    "sort_by_HITS('localhost', 9200, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_by_HITS('localhost', 9200, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose one of these:\n",
      " 1.crawl\n",
      " 2.indexing\n",
      " 3.evaluating papers\n",
      " 4.search\n",
      " 5.sort authors by HITS\n",
      " 6.exit\n",
      "2\n",
      "enter address of json file\n",
      "D:\\education\\98-99-2\\modern information retrieval\\project\\phase3\\phase3\\papers.json\n",
      "first enter host address then enter port on which elasticsearch is running\n",
      "host:localhost\n",
      "port:9200\n",
      "to save data in elasticsrach enter 1\n",
      "to delete data saved in elasticsearch press 2\n",
      "to exit this mode press 3\n",
      "1\n",
      "to save data in elasticsrach enter 1\n",
      "to delete data saved in elasticsearch press 2\n",
      "to exit this mode press 3\n",
      "3\n",
      "choose one of these:\n",
      " 1.crawl\n",
      " 2.indexing\n",
      " 3.evaluating papers\n",
      " 4.search\n",
      " 5.sort authors by HITS\n",
      " 6.exit\n",
      "5\n",
      "enter host and port of server and number of authors you want to be returned\n",
      "host:localhost\n",
      "port:9200\n",
      "authors number:10\n",
      "asf\n",
      "Yoshua Bengio 7579474926266793\n",
      "Ilya Sutskever 7210802856506559\n",
      "Geoffrey E. Hinton 5786486920067150\n",
      "Quoc V. Le 5608604046913263\n",
      "Oriol Vinyals 5107951659190343\n",
      "Andrew Y. Ng 4417444935661362\n",
      "Jason Weston 4088627336399566\n",
      "Christopher D. Manning 4078108434434145\n",
      "Kyunghyun Cho 3795177720018078\n",
      "Richard Socher 3779113738409020\n",
      "choose one of these:\n",
      " 1.crawl\n",
      " 2.indexing\n",
      " 3.evaluating papers\n",
      " 4.search\n",
      " 5.sort authors by HITS\n",
      " 6.exit\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print('choose one of these:\\n 1.crawl\\n 2.indexing\\n 3.evaluating papers\\n 4.search\\n 5.sort authors by HITS\\n 6.exit')\n",
    "    order = int(input())\n",
    "    if order == 1:\n",
    "        start_urls = []\n",
    "        print('enter start urls one by one and then press enter')\n",
    "        for i in range(3):\n",
    "            print('url number {}'.format(i+1), end='')\n",
    "            start_urls.append(input())\n",
    "        print('enter number of papers you want to crawl')\n",
    "        print('limit:', end='')\n",
    "        crawl_limit = int(input())\n",
    "        crawler(start_urls = start_urls, limit=crawl_limit)\n",
    "    elif order == 2:\n",
    "        print('enter address of json file')\n",
    "        json_addr = input()\n",
    "        print('first enter host address then enter port on which elasticsearch is running')\n",
    "        print('host:', end='')\n",
    "        host = input()\n",
    "        print('port:', end='')\n",
    "        port = int(input())\n",
    "        index = Index(host, port, json_addr)\n",
    "        while True:\n",
    "            print('to save data in elasticsrach enter 1\\nto delete data saved in elasticsearch press 2\\nto exit this mode press 3')\n",
    "            sub_order = int(input())\n",
    "            if sub_order == 1:\n",
    "                index.save_data()\n",
    "            elif sub_order == 2:\n",
    "                index.delete()\n",
    "            elif sub_order == 3:\n",
    "                break\n",
    "    elif order == 3:\n",
    "        print('enter host and port of server and then value of alpha you want to be applied')\n",
    "        print('host:', end='')\n",
    "        host = input()\n",
    "        print('port:', end='')\n",
    "        port = int(input())\n",
    "        print('alpha:', end='')\n",
    "        alpha = float(input())\n",
    "        calc_alpha(host, port, alpha)\n",
    "    elif order == 4:\n",
    "        print('enter host and port of server and then weights and values of fields. At the end specify if you want page rank to have an effect on the results or not')\n",
    "        print('host:', end='')\n",
    "        host = input()\n",
    "        print('port:', end='')\n",
    "        port = int(input())\n",
    "        print('title weight:', end='')\n",
    "        title_weight = int(input())\n",
    "        print('title:', end='')\n",
    "        title = input()\n",
    "        print('abstract weight:', end='')\n",
    "        abstract_weight = int(input())\n",
    "        print('abstract:', end='')\n",
    "        abstract = input()\n",
    "        print('date weight:', end='')\n",
    "        date_weight = int(input())\n",
    "        print('date:', end='')\n",
    "        date = input()\n",
    "        print('page rank effect: y or n', end='')\n",
    "        use_page_rank = True if input() == 'y' else False\n",
    "        search(host, port, title, title_weight, abstract, abstract_weight, date, date_weight, use_page_rank)\n",
    "    elif order == 5:\n",
    "        print('enter host and port of server and number of authors you want to be returned')\n",
    "        print('host:', end='')\n",
    "        host = input()\n",
    "        print('port:', end='')\n",
    "        port = int(input())    \n",
    "        print('authors number:', end='')\n",
    "        authors_number = int(input())   \n",
    "        sort_by_HITS(host, port, authors_number)\n",
    "    elif order == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/education/98-99-2/modern information retrieval/project/phase3/data/train.txt', mode='r') as file:\n",
    "    raw_train_data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/education/98-99-2/modern information retrieval/project/phase3/data/vali.txt', mode='r') as file:\n",
    "    raw_val_data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/education/98-99-2/modern information retrieval/project/phase3/data/test.txt', mode='r') as file:\n",
    "    raw_test_data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = {}\n",
    "val_docs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_doc in raw_val_data:\n",
    "    query_id = query_doc.split()[1].split(':')[-1]\n",
    "    doc = val_docs.get(query_id, False)\n",
    "    if doc:\n",
    "        val_docs[query_id].append({\"relevance\" : int(query_doc[0]), \"feature\" : list(map(lambda t:float(t.split(':')[-1]), query_doc.split()[2:48]))})\n",
    "    else:\n",
    "        val_docs[query_id] = [{\"relevance\" : int(query_doc[0]), \"feature\" : list(map(lambda t:float(t.split(':')[-1]), query_doc.split()[2:48]))}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_doc in raw_test_data:\n",
    "    query_id = query_doc.split()[1].split(':')[-1]\n",
    "    doc = test_docs.get(query_id, False)\n",
    "    if doc:\n",
    "        test_docs[query_id].append({\"relevance\" : int(query_doc[0]), \"feature\" : list(map(lambda t:float(t.split(':')[-1]), query_doc.split()[2:48]))})\n",
    "    else:\n",
    "        test_docs[query_id] = [{\"relevance\" : int(query_doc[0]), \"feature\" : list(map(lambda t:float(t.split(':')[-1]), query_doc.split()[2:48]))}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG(ranking, ground_truth):\n",
    "    gt_dcg = 0\n",
    "    for idx, r in enumerate(ground_truth):\n",
    "        if idx == 0:\n",
    "            gt_dcg += r\n",
    "        else:\n",
    "            gt_dcg += r / np.log2(idx+1)\n",
    "    dcg = 0\n",
    "    for idx, r in enumerate(ranking):\n",
    "        if idx == 0:\n",
    "            dcg += r\n",
    "        else:\n",
    "            dcg += r / np.log2(idx+1)\n",
    "    return dcg / gt_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubblesort(in_list):\n",
    "\n",
    "# Swap the elements to arrange in order\n",
    "    for iter_num in range(len(in_list)-1,0,-1):\n",
    "        for idx in range(iter_num):\n",
    "            if clf.predict([np.asarray(in_list[idx]['feature']) - np.asarray(in_list[idx+1]['feature'])]) == 0:\n",
    "                temp = in_list[idx]\n",
    "                in_list[idx] = in_list[idx+1]\n",
    "                in_list[idx+1] = temp\n",
    "    ranking = []\n",
    "    for doc in in_list[:5]:\n",
    "        ranking.append(doc['relevance'])\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_docs = None\n",
    "for k, v in groupby(raw_train_data, key=lambda t: t.split()[1].split(':')[-1]):\n",
    "    grouped_docs = sorted(list(v), key=lambda t : t[0], reverse=True)\n",
    "    docs_number = len(grouped_docs)\n",
    "    for i in range(docs_number - 1):\n",
    "        for j in range(i+1, docs_number):\n",
    "            if grouped_docs[i][0] != grouped_docs[j][0]:\n",
    "                first = np.array(list(map(lambda t:float(t.split(':')[-1]), grouped_docs[i].split()[2:48])))\n",
    "                second = np.array(list(map(lambda t:float(t.split(':')[-1]), grouped_docs[j].split()[2:48])))                \n",
    "                if grouped_docs[i][0] > grouped_docs[j][0]:\n",
    "                    train_data.append(first - second)\n",
    "                    train_label.append(1)\n",
    "                    train_data.append(second - first)\n",
    "                    train_label.append(0)\n",
    "                else:\n",
    "                    train_data.append(first - second)\n",
    "                    train_label.append(0)\n",
    "                    train_data.append(second - first)\n",
    "                    train_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711.9629595279694\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf = svm.SVC(C=0.6)\n",
    "clf.fit(train_data, train_label)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_weights_0.6.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf, 'svm_weights_0.6.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('svm_weights_0.8.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs = 0\n",
    "counter = 0\n",
    "for query_id in val_docs.keys():\n",
    "    aux = sorted(list(map(lambda d:d['relevance'], val_docs[query_id])), reverse=True)\n",
    "    if aux[0] != aux[-1]:\n",
    "        ndcgs += NDCG(bubblesort(val_docs[query_id]), aux[:5])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6795808823313301"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcgs/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs = 0\n",
    "counter = 0\n",
    "for query_id in test_docs.keys():\n",
    "    aux = sorted(list(map(lambda d:d['relevance'], test_docs[query_id])), reverse=True)\n",
    "    if aux[0] != aux[-1]:\n",
    "        ndcgs += NDCG(bubblesort(test_docs[query_id]), aux[:5])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6786381129994296"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcgs/counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
