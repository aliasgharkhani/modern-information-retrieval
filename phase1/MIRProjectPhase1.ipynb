{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTWsrYobJnd3"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=30>\n",
    "<p></p><p></p>\n",
    "به نام خدا\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<img src=\"Images/sharif.png\" width=\"25%\">\n",
    "<font color=blue>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=green>\n",
    "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=#FF7500>\n",
    "بهار ۹۹\n",
    "<br>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مقدمه </div>\n",
    "</font>\n",
    "<hr>\n",
    "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
    "<br>\n",
    "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
    "ارزیابی کنید.\n",
    "<br>\n",
    "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
    "import\n",
    "مناسب از کد‌هایتان استفاده کنید.\n",
    "<br>\n",
    "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-nJxAxdJnd8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مجموعه دادگان</div>\n",
    "</font>\n",
    "<hr>\n",
    "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
    "این مجموعه اسناد از دو بخش تشکیل شده است\n",
    ".\n",
    "<br>\n",
    "بخش اول که در فایل \n",
    "Persian.xml\n",
    "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
    "هر سند شامل شناسه\n",
    "(id)،\n",
    "عنوان\n",
    "(title)،   \n",
    "و متن \n",
    "(text)\n",
    "است.\n",
    "بخش دوم که در پوشه‌ی \n",
    "queries\n",
    "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
    "بخش سوم که در پوشه‌ی\n",
    "relevance\n",
    "آمده‌است،\n",
    "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avT4ky8EJnd-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" ><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
    "</font>\n",
    "<hr>\n",
    "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
    "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
    "مراجعه کنید.\n",
    "<br>\n",
    "<br>\n",
    "عملیات مورد انتظار:\n",
    "<ol>\n",
    "    <li>\n",
    "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
    "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
    "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
    "    </li>\n",
    "    <li> \n",
    "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
    "    </li>\n",
    "    <li>\n",
    "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
    "    </li>\n",
    "<li>\n",
    "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
    "ریشه\n",
    "(stemming)\n",
    "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
    "    </li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzEhdYtzJnd_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
    "prepare_text\n",
    "باید طوری بر روی متن ورودی با نام\n",
    "raw_text\n",
    "عمل کند که\n",
    "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "from xml.dom import minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import enum\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Item:\n",
    "#     def __init__(self, key):\n",
    "#         self.key = key\n",
    "#         self.posting_list = []\n",
    "\n",
    "        \n",
    "class BTree:\n",
    "    \n",
    "    class Node:\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.sons = []\n",
    "            self.items = []\n",
    "\n",
    "        def __repr__(self):\n",
    "            return 'Node' + str(list(map(lambda x : x.key, self.items))) + str(self.sons)\n",
    "        \n",
    "        def _lower_bound(self, key):\n",
    "            b = 0\n",
    "            e = len(self.sons) - 1\n",
    "            while b < e:\n",
    "                mid = (b + e + 1) // 2\n",
    "                if mid == 0: # mid is never 0 actually\n",
    "                    pass\n",
    "                elif self.items[mid - 1].key <= key:\n",
    "                    b = mid\n",
    "                else:\n",
    "                    e = mid - 1\n",
    "            return b\n",
    "\n",
    "    def __init__(self, t):\n",
    "        self.root = self.Node()\n",
    "        self.t = t\n",
    "    \n",
    "    \n",
    "    def _inorder(self, cur):\n",
    "        if cur == None: return\n",
    "        \n",
    "        for i, son in enumerate(cur.sons):\n",
    "            if i > 0:\n",
    "                yield cur.items[i - 1].key\n",
    "            yield from self._inorder(son)\n",
    "    \n",
    "    \n",
    "    def inorder(self):\n",
    "        yield from self._inorder(self.root)\n",
    "    \n",
    "    def _preorder(self, cur):\n",
    "        if cur == None: return\n",
    "        for item in cur.items:\n",
    "            yield item.key\n",
    "        for son in cur.sons:\n",
    "            yield from self._preorder(son)\n",
    "    \n",
    "    \n",
    "    def preorder(self):\n",
    "        yield from self._preorder(self.root)\n",
    "        \n",
    "                \n",
    "                \n",
    "    def _split(self, node, parnode, pos):\n",
    "        \n",
    "        # root case\n",
    "        if parnode is None:\n",
    "            self.root = self.Node()\n",
    "            left = self.Node()\n",
    "            right = self.Node()\n",
    "            left.items = node.items[:self.t - 1]\n",
    "            right.items = node.items[self.t:]\n",
    "            left.sons = node.sons[:self.t]\n",
    "            right.sons = node.sons[self.t:]\n",
    "            self.root.items = [ node.items[self.t - 1] ]\n",
    "            self.root.sons = [left, right]\n",
    "            return self.root\n",
    "        else:\n",
    "            left = self.Node()\n",
    "            right = self.Node()\n",
    "            left.items = node.items[:self.t - 1]\n",
    "            right.items = node.items[self.t:]\n",
    "            left.sons = node.sons[:self.t]\n",
    "            right.sons = node.sons[self.t:]\n",
    "            parnode.items = parnode.items[:pos] + [ node.items[self.t - 1] ] + parnode.items[pos:]\n",
    "            parnode.sons = parnode.sons[:pos] + [left, right] + parnode.sons[pos + 1:]\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    def _insert(self, item, node, parnode):\n",
    "        if node is None: return None\n",
    "\n",
    "        # node is full, and must be root\n",
    "        if len(node.items) == 2 * self.t - 1:\n",
    "            assert node == self.root\n",
    "            node = self._split(node, parnode, -1)\n",
    "            assert len(node.items) == 1\n",
    "            \n",
    "            # to the right\n",
    "            if node.items[0].key <= item.key:\n",
    "                self._insert(item, node.sons[1], node)\n",
    "            else:\n",
    "                self._insert(item, node.sons[0], node)\n",
    "            \n",
    "            return\n",
    "        \n",
    "        # only possible for root at the beginning\n",
    "        if len(node.sons) == 0:\n",
    "            assert node == self.root\n",
    "            node.sons.append(None)\n",
    "            node.items.append(item)\n",
    "            node.sons.append(None)\n",
    "            return\n",
    "        \n",
    "        \n",
    "        pos = node._lower_bound(item.key)\n",
    "\n",
    "        \n",
    "        # we are in a leaf\n",
    "        if node.sons[pos] is None:\n",
    "            node.items = node.items[:pos] + [item] + node.items[pos:]\n",
    "            node.sons.append(None)\n",
    "        else:\n",
    "            \n",
    "            # son is full, doing split from here\n",
    "            if node.sons[pos] is not None and len(node.sons[pos].items) == 2 * self.t - 1:\n",
    "                self._split(node.sons[pos], node, pos)\n",
    "                # go to right\n",
    "                if node.items[pos].key <= item.key:\n",
    "                    self._insert(item, node.sons[pos + 1], node)\n",
    "                else:\n",
    "                    self._insert(item, node.sons[pos], node)\n",
    "            else:\n",
    "                self._insert(item, node.sons[pos], node)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def insert(self, item):\n",
    "        self._insert(item, self.root, None)    \n",
    "    \n",
    "    \n",
    "    def _find(self, key, node):\n",
    "        if node is None or len(node.sons) == 0:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        pos = node._lower_bound(key)\n",
    "\n",
    "        \n",
    "        if pos >= 1 and node.items[pos - 1].key == key:\n",
    "            return node.items[pos - 1]\n",
    "        else:\n",
    "            return self._find(key, node.sons[pos])\n",
    "         \n",
    "         \n",
    "    def find(self, key):\n",
    "        return self._find(key, self.root)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _find_predecessor(self, item, node):\n",
    "        if node.sons[0] == None:\n",
    "            return node.items[-1]\n",
    "        else:\n",
    "            return self._find_predecessor(item, node.sons[-1])\n",
    "    \n",
    "    def _find_succesor(self, item, node):\n",
    "        if node.sons[0] == None:\n",
    "            return node.items[0]\n",
    "        else:\n",
    "            return self._find_succesor(item, node.sons[0])\n",
    "    \n",
    "    def _delete_key_leaf(self, item, node, pos):\n",
    "        \n",
    "        # condition for correctness of algorithm\n",
    "        assert node == self.root or len(node.sons) >= self.t\n",
    "        \n",
    "        assert node.items[pos].key == item.key\n",
    "        \n",
    "        node.items = node.items[:pos] + node.items[pos + 1:]\n",
    "        node.sons.pop()\n",
    "        \n",
    "        \n",
    "    def _merge_children_around_key(self, item, node, pos):\n",
    "        \n",
    "        assert pos >= 0 and pos < len(node.sons) - 1\n",
    "        \n",
    "        y = self.Node()\n",
    "        y.sons = node.sons[pos].sons + node.sons[pos + 1].sons\n",
    "        y.items = node.sons[pos].items + [node.items[pos]] + node.sons[pos + 1].items\n",
    "        \n",
    "        node.items = node.items[:pos] + node.items[pos + 1:]\n",
    "        node.sons = node.sons[:pos] + [y] + node.sons[pos + 2:]\n",
    "        \n",
    "        \n",
    "    def _move_node_from_left_child(self, node, pos):\n",
    "        \n",
    "        \n",
    "        \n",
    "        assert pos > 0 and len(node.sons[pos - 1].items) >= self.t\n",
    "        \n",
    "        \n",
    "        node.sons[pos].items = [node.items[pos - 1] ] + node.sons[pos].items\n",
    "        node.sons[pos].sons = [ node.sons[pos - 1].sons[-1] ] + node.sons[pos].sons\n",
    "        \n",
    "        node.items[pos - 1] = node.sons[pos - 1].items[-1]\n",
    "        \n",
    "        node.sons[pos - 1].sons = node.sons[pos - 1].sons[:-1]\n",
    "        node.sons[pos - 1].items = node.sons[pos - 1].items[:-1]\n",
    "        \n",
    "\n",
    "    \n",
    "    def _move_node_from_right_child(self, node, pos):\n",
    "        \n",
    "        \n",
    "        assert pos < len(node.sons) - 1 and len(node.sons[pos + 1].items) >= self.t\n",
    "        \n",
    "        \n",
    "        node.sons[pos].items = node.sons[pos].items + [node.items[pos] ]\n",
    "        node.sons[pos].sons =  node.sons[pos].sons + [ node.sons[pos + 1].sons[0] ] \n",
    "        \n",
    "        node.items[pos] = node.sons[pos + 1].items[0]\n",
    "        \n",
    "        node.sons[pos + 1].sons = node.sons[pos + 1].sons[1:]\n",
    "        node.sons[pos + 1].items = node.sons[pos + 1].items[1:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _fix_empty_root(self, node):\n",
    "        if node == self.root and len(node.sons) == 1:\n",
    "            self.root = node.sons[0]\n",
    "            return self.root\n",
    "        else:\n",
    "            return node\n",
    "    \n",
    "    \n",
    "    def _delete(self, item, node):\n",
    "        if node is None or len(node.sons) == 0: return\n",
    "        \n",
    "        \n",
    "        pos = node._lower_bound(item.key)\n",
    "        \n",
    "        \n",
    "        # the key to delete is here\n",
    "        if pos > 0 and node.items[pos - 1].key == item.key:\n",
    "            \n",
    "            # this node is a leaf\n",
    "            if node.sons[pos] is None:\n",
    "                self._delete_key_leaf(item, node, pos - 1)\n",
    "            # left child node has enough keys\n",
    "            elif len(node.sons[pos - 1].items) >= self.t:\n",
    "                kp = self._find_predecessor(item, node.sons[pos - 1])\n",
    "                node.items[pos - 1] = kp\n",
    "                self._delete(kp, node.sons[pos - 1])\n",
    "            # right child node has enough keys\n",
    "            elif len(node.sons[pos].items) >= self.t:\n",
    "                kp = self._find_succesor(item, node.sons[pos])\n",
    "                node.items[pos - 1] = kp\n",
    "                self._delete(kp, node.sons[pos])\n",
    "            # both children have minimal number of keys, must combine them\n",
    "            else:\n",
    "                self._merge_children_around_key(item, node, pos - 1)\n",
    "                \n",
    "                # here I should take care of missing root\n",
    "                node = self._fix_empty_root(node)\n",
    "                \n",
    "                self._delete(item, node)\n",
    "        else:\n",
    "            \n",
    "            # we are on a leave and haven't found the key, we have nothing to do\n",
    "            if node.sons[pos] is None:\n",
    "                pass\n",
    "            # the amount of keys in the child is enough, simply recurse\n",
    "            elif len(node.sons[pos].items) >= self.t:\n",
    "                self._delete(item, node.sons[pos])\n",
    "            # we must push a key to the child\n",
    "            else:\n",
    "                # left sibbling has enough keys\n",
    "                if pos > 0 and len(node.sons[pos - 1].items) >= self.t:\n",
    "                    self._move_node_from_left_child(node, pos)\n",
    "                    self._delete(item, node.sons[pos])\n",
    "                # right sibbling has enough keys\n",
    "                elif pos < len(node.sons) - 1 and len(node.sons[pos + 1].items) >= self.t:\n",
    "                    self._move_node_from_right_child(node, pos)\n",
    "                    self._delete(item, node.sons[pos])\n",
    "                # must merge with one of sibblings\n",
    "                else:\n",
    "                    \n",
    "                    if pos > 0:\n",
    "                        self._merge_children_around_key(item, node, pos - 1)\n",
    "                        \n",
    "                        # here I should take care of missing root\n",
    "                        node = self._fix_empty_root(node)\n",
    "                        \n",
    "                        self._delete(item, node)\n",
    "                    elif pos < len(node.sons) - 1:\n",
    "                        self._merge_children_around_key(item, node, pos)\n",
    "                        \n",
    "                        # here I should take care of missing root\n",
    "                        node = self._fix_empty_root(node)\n",
    "                        \n",
    "                        self._delete(item, node)\n",
    "                    # this shouldn't be possible\n",
    "                    else:\n",
    "                        assert False\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "        \n",
    "    def delete(self, item):\n",
    "        self._delete(item, self.root)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def _find_all(self, item, node, ans):\n",
    "        if node is None or len(node.sons) == 0: return\n",
    "        b = 0\n",
    "        e = len(node.sons) - 1\n",
    "        while b < e:\n",
    "            mid = (b + e + 1) // 2\n",
    "            if mid == 0: # mid is never 0 actually\n",
    "                pass\n",
    "            elif node.items[mid - 1].key < item.key:\n",
    "                b = mid\n",
    "            else:\n",
    "                e = mid - 1\n",
    "        \n",
    "        left = b\n",
    "        \n",
    "        \n",
    "        b = 0\n",
    "        e = len(node.sons) - 1\n",
    "        while b < e:\n",
    "            mid = (b + e + 1) // 2\n",
    "            if mid == 0: # mid is never 0 actually\n",
    "                pass\n",
    "            elif node.items[mid - 1].key > item.key:\n",
    "                e = mid - 1\n",
    "            else:\n",
    "                b = mid\n",
    "        right = b\n",
    "        \n",
    "        # print(left, right, len(node.sons))\n",
    "        for i in range(left, right + 1):\n",
    "            self._find_all(item, node.sons[i], ans)\n",
    "            \n",
    "            if i < right:\n",
    "                assert node.items[i].key == item.key\n",
    "                ans.append(node.items[i])\n",
    "        \n",
    "    \n",
    "    def find_all(self, item):\n",
    "        ans = []\n",
    "        self._find_all(item, self.root, ans)\n",
    "        return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_chars(string):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i >= len(string):\n",
    "            break\n",
    "        ascii_code = ord(string[i])\n",
    "        if not((ascii_code in range(1569, 1595)) or (ascii_code in range(1601, 1611)) or (ascii_code in range(1632, 1642)) or (ascii_code in range(1649, 1750)) or (ascii_code in range(1776, 1786)) or ascii_code == 32):\n",
    "            string = string[:i] + ' ' + string[i+1:]\n",
    "#             string[i] = ' '\n",
    "        i += 1\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text:str):\n",
    "    punctuations = ['.', '!', '?', '\"', \"'\", '`', ',', '-', '_', ':', ';', '(', ')', '[', ']', '/', '،', '{','}', '«', ':', '»', '؟', '؛', '|', '=', '+']\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, '')\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [],
   "source": [
    "def prepare_text(raw_text):\n",
    "    raw_text = cleanhtml(raw_text)\n",
    "    raw_text = remove_punctuation(raw_text)\n",
    "    normalizer = Normalizer()\n",
    "    text = normalizer.normalize(raw_text)\n",
    "    words = word_tokenize(text)\n",
    "    stemmer = Stemmer()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        stemmed = stemmer.stem(word)\n",
    "        words[i] = stemmed\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text1(raw_text):\n",
    "    raw_text = cleanhtml(raw_text)\n",
    "    raw_text = remove_punctuation(raw_text)\n",
    "    normalizer = Normalizer()\n",
    "    text = normalizer.normalize(raw_text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mlwmeLaQJneG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
    "<br>\n",
    "شرح نمایه‌های مورد انتظار:\n",
    "<br>\n",
    "<ol>\n",
    "<li>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
    "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
    "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
    "</li>\n",
    "<li>\n",
    "(*امتیازی*)\n",
    "نمایه‌ی \n",
    "Bigram: \n",
    "با استفاده از این نمایه می‌توان با دادن یک \n",
    "Bigram\n",
    "(ترکیب‌های دو حرفی) \n",
    "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
    "</li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjpoyRv9JneH"
   },
   "source": [
    " <div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "    (30 نمره)   \n",
    "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
    "construct_indexes\n",
    "با گرفتن مسیر مجموعه‌داده‌ها\n",
    "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = BTree(4)\n",
    "bigram_index = BTree(4)\n",
    "\n",
    "N = 0\n",
    "\n",
    "class Doc:\n",
    "    def __init__(self, id):\n",
    "        self.id = int(id)\n",
    "        self.text_positions = []\n",
    "        self.title_positions = []\n",
    "        self.text_tf = 0\n",
    "        self.title_tf = 0\n",
    "\n",
    "class Term:\n",
    "    def __init__(self, term):\n",
    "        self.key = term\n",
    "        self.posting_lists = []\n",
    "#         self.tf = {}\n",
    "    def insert_doc(self, doc:Doc):\n",
    "        start = 0\n",
    "        if len(self.posting_lists) == 0:\n",
    "            self.posting_lists.append(doc)\n",
    "            return\n",
    "        end = len(self.posting_lists) - 1\n",
    "        while start <= end: \n",
    "\n",
    "            mid = start + (end - start) // 2; \n",
    "\n",
    "            # Check if x is present at mid \n",
    "            if doc.id == self.posting_lists[mid].id: \n",
    "                self.posting_lists.insert(mid, doc)\n",
    "                return\n",
    "\n",
    "            # If x is greater, ignore left half \n",
    "            elif doc.id > self.posting_lists[mid].id: \n",
    "                start = mid + 1\n",
    "\n",
    "            # If x is smaller, ignore right half \n",
    "            else: \n",
    "                end = mid - 1\n",
    "        self.posting_lists.insert(start, doc)\n",
    "                \n",
    "    def get_doc(self, id, index=False):\n",
    "        id = int(id)\n",
    "        start = 0\n",
    "        end = len(self.posting_lists) - 1\n",
    "        while start <= end: \n",
    "\n",
    "            mid = start + (end - start) // 2; \n",
    "\n",
    "            # Check if x is present at mid \n",
    "            if id == self.posting_lists[mid].id: \n",
    "                if index:\n",
    "                    return mid\n",
    "                return self.posting_lists[mid]\n",
    "\n",
    "            # If x is greater, ignore left half \n",
    "            elif id > self.posting_lists[mid].id: \n",
    "                start = mid + 1\n",
    "\n",
    "            else: \n",
    "                end = mid - 1\n",
    "\n",
    "        return None\n",
    "\n",
    "class Bigram:\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.words = {}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_num(docs_path):\n",
    "    global N\n",
    "\n",
    "    docs = minidom.parse(docs_path)\n",
    "    \n",
    "    page_xmls = docs.getElementsByTagName('page')\n",
    "    \n",
    "    N = len(page_xmls)\n",
    "get_docs_num('./data/Persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1572/1572 [00:13<00:00, 117.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.85866141319275\n"
     ]
    }
   ],
   "source": [
    "def construct_positional_indexes(docs_path):\n",
    "    start = time.time()\n",
    "    global index\n",
    "\n",
    "    docs = minidom.parse(docs_path)\n",
    "    \n",
    "    page_xmls = docs.getElementsByTagName('page')\n",
    "    \n",
    "    all_text_words = []\n",
    "    all_title_words = []\n",
    "    \n",
    "    for i, page_xml in tqdm(enumerate(page_xmls), total=len(page_xmls)):\n",
    "        id = int(page_xml.getElementsByTagName('id')[0].firstChild.data)\n",
    "        text = page_xml.getElementsByTagName('text')[0].firstChild.data\n",
    "        text_words = prepare_text(text)\n",
    "        all_text_words += [(term, id, k) for k, term in enumerate(text_words)]\n",
    "\n",
    "        title = page_xml.getElementsByTagName('title')[0].firstChild.data\n",
    "        title_words = prepare_text(title)\n",
    "        all_title_words += [(term, id, k) for k, term in enumerate(title_words)]\n",
    "\n",
    "        \n",
    "    all_text_words = sorted(all_text_words, key=lambda x:x[0]) # sort by term\n",
    "    all_title_words = sorted(all_title_words, key=lambda x:x[0]) # sort by term\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(all_text_words):\n",
    "        if all_text_words[i][0] == '' or all_text_words[i][0] == '#':\n",
    "            while all_text_words[i][0] == '' or all_text_words[i][0] == '#':\n",
    "                i += 1\n",
    "        word = all_text_words[i][0]\n",
    "        term = Term(word)\n",
    "        while i < len(all_text_words) and all_text_words[i][0] == word:\n",
    "            \n",
    "            id = all_text_words[i][1]\n",
    "            j = all_text_words[i][2]\n",
    "            doc = term.get_doc(id)\n",
    "            \n",
    "            if doc == None:\n",
    "                doc = Doc(id)\n",
    "                doc.text_positions.append(j)\n",
    "                doc.text_tf += 1\n",
    "                term.insert_doc(doc)\n",
    "                i += 1\n",
    "                continue\n",
    "            doc.text_positions.append(j)\n",
    "            doc.text_tf += 1\n",
    "            i += 1\n",
    "        index.insert(term)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(all_title_words):\n",
    "        if all_title_words[i][0] == '' or all_title_words[i][0] == '#':\n",
    "            while all_title_words[i][0] == '' or all_title_words[i][0] == '#':\n",
    "                i += 1\n",
    "        word = all_title_words[i][0]\n",
    "        term = index.find(word)\n",
    "        term_exist = True\n",
    "        if not term:\n",
    "            term_exist = False\n",
    "            term = Term(word)\n",
    "        while i < len(all_title_words) and all_title_words[i][0] == word:\n",
    "            id = all_title_words[i][1]\n",
    "            j = all_title_words[i][2]\n",
    "            \n",
    "            doc = term.get_doc(id)\n",
    "            if doc == None:\n",
    "                doc = Doc(id)\n",
    "                doc.title_positions.append(j)\n",
    "                doc.title_tf += 1\n",
    "                term.insert_doc(doc)\n",
    "                i += 1\n",
    "                continue\n",
    "            doc.title_positions.append(j)\n",
    "            doc.title_tf += 1\n",
    "            i += 1    \n",
    "\n",
    "        if not term_exist:\n",
    "            index.insert(term)        \n",
    "    print(time.time() - start)\n",
    "\n",
    "construct_positional_indexes('./data/Persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1572/1572 [00:48<00:00, 32.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def construct_bigram_indexes(docs_path):\n",
    "    global bigram_index\n",
    "    \n",
    "    docs = minidom.parse(docs_path)\n",
    "    \n",
    "    page_xmls = docs.getElementsByTagName('page')\n",
    "    \n",
    "    \n",
    "    for i, page_xml in tqdm(enumerate(page_xmls), total=len(page_xmls)):\n",
    "        text = page_xml.getElementsByTagName('text')[0].firstChild.data\n",
    "        text_words = set(prepare_text1(text))\n",
    "        title = page_xml.getElementsByTagName('title')[0].firstChild.data\n",
    "        title_words = set(prepare_text1(title))\n",
    "        id = int(page_xml.getElementsByTagName('id')[0].firstChild.data)\n",
    "        for i, word in enumerate(text_words|title_words):\n",
    "            for k in range(len(word)-1):\n",
    "                bigram = word[k:k+2]\n",
    "                bigram_term = bigram_index.find(bigram)\n",
    "                if bigram_term is None:\n",
    "                    bigram_term = Bigram(bigram)\n",
    "                    bigram_term.words[word] = [id]\n",
    "                    bigram_index.insert(bigram_term)\n",
    "                    continue\n",
    "                if word not in bigram_term.words:\n",
    "                    bigram_term.words[word] = [id]\n",
    "                    continue\n",
    "                bigram_term.words[word].append(id)        \n",
    "construct_bigram_indexes('./data/Persian.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51VMBVg3JneM"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای مشاهده \n",
    "posting list\n",
    "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
    "get_posting_list\n",
    "با گرفتن\n",
    "word\n",
    "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "title\n",
    "و\n",
    "text\n",
    "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_posting_list(word_list):\n",
    "    global index\n",
    "    word = word_list[0] \n",
    "    term = index.find(word)\n",
    "    if not term is None:\n",
    "        \n",
    "        c = {}\n",
    "        for doc in term.posting_lists:\n",
    "            c[doc.id] = {'title' : [], 'text' : []}\n",
    "            for pos in doc.title_positions:\n",
    "                c[doc.id]['title'].append(pos)\n",
    "            for pos in doc.text_positions:\n",
    "                c[doc.id]['text'].append(pos)\n",
    "        \n",
    "        return c\n",
    "        \n",
    "    return 'Sorry! The word you are looking for is not in index!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__qCYhAsJneS"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
    "get_words_with_bigram\n",
    "یک ورودی به عنوان\n",
    "bigram\n",
    "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ایالات',\n",
       " 'میلاد',\n",
       " 'میانگین\\u200cبارش\\u200cسالانه',\n",
       " 'اعلام',\n",
       " 'تلاقی',\n",
       " 'سابلاغ',\n",
       " 'ولایات',\n",
       " 'ساوجبلاغ',\n",
       " 'اسقلال',\n",
       " 'دلالت',\n",
       " 'قبلا',\n",
       " 'طولانی\\u200cمدت',\n",
       " 'علاقه\\u200cتان',\n",
       " 'خلاصهٔ',\n",
       " 'علاقهٔ',\n",
       " 'اطلاعات',\n",
       " 'بالای',\n",
       " 'علاوه',\n",
       " 'علامت',\n",
       " 'لازم',\n",
       " 'خلاصه',\n",
       " 'مقالات',\n",
       " 'اطلاعاتی',\n",
       " 'علاقه\\u200cمندان',\n",
       " 'اولاد',\n",
       " 'الغزلان',\n",
       " 'میلادی',\n",
       " 'الاستعمار',\n",
       " 'سنگلاخی',\n",
       " 'هلال',\n",
       " 'اسلام',\n",
       " 'خلاف',\n",
       " 'ولایة',\n",
       " 'الان\\u200cها',\n",
       " 'اسلامی',\n",
       " 'انقلاب',\n",
       " 'بالاتر',\n",
       " 'سطح\\u200cبالای',\n",
       " 'زینت\\u200cآلات',\n",
       " 'استقلال',\n",
       " 'تقویممیلادی',\n",
       " 'حملات',\n",
       " 'الاستعماریه',\n",
       " 'الانوار',\n",
       " 'التحصیلان',\n",
       " 'بالاترین',\n",
       " 'سالانه',\n",
       " 'اسلام\\u200cگرایی',\n",
       " 'مسلاته',\n",
       " 'جبل\\u200cالاخضر',\n",
       " 'علاقه',\n",
       " 'کاملا',\n",
       " 'کالاهای',\n",
       " 'محصولات',\n",
       " 'الاسلام',\n",
       " 'جبل\\u200cالاخضرجبل\\u200cالاخضر',\n",
       " 'خلافت',\n",
       " 'اسلام\\u200cگراها',\n",
       " 'معمولا',\n",
       " 'بالاترین\\u200cها',\n",
       " 'الاخضر',\n",
       " 'الاخضرحزام',\n",
       " 'دلار',\n",
       " 'ماشین\\u200cآلات',\n",
       " 'مبتلا',\n",
       " 'مردم\\u200cسالاری',\n",
       " 'انقلابی',\n",
       " 'لاکربی',\n",
       " 'لاتین',\n",
       " 'لاذقیه',\n",
       " 'اسلاوی',\n",
       " 'مالاگا',\n",
       " 'عربیاللاذقیة',\n",
       " 'langarاللاذقیة',\n",
       " 'ردهلاذقیه',\n",
       " 'لاتینیLaodicea',\n",
       " 'للمیلاد',\n",
       " 'ردههلال',\n",
       " 'والاسلامیة',\n",
       " 'باجلانی\\u200cها',\n",
       " 'اردلانی',\n",
       " 'اصطلاح',\n",
       " 'اختلاط',\n",
       " 'الارض',\n",
       " 'مثلا',\n",
       " 'ولادیمیر',\n",
       " 'تحولات',\n",
       " 'احتمالا',\n",
       " 'ایلامایلام',\n",
       " 'ملاحظه\\u200cای',\n",
       " 'ملای',\n",
       " 'اخلاف',\n",
       " 'لاتینی',\n",
       " 'اطلاعات\\u200cنامه',\n",
       " 'مجلات',\n",
       " 'الصلاه',\n",
       " 'الانبیا',\n",
       " 'اطلاق',\n",
       " 'ایلام',\n",
       " 'اسلامناشر',\n",
       " 'تلاش',\n",
       " 'تازه\\u200cاستقلال\\u200cیافته',\n",
       " 'السلام',\n",
       " 'طولانی',\n",
       " 'غلامرضا',\n",
       " 'ولادمیر',\n",
       " 'فلات',\n",
       " 'حاصلخیزهلال',\n",
       " 'الانبیاء',\n",
       " 'گیلان',\n",
       " 'ایلات',\n",
       " 'سلاطین',\n",
       " 'الاکراد',\n",
       " 'ایلامی',\n",
       " 'ملا',\n",
       " 'اختلافی',\n",
       " 'ایلامسال',\n",
       " 'دالاهو',\n",
       " 'ملاطیه',\n",
       " 'روژهه\\u200cلات',\n",
       " 'هه\\u200cلاله',\n",
       " 'pxهلاله',\n",
       " 'هلاله',\n",
       " 'مندالان',\n",
       " 'بالا',\n",
       " 'سالار',\n",
       " 'کهلان',\n",
       " 'دلاوران',\n",
       " 'لایق\\u200cتر',\n",
       " 'جلال',\n",
       " 'پلاک',\n",
       " 'سونگورلاره',\n",
       " 'زلاندنو',\n",
       " 'لارنس',\n",
       " 'زلاند',\n",
       " 'سلاح',\n",
       " 'ملاقات',\n",
       " 'عملا',\n",
       " 'کیالاکه\\u200cکوا',\n",
       " 'آلاسکا',\n",
       " 'بلافاصله',\n",
       " 'اختلاف',\n",
       " 'دلایل',\n",
       " 'سولاندر',\n",
       " 'تلاششان',\n",
       " 'لارنسرودخانهٔ',\n",
       " 'خلال',\n",
       " 'میلادیسدهٔ',\n",
       " 'بعلاوه',\n",
       " 'اصلاح',\n",
       " 'کلاسیککلاسیک',\n",
       " 'شدهاصلاح',\n",
       " 'بلاویا',\n",
       " 'ایرلاینزSCAT',\n",
       " 'فلای\\u200cدبی',\n",
       " 'ایرلاینز',\n",
       " 'لارناکا',\n",
       " 'ایرلاینزفرودگاه',\n",
       " 'ایرلاینزفصلی',\n",
       " 'آمریکادلار',\n",
       " 'آمریکاایالات',\n",
       " 'بلامانع',\n",
       " 'بالاخره',\n",
       " 'ردهولایت\\u200cهای',\n",
       " 'الایام',\n",
       " 'علاقه\\u200cداری\\u200cها',\n",
       " 'مشکلات',\n",
       " 'حالا',\n",
       " 'جنگلات',\n",
       " 'ولایت',\n",
       " 'بالامرغاب',\n",
       " 'اصلا',\n",
       " 'فلادلفیا',\n",
       " 'ردهولایت',\n",
       " 'الاخبار',\n",
       " 'ولایاتی',\n",
       " 'حاصلات',\n",
       " 'ولایت\\u200cهای',\n",
       " 'مواصلاتی',\n",
       " 'سال\\u200cولایت\\u200cشدن',\n",
       " 'ولایتی',\n",
       " 'غلات',\n",
       " 'افغانستانبغلان',\n",
       " 'بغلان',\n",
       " 'مشهورولایت',\n",
       " 'دلاور',\n",
       " 'تحصیلات',\n",
       " 'فعلا',\n",
       " 'بلخیمولانا',\n",
       " 'مولانای',\n",
       " 'غلام\\u200cحضرت',\n",
       " 'pxولایت',\n",
       " 'آلاتش',\n",
       " 'پلانگذاری',\n",
       " 'سلامت',\n",
       " 'عبدالاحمد',\n",
       " 'مرکزولایت',\n",
       " 'ولایتمیدان',\n",
       " 'nationایالات',\n",
       " 'ردهایالات',\n",
       " 'تشکیلاتی',\n",
       " 'اصلاح\\u200cطلبان',\n",
       " 'گلایه',\n",
       " 'تلاشهایی',\n",
       " 'اعلامیه\\u200cای',\n",
       " 'اطلاع',\n",
       " 'انقلابهای',\n",
       " 'اطلاع\\u200cرسانی',\n",
       " 'اسلامیhttpwwwirdcirfacalendar',\n",
       " 'اطلاعاتبازگشت',\n",
       " 'اصلاحات',\n",
       " 'اعلامیه',\n",
       " 'علاقه\\u200cمند',\n",
       " 'سکولار',\n",
       " 'طولانی\\u200cترین',\n",
       " 'ایرانانقلاب',\n",
       " 'ردهانقلاب\\u200cهای',\n",
       " 'انحلال',\n",
       " 'اصلاح\\u200cنشده',\n",
       " 'کلاسهای',\n",
       " 'ارباب\\u200cسالاری',\n",
       " 'میلانی',\n",
       " 'برخلاف',\n",
       " 'ممنوع\\u200cالانتشار',\n",
       " 'محصلان',\n",
       " 'تلاش\\u200cهای',\n",
       " 'کلانتریها',\n",
       " 'فی\\u200cالارض',\n",
       " 'انقلابیون',\n",
       " 'ملازم',\n",
       " 'اسلامگرای',\n",
       " 'تغییرمسیرانقلاب',\n",
       " 'طولانیش',\n",
       " 'استدلال',\n",
       " 'اصلاحگری',\n",
       " 'اصلاح\\u200cاندیش',\n",
       " 'دیوانسالاری',\n",
       " 'ائتلاف',\n",
       " 'کالاها',\n",
       " 'اعلان',\n",
       " 'خلاص',\n",
       " 'کندپکمیلانی',\n",
       " 'دلالان',\n",
       " 'بی\\u200cملاحظه',\n",
       " 'محلات',\n",
       " 'علامه',\n",
       " 'ردهانقلاب',\n",
       " 'انقلابدادگاه\\u200cهای',\n",
       " 'پالایشگاه\\u200cهای',\n",
       " 'انقلابیJPG',\n",
       " 'اختلال',\n",
       " 'نشانیhttpwwwdwcomfairواژههاییکهباجمهوریاسلامیبهایرانآمدندg',\n",
       " 'طلاب',\n",
       " 'اسلامیکمیته',\n",
       " 'لاهوتی',\n",
       " 'تلاشی',\n",
       " 'اعلامیه\\u200cهای',\n",
       " 'لاهیجی',\n",
       " 'ابلاغ',\n",
       " 'انقلابیJPGبندانگشتیچپ',\n",
       " 'لاله',\n",
       " 'اسلام\\u200cگرا',\n",
       " 'دلاری',\n",
       " 'آتلانتا',\n",
       " 'فیلادلفیاکمدن',\n",
       " 'کلانشهر',\n",
       " 'لانگ',\n",
       " 'کلانتر',\n",
       " 'ونزوئلا',\n",
       " 'گولاش',\n",
       " 'سکولارکشور',\n",
       " 'اصلاحیه',\n",
       " 'متقابلا',\n",
       " 'دالاس',\n",
       " 'اصلاحی',\n",
       " 'مبادلات',\n",
       " 'لاتینو',\n",
       " 'اصلاحیهٔ',\n",
       " 'Statesایالات',\n",
       " 'پالایش',\n",
       " 'اقلام',\n",
       " 'سلاح\\u200cهای',\n",
       " 'آمریکاآتلانتیک',\n",
       " 'بنگلادش',\n",
       " 'وارهولاندی',\n",
       " 'نیویورکنیوآرکلانگ',\n",
       " 'فیلادلفیا',\n",
       " 'لادردیلوست',\n",
       " 'آمریکااعلامسخمعاهده',\n",
       " 'گواتمالا۹٬',\n",
       " 'بالاتری',\n",
       " 'کبیرسخاعلامیه',\n",
       " 'لادردیل',\n",
       " 'معاملات',\n",
       " 'دلاریادکرد',\n",
       " 'دالاسفورت',\n",
       " 'فولاد',\n",
       " 'آمریکااعلامیه',\n",
       " 'pdfسخآلاسکا',\n",
       " 'دلاویر',\n",
       " 'لابی',\n",
       " 'کلان\\u200cشهر',\n",
       " 'خلاقیت',\n",
       " 'بالایی',\n",
       " 'اعلامیهٔ',\n",
       " 'سیلای',\n",
       " 'بلاعوض',\n",
       " 'کلاسیک',\n",
       " 'لاابالی\\u200cگری',\n",
       " 'بلاغت',\n",
       " 'نیوکلاسیک',\n",
       " 'غلامحسین',\n",
       " 'الافکار',\n",
       " 'تخیلات',\n",
       " 'جلال\\u200cالدین',\n",
       " 'غلامحسن',\n",
       " 'اطلال',\n",
       " 'اصطلاحات',\n",
       " 'مثلااستخوان',\n",
       " 'گلابی',\n",
       " 'بطلان',\n",
       " 'سه\\u200cلایه\\u200cازجنس',\n",
       " 'غیراخلاقی',\n",
       " 'عضلات',\n",
       " 'اخلاقی',\n",
       " 'تالار',\n",
       " 'لاویل',\n",
       " 'پالاس',\n",
       " 'اولاندسخمانوئل',\n",
       " 'پرلاشز',\n",
       " 'ماشینماشین\\u200cآلات',\n",
       " 'نیکولا',\n",
       " 'سخhttpwwwinseefrfrffcpopage۲htmاطلاعات',\n",
       " 'آتلانتیک',\n",
       " 'لاسکو',\n",
       " 'اعلامیۀ',\n",
       " 'اولاند',\n",
       " 'صلاحیت',\n",
       " 'دلایلی',\n",
       " 'ناعادلانه',\n",
       " 'تلاشها',\n",
       " 'دلارcite',\n",
       " 'شیلات',\n",
       " 'چکدانمارکاستونیفنلاندفرانسهآلمانیونانمجارستانجزیره',\n",
       " 'لاتویا',\n",
       " 'اصلاحاتی',\n",
       " 'فنلاند',\n",
       " 'عادلانه',\n",
       " 'دلارhttpwwwimforgexternalpubsftweo',\n",
       " 'اختلافات',\n",
       " 'سالانه\\u200cاست',\n",
       " 'بلاروس',\n",
       " 'تلاشهای',\n",
       " 'ژولای',\n",
       " 'فنلاندی',\n",
       " 'اصطلاحی',\n",
       " 'کلام',\n",
       " 'موزیلا',\n",
       " 'ردهموزیلا',\n",
       " 'ملاحظه',\n",
       " 'پلانک',\n",
       " 'خلاء',\n",
       " 'طلا',\n",
       " 'عیلامی',\n",
       " 'اسلامیکاتاریختاریخ',\n",
       " 'لاژورد',\n",
       " 'کلاه\\u200cتیزخود',\n",
       " 'تعطیلات',\n",
       " 'کلاه',\n",
       " 'آلاینده\\u200cهای',\n",
       " 'املایی',\n",
       " 'دالان',\n",
       " 'علائم',\n",
       " 'مسئولان',\n",
       " 'لای',\n",
       " 'والا',\n",
       " 'تالارهای',\n",
       " 'ملات',\n",
       " 'آنلاین',\n",
       " 'ایلامی\\u200cها',\n",
       " 'جلالی',\n",
       " 'ملاتی',\n",
       " 'ملازمش',\n",
       " 'تالارها',\n",
       " 'لاجورد',\n",
       " 'pxپلان',\n",
       " 'والایی',\n",
       " 'دولاب',\n",
       " 'بلاغی',\n",
       " 'روزآنلاین',\n",
       " 'ایلامیها',\n",
       " 'طلایی',\n",
       " 'استعلامه',\n",
       " 'کارلا',\n",
       " 'نیلاندر',\n",
       " 'ردهتعطیلات',\n",
       " 'کلاهی',\n",
       " 'ابهام\\u200cزداییلاتین',\n",
       " 'اسلام\\u200cگرایان',\n",
       " 'املای',\n",
       " 'همشهری\\u200cآنلایننشانی',\n",
       " 'بلاغزبانفارسی',\n",
       " 'غلامان',\n",
       " 'میشلاوباماوتبریکنوروزبهزبانفارسیhttpmelimazhabicomvideoobamahttpgardoonakir',\n",
       " 'جلالیتقویم',\n",
       " 'سلام',\n",
       " 'سالارمردم',\n",
       " 'اسلامینشانی',\n",
       " 'ملک\\u200cشاهجلال\\u200cالدین',\n",
       " 'کالا',\n",
       " 'اسلامEncyclopaedia',\n",
       " 'تجملات',\n",
       " 'جمادی\\u200cالاول',\n",
       " 'سالاری',\n",
       " 'محمدخلافت',\n",
       " 'میلادی۱',\n",
       " 'نهج\\u200cالبلاغه',\n",
       " 'خلافتی',\n",
       " 'عاملانی',\n",
       " 'ربیع\\u200cالاول',\n",
       " 'بلال',\n",
       " 'الامکان',\n",
       " 'شدمقالات',\n",
       " 'بوده\\u200cاستمقالات',\n",
       " 'فلان',\n",
       " 'حسنکتابمقالات',\n",
       " 'اصطلاحمنا',\n",
       " 'علاوهٔ',\n",
       " 'مشکلاتی',\n",
       " 'موزیلاام\\u200cپی\\u200cالcitation',\n",
       " 'پلاگین',\n",
       " 'غلاف',\n",
       " 'هسته\\u200cایسلاح\\u200cهای',\n",
       " 'بالاست',\n",
       " 'غلهغلات',\n",
       " 'قلاع',\n",
       " 'دالانی',\n",
       " 'قزل\\u200cآلا',\n",
       " 'واژگونلاله\\u200cهای',\n",
       " 'لاله\\u200cهای',\n",
       " 'خاتم\\u200cالانبیا',\n",
       " 'کهنسالانش',\n",
       " 'سیلاخور',\n",
       " 'نیکلای',\n",
       " 'رومیجلال',\n",
       " 'اسلامگرایان',\n",
       " 'بالابان',\n",
       " 'آلات',\n",
       " 'حالات',\n",
       " 'کالای',\n",
       " 'الاقصی',\n",
       " 'پالایه',\n",
       " 'صلاح',\n",
       " 'کلان',\n",
       " 'طلال',\n",
       " 'ائتلافی',\n",
       " 'شکلات',\n",
       " 'لاتینلاتین',\n",
       " 'سینالا',\n",
       " 'پالاینده',\n",
       " '٫۶چهارلا',\n",
       " '٫۳چهارلا',\n",
       " 'نتلا',\n",
       " 'اصطلاحا',\n",
       " 'لابمل',\n",
       " 'گلابی\\u200cشکل',\n",
       " 'ارسلان',\n",
       " '٫۵چهارلا',\n",
       " 'لایه',\n",
       " 'سه\\u200cلایی',\n",
       " '٫۰چهارلا',\n",
       " 'لاکرن',\n",
       " 'چهارلایی',\n",
       " '٫۷چهارلا',\n",
       " 'پلاستیک',\n",
       " '٫۹چهارلا',\n",
       " 'می\\u200cلا',\n",
       " 'لا',\n",
       " '٫۴چهارلا',\n",
       " '٫۲چهارلا',\n",
       " 'چهارلا',\n",
       " 'ملایم',\n",
       " 'ردهایلات',\n",
       " 'اسلاممسلمان\\u200cها',\n",
       " 'حملاتی',\n",
       " 'سکولاریسمسیستم',\n",
       " 'سالانهٔ',\n",
       " 'برایلان',\n",
       " 'مسئولانه',\n",
       " 'فعالان',\n",
       " 'کلاهک\\u200cهای',\n",
       " 'کلاهک',\n",
       " 'سکولاریسم',\n",
       " 'فارغ\\u200cالتحصیلان',\n",
       " 'محصولاتی',\n",
       " 'موتورولا',\n",
       " 'اسرائیلاحزاب',\n",
       " 'جولان',\n",
       " 'اسرائیلاستان',\n",
       " 'میلادیطرح',\n",
       " 'pxسخاستقلال',\n",
       " 'لاییک',\n",
       " 'اسلاو',\n",
       " 'لاتینی\\u200cسازی',\n",
       " 'مالاکاستر',\n",
       " 'استیلای',\n",
       " 'ولاش',\n",
       " 'اسلاوها',\n",
       " 'آلبانیاستقلال',\n",
       " 'ملایان',\n",
       " 'لابئات',\n",
       " 'تاؤلانت',\n",
       " 'مالاوی',\n",
       " 'لاگوس',\n",
       " 'کوالالامپور',\n",
       " 'اولانباتار',\n",
       " 'ولا',\n",
       " 'گلاسگو',\n",
       " 'براتیسلاوا',\n",
       " 'لاهه',\n",
       " 'لایپزیگ',\n",
       " 'گواتمالا',\n",
       " 'دلاس',\n",
       " 'لائوس',\n",
       " 'کازابلانکا',\n",
       " 'میلان',\n",
       " 'تلاوی',\n",
       " 'دارالسلام',\n",
       " 'ماچالا',\n",
       " 'لاپلاتا',\n",
       " 'آخالکالاکی',\n",
       " 'کالائو',\n",
       " 'پورت\\u200cویلا',\n",
       " 'لاشنجه',\n",
       " 'اکلاهماسیتی',\n",
       " 'لانژو',\n",
       " 'چیلان',\n",
       " 'آدلاید',\n",
       " 'بارانکیلا',\n",
       " 'کالاما',\n",
       " 'سولا',\n",
       " 'کامپالا',\n",
       " 'سریلانکا',\n",
       " 'گوادالاخارا',\n",
       " 'پوکاپالا',\n",
       " 'آنگولا',\n",
       " 'باهیابلانکا',\n",
       " 'لاپاز',\n",
       " 'سانتاکلارا',\n",
       " 'سلاله',\n",
       " 'ولادی\\u200cوستوک',\n",
       " 'مالابو',\n",
       " 'لاس',\n",
       " 'پلاتا',\n",
       " 'لاهور',\n",
       " 'اختلالات',\n",
       " 'اختلالاتی',\n",
       " 'همسالان',\n",
       " 'علائمی',\n",
       " 'اسلاواسلاوها',\n",
       " 'الاءعلاق\\u200cالنفیسة',\n",
       " 'افلاک',\n",
       " 'لانتانید',\n",
       " 'لانتانیدها',\n",
       " 'ردهلانتانیدها',\n",
       " 'ردهلانتان',\n",
       " 'لانتان',\n",
       " 'ولایتهلمند',\n",
       " 'علاقه\\u200cداری',\n",
       " 'مولانا',\n",
       " 'اتولا',\n",
       " 'بلا',\n",
       " 'الان',\n",
       " 'طلاق',\n",
       " 'می\\u200cگویدگفتاوردحالا',\n",
       " 'ملاهادی',\n",
       " 'خانم\\u200cبالا',\n",
       " 'لیلا',\n",
       " 'جلال\\u200cالممالک',\n",
       " 'افلاکی',\n",
       " 'گیلانی',\n",
       " 'تمایلات',\n",
       " 'تحصیلاتش',\n",
       " 'کلاغ',\n",
       " 'لابه\\u200cلای',\n",
       " 'خلقطلاب',\n",
       " 'ولای',\n",
       " 'لاغراندام',\n",
       " 'غلامحسین\\u200cمیرزا',\n",
       " 'اسلامییادکردفصلمقدمهکتابافکار',\n",
       " 'کلامی',\n",
       " 'خلاصه\\u200cهای',\n",
       " 'زرکوبصلاح',\n",
       " 'مولاناست',\n",
       " 'میرعلایی',\n",
       " 'الا',\n",
       " 'زواهرالانوار',\n",
       " 'فاتح\\u200cالابیات',\n",
       " 'زدندبلبلان',\n",
       " 'تاویلات',\n",
       " 'اولا',\n",
       " 'اخلاق',\n",
       " 'ملاحسین',\n",
       " 'میلادیفروزانفر',\n",
       " 'علا',\n",
       " 'استعلامی',\n",
       " 'روزمولاناستولیانگارنهانگارعنوانروز',\n",
       " 'جواهرالاسرار',\n",
       " 'لاهجان',\n",
       " 'سپه\\u200cسالاری',\n",
       " 'دلایلهشتگانهضرورتتغییرپولملیخداحافظریال',\n",
       " 'به\\u200cحلال',\n",
       " 'لاینپالیزه',\n",
       " 'لاین',\n",
       " 'جولانگاه',\n",
       " 'کلاسیسم',\n",
       " 'کلاسیکها',\n",
       " 'کلاسیسیسم',\n",
       " 'لایه\\u200cهای',\n",
       " 'هیمالایا',\n",
       " 'بالائی',\n",
       " 'کلارک',\n",
       " 'الارب',\n",
       " 'متلاشی',\n",
       " 'داگلاس',\n",
       " 'ماکس\\u200cپلانک',\n",
       " 'نشسته\\u200cاندنولا',\n",
       " 'می\\u200cشدنولا',\n",
       " 'کلادون',\n",
       " 'پلاستیکی',\n",
       " 'موادلازم',\n",
       " 'پلاستکی',\n",
       " 'کولادون',\n",
       " 'لایه\\u200cنشانی',\n",
       " 'نورلامپهای',\n",
       " 'ابزارآلات',\n",
       " 'لاشار',\n",
       " 'لاشارلاشار',\n",
       " 'کلات',\n",
       " 'ملاءعام',\n",
       " 'تشکیلات',\n",
       " 'آلمانآلمانی\\u200cالاصل',\n",
       " 'تلاش\\u200cها',\n",
       " 'الاولی',\n",
       " 'الإسلام',\n",
       " 'فلامینگو',\n",
       " 'علاقات',\n",
       " 'وکلا',\n",
       " 'شلاق',\n",
       " 'صالحاسلام',\n",
       " 'بالاستhttpwwwarabnewscompage۷&section۰&article',\n",
       " 'بالارتبه\\u200cترین',\n",
       " 'اسلاماسلامی',\n",
       " 'کلاس\\u200cهای',\n",
       " 'غلام',\n",
       " 'لاوری',\n",
       " 'ملامحمد',\n",
       " 'لائودیسه',\n",
       " 'لااودیسه',\n",
       " 'ملایر',\n",
       " 'اشکالات',\n",
       " 'کلانتری',\n",
       " 'مشهدپایتختفرهنگیجهاناسلاممیشودفرصتیناببرایترویجفرهنگ',\n",
       " 'جوادالائمه',\n",
       " 'مشهدولاهورخواهرخواندهشدند',\n",
       " 'کربلا',\n",
       " 'لاهیجانی',\n",
       " 'کلان\\u200cشهرهایی',\n",
       " 'ییلاقات',\n",
       " 'کمپوستلا',\n",
       " 'الاسفزاری',\n",
       " 'ایستگاهسوارکلاتhtml',\n",
       " 'ایرانیاسلامی',\n",
       " 'خاتم\\u200cالانبیاء',\n",
       " 'معلا',\n",
       " 'دارالولایه',\n",
       " 'کلاتکلات',\n",
       " 'سوالات',\n",
       " 'اسلامتاریخ',\n",
       " 'نونهالان',\n",
       " 'ثامن\\u200cالائمه',\n",
       " 'خلافتش',\n",
       " 'سیلابی',\n",
       " 'دارالخلافه\\u200cاش',\n",
       " 'طلاق\\u200cها',\n",
       " 'گیلاس',\n",
       " 'اعتلاء',\n",
       " 'دلایلگسترشحاشیهنشینیدرمشهدتدوینسندتوانمندسازیسکونتگاههای',\n",
       " 'الاشرف',\n",
       " 'کلاس',\n",
       " 'بازنگریمعماریوشهرسازیکلانشهرمشهدلزومتبدیلبهپایتختفرهنگیجهاناسلام',\n",
       " 'الاقالیم',\n",
       " 'قولبرخلاف',\n",
       " 'اسلامص',\n",
       " 'خلایق',\n",
       " 'اسلامیمدرن',\n",
       " 'درصدیمشکلاتزیستمحیطیدرمشهد',\n",
       " 'کلاتیادکرد',\n",
       " 'طلای',\n",
       " 'معضلات',\n",
       " 'غلام\\u200cحسین',\n",
       " 'کربلای',\n",
       " 'محلاتناشر',\n",
       " 'فتوکلاژ',\n",
       " 'جالیزمحصولات',\n",
       " 'افزایشدرصدیطلاقدرمشهد',\n",
       " 'بعدازاعتیادطلاقخشونتسومینآسیبعمدهاجتماعیدرمشهد',\n",
       " 'آنلاینتاریخ',\n",
       " 'بالاسر',\n",
       " 'ییلاقاتhtml',\n",
       " 'اعتلای',\n",
       " 'نشانههایاسلامیدرمعماریوشهرسازیمشهدرعایتنمیشود',\n",
       " 'اجلاس',\n",
       " 'رضویکلات',\n",
       " 'اسلامیبنیاد',\n",
       " 'اسلامیتاریخ',\n",
       " 'الاقالیماحسن',\n",
       " 'درصدطلاقهادرمشهداستراههایافزایشکیفیترابطهجنسی',\n",
       " 'مداخلات',\n",
       " 'خاتم\\u200cالانبیابیمارستان',\n",
       " 'کلان\\u200cشهری',\n",
       " 'کلان\\u200cشهرهای',\n",
       " 'مناطقگانهشهرمشهدتفکیکمحلاتhtml',\n",
       " 'پالاندوز',\n",
       " 'آلایندهٔ',\n",
       " 'پلانکتونها',\n",
       " 'باب\\u200cالابواب',\n",
       " 'کلانی',\n",
       " 'خبرآنلاینتاریخ',\n",
       " 'سولاک',\n",
       " 'اصلیاختلاف',\n",
       " 'آنلاینhttpwwwnoormagscomviewfaarticlepage',\n",
       " 'کشورعملا',\n",
       " 'آنلاینیادکرد',\n",
       " 'سفلای',\n",
       " 'لاکادیو',\n",
       " 'میلا',\n",
       " 'لاکشادویپLaccadive',\n",
       " 'ارسلانعضدالدوله',\n",
       " 'ردهاسلام',\n",
       " 'الادویه',\n",
       " 'سلاجقه',\n",
       " 'ملکشاهجلال\\u200cالدوله',\n",
       " 'ملازگرد',\n",
       " 'الابنیه',\n",
       " 'امجدالاشراف',\n",
       " 'دهگلان',\n",
       " 'تالاری',\n",
       " 'دارالایاله',\n",
       " 'قشلاق',\n",
       " 'اردلان\\u200cها',\n",
       " 'اردلاناردلانیمدرک',\n",
       " 'پلانی',\n",
       " 'اردلاناردلان',\n",
       " 'کلاو',\n",
       " 'دارالاحسان',\n",
       " 'بلافصل',\n",
       " 'گلابتون\\u200cدوزی',\n",
       " 'علاقه\\u200cمندی',\n",
       " 'چهارلان',\n",
       " 'قشلاقپل',\n",
       " 'ملالطف',\n",
       " 'هلاج',\n",
       " 'شیخ\\u200cالاسلام',\n",
       " 'قشلاقسد',\n",
       " 'پلان',\n",
       " 'صلاحی',\n",
       " 'غلامشاه',\n",
       " 'لان',\n",
       " 'قدیم\\u200cالایام',\n",
       " 'شلایی',\n",
       " 'حلاج',\n",
       " 'اردلان',\n",
       " 'دارالامان',\n",
       " 'کلاقاه',\n",
       " 'زیورآلات',\n",
       " 'چوارلان',\n",
       " 'ملاکاوو',\n",
       " 'گلیمسخگیوهکلاش',\n",
       " 'آوالان',\n",
       " 'اسلام\\u200cآباد',\n",
       " 'محلاتی',\n",
       " 'محلاتتاریخ',\n",
       " 'صدرالاشراف',\n",
       " 'خبرآنلاین',\n",
       " 'محلاتیو',\n",
       " 'محلاتمرکزی',\n",
       " 'ردهمحلات',\n",
       " 'احتمالات',\n",
       " 'باشدعلامه',\n",
       " 'فلاسفه',\n",
       " 'میکنندمثلا',\n",
       " 'الاعتقاد',\n",
       " 'الاسلامی',\n",
       " 'شوداخلاق',\n",
       " 'الاهی',\n",
       " 'عقلانی',\n",
       " 'کلامکلام',\n",
       " 'خلاقانه',\n",
       " 'سؤالاتی',\n",
       " 'جملاتی',\n",
       " 'سؤالات',\n",
       " 'آنلاینتان',\n",
       " 'ردهبلایای',\n",
       " 'بلایای',\n",
       " 'ولانس',\n",
       " 'جلا',\n",
       " 'جلالوند',\n",
       " 'بلاغ',\n",
       " 'کلاوه',\n",
       " 'پالایشگاه',\n",
       " 'بیلا',\n",
       " 'ردهبلایای\\u200cهای',\n",
       " 'سری\\u200cلانکا',\n",
       " 'لانکا',\n",
       " 'پالارپالارها',\n",
       " 'عیلام',\n",
       " 'هیولای',\n",
       " 'لایه\\u200cها',\n",
       " 'تعاملات',\n",
       " 'گیلانگیلان',\n",
       " 'اصولا',\n",
       " 'اردبیلاردبیل',\n",
       " 'اتصالات',\n",
       " 'هیولا',\n",
       " 'بوسلامه',\n",
       " 'فولادزره',\n",
       " 'اسلاواسلاوی',\n",
       " 'هیولاهای',\n",
       " 'امیدسالار',\n",
       " 'الثقلان',\n",
       " 'الإسلامیة',\n",
       " 'عاملان',\n",
       " 'ملاقاتهایی',\n",
       " 'کلاته',\n",
       " 'معلم\\u200cکلایه',\n",
       " 'شهراهلاستان',\n",
       " 'ملایرزنگنه',\n",
       " 'اردلاستان',\n",
       " 'اسلامیه',\n",
       " 'بیدگلاستان',\n",
       " 'ملارد',\n",
       " 'شهربابلاستان',\n",
       " 'لالجین',\n",
       " 'باباجانیثلاث',\n",
       " 'گیلانرحیم\\u200cآباد',\n",
       " 'سلامی',\n",
       " 'آلاشتاستان',\n",
       " 'گیلانگیلانشهرستان',\n",
       " 'لالی',\n",
       " 'کلاله',\n",
       " 'سیمرغکیاکلا',\n",
       " 'اسلام\\u200cشهراستان',\n",
       " 'ایلاماستان',\n",
       " 'گیلانشفت',\n",
       " 'میلاجرد',\n",
       " 'لاهیجان',\n",
       " 'معمولان',\n",
       " 'چالانچولان',\n",
       " 'شهرجنگلاستان',\n",
       " 'آق\\u200cقلااستان',\n",
       " 'بردسیرلاله',\n",
       " 'لامردلامرد',\n",
       " 'اردبیلاردبیلشهرستان',\n",
       " 'امیرکلااستان',\n",
       " 'کیلان',\n",
       " 'ملایرملایر',\n",
       " 'کلاچای',\n",
       " 'ایلامایلامشهرستان',\n",
       " 'علامرودشت',\n",
       " 'مرزیکلا',\n",
       " 'اسلام\\u200cشهرچهاردانگه',\n",
       " 'گیلانغرب',\n",
       " 'آملاستان',\n",
       " 'کیاکلا',\n",
       " 'کلاردشت',\n",
       " 'فلاورجان',\n",
       " 'برزولاستان',\n",
       " 'فولادشهر',\n",
       " 'دهگلاندهگلان',\n",
       " 'لار',\n",
       " 'نوشهرپولاستان',\n",
       " 'لاهرود',\n",
       " 'ثلاث',\n",
       " 'اصلاندوزاستان',\n",
       " 'لیلان',\n",
       " 'گیلانرستم\\u200cآباد',\n",
       " 'لامرد',\n",
       " 'ایلاممیمه',\n",
       " 'ملاثانی',\n",
       " 'سرخنکلاته',\n",
       " 'فردوساسلامیهاستان',\n",
       " 'لارستانلارستان',\n",
       " 'فلاورجانفلاورجان',\n",
       " 'رستمکلا',\n",
       " 'آریاشهربالادهاستان',\n",
       " 'کلارآباد',\n",
       " 'غرباسلام\\u200cآباد',\n",
       " 'اردلاردل',\n",
       " 'اسلامشهراسلامشهر',\n",
       " 'اردبیلاستان',\n",
       " 'آق\\u200cقلاآق\\u200cقلا',\n",
       " 'الانوارالنعمانیه',\n",
       " 'بلاذری',\n",
       " 'حلال',\n",
       " 'مغولان',\n",
       " 'کلاهخود',\n",
       " 'لایحضره',\n",
       " 'کربلایی',\n",
       " 'ملامراد',\n",
       " 'اتومبیلایران',\n",
       " 'ییلاق',\n",
       " 'الامر',\n",
       " 'قرلان',\n",
       " 'میرعلام',\n",
       " 'دررالاشعار',\n",
       " 'مصلا',\n",
       " 'قشلاققشلاق',\n",
       " 'جلایر',\n",
       " 'فاضلاب',\n",
       " 'دلان',\n",
       " 'سوته\\u200cدلان',\n",
       " 'اسلامیایرنا',\n",
       " 'دستانمیلادی',\n",
       " 'مولانابلی',\n",
       " 'طلاییبلیتاریخ',\n",
       " 'مولاناشکل',\n",
       " 'الاصل',\n",
       " 'مولانابلیاشعار',\n",
       " 'طلاییبیژن',\n",
       " 'لایقبلیاین',\n",
       " 'مولویمولانا',\n",
       " 'فیلارمونیک',\n",
       " 'دلاویل',\n",
       " 'ذوالفنونجلال',\n",
       " 'ساپودیلا',\n",
       " 'علاقه\\u200cی',\n",
       " 'امیرارسلان',\n",
       " 'اسطرلاب',\n",
       " 'زیلانت',\n",
       " 'لاخ',\n",
       " 'کتابدراکولا',\n",
       " 'کوپولا',\n",
       " 'دراکولای',\n",
       " 'دراکولادراکولا',\n",
       " 'دراکولا',\n",
       " 'لایکن\\u200cها',\n",
       " 'بالابردن',\n",
       " 'میلادیسالینجر',\n",
       " 'وبلاگ',\n",
       " 'بلندبالایی',\n",
       " 'کتابلاگ',\n",
       " 'میلادیدههٔ',\n",
       " 'پائولا',\n",
       " 'داگلاسکالین',\n",
       " 'بالابلندتر',\n",
       " 'نیلا',\n",
       " 'متلاطم',\n",
       " 'ملامت',\n",
       " 'والای',\n",
       " 'میلادی۹',\n",
       " 'ارمنستانآلاشکرت',\n",
       " '*آلاشکرت',\n",
       " 'لازاریانژانت',\n",
       " 'لازاریان',\n",
       " 'تبادلات',\n",
       " 'املائی',\n",
       " 'نام\\u200cرسمیکیلان',\n",
       " 'استhttpwwwptewikicomwikiindexphp۵titleکیلان',\n",
       " 'کالاآلیت',\n",
       " 'کالالیت',\n",
       " 'کالالیت\\u200cها',\n",
       " 'توپیلاک\\u200cها',\n",
       " 'توپیلاک',\n",
       " 'پردلانه',\n",
       " 'کلالیت',\n",
       " 'کلالیت*',\n",
       " 'کلاممتکمین',\n",
       " 'رسالاتی',\n",
       " 'ذیلا',\n",
       " 'کلاما',\n",
       " 'الاختیار',\n",
       " 'آثاراخلاق',\n",
       " 'کلاممتکلم',\n",
       " 'کاملابوجعفر',\n",
       " 'اساس\\u200cالاقتباس',\n",
       " 'الالقاب',\n",
       " 'الاعتقادات',\n",
       " 'هلاکوخان',\n",
       " 'هلاکوهلاکوی',\n",
       " 'البلاغة',\n",
       " 'اعلاء',\n",
       " 'اسلامی\\u200cست',\n",
       " 'جمادی\\u200cالاولی',\n",
       " 'تجریک\\u200cالکلام',\n",
       " 'کلامکلامی',\n",
       " 'الاقتباس',\n",
       " 'الاعراق',\n",
       " 'هلاکو',\n",
       " 'البلاغه',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    global bigram_index\n",
    "    result = bigram_index.find(bigram)\n",
    "    if result:\n",
    "        return list(result.words.keys())\n",
    "    \n",
    "    # TODO\n",
    "    return 'bigram does not exist'\n",
    "\n",
    "get_words_with_bigram('لا')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rz7JnPLJneY"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
    "تابع\n",
    "add_document_to_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
    "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    start = time.time()\n",
    "    global index\n",
    "    \n",
    "    all_text_words = []\n",
    "    all_title_words = []\n",
    "    \n",
    "    mydoc = minidom.parse(docs_path)\n",
    "    page_xmls = mydoc.getElementsByTagName('page')\n",
    "    \n",
    "    for i, page_xml in enumerate(page_xmls):\n",
    "        id = int(page_xml.getElementsByTagName('id')[0].firstChild.data)\n",
    "        if id == doc_num:\n",
    "            text = page_xml.getElementsByTagName('text')[0].firstChild.data\n",
    "            text_words = prepare_text(text)\n",
    "            title = page_xml.getElementsByTagName('title')[0].firstChild.data\n",
    "            title_words = prepare_text(title)\n",
    "            \n",
    "            all_text_words += [(term, id, k) for k, term in enumerate(text_words)]\n",
    "            all_title_words += [(term, id, k) for k, term in enumerate(title_words)]\n",
    "            \n",
    "            text_words = set(text_words)\n",
    "            title_words = set(title_words)\n",
    "            for i, word in enumerate(text_words|title_words):\n",
    "                for k in range(len(word)-1):\n",
    "                    bigram = word[k:k+2]\n",
    "                    bigram_term = bigram_index.find(bigram)\n",
    "                    if bigram_term is None:\n",
    "                        bigram_term = Bigram(bigram)\n",
    "                        bigram_term.words[word] = [id]\n",
    "                        bigram_index.insert(bigram_term)\n",
    "                        continue\n",
    "                    if word not in bigram_term.words:\n",
    "                        bigram_term.words[word] = [id]\n",
    "                        continue\n",
    "                    bigram_term.words[word].append(id) \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    all_text_words = sorted(all_text_words, key=lambda x:x[0])\n",
    "    all_title_words = sorted(all_title_words, key=lambda x:x[0])        \n",
    "    \n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    while i < len(all_text_words):\n",
    "        if all_text_words[i][0] == '' or all_text_words[i][0] == '#':\n",
    "            while all_text_words[i][0] == '' or all_text_words[i][0] == '#':\n",
    "                i += 1\n",
    "        word = all_text_words[i][0]\n",
    "        term = index.find(word)\n",
    "        term_exists = True\n",
    "        if not term:\n",
    "            term_exists = False\n",
    "            term = Term(word)\n",
    "        while i < len(all_text_words) and all_text_words[i][0] == word:\n",
    "            id = all_text_words[i][1]\n",
    "            j = all_text_words[i][2]\n",
    "            \n",
    "            doc = term.get_doc(id)\n",
    "            \n",
    "            if doc == None:\n",
    "                doc = Doc(id)\n",
    "                doc.text_positions.append(j)\n",
    "                doc.text_tf += 1\n",
    "                term.insert_doc(doc)\n",
    "                i += 1\n",
    "                continue\n",
    "            doc.text_positions.append(j)\n",
    "            doc.text_tf += 1\n",
    "            i += 1\n",
    "        if not term_exists:\n",
    "            index.insert(term)    \n",
    "    \n",
    "    i = 0\n",
    "    while i < len(all_title_words):\n",
    "        if all_title_words[i][0] == '' or all_title_words[i][0] == '#':\n",
    "            while all_title_words[i][0] == '' or all_title_words[i][0] == '#':\n",
    "                i += 1\n",
    "        word = all_title_words[i][0]\n",
    "        term = index.find(word)\n",
    "        term_exists = True\n",
    "        if not term:\n",
    "            term_exists = False\n",
    "            term = Term(word)\n",
    "        while i < len(all_title_words) and all_title_words[i][0] == word:\n",
    "            id = all_title_words[i][1]\n",
    "            j = all_title_words[i][2]\n",
    "            doc = term.get_doc(id)\n",
    "            \n",
    "            if doc == None:\n",
    "                doc = Doc(id)\n",
    "                doc.title_positions.append(j)\n",
    "                doc.title_tf += 1\n",
    "                term.insert_doc(doc)\n",
    "                i += 1\n",
    "                continue\n",
    "            doc.title_positions.append(j)\n",
    "            doc.title_tf += 1\n",
    "            i += 1\n",
    "        if not term_exists:\n",
    "            index.insert(term)        \n",
    "    print(time.time() - start)\n",
    "    \n",
    "    \n",
    "    print('doc added')\n",
    "\n",
    "# add_document_to_indexes('./data/Persian.xml', 4766)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI62QI7FJnec"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای حذف کردن یک سند از نمایه است.\n",
    "تابع\n",
    "delete_document_from_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
    "در صورتی که پس از حذف یک سند، \n",
    "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
    "به طور کامل حذف شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    global index\n",
    "\n",
    "    mydoc = minidom.parse(docs_path)\n",
    "\n",
    "    page_xmls = mydoc.getElementsByTagName('page')\n",
    "    \n",
    "    for i, page_xml in enumerate(page_xmls):\n",
    "        id = int(page_xml.getElementsByTagName('id')[0].firstChild.data)\n",
    "        if id == doc_num:\n",
    "            text = page_xml.getElementsByTagName('text')[0].firstChild.data\n",
    "            text_words = prepare_text(text)\n",
    "            title = page_xml.getElementsByTagName('title')[0].firstChild.data\n",
    "            title_words = prepare_text(title)\n",
    "            \n",
    "            for i, word in enumerate(text_words):\n",
    "                if word == '' or word == '#':\n",
    "                    continue\n",
    "                term = index.find(word)\n",
    "                if term is None:\n",
    "                    continue\n",
    "                doc_index = term.get_doc(id, index=True)\n",
    "            \n",
    "                if not (doc_index is None):\n",
    "                    del term.posting_lists[doc_index]\n",
    "\n",
    "            for i, word in enumerate(title_words):\n",
    "                if word == '' or word == '#':\n",
    "                    continue\n",
    "                term = index.find(word)\n",
    "                if term is None:\n",
    "                    continue\n",
    "                doc_index = term.get_doc(id, index=True)\n",
    "                if not (doc_index is None):\n",
    "                    del term.posting_lists[doc_index]\n",
    "                    \n",
    "            text_words = set(text_words)\n",
    "            title_words = set(title_words)      \n",
    "            for i, word in enumerate(text_words|title_words):\n",
    "                for k in range(len(word)-1):\n",
    "                    bigram = word[k:k+2] \n",
    "                    bigram_term = bigram_index.find(bigram)\n",
    "                    if not (bigram_term is None):\n",
    "                        word_dic = bigram_term.words.get(word)\n",
    "                        if not (word_dic is None):\n",
    "                            try:\n",
    "                                word_dic.remove(id)\n",
    "                                if len(word_dic) == 0:\n",
    "                                    bigram_term.words.pop(word)\n",
    "                            except:\n",
    "                                pass\n",
    "                        continue\n",
    "            print('doc deleted')\n",
    "            break\n",
    "\n",
    "\n",
    "# delete_document_from_indexes('./data/Persian.xml', 4766)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL-19qvrJnek"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
    "و نیازی به ذخیره‌سازی نمایه \n",
    "Bigram نیست \n",
    ".\n",
    "تابع \n",
    "save_index\n",
    "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
    "destination\n",
    "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(p_destination, b_destination):\n",
    "    global index\n",
    "    global bigram_index\n",
    "    print('saving index into file...')\n",
    "    pickle_out = open(p_destination,\"wb\")\n",
    "    pickle.dump(index, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    pickle_out = open(b_destination,\"wb\")\n",
    "    pickle.dump(bigram_index, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    print('index saved')\n",
    "# save_index(\"positional_index.pickle\", \"bigram_index.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPL51GrmJnep"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
    "load_index\n",
    "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
    "source\n",
    "نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(p_source, b_source):\n",
    "    global index\n",
    "    global bigram_index\n",
    "    if os.path.exists(p_source):\n",
    "        print('loading positional index from file...')\n",
    "        pickle_in = open(p_source,\"rb\")\n",
    "        index = pickle.load(pickle_in)\n",
    "        print('index loaded')\n",
    "    \n",
    "    if os.path.exists(b_source):\n",
    "        print('loading bigram index from file...')\n",
    "        pickle_in = open(b_source,\"rb\")\n",
    "        bigram_index = pickle.load(pickle_in)\n",
    "        print('index loaded')\n",
    "\n",
    "# load_index(\"positional_index.pickle\", \"bigram_index.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5JeSxd2Jnet"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
    "(*امتیازی*)\n",
    "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "(*امتیازی*)\n",
    "اصلاح پرسمان\n",
    "</font>\n",
    "<br>\n",
    "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
    "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
    "bigram\n",
    "(با استفاده از معیار فاصله جاکارد) \n",
    "انتخاب شده و سپس\n",
    "بهترین آنها با معیار \n",
    "edit distance\n",
    "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
    "</font>\n",
    "<br>\n",
    " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
    "<br>\n",
    "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
    "(ltn-lnn\n",
    "و\n",
    "ltc-lnc)\n",
    "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "جستجوی دقیق \n",
    "(phrasal search)\n",
    "</font>\n",
    "<br>\n",
    "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
    "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
    "<br>\n",
    "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
    "عنوان نمونه برای پرسمان\n",
    "<br>\n",
    "\"q5 q4\" q3 \"q2 q1\"\n",
    "<br>\n",
    "سند\n",
    "<br>\n",
    "q3 q2 q1 q5 q4\n",
    "<br>\n",
    "مرتبط محسوب می‌شود. \n",
    "<br>\n",
    "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UnpNX56Jneu"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
    "correct_query\n",
    "پرسمان کاربر  \n",
    "(query)\n",
    "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(s, t, costs=(1, 1, 1)):\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    deletes, inserts, substitutes = costs\n",
    "    \n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "\n",
    "    for row in range(1, rows):\n",
    "        dist[row][0] = row * deletes\n",
    "\n",
    "    for col in range(1, cols):\n",
    "        dist[0][col] = col * inserts\n",
    "        \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = substitutes\n",
    "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
    "                                 dist[row][col-1] + inserts,\n",
    "                                 dist[row-1][col-1] + cost) # substitution    \n",
    " \n",
    "    return dist[row][col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(str1, str2): \n",
    "    a = set(str1) \n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'بلام حالا برسیان دسک شد'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_query(query):\n",
    "    distance = 1000\n",
    "    result = ''\n",
    "    global index\n",
    "    global bigram_index\n",
    "    query_terms = prepare_text1(query)\n",
    "    for i, query_term in enumerate(query_terms):\n",
    "        if not index.find(query_term):\n",
    "            for k in range(len(query_term)-1):\n",
    "                bigram = query_term[k:k+2]\n",
    "                words = list(bigram_index.find(bigram).words.keys())\n",
    "                for word in words:\n",
    "                    aux1 = get_jaccard_sim(word, query_term)\n",
    "                    aux = edit_distance(word, query_term)\n",
    "                    if aux1 >= 0.5 and aux <= distance:\n",
    "                        distance = aux\n",
    "                        result = word\n",
    "            if result != '':\n",
    "                query_terms[i] = result            \n",
    "    correct_query = ' '.join(query_terms)\n",
    "    return correct_query\n",
    "\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9HMqDMZJney"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
    "search\n",
    "به عنوان اولین پارامتر پرسمان \n",
    "(query)\n",
    "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
    "پارامتر سوم \n",
    "(weight)\n",
    "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_char(string, char):\n",
    "    return np.array([m.start() for m in re.finditer(char, string)])\n",
    "\n",
    "def extract_terms(query):\n",
    "    exact_queries = []\n",
    "    indexes = find_char(query, '\"')\n",
    "    for i in range(0, len(indexes), 2):\n",
    "        exact_queries.append(query[indexes[i] + 1: indexes[i + 1]])        \n",
    "\n",
    "    terms = prepare_text(query)\n",
    "    query_terms = {'exact_queries':{}, 'terms':{}}\n",
    "    for term in terms:\n",
    "        if term in query_terms['terms']:\n",
    "            query_terms['terms'][term] += 1\n",
    "        else:\n",
    "            query_terms['terms'][term] = 1\n",
    "            \n",
    "    for term in exact_queries:\n",
    "        if term in query_terms['exact_queries']:\n",
    "            query_terms['exact_queries'][term] += 1\n",
    "        else:\n",
    "            query_terms['exact_queries'][term] = 1\n",
    "    \n",
    "    return query_terms, query.replace('\"', ' ')\n",
    "\n",
    "def weightd_tf(tf):\n",
    "    if tf == 0:\n",
    "        return 0\n",
    "    return 1 + math.log10(tf)\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    temp = set(lst2) \n",
    "    lst3 = [value for value in lst1 if value in temp] \n",
    "    return lst3 \n",
    "\n",
    "\n",
    "class Methods(enum.Enum):\n",
    "    ltn_lnn = 'ltn-lnn'\n",
    "    ltc_lnc = 'ltc-lnc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrasal_search(query_terms, context_to_search='text'):\n",
    "    global index\n",
    "\n",
    "    results = []\n",
    "    if context_to_search == 'text':\n",
    "        for k, query_term in enumerate(query_terms.items()):\n",
    "            tmp = []\n",
    "            exact_terms = prepare_text(query_term[0])\n",
    "            base_term = index.find(exact_terms[0])\n",
    "            term_exists = True\n",
    "            if not base_term:\n",
    "                term_exists = False\n",
    "                continue\n",
    "            for doc in base_term.posting_lists:\n",
    "                for position in doc.text_positions:\n",
    "                    doc_exists_in_term = True\n",
    "                    term_is_in_that_position = True\n",
    "                    for i, term in enumerate(exact_terms[1:]):\n",
    "                        term = index.find(term)\n",
    "                        if not term:\n",
    "                            term_exists = False\n",
    "                            break\n",
    "                        result_doc = term.get_doc(doc.id)\n",
    "                        if result_doc is None:\n",
    "                            doc_exists_in_term = False\n",
    "                            break\n",
    "                        result_positions = result_doc.text_positions\n",
    "                        if (not result_positions) or (position + i + 1 not in result_positions):\n",
    "                            term_is_in_that_position = False\n",
    "                            break\n",
    "\n",
    "                    if (not doc_exists_in_term) or (not term_is_in_that_position) or (not term_exists):\n",
    "                        break\n",
    "                    else:\n",
    "                        tmp.append(doc.id)\n",
    "                        break\n",
    "            if not term_exists:\n",
    "                continue\n",
    "\n",
    "            if k == 0:\n",
    "                results = tmp\n",
    "            else:\n",
    "                result = intersection(result, tmp)\n",
    "        return list(results)\n",
    "\n",
    "    if context_to_search == 'title':\n",
    "        for k, query_term in enumerate(query_terms.items()):\n",
    "            tmp = []\n",
    "            exact_terms = prepare_text(query_term[0])\n",
    "            base_term = index.find(exact_terms[0])\n",
    "            term_exists = True\n",
    "            if not base_term:\n",
    "                term_exists = False\n",
    "                continue\n",
    "            for doc in base_term.posting_lists:\n",
    "                for position in doc.title_positions:\n",
    "                    doc_exists_in_term = True\n",
    "                    term_is_in_that_position = True\n",
    "                    for i, term in enumerate(exact_terms[1:]):\n",
    "                        term = index.find(term)\n",
    "                        if not term:\n",
    "                            term_exists = False\n",
    "                            break\n",
    "                        result_doc = term.get_doc(doc.id)\n",
    "                        if result_doc is None:\n",
    "                            doc_exists_in_term = False\n",
    "                            break\n",
    "                        result_positions = result_doc.title_positions\n",
    "                        if (not result_positions) or (position + i + 1 not in result_positions):\n",
    "                            term_is_in_that_position = False\n",
    "                            break\n",
    "                    if (not doc_exists_in_term) or (not term_is_in_that_position):\n",
    "                        break\n",
    "                    else:\n",
    "                        tmp.append(doc.id)\n",
    "                        break\n",
    "\n",
    "            if k == 0:\n",
    "                results = tmp\n",
    "            else:\n",
    "                result = intersection(result, tmp)\n",
    "        return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [],
   "source": [
    "def search(query, method=Methods.ltn_lnn, weight=2):\n",
    "    global index\n",
    "    global N\n",
    "\n",
    "    query_terms, all_query = extract_terms(query)\n",
    "\n",
    "    scores = {}\n",
    "    if len(query_terms['exact_queries']) > 0:\n",
    "        docs_to_be_searched = phrasal_search(query_terms['exact_queries'], context_to_search='text')\n",
    "        docs_to_be_searched += phrasal_search(query_terms['exact_queries'], context_to_search='title')\n",
    "    if method == Methods.ltn_lnn:\n",
    "        for query_term, tf in query_terms['terms'].items():\n",
    "            q_tf_wt = weightd_tf(tf)\n",
    "\n",
    "            data = index.find(query_term)\n",
    "            if data:\n",
    "                for doc in data.posting_lists:\n",
    "                    if ((len(query_terms['exact_queries']) > 0) and (doc.id in docs_to_be_searched)) or len(\n",
    "                            query_terms['exact_queries']) == 0:\n",
    "                        d_tf_wt = weightd_tf(doc.text_tf)\n",
    "                        d_tf_wt += weight * weightd_tf(doc.title_tf)\n",
    "                        df_t = len(data.posting_lists)\n",
    "                        idf = math.log10(N / df_t)\n",
    "\n",
    "                        if doc.id in scores:\n",
    "                            scores[doc.id] += (q_tf_wt * (d_tf_wt * idf))\n",
    "                        else:\n",
    "                            scores[doc.id] = (q_tf_wt * (d_tf_wt * idf))\n",
    "\n",
    "        relevant_docs = scores.items()\n",
    "        relevant_docs = sorted(relevant_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    elif method == Methods.ltc_lnc:\n",
    "\n",
    "        doc_weights = 0\n",
    "        query_weights = 0\n",
    "        for query_term, tf in query_terms['terms'].items():\n",
    "            q_tf_wt = weightd_tf(tf)\n",
    "            data = index.find(query_term)\n",
    "            if data:\n",
    "                for doc in data.posting_lists:\n",
    "                    if ((len(query_terms['exact_queries']) > 0) and (doc.id in docs_to_be_searched)) or len(\n",
    "                            query_terms['exact_queries']) == 0:\n",
    "                        d_tf_wt = weightd_tf(doc.text_tf)\n",
    "                        d_tf_wt += weight * weightd_tf(doc.title_tf)\n",
    "                        df_t = len(data.posting_lists)\n",
    "                        idf = math.log10(N / df_t)\n",
    "\n",
    "                        doc_weights += d_tf_wt ** 2\n",
    "                        query_weights += q_tf_wt ** 2\n",
    "                        if doc.id in scores:\n",
    "                            scores[doc.id] += (q_tf_wt * (d_tf_wt * idf))\n",
    "                        else:\n",
    "                            scores[doc.id] = (q_tf_wt * (d_tf_wt * idf))\n",
    "        doc_weights = math.sqrt(doc_weights)\n",
    "        query_weights = math.sqrt(query_weights)\n",
    "        relevant_docs = list(map(lambda x: (x[0], x[1] / (doc_weights * query_weights)), scores.items()))\n",
    "        relevant_docs = sorted(relevant_docs, key=lambda x: x[1], reverse=True)\n",
    "    return relevant_docs[:15]\n",
    "\n",
    "\n",
    "# search('نظرخواهی انجام شده توسط دانشگاه \"شهر نیویورک\"', \"ltn-lnn\", 3)\n",
    "# search('نظرخواهی انجام شده توسط دانشگاه \"شهر نیویورک\"', Methods.ltn_lnn, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTXhfuP0Jne5"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
    "detailed_search\n",
    "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
    "(title_query)\n",
    "و پرسمان بر روی متن\n",
    "(text_query)\n",
    "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    global index\n",
    "    global N\n",
    "\n",
    "    text_query_terms, all_query = extract_terms(text_query)\n",
    "    title_query_terms, all_query = extract_terms(title_query)\n",
    "    scores = {}\n",
    "    text_docs_to_be_searched = phrasal_search(text_query_terms['exact_queries'], context_to_search='text')\n",
    "    title_docs_to_be_searched = phrasal_search(title_query_terms['exact_queries'], context_to_search='title')\n",
    "    docs_to_be_searched = intersection(text_docs_to_be_searched, title_docs_to_be_searched)\n",
    "    if method == Methods.ltn_lnn:\n",
    "        for query_term, tf in text_query_terms['terms'].items():\n",
    "            q_tf_wt = weightd_tf(tf)\n",
    "\n",
    "            data = index.find(query_term)\n",
    "            if data:\n",
    "                for doc in data.posting_lists:\n",
    "                    if ((len(text_query_terms['exact_queries']) > 0) and (doc_id in docs_to_be_searched)) or len(\n",
    "                            text_query_terms['exact_queries']) == 0:\n",
    "                        d_tf_wt = weightd_tf(doc.text_tf)\n",
    "\n",
    "                        df_t = len(data.posting_lists)\n",
    "                        idf = math.log10(N / df_t)\n",
    "\n",
    "                        if doc.id in scores:\n",
    "                            scores[doc.id] += (q_tf_wt * (d_tf_wt * idf))\n",
    "                        else:\n",
    "                            scores[doc.id] = (q_tf_wt * (d_tf_wt * idf))\n",
    "\n",
    "        for query_term, tf in title_query_terms['terms'].items():\n",
    "            q_tf_wt = weightd_tf(tf)\n",
    "\n",
    "            data = index.find(query_term)\n",
    "\n",
    "            if data:\n",
    "                for doc in data.posting_lists:\n",
    "                    if ((len(title_query_terms['exact_queries']) > 0) and (doc_id in docs_to_be_searched)) or len(\n",
    "                            title_query_terms['exact_queries']) == 0:\n",
    "                        d_tf_wt = weightd_tf(doc.title_tf)\n",
    "\n",
    "                        df_t = len(data.posting_lists)\n",
    "                        idf = math.log10(N / df_t)\n",
    "\n",
    "                        if doc.id in scores:\n",
    "                            scores[doc.id] += (q_tf_wt * (d_tf_wt * idf))\n",
    "                        else:\n",
    "                            scores[doc.id] = (q_tf_wt * (d_tf_wt * idf))\n",
    "\n",
    "        relevant_docs = scores.items()\n",
    "        relevant_docs = sorted(relevant_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    elif method == Methods.ltc_lnc:\n",
    "        #         for query_term in query_trems['exact_queries'].items():\n",
    "        #             pass # exact search\n",
    "        doc_weights = 0\n",
    "        query_weights = 0\n",
    "        for query_term, tf in text_query_terms['terms'].items():\n",
    "            q_tf_wt = weightd_tf(tf)\n",
    "            data = index.find(query_term)\n",
    "            if data:\n",
    "                for doc in data.posting_lists:\n",
    "                    #                     if ((len(query_trems['exact_queries']) > 0) and (doc_id in docs_to_be_searched)) or len(query_trems['exact_queries']) == 0:\n",
    "                    d_tf_wt = weightd_tf(doc.text_tf)\n",
    "\n",
    "                    df_t = len(data.posting_lists)\n",
    "                    idf = math.log10(N / df_t)\n",
    "                    doc_weights += d_tf_wt ** 2\n",
    "                    query_weights += q_tf_wt ** 2\n",
    "                    if doc.id in scores:\n",
    "                        scores[doc.id] += (q_tf_wt * (d_tf_wt * idf))\n",
    "                    else:\n",
    "                        scores[doc.id] = (q_tf_wt * (d_tf_wt * idf))\n",
    "\n",
    "        for query_term, tf in title_query_terms['terms'].items():\n",
    "            q_tf_wt = weightd_tf(tf)\n",
    "            data = index.find(query_term)\n",
    "            if data:\n",
    "                for doc in data.posting_lists:\n",
    "                    #                     if ((len(query_trems['exact_queries']) > 0) and (doc_id in docs_to_be_searched)) or len(query_trems['exact_queries']) == 0:\n",
    "                    d_tf_wt = weightd_tf(doc.title_tf)\n",
    "\n",
    "                    df_t = len(data.posting_lists)\n",
    "                    idf = math.log10(N / df_t)\n",
    "                    doc_weights += d_tf_wt ** 2\n",
    "                    query_weights += q_tf_wt ** 2\n",
    "                    if doc.id in scores:\n",
    "                        scores[doc.id] += (q_tf_wt * (d_tf_wt * idf))\n",
    "                    else:\n",
    "                        scores[doc.id] = (q_tf_wt * (d_tf_wt * idf))\n",
    "\n",
    "        doc_weights = math.sqrt(doc_weights)\n",
    "        query_weights = math.sqrt(query_weights)\n",
    "        relevant_docs = list(map(lambda x: (x[0], x[1] / (doc_weights * query_weights)), scores.items()))\n",
    "        relevant_docs = sorted(relevant_docs, key=lambda x: x[1], reverse=True)\n",
    "    return relevant_docs[:15]\n",
    "\n",
    "\n",
    "#     relevant_docs = []\n",
    "#     #print(title_query)\n",
    "#     #print(desc_query)\n",
    "#     return relevant_docs\n",
    "\n",
    "# detailed_search('عجایب هفت‌گانه', 'چشمگیرترین بناهای تاریخی جهان', Methods.ltn_lnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpAO-xvEJne-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
    "</font>\n",
    "<hr>\n",
    "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
    "<ol>\n",
    "<li>\n",
    "MAP\n",
    "</li>\n",
    "<li>\n",
    "F-Measure\n",
    "</li>\n",
    "<li>\n",
    "R-Precision\n",
    "</li>\n",
    "<li>\n",
    "NDCG\n",
    "</li>\n",
    "</ol>\n",
    "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
    "search \n",
    "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
    "detailed_search\n",
    "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
    "<br>\n",
    "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l52cf_R-Jne_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
    "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
    "all\n",
    "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    python_open\n",
    "    print(\"Already done!\")\n",
    "except NameError:\n",
    "    python_open = open\n",
    "    def open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):\n",
    "        encoding=\"utf-8\"\n",
    "        return python_open(file, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd, opener=opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def precision(query_id):\n",
    "    search_mode = 'normal'\n",
    "    with open('./data/queries/%s.txt'%(query_id,)) as query_file: \n",
    "        query = query_file.readlines()\n",
    "        if len(query) > 1:\n",
    "            search_mode = 'detailed'\n",
    "    with open('./data/relevance/%s.txt'%(query_id,)) as query_file: \n",
    "        ground_truth = list(map(int, query_file.read().split(',')))\n",
    "\n",
    "    if search_mode == 'normal':\n",
    "        retrieved = list(map(lambda x:x[0],search(query[0])))\n",
    "    else:\n",
    "        retrieved = list(map(lambda x:x[0],detailed_search(query[0], query[1])))\n",
    "    tp = intersection(retrieved, ground_truth)\n",
    "    result = len(tp) / len(retrieved)\n",
    "    return result\n",
    "\n",
    "def R_Precision(query_id='all'):\n",
    "    search_mode = 'normal'\n",
    "    # sample code for read query! You can change anyway\n",
    "    if query_id == 'all':\n",
    "        result = 0\n",
    "        queries_ids = list(map(lambda x : int(x.split('.')[0]), os.listdir('./data/queries/')))\n",
    "        for query_id in queries_ids:\n",
    "            result += precision(query_id)\n",
    "        return result / len(queries_ids)    \n",
    "    else:\n",
    "        return precision(query_id)\n",
    "    \n",
    "def recall(query_id):\n",
    "    search_mode = 'normal'\n",
    "    with open('./data/queries/%s.txt'%(query_id,)) as query_file: \n",
    "        query = query_file.readlines()\n",
    "        if len(query) > 1:\n",
    "            search_mode = 'detailed'\n",
    "    with open('./data/relevance/%s.txt'%(query_id,)) as query_file: \n",
    "        ground_truth = list(map(int, query_file.read().split(',')))\n",
    "\n",
    "    if search_mode == 'normal':\n",
    "        retrieved = list(map(lambda x:x[0],search(query[0])))\n",
    "    else:\n",
    "        retrieved = list(map(lambda x:x[0],detailed_search(query[0], query[1])))\n",
    "    tp = intersection(retrieved, ground_truth)\n",
    "    result = len(tp) / len(ground_truth)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def f_measure(query_id):\n",
    "    p = precision(query_id)\n",
    "    r = recall(query_id)\n",
    "    return (2*p*r) / (p+r)\n",
    "    \n",
    "def F_measure(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        result = 0\n",
    "        queries_ids = list(map(lambda x : int(x.split('.')[0]), os.listdir('./data/queries/')))\n",
    "        for query_id in queries_ids:\n",
    "            result += f_measure(query_id)\n",
    "        return result / len(queries_ids)   \n",
    "\n",
    "    else:\n",
    "        return f_measure(query_id)\n",
    "\n",
    "def mean_average_precision(query_id):\n",
    "    search_mode = 'normal'\n",
    "    with open('./data/queries/%s.txt'%(query_id,)) as query_file: \n",
    "        query = query_file.readlines()\n",
    "        if len(query) > 1:\n",
    "            search_mode = 'detailed'\n",
    "    with open('./data/relevance/%s.txt'%(query_id,)) as query_file: \n",
    "        ground_truth = list(map(int, query_file.read().split(',')))\n",
    "\n",
    "    if search_mode == 'normal':\n",
    "        retrieved = list(map(lambda x:x[0],search(query[0])))\n",
    "    else:\n",
    "        retrieved = list(map(lambda x:x[0],detailed_search(query[0], query[1])))\n",
    "    precisions = []\n",
    "    for i, doc in enumerate(retrieved):\n",
    "        if doc in ground_truth:\n",
    "            p = (len(precisions) + 1) / (i+1)\n",
    "            precisions.append(p)\n",
    "    return sum(precisions) / len(precisions)\n",
    "    \n",
    "    \n",
    "def MAP(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        result = 0\n",
    "        queries_ids = list(map(lambda x : int(x.split('.')[0]), os.listdir('./data/queries/')))\n",
    "        for query_id in queries_ids:\n",
    "            result += mean_average_precision(query_id)\n",
    "        return result / len(queries_ids)   \n",
    "\n",
    "    else:\n",
    "        return mean_average_precision(query_id)\n",
    "def get_DCG(docs_id):\n",
    "    result = docs_id[0]\n",
    "    for i, doc_id in enumerate(docs_id[1:]):\n",
    "        result += doc_id / math.log2(i+2)\n",
    "    return result    \n",
    "\n",
    "def ndcg(query_id):\n",
    "    search_mode = 'normal'\n",
    "    with open('./data/queries/%s.txt'%(query_id,)) as query_file: \n",
    "        query = query_file.readlines()\n",
    "        if len(query) > 1:\n",
    "            search_mode = 'detailed'\n",
    "    with open('./data/relevance/%s.txt'%(query_id,)) as query_file: \n",
    "        ground_truth = list(map(int, query_file.read().split(',')))\n",
    "\n",
    "    if search_mode == 'normal':\n",
    "        retrieved = list(map(lambda x:x[0],search(query[0])))\n",
    "    else:\n",
    "        retrieved = list(map(lambda x:x[0],detailed_search(query[0], query[1])))\n",
    "    gt_dcg = get_DCG([1 for i in range(len(ground_truth))])\n",
    "    r = []\n",
    "    for doc in retrieved:\n",
    "        if doc in ground_truth:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    r_dgc = get_DCG(r)\n",
    "    return r_dgc / gt_dcg\n",
    "        \n",
    "\n",
    "def NDCG(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        result = 0\n",
    "        queries_ids = list(map(lambda x : int(x.split('.')[0]), os.listdir('./data/queries/')))\n",
    "        for query_id in queries_ids:\n",
    "            result += ndcg(query_id)\n",
    "        return result / len(queries_ids)   \n",
    "\n",
    "    else:\n",
    "        return ndcg(query_id)\n",
    "\n",
    "\n",
    "# R_Precision('all')\n",
    "# F_measure(1)\n",
    "# MAP('all')\n",
    "# NDCG(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "R_Precision:\n",
      "1:\t1.00\tTrue\n",
      "2:\t1.00\tFalse\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "F_measure:\n",
      "1:\t1.00\tFalse\n",
      "2:\t0.62\tTrue\n",
      "division by zero\n",
      "------------------------------\n",
      "MAP:\n",
      "1:\t1.00\tFalse\n",
      "2:\t1.00\tFalse\n",
      "division by zero\n",
      "------------------------------\n",
      "NDCG:\n",
      "1:\t1.00\tFalse\n",
      "2:\t0.63\tTrue\n",
      "3:\t0.00\tTrue\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = [1, 2, 3]\n",
    "rels = [\n",
    "    [6753, 7134, 6978, 7136, 4530, 6798, 6885, 5381, 6900, 4537, 5509, 6794, 4094, 6417, 3666, 5967],\n",
    "    [6753, 5509, 4718, 6798, 6850, 6417, 6978, 6871],\n",
    "    list(range(20))\n",
    "]\n",
    "outputs = [{'R_Precision': 1.0, 'F_measure': 0.967741935483871, 'MAP': 0.9375, 'NDCG': 0.9635640110263509},\n",
    "           {'R_Precision': 0.4444444444444444, 'F_measure': 0.6153846153846153, 'MAP': 0.4444444444444444, 'NDCG': 0.6313802022799658},\n",
    "           {'R_Precision': 0.0, 'F_measure': 0.0, 'MAP': 0.0, 'NDCG': 0.0}]\n",
    "\n",
    "functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "idx = 0 \n",
    "\n",
    "ds = detailed_search\n",
    "s = search\n",
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    return list(map(lambda x:(x, 0),rels[idx]))\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    return list(map(lambda x:(x, 0),rels[idx]))\n",
    "\n",
    "for f in functions.keys():\n",
    "    print(\"{}\\n{}:\".format('-'*30, f))\n",
    "    idx = 0\n",
    "    for doc in test_docs:    \n",
    "        expected = outputs[idx][f]\n",
    "        try:\n",
    "            out = functions[f](doc)\n",
    "            print(\"{}:\\t{:.2f}\\t{}\".format(doc, out, abs(out-expected)<1e-3))\n",
    "        except:\n",
    "            print(\"division by zero\")\n",
    "        \n",
    "        \n",
    "        idx += 1\n",
    "\n",
    "detailed_search = ds\n",
    "search = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUs450l5JnfD"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>نکات پایانی</div>\n",
    "</font>\n",
    "<hr>\n",
    "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
    "<br>\n",
    "۲- فایل‌های \n",
    "ipynb\n",
    "و پایتون \n",
    "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
    "<br>\n",
    "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
    "<br>\n",
    "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
    "<br>\n",
    "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
    "<br>\n",
    "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2AOY8hCJnfE"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B titr\" size=30>\n",
    "<p></p>\n",
    "<font color=#FF7500> \n",
    "موفق باشید\n",
    ":)\n",
    "<br>\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
