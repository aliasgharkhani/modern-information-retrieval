[{"id": "f90720ed12e045ac84beb94c27271d6fb8ad48cf", "title": "The Lottery Ticket Hypothesis: Training Pruned Neural Networks", "authors": ["Jonathan Frankle", "Michael Carbin"], "date": "2018", "abstract": "Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets… ", "references": ["c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "cc46229a7c47f485e090857cbab6e6bf68c09811", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "642d0f49b7826adcf986616f4af77e736229990f"]},{"id": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets", "authors": ["Hao Li", "Asim Kadav", "Hans Peter Graf"], "date": "2016", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs.", "references": ["021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "8ad35df17ae4064dd174690efb04d347428f1117", "7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "397de65a9a815ec39b3704a79341d687205bc80a", "397de65a9a815ec39b3704a79341d687205bc80a", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "751c8884c1e857e675d85d8594c5f9b608005ed5", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "397de65a9a815ec39b3704a79341d687205bc80a", "397de65a9a815ec39b3704a79341d687205bc80a"]},{"id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Illia Polosukhin"], "date": "2017", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration.", "references": ["735d547fc75e0772d2a78c46a1cc5fad7da1474c", "98445f4172659ec5e891e031d8202c102135c644", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "98445f4172659ec5e891e031d8202c102135c644", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "510e26733aaff585d65701b9f1be7ca9d5afc586", "510e26733aaff585d65701b9f1be7ca9d5afc586"]},{"id": "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes", "authors": ["Zelda Mariet", "Suvrit Sra"], "date": "2015", "abstract": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables… ", "references": ["ec46bcbced500820521e9f65b0f9ffef5a83ae11", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "ec46bcbced500820521e9f65b0f9ffef5a83ae11", "8ba555d9587688bd3225d71ef9d686dad288e1f1", "efb5032e6199c80f83309fd866b25be9545831fd", "87d810fcea61068e8b29f2b75fa1cbb00c190bea", "31f88db95eb5c66b95cd7335b0cd4f27f0f271f2", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "efb5032e6199c80f83309fd866b25be9545831fd", "a42954d4b9d0ccdf1036e0af46d87a01b94c3516"]},{"id": "cc46229a7c47f485e090857cbab6e6bf68c09811", "title": "Understanding Dropout", "authors": ["Pierre Baldi", "Peter Sadowski"], "date": "NIPS", "abstract": "Dropout is a relatively new algorithm for training neural networks which relies on stochastically \"dropping out\" neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are… ", "references": ["0688fbcbfb08d7b91238bc90589209b31f97290f", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "ba15f09796d53adfbe9e78cf79182e59b6045543", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "ba15f09796d53adfbe9e78cf79182e59b6045543", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "ba15f09796d53adfbe9e78cf79182e59b6045543", "1366de5bb112746a555e9c0cd00de3ad8628aea8"]},{"id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kristina Toutanova"], "date": "2019", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.", "references": ["0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "8c1b00128e74f1cd92aede3959690615695d5101", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "687bac2d3320083eb4530bf18bb8f8f721477600", "27e98e09cf09bc13c913d01676e5f32624011050", "93b8da28d006415866bf48f9a6e06b5242129195", "27e98e09cf09bc13c913d01676e5f32624011050"]},{"id": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning", "authors": ["Babak Hassibi", "David G. Stork", "Gregory J. Wolff"], "date": "1993", "abstract": "The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optimal Brain Damage, which often… ", "references": ["57dc98cfb48247b400cc8decb93380e022864905", "1b29884885401d12299a01b0eae099f425dd32e1", "1b29884885401d12299a01b0eae099f425dd32e1", "5887de8eed53c444b2ef93d8ab9c8cc685cd7ac5", "a42954d4b9d0ccdf1036e0af46d87a01b94c3516"]},{"id": "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression", "authors": ["Jian-Hao Luo", "Jianxin Wu", "Weiyao Lin"], "date": "2017", "abstract": "We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages.", "references": ["60ae4f18cb53efff0174e3fea7064049737e1e67", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "7601b995303f953955004db7b9b8b206c0e02ff8", "642d0f49b7826adcf986616f4af77e736229990f", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "60ae4f18cb53efff0174e3fea7064049737e1e67", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "60ae4f18cb53efff0174e3fea7064049737e1e67"]},{"id": "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "title": "Data-free Parameter Pruning for Deep Neural Networks", "authors": ["Suraj Srinivas", "R. Venkatesh Babu"], "date": "BMVC", "abstract": "Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "eb42cf88027de515750f230b23b1a057dc782108", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "0c908739fbff75f03469d13d4a1a07de3414ee19", "e8650503ab80ad7299f0845b1843abf3a97f313a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "8ad35df17ae4064dd174690efb04d347428f1117", "title": "Convolutional neural networks at constrained time cost", "authors": ["Kaiming He"], "date": "2015", "abstract": "Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs… ", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "d67175d17c450ab0ac9c256103828f9e9a0acb85", "6270baedeba28001cd1b563a199335720d6e0fe0", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "cbb19236820a96038d000dc629225d36e0b6294a", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "6270baedeba28001cd1b563a199335720d6e0fe0", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network", "authors": ["Song Han", "Jeff Pool", "William J. Dally"], "date": "2015", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems.", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "642d0f49b7826adcf986616f4af77e736229990f", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "081651b38ff7533550a3adfc1c00da333a8fe86c", "081651b38ff7533550a3adfc1c00da333a8fe86c", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "34f25a8704614163c4095b3ee2fc969b60de4698", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16"]},{"id": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "date": "2016", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e7bf9803705f2eb608db1e59e5c7636a3f171916", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "6bdb186ec4726e00a8051119636d4df3b94043b5", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "6bdb186ec4726e00a8051119636d4df3b94043b5", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "a6373454105df0c5511ca5f6cae4d20c48214272"]},{"id": "7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "title": "Structured Pruning of Deep Convolutional Neural Networks", "authors": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "date": "2017", "abstract": "Real-time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "397de65a9a815ec39b3704a79341d687205bc80a", "397de65a9a815ec39b3704a79341d687205bc80a", "397de65a9a815ec39b3704a79341d687205bc80a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "397de65a9a815ec39b3704a79341d687205bc80a", "2cc157afda51873c30b195fff56e917b9c06b853"]},{"id": "397de65a9a815ec39b3704a79341d687205bc80a", "title": "A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "date": "2015", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources.", "references": ["e7bf9803705f2eb608db1e59e5c7636a3f171916", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "27a99c21a1324f087b2f144adc119f04137dfd87", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "2a4117849c88d4728c33b1becaa9fb6ed7030725"]},{"id": "751c8884c1e857e675d85d8594c5f9b608005ed5", "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification", "authors": ["Yani A Ioannou", "Duncan P. Robertson", "Antonio Criminisi"], "date": "2016", "abstract": "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight… ", "references": ["f075f89b4f4026748cbf2fb9f989a9934c42ee8f", "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5d90f06bb70a0a3dced62413346235c02b1aa086", "5d90f06bb70a0a3dced62413346235c02b1aa086", "5a434953b58c72fe2089531d6c4b4fc1325defcb", "f075f89b4f4026748cbf2fb9f989a9934c42ee8f", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "f075f89b4f4026748cbf2fb9f989a9934c42ee8f"]},{"id": "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "authors": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "date": "2014", "abstract": "The focus of this paper is speeding up the application of convolutional neural networks.", "references": ["1e80f755bcbf10479afd2338cec05211fdbd325c", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1e80f755bcbf10479afd2338cec05211fdbd325c", "4dbc68cf2e14155edb6da0def30661aca8c96c22", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b3d8dffb73bc93de239998548386c84177caa2ad", "1e80f755bcbf10479afd2338cec05211fdbd325c", "fbeaa499e10e98515f7e1c4ad89165e8c0677427"]},{"id": "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "title": "Can Active Memory Replace Attention?", "authors": ["Lukasz Kaiser", "Samy Bengio"], "date": "2016", "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory… ", "references": ["33108287fbc8d94160787d7b2c7ef249d3ad6437", "0811597b0851b7ebe21aadce7cb4daac4664b44f", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "33108287fbc8d94160787d7b2c7ef249d3ad6437", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "0811597b0851b7ebe21aadce7cb4daac4664b44f", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "98445f4172659ec5e891e031d8202c102135c644", "title": "Neural Machine Translation in Linear Time", "authors": ["Nal Kalchbrenner", "Lasse Espeholt", "Koray Kavukcuoglu"], "date": "2016", "abstract": "We present a novel neural network for processing sequences.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "a17745f1d7045636577bcd5d513620df5860e9e5", "b60abe57bc195616063be10638c6437358c81d1e", "93499a7c7f699b6630a86fad964536f9423bb6d0", "b60abe57bc195616063be10638c6437358c81d1e", "0b544dfe355a5070b60986319a3f51fb45d1348e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "b60abe57bc195616063be10638c6437358c81d1e", "b60abe57bc195616063be10638c6437358c81d1e", "0b544dfe355a5070b60986319a3f51fb45d1348e"]},{"id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "date": "2015", "abstract": "Neural machine translation is a recently proposed approach to machine translation.", "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "396aabd694da04cdb846cb724ca9f866f345cbd5", "0b544dfe355a5070b60986319a3f51fb45d1348e", "5f08df805f14baa826dbddcb002277b15d3f1556", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "0b544dfe355a5070b60986319a3f51fb45d1348e", "5f08df805f14baa826dbddcb002277b15d3f1556", "5f08df805f14baa826dbddcb002277b15d3f1556", "944a1cfd79dbfb6fef460360a0765ba790f4027a"]},{"id": "ec46bcbced500820521e9f65b0f9ffef5a83ae11", "title": "k-DPPs: Fixed-Size Determinantal Point Processes", "authors": ["Alex Kulesza", "Ben Taskar"], "date": "ICML", "abstract": "Determinantal point processes (DPPs) have recently been proposed as models for set selection problems where diversity is preferred. For example, they can be used to select diverse sets of sentences to form document summaries, or to find multiple non-overlapping human poses in an image. However, DPPs conflate the modeling of two distinct characteristics: the size of the set, and its content. For many realistic tasks, the size of the desired set is known up front; e.g., in search we may want to… ", "references": ["a22ac183c8b37824e32cae970db170b861a13438", "1840c15f0ce85a7f4756c185cd29508d6b53c66c", "9d94fc289d82738a4d1071470b16ba861ea12169", "b91afd46236a9c9eda9056bf4e70fe9235867571", "fabe20a21465511a1d56d319e52eec17005b3452", "a22ac183c8b37824e32cae970db170b861a13438", "1840c15f0ce85a7f4756c185cd29508d6b53c66c", "5f10ce4992742b7134d146d91af6a66077140f5f", "a22ac183c8b37824e32cae970db170b861a13438", "f9f836d28f52ad260213d32224a6d227f8e8849a"]},{"id": "8c1b00128e74f1cd92aede3959690615695d5101", "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension", "authors": ["Adams Wei Yu", "David Dohan", "Quoc V. Le"], "date": "2018", "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention.", "references": ["b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "93499a7c7f699b6630a86fad964536f9423bb6d0", "12e20e4ea572dbe476fd894c5c9a9930cf250dd2", "c6e5df6322659276da6133f9b734a389d7a255e8", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "adc276e6eae7051a027a4c269fb21dae43cadfed", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "de0c30321b22c56d637e7c29cb59180f157272a8", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "de0c30321b22c56d637e7c29cb59180f157272a8"]},{"id": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "authors": ["Noam Shazeer", "Azalia Mirhoseini", "Jeff Dean"], "date": "2017", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation… ", "references": ["2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "cf3229e74f912ef365d67d1954441b32ce2573ee", "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "66b8d34477cf1736f91fd22b27e37ce0b703c86e", "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "44ddac48353ead135eef4096859956eaa31be2a5", "b60abe57bc195616063be10638c6437358c81d1e"]},{"id": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "title": "MaskGAN: Better Text Generation via Filling in the ______", "authors": ["William Fedus", "Ian J. Goodfellow", "Andrew M. Dai"], "date": "2018", "abstract": "Neural text generation models are often autoregressive language models or seq2seq models.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "424aef7340ee618132cc3314669400e23ad910ba", "a8176a160777bfe82b1c67506835c60073e6fbe8", "db38edba294b7d2fd8ca3aad65721bd9dce32619", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "424aef7340ee618132cc3314669400e23ad910ba", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "176f1d608b918eec8dc4b75e7b6e0acaba84a447", "cea967b59209c6be22829699f05b8b1ac4dc092d", "424aef7340ee618132cc3314669400e23ad910ba"]},{"id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention", "authors": ["Rami Al-Rfou", "Dokook Choe", "Llion Jones"], "date": "AAAI", "abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling.", "references": ["58c6f890a1ae372958b7decf56132fe258152722", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "84ca430856a92000e90cd728445ca2241c10ddc3", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "58c6f890a1ae372958b7decf56132fe258152722", "88caa4a0253a8b0076176745ebc072864eab66e1", "4db8cd9117254d21c9c828b8ba2aea58e57ee2c4", "88caa4a0253a8b0076176745ebc072864eab66e1", "f9a1b3850dfd837793743565a8af95973d395a4e"]},{"id": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "date": "2014", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks.", "references": ["c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b544dfe355a5070b60986319a3f51fb45d1348e", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6", "9819b600a828a57e1cde047bbe710d3446b30da5", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "6658bbf68995731b2083195054ff45b4eca38b3a", "9819b600a828a57e1cde047bbe710d3446b30da5", "167ad306d84cca2455bc50eb833454de9f2dcd02"]},{"id": "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "title": "Semi-Supervised Sequence Modeling with Cross-View Training", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le"], "date": "EMNLP", "abstract": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text.", "references": ["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "ac17cfa150d802750b46220084d850cfdb64d1c1", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "ac17cfa150d802750b46220084d850cfdb64d1c1", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "c35c2ce83221f909f3ee97bce3d49fae82b795c0", "85f94d8098322f8130512b4c6c4627548ce4a6cc", "85f94d8098322f8130512b4c6c4627548ce4a6cc"]},{"id": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "title": "Semi-supervised sequence tagging with bidirectional language models", "authors": ["Matthew E. Peters", "Waleed Ammar", "Russell Power"], "date": "2017", "abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks.", "references": ["6e795c6e9916174ae12349f5dc3f516570c17ce8", "2c821e2ec8ef976d3abb36fb0dc1946f04208512", "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "7ece4e8d31f872d928369ac2cf58a616a7182112", "26e743d5bd465f49b9538deaf116c15e61b7951f", "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "ade0c116120b54b57a91da51235108b75c28375a", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "7ece4e8d31f872d928369ac2cf58a616a7182112", "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4"]},{"id": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "authors": ["Richard Socher", "Alex Perelygin", "Christopher Potts"], "date": "EMNLP", "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way.", "references": ["553fb89d5858826c02f26e94262e8958debc777e", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "553fb89d5858826c02f26e94262e8958debc777e", "2063745d08868c928455f422202b72146a1960fb", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "27e38351e48fe4b7da2775bf94341738bc4da07e", "57458bc1cffe5caa45a885af986d70f723f406b4", "cfa2646776405d50533055ceb1b7f050e9014dcb", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "553fb89d5858826c02f26e94262e8958debc777e"]},{"id": "8ba555d9587688bd3225d71ef9d686dad288e1f1", "title": "Fast Determinantal Point Process Sampling with Application to Clustering", "authors": ["Byungkon Kang"], "date": "NIPS", "abstract": "Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that defines the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from… ", "references": ["8faad7901db9a73cacaf92ecdedbaece87d95f92", "2b5db2ef319226e1a019c10bd17af0c283b56cf7", "15acca25f75076b80b0bd24c5710c70733308c11", "8faad7901db9a73cacaf92ecdedbaece87d95f92", "48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016", "b91afd46236a9c9eda9056bf4e70fe9235867571", "5c8fe9a0412a078e30eb7e5eeb0068655b673e86", "2b5db2ef319226e1a019c10bd17af0c283b56cf7", "8faad7901db9a73cacaf92ecdedbaece87d95f92", "15acca25f75076b80b0bd24c5710c70733308c11"]},{"id": "27e98e09cf09bc13c913d01676e5f32624011050", "title": "U-Net: Machine Reading Comprehension with Unanswerable Questions", "authors": ["Fu Sun", "Linyang Li", "Yang P. Liu"], "date": "2018", "abstract": "Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the… ", "references": ["9a5ba9aee44ab873f3d60b05e2773c693707da88", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "9a5ba9aee44ab873f3d60b05e2773c693707da88", "fa025e5d117929361bcf798437957762eb5bb6d4", "c636a2dd242908fe2e598a1077c0c57bfdea8633", "bb6daf3bd95668ea9775d7e06d8b3f6994306cb7"]},{"id": "31f88db95eb5c66b95cd7335b0cd4f27f0f271f2", "title": "A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data", "authors": ["Jasper Snoek", "Richard S. Zemel", "Ryan P. Adams"], "date": "NIPS", "abstract": "Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over… ", "references": ["2fbeef5ca17328590cf74353ff5654dad7800d67", "c76d84d779e28e3b236054a0f34c3e48910399d8", "0d92ce89894d50af18c902e6b89f85eb0d4fcde2", "ef7bb077881079f686715e2de711b905f6a18829", "3e568fb5a3ce754abe9f029aa80610f16b9f9f92", "0d92ce89894d50af18c902e6b89f85eb0d4fcde2", "3e568fb5a3ce754abe9f029aa80610f16b9f9f92", "eb84341018bc6e5775815a37c74876e1e5a1e5fb", "ef7bb077881079f686715e2de711b905f6a18829", "59d435c69a34efec487447c52f37cb96b36df570"]},{"id": "87d810fcea61068e8b29f2b75fa1cbb00c190bea", "title": "Reshaping deep neural network for fast decoding by node-pruning", "authors": ["Tianxing He", "Yuchen Fan", "Kai Yu"], "date": "2014", "abstract": "Although deep neural networks (DNN) has achieved significant accuracy improvements in speech recognition, it is computationally expensive to deploy large-scale DNN in decoding due to huge number of parameters. Weights truncation and decomposition methods have been proposed to speed up decoding by exploiting the sparseness of DNN. This paper summarizes different approaches of restructuring DNN and proposes a new node pruning approach to reshape DNN for fast decoding. In this approach, hidden… ", "references": ["5cea23330c76994cb626df20bed31cc2588033df", "6d9429b96d9bb40e2d0d9c3f57d0b97f61db8503", "6658bbf68995731b2083195054ff45b4eca38b3a", "5cea23330c76994cb626df20bed31cc2588033df", "8d52f379250a9755f463a9f1a86d555341329001", "077b54784ffc0fc4b335392a0bdf630f595a12ce", "077b54784ffc0fc4b335392a0bdf630f595a12ce", "6d9429b96d9bb40e2d0d9c3f57d0b97f61db8503", "e354ec85b8287bf15ed596be16ef6e422ccc29e7", "8d52f379250a9755f463a9f1a86d555341329001"]},{"id": "93b8da28d006415866bf48f9a6e06b5242129195", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "authors": ["Alex Wang", "Amanpreet Singh", "Samuel R. Bowman"], "date": "BlackboxNLP@EMNLP", "abstract": "For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset.", "references": ["ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "afc2850945a871e72c245818f9bc141bd659b453", "ade0c116120b54b57a91da51235108b75c28375a", "ade0c116120b54b57a91da51235108b75c28375a", "afc2850945a871e72c245818f9bc141bd659b453", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "93b4cc549a1bc4bc112189da36c318193d05d806", "8f1c9b656157b1d851563fb42129245701d83175", "93b4cc549a1bc4bc112189da36c318193d05d806", "8f1c9b656157b1d851563fb42129245701d83175"]},{"id": "0688fbcbfb08d7b91238bc90589209b31f97290f", "title": "A CONVERGENCE THEOREM FOR NON NEGATIVE ALMOST SUPERMARTINGALES AND SOME APPLICATIONS", "authors": ["Herbert Robbins", "David Siegmund"], "date": "1985", "abstract": "The purpose of this paper is to give a unified treatment of a number of almost sure convergence theorems by exploiting the fact that the processes involved possess a common “almost supermartingale” properties To be precise, let (Ω,F,P) be a probability space and F1 ⊂ F2 ⊂ … a sequence of sub-σ-algebras of F. For each n= 1,2, … let zn, βn, ξn, and ζn be non-negative Fn -measurable random variables such that \n \n$$E({z_{n + 1}}|{F_n}) \\leqslant {z_n}(1 + {\\beta _n}) + {\\xi _n} - {\\zeta _n… ", "references": []},{"id": "efb5032e6199c80f83309fd866b25be9545831fd", "title": "Compressing Neural Networks with the Hashing Trick", "authors": ["Wenlin Chen", "James T. Wilson", "Yixin Chen"], "date": "ICML", "abstract": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to… ", "references": ["851c27d7cdb74b0b21bd84a9333bca106f486713", "7a6fd5573d2679506765d461ec4892fd4017b745", "be9a17321537d9289875fe475b71f4821457b435", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "851c27d7cdb74b0b21bd84a9333bca106f486713", "d1208ac421cf8ff67b27d93cd19ae42b8d596f95", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "34f25a8704614163c4095b3ee2fc969b60de4698", "72e93aa6767ee683de7f001fa72f1314e40a8f35"]},{"id": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon", "authors": ["Babak Hassibi", "David G. Stork"], "date": "NIPS", "abstract": "We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction.", "references": ["de996c32045df6f7b404dda2a753b6a9becf3c08", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5"]},{"id": "ba15f09796d53adfbe9e78cf79182e59b6045543", "title": "On the Ky Fan inequality and related inequalities II", "authors": ["Edward Neuman", "József Sándor"], "date": "2005", "abstract": "Disclosed is an information processing apparatus including a display unit displaying information on a display screen, an operation unit including a text input key, and a control unit. The control unit displays text in response to input from the operation unit in a state of displaying an initial screen on the display screen, shows an application using text to a user for selection, starts up the application in accordance with the user's selection, and executes the application, using the text… ", "references": ["6d02ef4ef79f3cb57dcf4eb20f890aa1be859219", "94fae0f11be9f67bc6818553e10599ced559422d", "a1ea19d292460e3b7fca3df1313482f5abc0897e", "94fae0f11be9f67bc6818553e10599ced559422d", "a419456b897eb646d5c9b8f075ffd9d283fff26e", "a1ea19d292460e3b7fca3df1313482f5abc0897e", "94fae0f11be9f67bc6818553e10599ced559422d", "a1ea19d292460e3b7fca3df1313482f5abc0897e", "a1ea19d292460e3b7fca3df1313482f5abc0897e", "1295e0ec83fa4e5efbbfaf156d8525fdc8907929"]},{"id": "57dc98cfb48247b400cc8decb93380e022864905", "title": "Introduction to the Theory of Neural Computation", "authors": ["John Hertz", "Anders Krogh", "Roderick V. Jensen"], "date": "1994", "abstract": null, "references": []},{"id": "1366de5bb112746a555e9c0cd00de3ad8628aea8", "title": "Improving neural networks by preventing co-adaptation of feature detectors", "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Ruslan Salakhutdinov"], "date": "2012", "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the… ", "references": ["d779f5c56a7121bdb62d73c1894a1ab0d182cbc2", "db869fa192a3222ae4f2d766674a378e47013b1b", "db869fa192a3222ae4f2d766674a378e47013b1b", "5d90f06bb70a0a3dced62413346235c02b1aa086", "db869fa192a3222ae4f2d766674a378e47013b1b", "db869fa192a3222ae4f2d766674a378e47013b1b", "5d90f06bb70a0a3dced62413346235c02b1aa086", "950bb724db08b009e6305157ac21b484cd2771fe", "d779f5c56a7121bdb62d73c1894a1ab0d182cbc2", "d779f5c56a7121bdb62d73c1894a1ab0d182cbc2"]},{"id": "5887de8eed53c444b2ef93d8ab9c8cc685cd7ac5", "title": "A Frobenius approximation reduction method (FARM) for determining optimal number of hidden units", "authors": ["S. Kung", "Yu Hen Hu"], "date": "1991", "abstract": "A least-square approximation method is proposed to reduce the number of hidden units of a trained multilayer perceptron artificial neural network structure. In this method, the hidden neurons that contribute the most to the net function of the output layer are retained while the hidden units that contribute the least are removed. It is shown theoretically that the proposed method minimizes the Frobenius norm of the approximation error, hence the name Frobenius approximation reduction method… ", "references": []},{"id": "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "title": "Improving Neural Networks with Dropout", "authors": ["Nitish Srivastava"], "date": "2013", "abstract": "Improving Neural Networks with Dropout Nitish Srivastava Master of Science Graduate Department of Computer Science University of Toronto 2013 Deep neural nets with a huge number of parameters are very powerful machine learning systems.", "references": ["1366de5bb112746a555e9c0cd00de3ad8628aea8", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6"]},{"id": "1b29884885401d12299a01b0eae099f425dd32e1", "title": "Interpretation of Artificial Neural Networks: Mapping Knowledge-Based Neural Networks into Rules", "authors": ["Geoffrey G. Towell", "Jude W. Shavlik"], "date": "NIPS", "abstract": "We propose and empirically evaluate a method for the extraction of expert-comprehensible rules from trained neural networks. Our method operates in the context of a three-step process for learning that uses rule-based domain knowledge in combination with neural networks. Empirical tests using real-worlds problems from molecular biology show that the rules our method extracts from trained neural networks: closely reproduce the accuracy of the network from which they came, are superior to the… ", "references": ["d2f6ac8b70044a437a7d5444252d6af6037b46c2", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "fdfad550280c6a850d92424b6075e7fb58e8e415", "d8c9a210221e3c925b4119a4ab90aa8b57bb31fc", "d2f6ac8b70044a437a7d5444252d6af6037b46c2", "d14670a0c65a007912b37e2436ee2d7caf70fd76", "a57c6d627ffc667ae3547073876c35d6420accff", "d2f6ac8b70044a437a7d5444252d6af6037b46c2", "a080a28ff7fb3b58fa8cd7123a473c5e75bf46e1", "d8c9a210221e3c925b4119a4ab90aa8b57bb31fc"]},{"id": "e7bf9803705f2eb608db1e59e5c7636a3f171916", "title": "Compressing Deep Convolutional Networks using Vector Quantization", "authors": ["Yunchao Gong", "Liu Liu", "Lubomir D. Bourdev"], "date": "2014", "abstract": "Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we… ", "references": ["fbeaa499e10e98515f7e1c4ad89165e8c0677427", "6d77482b5e3478f4616f7467054ad50505207958", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "6d77482b5e3478f4616f7467054ad50505207958", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "6d77482b5e3478f4616f7467054ad50505207958", "a99add9d76d849a8d47b93532703e4ca0f683b92", "a99add9d76d849a8d47b93532703e4ca0f683b92"]},{"id": "60ae4f18cb53efff0174e3fea7064049737e1e67", "title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures", "authors": ["Hengyuan Hu", "Rui Peng", "Chi-Keung Tang"], "date": "2016", "abstract": "State-of-the-art neural networks are getting deeper and wider. While their performance increases with the increasing number of layers and neurons, it is crucial to design an efficient deep architecture in order to reduce computational and memory costs. Designing an efficient neural network, however, is labor intensive requiring many experiments, and fine-tunings. In this paper, we introduce network trimming which iteratively optimizes the network by pruning unimportant neurons based on analysis… ", "references": ["e15cf50aa89fee8535703b9f9512fca5bfc43327", "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "eb42cf88027de515750f230b23b1a057dc782108", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "eb42cf88027de515750f230b23b1a057dc782108", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270"]},{"id": "7601b995303f953955004db7b9b8b206c0e02ff8", "title": "Learning Structured Sparsity in Deep Neural Networks", "authors": ["Wei Wen", "Chunpeng Wu", "Hai Li"], "date": "NIPS", "abstract": "High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices.", "references": ["6cf7f474eb493b0e5aae74ccfd9cdc79e506060e", "642d0f49b7826adcf986616f4af77e736229990f", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "d559dd84fc473fca7e91b9075675750823935afa", "d5b4721c8188269b120d3d06149a04435753e755", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "2a4117849c88d4728c33b1becaa9fb6ed7030725", "title": "Memory Bounded Deep Convolutional Networks", "authors": ["Maxwell D. Collins", "Pushmeet Kohli"], "date": "2014", "abstract": "In this work, we investigate the use of sparsity-inducing regularizers during training of Convolution Neural Networks (CNNs.", "references": ["b64601d509711468f5d085261d463846f36785b2", "843959ffdccf31c6694d135fad07425924f785b1", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "aa7bfd2304201afbb19971ebde87b17e40242e91", "aa7bfd2304201afbb19971ebde87b17e40242e91", "aa7bfd2304201afbb19971ebde87b17e40242e91", "aa7bfd2304201afbb19971ebde87b17e40242e91", "5d90f06bb70a0a3dced62413346235c02b1aa086", "b64601d509711468f5d085261d463846f36785b2"]},{"id": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "authors": ["Karen Simonyan", "Andrew Zisserman"], "date": "2015", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting.", "references": ["6270baedeba28001cd1b563a199335720d6e0fe0", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71", "6270baedeba28001cd1b563a199335720d6e0fe0", "6270baedeba28001cd1b563a199335720d6e0fe0", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "1109b663453e78a59e4f66446d71720ac58cec25", "1109b663453e78a59e4f66446d71720ac58cec25", "14d9be7962a4ec5a6e55755f4c7588ea00793652"]},{"id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes.", "references": ["398c296d0cc7f9d180f84969f8937e6d3a413796", "1e80f755bcbf10479afd2338cec05211fdbd325c", "5d90f06bb70a0a3dced62413346235c02b1aa086", "5562a56da3a96dae82add7de705e2bd841eb00fc", "1e80f755bcbf10479afd2338cec05211fdbd325c", "82b9099ddf092463f497bd48bb112c46ca52c4d1", "5d90f06bb70a0a3dced62413346235c02b1aa086", "c43025c429b1fbf6f1379f61801a1b40834d62e7", "5562a56da3a96dae82add7de705e2bd841eb00fc", "82b9099ddf092463f497bd48bb112c46ca52c4d1"]},{"id": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network", "authors": ["Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean"], "date": "2015", "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions.", "references": ["34f25a8704614163c4095b3ee2fc969b60de4698", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "3127190433230b3dc1abd0680bb58dced4bcd90e", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "8d25d04051074be7590cbe5e4e34c45bb26674e1", "34f25a8704614163c4095b3ee2fc969b60de4698"]},{"id": "e8650503ab80ad7299f0845b1843abf3a97f313a", "title": "Predicting Parameters in Deep Learning", "authors": ["Misha Denil", "Babak Shakibi", "Nando de Freitas"], "date": "2013", "abstract": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of… ", "references": ["052b1d8ce63b07fec3de9dbb583772d860b7c769", "51e93552fe55be91a5711ff2aabc04b742503e68", "51e93552fe55be91a5711ff2aabc04b742503e68", "72d32c986b47d6b880dad0c3f155fe23d2939038", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "182015c5edff1956cbafbcb3e7bbe294aa54f9fc", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "5352b7ca90cbe4938f8e71a25d49517e7f94670a", "052b1d8ce63b07fec3de9dbb583772d860b7c769"]},{"id": "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "title": "Improving the speed of neural networks on CPUs", "authors": ["Vincent Vanhoucke", "Andrew W. Senior", "Mark Z. Mao"], "date": "2011", "abstract": "Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing.", "references": ["a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "fa5cf89c59b834ec7573673657c99c77f53f7add", "0ea90fac0958d84bcf4a2875c2b169478358b480", "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "80664dab16a1f18ce1998e38a03f080c5e98363a", "94fd94b7ebcdd80e47706376aa0540cbeb009262", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "0409187ebfce03de95677aaeb499e8f1953bdbaf", "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37"]},{"id": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions", "authors": ["Christian Szegedy", "Wei Liu", "Andrew Rabinovich"], "date": "2015", "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the… ", "references": ["1109b663453e78a59e4f66446d71720ac58cec25", "67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6", "3127190433230b3dc1abd0680bb58dced4bcd90e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "1109b663453e78a59e4f66446d71720ac58cec25", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1109b663453e78a59e4f66446d71720ac58cec25", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6"]},{"id": "081651b38ff7533550a3adfc1c00da333a8fe86c", "title": "How transferable are features in deep neural networks?", "authors": ["Jason Yosinski", "Jeff Clune", "Hod Lipson"], "date": "2014", "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs.", "references": ["1e80f755bcbf10479afd2338cec05211fdbd325c", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "b8de958fead0d8a9619b55c7299df3257c624a96", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "51e93552fe55be91a5711ff2aabc04b742503e68", "1109b663453e78a59e4f66446d71720ac58cec25", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1e80f755bcbf10479afd2338cec05211fdbd325c", "b8de958fead0d8a9619b55c7299df3257c624a96"]},{"id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "title": "Network In Network", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "date": "2014", "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field.", "references": ["5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "523b12db4004b89284387f978c2af8ae0e79d54b", "38f35dd624cd1cf827416e31ac5e0e0454028eca", "b3d8dffb73bc93de239998548386c84177caa2ad", "822f3b9a392a9abccdaa7ef5ae4183d2d4d3d6db", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "f9def788d4ae040edb8bde18b8aeea635444a4d1"]},{"id": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting", "authors": ["Nitish Srivastava", "Geoffrey E. Hinton", "Ruslan Salakhutdinov"], "date": "2014", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems.", "references": ["ec92efde21707ddf4b81f301cd58e2051c1a2443", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "3c20df69865df6a627cc45c524869ccc0297048f", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "5d90f06bb70a0a3dced62413346235c02b1aa086", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "db869fa192a3222ae4f2d766674a378e47013b1b", "de75e4e15e22d4376300e5c968e2db44be29ac9e"]},{"id": "6bdb186ec4726e00a8051119636d4df3b94043b5", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "authors": ["Yangqing Jia", "Evan Shelhamer", "Trevor Darrell"], "date": "2014", "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a… ", "references": []},{"id": "33108287fbc8d94160787d7b2c7ef249d3ad6437", "title": "Modeling Coverage for Neural Machine Translation", "authors": ["Zhaopeng Tu", "Zhengdong Lu", "Hang Li"], "date": "2016", "abstract": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to… ", "references": ["de45a6f4473a321d3dd70b0f5e327a0783b57326", "cea967b59209c6be22829699f05b8b1ac4dc092d", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "9f2a8e923965b23c11066a2ead79658208f1fae1", "93499a7c7f699b6630a86fad964536f9423bb6d0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "ada937c9f51316c6ac87f9d1d4509383d23e0c21", "22358c1e6f371db45a0d237baff6052e0a50e498"]},{"id": "0811597b0851b7ebe21aadce7cb4daac4664b44f", "title": "One-Shot Generalization in Deep Generative Models", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "date": "ICML", "abstract": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the… ", "references": ["8948bea1e2436e51316f131170923cb5b7d870db", "7fc190b8f610e0168ab648c5544d3154b270d58c", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "8948bea1e2436e51316f131170923cb5b7d870db", "3e47c4c2dd98c49b7771c7228812d5fd9eee56a3", "484ad17c926292fbe0d5211540832a8c8a8e958b", "8948bea1e2436e51316f131170923cb5b7d870db", "2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1", "815c84ab906e43f3e6322f2ca3fd5e1360c64285", "7fc190b8f610e0168ab648c5544d3154b270d58c"]},{"id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "title": "Long Short-Term Memory", "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber"], "date": "1997", "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow.", "references": ["32e97eef94beacace020e79322cef0e1e5a76ee0", "6d72a0e83e772468c6084ae7c79e43a4f5989feb", "d0be39ee052d246ae99c082a565aba25b811be2d", "50c770b425a5bb25c77387f687a9910a9d130722", "e141d68065ce638f9fc4f006eab2f66711e89768", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "2f7c4048a03281e976f28d35c2f9fef3a58346e6", "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4", "f6e91c9e7e8f8a577a98ecfcfa998212a683195a", "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4"]},{"id": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Yoshua Bengio"], "date": "2014", "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks.", "references": ["89b1f4740ae37fd04f6ac007577bdd34621f0861", "e27d81521dc4e8b6ea93947c05ffccf06784f569", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "0b544dfe355a5070b60986319a3f51fb45d1348e", "396aabd694da04cdb846cb724ca9f866f345cbd5", "e27d81521dc4e8b6ea93947c05ffccf06784f569", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "89eca547b1a2f6208ae529d45a65a51cf49adbff", "396aabd694da04cdb846cb724ca9f866f345cbd5", "396aabd694da04cdb846cb724ca9f866f345cbd5"]},{"id": "a6373454105df0c5511ca5f6cae4d20c48214272", "title": "Fixed point optimization of deep convolutional neural networks for object recognition", "authors": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "date": "2015", "abstract": "Deep convolutional neural networks have shown promising results in image and speech recognition applications.", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "5562a56da3a96dae82add7de705e2bd841eb00fc", "c3c82b476162d2d006e02180530875a64af18154", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "162d958ff885f1462aeda91cd72582323fd6a1f4", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33"]},{"id": "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "title": "Encoding Source Language with Convolutional Neural Network for Machine Translation", "authors": ["Fandong Meng", "Zhengdong Lu", "Qun Liu"], "date": "ACL", "abstract": "The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can… ", "references": ["ebbd9e5fbc9c663d9dd60a08e1c3a09b15e65278", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "ebbd9e5fbc9c663d9dd60a08e1c3a09b15e65278", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0894b06cff1cd0903574acaa7fcf071b144ae775", "ebbd9e5fbc9c663d9dd60a08e1c3a09b15e65278", "0894b06cff1cd0903574acaa7fcf071b144ae775", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "fb4e8945040ff372be0bff598b1b8a9676b8e9f6"]},{"id": "a17745f1d7045636577bcd5d513620df5860e9e5", "title": "Deep Neural Network Language Models", "authors": ["Ebru Arisoy", "Tara N. Sainath", "Bhuvana Ramabhadran"], "date": "WLM@NAACL-HLT", "abstract": "In recent years, neural network language models (NNLMs) have shown success in both peplexity and word error rate (WER) compared to conventional n-gram language models.", "references": ["0fcc184b3b90405ec3ceafd6a4007c749df7c363", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "8b395470a57c48d174c4216ea21a7a58bc046917", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "cb45e9217fe323fbc199d820e7735488fca2a9b3", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "cb45e9217fe323fbc199d820e7735488fca2a9b3", "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb"]},{"id": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation", "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning"], "date": "EMNLP", "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.", "references": ["944a1cfd79dbfb6fef460360a0765ba790f4027a", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "52805ca2a7f5f6e73dc90ff20f1ca2f198dd031b", "0b544dfe355a5070b60986319a3f51fb45d1348e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "52805ca2a7f5f6e73dc90ff20f1ca2f198dd031b", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b"]},{"id": "5f08df805f14baa826dbddcb002277b15d3f1556", "title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation", "authors": ["Holger Schwenk"], "date": "COLING", "abstract": "This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system.", "references": ["e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "1f12451245667a85d0ee225a80880fc93c71cc8b", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "1f12451245667a85d0ee225a80880fc93c71cc8b", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97"]},{"id": "396aabd694da04cdb846cb724ca9f866f345cbd5", "title": "Domain Adaptation via Pseudo In-Domain Data Selection", "authors": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao"], "date": "EMNLP", "abstract": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora -- 1% the size of the original -- can then used to train small… ", "references": ["8a601640a709eb96e65d130ef8aac69e9ea3602d", "8a601640a709eb96e65d130ef8aac69e9ea3602d", "e923e85b2d491c37bac08258deff0af485ab71b9", "8a601640a709eb96e65d130ef8aac69e9ea3602d", "e923e85b2d491c37bac08258deff0af485ab71b9", "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "e923e85b2d491c37bac08258deff0af485ab71b9", "9b53e2a30f070a3bef3ca17a1872a70acfe478f9", "d7b2656177ed5a35ee1d1dc7f1fa57da54ed14d6", "d7b2656177ed5a35ee1d1dc7f1fa57da54ed14d6"]},{"id": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Yoshua Bengio"], "date": "EMNLP", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN.", "references": ["167ad306d84cca2455bc50eb833454de9f2dcd02", "70600593f870f460624c56c2a57b9a03b94f94a5", "0894b06cff1cd0903574acaa7fcf071b144ae775", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "0894b06cff1cd0903574acaa7fcf071b144ae775", "167ad306d84cca2455bc50eb833454de9f2dcd02", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "167ad306d84cca2455bc50eb833454de9f2dcd02", "d49fc0b584012532e4fd7725149a29e25ac835bc", "a4b828609b60b06e61bea7a4029cc9e1cad5df87"]},{"id": "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "title": "Statistical Phrase-Based Translation", "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "date": "HLT-NAACL", "abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic… ", "references": ["8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "3a7a536a17bd19587c05f3f8d1f4571c13388e9c", "d7da009f457917aa381619facfa5ffae9329a6e9", "c9214ebe91454e6369720136ab7dd990d52a07d4", "0ffa423a5283396c88ff3d4033d541796bd039cc", "3a7a536a17bd19587c05f3f8d1f4571c13388e9c", "3a7a536a17bd19587c05f3f8d1f4571c13388e9c", "62864f78fa4cb5f1ab45ebbe5a420b546b62d7a6", "dd5514876b7e1c09b6d2f931d90bb34aa3501441", "dd5514876b7e1c09b6d2f931d90bb34aa3501441"]},{"id": "b60abe57bc195616063be10638c6437358c81d1e", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "authors": ["Ying Cao", "Wei Xu"], "date": "2016", "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional… ", "references": ["a739ae988ba0e3ff232f4507627dfc282ba7b3f4", "c34e41312b47f60986458759d5cc546c2b53f748", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "c34e41312b47f60986458759d5cc546c2b53f748", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "a739ae988ba0e3ff232f4507627dfc282ba7b3f4", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},{"id": "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering", "authors": ["Wenhui Wang", "Nan Yang", "Ming Zhou"], "date": "ACL", "abstract": "In this paper, we present the gated selfmatching networks for reading comprehension style question answering, which aims to answer questions from a given passage.", "references": ["0f2ea810c16275dc74e880296e20dbd83b1bae1c", "62f88e8fc3b44c5627f2b4721b08498d78103893", "e94697b98b707f557436e025bdc8498fa261d3bc", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "62f88e8fc3b44c5627f2b4721b08498d78103893", "e94697b98b707f557436e025bdc8498fa261d3bc", "c6e5df6322659276da6133f9b734a389d7a255e8", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "f2e50e2ee4021f199877c8920f1f984481c723aa", "13fe71da009484f240c46f14d9330e932f8de210"]},{"id": "12e20e4ea572dbe476fd894c5c9a9930cf250dd2", "title": "MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension", "authors": ["Boyuan Pan", "Hao Li", "Xiaofei He"], "date": "2017", "abstract": "Machine comprehension(MC) style question answering is a representative problem in natural language processing.", "references": ["0f2ea810c16275dc74e880296e20dbd83b1bae1c", "525f65936c331b0b766c7aea0eae64c595704c50", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "b1e20420982a4f923c08652941666b189b11b7fe", "b1e20420982a4f923c08652941666b189b11b7fe", "e94697b98b707f557436e025bdc8498fa261d3bc", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4"]},{"id": "944a1cfd79dbfb6fef460360a0765ba790f4027a", "title": "Recurrent Continuous Translation Models", "authors": ["Nal Kalchbrenner", "Phil Blunsom"], "date": "EMNLP", "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units.", "references": ["27e38351e48fe4b7da2775bf94341738bc4da07e", "d1275b2a2ab53013310e759e5c6878b96df643d4", "cd96a6e0b6bb099c515be8770764d2fd18e7b878", "27e38351e48fe4b7da2775bf94341738bc4da07e", "27e38351e48fe4b7da2775bf94341738bc4da07e", "27e38351e48fe4b7da2775bf94341738bc4da07e", "5f08df805f14baa826dbddcb002277b15d3f1556", "ab7b5917515c460b90451e67852171a531671ab8", "cd96a6e0b6bb099c515be8770764d2fd18e7b878", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8"]},{"id": "c6e5df6322659276da6133f9b734a389d7a255e8", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "authors": ["Yiming Cui", "Zhipeng Chen", "Guoping Hu"], "date": "ACL", "abstract": "Cloze-style queries are representative problems in reading comprehension.", "references": ["0f2ea810c16275dc74e880296e20dbd83b1bae1c", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "e978d832a4d86571e1b52aa1685dc32ccb250f50", "f2e50e2ee4021f199877c8920f1f984481c723aa", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "d1505c6123c102e53eb19dff312cb25cea840b72", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5"]},{"id": "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "authors": ["Shuohang Wang", "Jing Jiang"], "date": "2017", "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths… ", "references": ["596c882de006e4bb4a93f1fa08a5dd467bee060a", "452059171226626718eb677358836328f884298e", "0680f04750b1e257ffdd161e85382031dc73ea7f", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "564257469fa44cdb57e4272f85253efb9acfd69d", "0680f04750b1e257ffdd161e85382031dc73ea7f", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "05dd7254b632376973f3a1b4d39485da17814df5", "6d8994b90b9cc1df522bd56cceeaba4a4a6e9e1e", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5"]},{"id": "de0c30321b22c56d637e7c29cb59180f157272a8", "title": "Globally Normalized Reader", "authors": ["Jonathan Raiman", "John L. Miller"], "date": "EMNLP", "abstract": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer's sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to… ", "references": ["46147f08468e873ff90d1d51e65493f262c7bb57", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "e978d832a4d86571e1b52aa1685dc32ccb250f50", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "e4600ece1f09236d082eca4537ee9c1efe687f6c", "05dd7254b632376973f3a1b4d39485da17814df5", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "3eda43078ae1f4741f09be08c4ecab6229046a5c"]},{"id": "adc276e6eae7051a027a4c269fb21dae43cadfed", "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding", "authors": ["Tao Shen", "Tianyi Zhou", "Chengqi Zhang"], "date": "AAAI", "abstract": "Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely used on NLP tasks to capture the long-term and local dependencies, respectively.", "references": ["a79ac27b270772c79b80d2235ca5ff2df2d2d370", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "93499a7c7f699b6630a86fad964536f9423bb6d0", "cff79255a94b9b05a4ce893eb403a522e0923f04", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "93499a7c7f699b6630a86fad964536f9423bb6d0"]},{"id": "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "title": "Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning", "authors": ["Kyunghyun Cho", "Yoshua Bengio"], "date": "2014", "abstract": "Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural… ", "references": ["72e93aa6767ee683de7f001fa72f1314e40a8f35", "3127190433230b3dc1abd0680bb58dced4bcd90e", "c965bac486a714d47a6362248f0a959c77622738", "3127190433230b3dc1abd0680bb58dced4bcd90e", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "be9a17321537d9289875fe475b71f4821457b435", "c965bac486a714d47a6362248f0a959c77622738", "be9a17321537d9289875fe475b71f4821457b435", "e60ff004dde5c13ec53087872cfcdd12e85beb57"]},{"id": "cf3229e74f912ef365d67d1954441b32ce2573ee", "title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "authors": ["Andrew S. Davis", "Itamar Arel"], "date": "2014", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be… ", "references": ["e8650503ab80ad7299f0845b1843abf3a97f313a", "03f34688ef4ee4239464633784235387e9bff4bb", "f9f19bee621faf46f90b023f8de8248b57becbc4", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "e8650503ab80ad7299f0845b1843abf3a97f313a", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "e64a9960734215e2b1866ea3cb723ffa5585ac14"]},{"id": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "title": "Exploring the Limits of Language Modeling", "authors": ["Rafal Józefowicz", "Oriol Vinyals", "Yonghui Wu"], "date": "2016", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly… ", "references": ["5d833331b0e22ff359db05c62a8bca18c4f04b68", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b", "a17745f1d7045636577bcd5d513620df5860e9e5", "ac973bbfd62a902d073a85ca621fd297e8660a82", "71480da09af638260801af1db8eff6acb4e1122f", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "ac973bbfd62a902d073a85ca621fd297e8660a82"]},{"id": "66b8d34477cf1736f91fd22b27e37ce0b703c86e", "title": "Expert Gate: Lifelong Learning with a Network of Experts", "authors": ["Rahaf Aljundi", "Punarjay Chakravarty", "Tinne Tuytelaars"], "date": "2017", "abstract": "In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating… ", "references": ["53c9443e4e667170acc60ca1b31a0ec7151fe753", "53c9443e4e667170acc60ca1b31a0ec7151fe753", "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a", "0c908739fbff75f03469d13d4a1a07de3414ee19", "5776d0fea69d826519ee3649f620e8755a490efe", "0c908739fbff75f03469d13d4a1a07de3414ee19", "5776d0fea69d826519ee3649f620e8755a490efe", "aba48504f4f9563eafa44e0cfb22e1345d767c80", "aba48504f4f9563eafa44e0cfb22e1345d767c80", "aba48504f4f9563eafa44e0cfb22e1345d767c80"]},{"id": "424aef7340ee618132cc3314669400e23ad910ba", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling", "authors": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher"], "date": "2017", "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel… ", "references": ["49899bd9e5a7f59aa14e6d21ed501e3c3acd5852", "687bac2d3320083eb4530bf18bb8f8f721477600", "9819b600a828a57e1cde047bbe710d3446b30da5", "687bac2d3320083eb4530bf18bb8f8f721477600", "d1275b2a2ab53013310e759e5c6878b96df643d4", "9819b600a828a57e1cde047bbe710d3446b30da5", "efbd381493bb9636f489b965a2034d529cd56bcd", "49899bd9e5a7f59aa14e6d21ed501e3c3acd5852", "5762b7deff7e95febe193196d548379ff34b34f1", "687bac2d3320083eb4530bf18bb8f8f721477600"]},{"id": "44ddac48353ead135eef4096859956eaa31be2a5", "title": "Learning Factored Representations in a Deep Mixture of Experts", "authors": ["David Eigen", "Marc'Aurelio Ranzato", "Ilya Sutskever"], "date": "2014", "abstract": "Mixtures of Experts combine the outputs of several \"expert\" networks, each of which specializes in a different part of the input space. This is achieved by training a \"gating\" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple… ", "references": ["a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "f6d8a7fc2e2d53923832f9404376512068ca2a57", "5bf65452ae566a052b00d919404f462470869600", "f6d8a7fc2e2d53923832f9404376512068ca2a57", "f6d8a7fc2e2d53923832f9404376512068ca2a57", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "c8d90974c3f3b40fa05e322df2905fc16204aa56"]},{"id": "a8176a160777bfe82b1c67506835c60073e6fbe8", "title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "authors": ["Ofir Press", "Amir Bar", "Lior Wolf"], "date": "2017", "abstract": "Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch… ", "references": ["428818a9edfb547431be6d7ec165c6af576c83d5", "428818a9edfb547431be6d7ec165c6af576c83d5", "042116e805aa3b5171efaf0c822dc142310ceefe", "042116e805aa3b5171efaf0c822dc142310ceefe", "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c", "4fc0ea6db600850908264652e1a5d7904f66ca58", "042116e805aa3b5171efaf0c822dc142310ceefe", "4fc0ea6db600850908264652e1a5d7904f66ca58", "4fc0ea6db600850908264652e1a5d7904f66ca58", "042116e805aa3b5171efaf0c822dc142310ceefe"]},{"id": "db38edba294b7d2fd8ca3aad65721bd9dce32619", "title": "Professor Forcing: A New Algorithm for Training Recurrent Networks", "authors": ["Anirudh Goyal", "Alex Lamb", "Yoshua Bengio"], "date": "NIPS", "abstract": "The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network’s own one-step-ahead predictions to do multi-step sampling.", "references": ["0d24a0695c9fc669e643bad51d4e14f056329dec", "0d24a0695c9fc669e643bad51d4e14f056329dec", "d1275b2a2ab53013310e759e5c6878b96df643d4", "d1275b2a2ab53013310e759e5c6878b96df643d4", "cea967b59209c6be22829699f05b8b1ac4dc092d", "d1275b2a2ab53013310e759e5c6878b96df643d4", "cea967b59209c6be22829699f05b8b1ac4dc092d", "b624504240fa52ab76167acfe3156150ca01cf3b", "c3b38c2fd30adb316d0bdb32e983804be5595c30", "c3b38c2fd30adb316d0bdb32e983804be5595c30"]},{"id": "176f1d608b918eec8dc4b75e7b6e0acaba84a447", "title": "Adversarial Learning for Neural Dialogue Generation", "authors": ["Jiwei Li", "Will Monroe", "Dan Jurafsky"], "date": "2017", "abstract": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances.", "references": ["339c6e6d46836c173fb6a23b493c724896d4cc70", "609e0f0e60ddfe83fdc71bf5397205323888289d", "7f9135f3584e4e1715b2990a4f389c94af0313a5", "609e0f0e60ddfe83fdc71bf5397205323888289d", "2966ecd82505ecd55ead0e6a327a304c8f9868e3", "339c6e6d46836c173fb6a23b493c724896d4cc70", "339c6e6d46836c173fb6a23b493c724896d4cc70", "17f5c7411eeeeedf25b0db99a9130aa353aee4ba", "339c6e6d46836c173fb6a23b493c724896d4cc70", "1298dae5751fb06184f6b067d1503bde8037bdb7"]},{"id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Illia Polosukhin"], "date": "2017", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration.", "references": ["032274e57f7d8b456bd255fe76b909b2c1d7458e", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "43428880d75b3a14257c3ee9bda054e61eb869c0", "98445f4172659ec5e891e031d8202c102135c644", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "98445f4172659ec5e891e031d8202c102135c644", "13d9323a8716131911bfda048a40e2cde1a76a46", "13d9323a8716131911bfda048a40e2cde1a76a46", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd"]},{"id": "58c6f890a1ae372958b7decf56132fe258152722", "title": "Regularizing and Optimizing LSTM Language Models", "authors": ["Stephen Merity", "Nitish Shirish Keskar", "Richard Socher"], "date": "2018", "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering.", "references": ["63e39cdf1ad884da6bc69096bb3413b5b1100559", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "fc18e99f918d8906ec44be3f7d90d8f9ebabae96", "9819b600a828a57e1cde047bbe710d3446b30da5", "d1275b2a2ab53013310e759e5c6878b96df643d4", "d1275b2a2ab53013310e759e5c6878b96df643d4", "67d968c7450878190e45ac7886746de867bf673d"]},{"id": "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "title": "Sequence Level Training with Recurrent Neural Networks", "authors": ["Marc'Aurelio Ranzato", "Sumit Chopra", "Wojciech Zaremba"], "date": "2016", "abstract": "Many natural language processing applications use language models to generate text.", "references": ["df137487e20ba7c6e1e2b9a1e749f2a578b5ad99", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "25a4fb7025453ce73feef36eeaa45dbd0eb215e5", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "5082a1a13daea5c7026706738f8528391a1e6d59", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "25a4fb7025453ce73feef36eeaa45dbd0eb215e5"]},{"id": "4db8cd9117254d21c9c828b8ba2aea58e57ee2c4", "title": "MuFuRU: The Multi-Function Recurrent Unit", "authors": ["Dirk Weissenborn", "Tim Rocktäschel"], "date": "2016", "abstract": "Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable… ", "references": ["04cca8e341a5da42b29b0bc831cb25a0f784fa01", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "9819b600a828a57e1cde047bbe710d3446b30da5", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "9819b600a828a57e1cde047bbe710d3446b30da5", "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "adfcf065e15fd3bc9badf6145034c84dfb08f204"]},{"id": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "title": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS", "authors": ["Ilya Sutskever", "Jan Černocký"], "date": "2011", "abstract": "We explore the performance of several types of language mode ls on the word-level and the character-level language modelin g tasks. This includes two recently proposed recurrent neural netwo rk architectures, a feedforward neural network model, a maximum ent ropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from th e data, and show that it combines advantages of both character and wo rdlevel models. Finally, we show that neural… ", "references": ["472bd5b90c289b715340708536ade437d20b237e", "1596e3cb20ba3b9aefe440e30660c2a9f035f683", "cb45e9217fe323fbc199d820e7735488fca2a9b3", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "cb45e9217fe323fbc199d820e7735488fca2a9b3", "1596e3cb20ba3b9aefe440e30660c2a9f035f683", "cb45e9217fe323fbc199d820e7735488fca2a9b3", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "472bd5b90c289b715340708536ade437d20b237e", "5999fd9b9712fee3184989d043bff899935b4208"]},{"id": "84ca430856a92000e90cd728445ca2241c10ddc3", "title": "Very Deep Convolutional Networks for Natural Language Processing", "authors": ["Alexis Conneau", "Holger Schwenk", "Yann LeCun"], "date": "2016", "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks.", "references": ["1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "f9a1b3850dfd837793743565a8af95973d395a4e", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "f9a1b3850dfd837793743565a8af95973d395a4e", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "bc1022b031dc6c7019696492e8116598097a8c12", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba"]},{"id": "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "title": "Multiplicative LSTM for sequence modelling", "authors": ["Ben Krause", "Liang Lu", "Steve Renals"], "date": "2017", "abstract": "We introduce multiplicative LSTM (mLSTM), a recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures.", "references": ["952454718139dba3aafc6b3b67c4f514ac3964af", "4ef03716945bd3907458efbe1bbf8928dafc1efc", "98445f4172659ec5e891e031d8202c102135c644", "4ef03716945bd3907458efbe1bbf8928dafc1efc", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "7dded890956b37df5ac4c42b8ffbc142725f2801", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "952454718139dba3aafc6b3b67c4f514ac3964af"]},{"id": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks", "authors": ["Yann Dauphin", "Angela Fan", "David Grangier"], "date": "ICML", "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks.", "references": ["759956bb98689dbcc891528636d8994e54318f85", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "b7cfccf123f86785476a06c8039889a2eb1e2d73", "12a5b7190b981bf478b4c9c04d3c0d41f13b9023", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "0dc9eb7d17f2def56ad930945f2521653f04c3fa", "0dc9eb7d17f2def56ad930945f2521653f04c3fa", "759956bb98689dbcc891528636d8994e54318f85", "efbd381493bb9636f489b965a2034d529cd56bcd"]},{"id": "c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6", "title": "Multilingual Distributed Representations without Word Alignment", "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "date": "2013", "abstract": "Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment… ", "references": ["3c5126da7ce388c64b796c80d15a3c3629d6ad58", "6a4007e60346e4501acc936b49b7a476e73afa1e", "27e38351e48fe4b7da2775bf94341738bc4da07e", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "d1f37d9cab68eb8cda669cc949394732f33264b4", "3b6b08fc7709016a6444389f129323fa8fce66b1", "27e38351e48fe4b7da2775bf94341738bc4da07e", "3b6b08fc7709016a6444389f129323fa8fce66b1", "3b6b08fc7709016a6444389f129323fa8fce66b1"]},{"id": "f9a1b3850dfd837793743565a8af95973d395a4e", "title": "LSTM Neural Networks for Language Modeling", "authors": ["Martin Sundermeyer", "Ralf Schlüter", "Hermann Ney"], "date": "INTERSPEECH", "abstract": "Neural networks have become increasingly popular for the task of language modeling.", "references": ["0d6203718c15f137fda2f295c96269bc2b254644", "e4a94d6eef25cdebdde2c91fb3c45a737d5e3141", "047655e733a9eed9a500afd916efa566915b9110", "d0be39ee052d246ae99c082a565aba25b811be2d", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "d0be39ee052d246ae99c082a565aba25b811be2d", "e4a94d6eef25cdebdde2c91fb3c45a737d5e3141", "2f83f6e1afadf0963153974968af6b8342775d82", "0d6203718c15f137fda2f295c96269bc2b254644", "0fcc184b3b90405ec3ceafd6a4007c749df7c363"]},{"id": "167ad306d84cca2455bc50eb833454de9f2dcd02", "title": "Joint Language and Translation Modeling with Recurrent Neural Networks", "authors": ["Michael Auli", "Michel Galley", "Geoffrey Zweig"], "date": "EMNLP", "abstract": "We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural… ", "references": ["1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "d1275b2a2ab53013310e759e5c6878b96df643d4", "a17745f1d7045636577bcd5d513620df5860e9e5", "a17745f1d7045636577bcd5d513620df5860e9e5", "d49fc0b584012532e4fd7725149a29e25ac835bc", "d49fc0b584012532e4fd7725149a29e25ac835bc", "fcfb39e64678fe9cb681f11b9a3314becec82bb2", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "c36d355e01e1ed90e57bffbbfc274d4d98952b96"]},{"id": "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "title": "Statistical Language Models Based on Neural Networks", "authors": ["Vysoké Učení", "Technické V Brně", "Disertační Práce"], "date": "2012", "abstract": "Statistical language models are crucial part of many successful applications, such as automatic speech recognition and statistical machine translation (for example well-known Google Translate). Traditional techniques for estimating these models are based on N gram counts. Despite known weaknesses of N -grams and huge efforts of research communities across many fields (speech recognition, machine translation, neuroscience, artificial intelligence, natural language processing, data compression… ", "references": ["b4fc91e543ec868658cde6170f1e59c33292e595", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "ff80a400198f0ce26887672407d8872825e663bf", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "0687165a9f0360bde0469fd401d966540e0897c3", "0687165a9f0360bde0469fd401d966540e0897c3", "ff80a400198f0ce26887672407d8872825e663bf", "0687165a9f0360bde0469fd401d966540e0897c3"]},{"id": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "authors": ["Alexis Conneau", "Douwe Kiela", "Antoine Bordes"], "date": "2017", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford… ", "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "f93a0a3e8a3e6001b4482430254595cf737697fa", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "57458bc1cffe5caa45a885af986d70f723f406b4", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "395044a2e3f5624b2471fb28826e7dbb1009356e", "bc1022b031dc6c7019696492e8116598097a8c12"]},{"id": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model", "authors": ["Martin Karafiát", "Sanjeev Khudanpur"], "date": "INTERSPEECH", "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05… ", "references": ["fc999072ce188ee1d57b6bb744cb276b09a491bb", "119b5e8927f98e3fea76cbb57d7e053c24ac5c18", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "43e5b43965aa4099e75110e4fd8e5458efb82fd8", "8a0e2e4dd10e1be7fd28474504cbdaa1237e4926", "fc999072ce188ee1d57b6bb744cb276b09a491bb", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "d0be39ee052d246ae99c082a565aba25b811be2d", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "c19fbefdeead6a4154a22a9c8551a18b1530033a"]},{"id": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning", "authors": ["Andrew M. Dai", "Quoc V. Le"], "date": "2015", "abstract": "We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "944e1a7b2c5c62e952418d7684e3cade89c76f87"]},{"id": "ac17cfa150d802750b46220084d850cfdb64d1c1", "title": "Semi-supervised Multitask Learning for Sequence Labeling", "authors": ["Marek Rei"], "date": "ACL", "abstract": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition… ", "references": ["f8f92f791d67883f77525c5aecdb7b10b3488b29", "bc1022b031dc6c7019696492e8116598097a8c12", "f6223a7549a6f35de2e9dfa5a27a9f93de304f47", "acd87e4f672f0b92ea4164414c213560c23bee52", "acd87e4f672f0b92ea4164414c213560c23bee52", "dcab6c0b486633f2f7722cde18371a18ca9da3bd", "f6223a7549a6f35de2e9dfa5a27a9f93de304f47", "acd87e4f672f0b92ea4164414c213560c23bee52", "bc1022b031dc6c7019696492e8116598097a8c12", "57458bc1cffe5caa45a885af986d70f723f406b4"]},{"id": "85f94d8098322f8130512b4c6c4627548ce4a6cc", "title": "Unsupervised Pretraining for Sequence to Sequence Learning", "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "date": "2017", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models. Our main result is… ", "references": ["1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6"]},{"id": "6658bbf68995731b2083195054ff45b4eca38b3a", "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition", "authors": ["George E. Dahl", "Dong Yu", "Alex Acero"], "date": "2012", "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks… ", "references": ["622a40854f79fb385b61f1a3de1cdce4999e9f4b", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "c1116b32168ca91607d81e8aa6be64ee7b539449", "90b63e917d5737b06357d50aa729619e933d9614", "90b63e917d5737b06357d50aa729619e933d9614", "df5b82595a29724467a98eed4d7e2a45e804579e", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "c1116b32168ca91607d81e8aa6be64ee7b539449"]},{"id": "c35c2ce83221f909f3ee97bce3d49fae82b795c0", "title": "Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions", "authors": ["Emma Strubell", "Pat Verga", "Andrew McCallum"], "date": "2017", "abstract": "Bi-directional LSTMs have emerged as a standard method for obtaining per-token vector representations serving as input to various token labeling tasks (whether followed by Viterbi prediction or independent classification). This paper proposes an alternative to Bi-LSTMs for this purpose: iterated dilated convolutional neural networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. We describe a distinct combination of network structure… ", "references": ["4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "7f5fc84819c0cf94b771fe15141f65b123f7b8ec", "39ad6c911f3351a3b390130a6e4265355b4d593b", "7f5fc84819c0cf94b771fe15141f65b123f7b8ec", "39ad6c911f3351a3b390130a6e4265355b4d593b", "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "39ad6c911f3351a3b390130a6e4265355b4d593b", "10a4db59e81d26b2e0e896d3186ef81b4458b93f", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292"]},{"id": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "title": "Skip-Thought Vectors", "authors": ["Ryan Kiros", "Yukun Zhu", "Sanja Fidler"], "date": "2015", "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder.", "references": ["d41cfe9b2ada4e09d53262bc75c473d8043936fc", "cea967b59209c6be22829699f05b8b1ac4dc092d", "d41cfe9b2ada4e09d53262bc75c473d8043936fc", "0b544dfe355a5070b60986319a3f51fb45d1348e", "d41cfe9b2ada4e09d53262bc75c473d8043936fc", "51239b320c73f3f2219286bf62f24d6763379328", "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "d41cfe9b2ada4e09d53262bc75c473d8043936fc"]},{"id": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification", "authors": ["Jeremy Howard", "Sebastian Ruder"], "date": "ACL", "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on… ", "references": ["7647a06965d868a4f6451bef0818994100a142e8", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "d7db74be6cda0ec2bd28ec187563def85ccef78f", "d7db74be6cda0ec2bd28ec187563def85ccef78f", "7647a06965d868a4f6451bef0818994100a142e8", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "7647a06965d868a4f6451bef0818994100a142e8", "ac17cfa150d802750b46220084d850cfdb64d1c1", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292"]},{"id": "2c821e2ec8ef976d3abb36fb0dc1946f04208512", "title": "A Bidirectional Recurrent Neural Language Model for Machine Translation", "authors": ["Álvaro Peris", "Francisco Casacuberta"], "date": "2015", "abstract": "A language model based in continuous representations of words is pre- sented, which has been applied to a statistical machine translation task.", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "f83d712e1afb83fd45628261ede23fb112ec1666", "0894b06cff1cd0903574acaa7fcf071b144ae775", "ade34bf617f7733bfa0676f2bd57fa5658d4e54c", "3090262c765a95fe9cd975fa12b4ec15391ece6d", "d36b19b4c5977dd2a2796a5ad3508a3d8a087809", "f83d712e1afb83fd45628261ede23fb112ec1666", "1956c239b3552e030db1b78951f64781101125ed", "f9a1b3850dfd837793743565a8af95973d395a4e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},{"id": "7ece4e8d31f872d928369ac2cf58a616a7182112", "title": "Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data", "authors": ["Jun Suzuki", "Hideki Isozaki"], "date": "ACL", "abstract": "This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task… ", "references": ["897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "03a460c8ca331d36e8f2e11edd49e7dbc35c7e43", "e2de29049d62de925cf709024b92774cd82b0a5a", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "0d1e90fa2457d34e6efead247a1a75ef76a48160", "0d1e90fa2457d34e6efead247a1a75ef76a48160", "e2de29049d62de925cf709024b92774cd82b0a5a", "0d1e90fa2457d34e6efead247a1a75ef76a48160"]},{"id": "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks", "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen"], "date": "2017", "abstract": "Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks.", "references": ["bc1022b031dc6c7019696492e8116598097a8c12", "e2d4df61a787210c67041929f6a43203bee99edf", "e2d4df61a787210c67041929f6a43203bee99edf", "e2d4df61a787210c67041929f6a43203bee99edf", "4dabd6182ce2681c758f654561d351739e8df7bf", "bc1022b031dc6c7019696492e8116598097a8c12", "4dabd6182ce2681c758f654561d351739e8df7bf", "10a4db59e81d26b2e0e896d3186ef81b4458b93f", "e2d4df61a787210c67041929f6a43203bee99edf", "ad945987071d3c5b4b915b85e09ae3488b2212c0"]},{"id": "26e743d5bd465f49b9538deaf116c15e61b7951f", "title": "Learning Distributed Representations of Sentences from Unlabelled Data", "authors": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen"], "date": "HLT-NAACL", "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but… ", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2", "c1088267b10e19d565865c2dcf0bc0f94696bf2e", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "c1088267b10e19d565865c2dcf0bc0f94696bf2e", "bc1022b031dc6c7019696492e8116598097a8c12", "d86227948b6000e5d7ed63cf2054ad600b7994a0", "d86227948b6000e5d7ed63cf2054ad600b7994a0", "330da625c15427c6e42ccfa3b747fb29e5835bf0"]},{"id": "ade0c116120b54b57a91da51235108b75c28375a", "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Richard Socher"], "date": "2017", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks.", "references": ["8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "03ad06583c9721855ccd82c3d969a01360218d86", "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2", "bc1022b031dc6c7019696492e8116598097a8c12", "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "d76c07211479e233f7c6a6f32d5346c983c5598f", "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2", "03ad06583c9721855ccd82c3d969a01360218d86"]},{"id": "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "title": "Semi-Supervised Sequence Modeling with Syntactic Topic Models", "authors": ["Wei Li", "Andrew McCallum"], "date": "AAAI", "abstract": "Although there has been significant previous work on semi-supervised learning for classification, there has been relatively little in sequence modeling. This paper presents an approach that leverages recent work in manifold-learning on sequences to discover word clusters from language data, including both syntactic classes and semantic topics. From unlabeled data we form a smooth. low-dimensional feature space, where each word token is projected based on its underlying role as a function or… ", "references": ["602af317fe1ff52f254a824df5880505e086c76d", "125842668eab7decac136db8a59d392dc5e4e395", "e2de29049d62de925cf709024b92774cd82b0a5a", "b40719006ef3d7d90a9b826341bc463a8c9c1fd8", "0ecc5ffeae38689dd2fe6ed4c32a6745744d7641", "602af317fe1ff52f254a824df5880505e086c76d", "3de5d40b60742e3dfa86b19e7f660962298492af", "ad269ba941949a1d66b6649a71d752784c576dc3", "602af317fe1ff52f254a824df5880505e086c76d", "c76c62c5ab6c076a80f925d277ef04dd36f6bf9c"]},{"id": "553fb89d5858826c02f26e94262e8958debc777e", "title": "Distributional Memory: A General Framework for Corpus-Based Semantics", "authors": ["Marco Baroni", "Alessandro Lenci"], "date": "2010", "abstract": "Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third… ", "references": ["83d9fec6bee4cf6e01445d6ff51aae58b6d96d5a", "4b88329392a75287942d85f42012f47f356a2714", "509a2ca90a85c62d66a16b37e0de28715dd4e89f", "e02837ac075543fe1b04f3003133a6015564d443", "afd5da111e80fd0ee783b1ecf1bcb1be8cbff8f4", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "4b88329392a75287942d85f42012f47f356a2714", "2cafda848586a9a4061a11227adbaf70720644c8", "2cda689911f9dcf7e1555e5cb0a25ae93b95ad0b"]},{"id": "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "title": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks", "authors": ["Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "date": "2010", "abstract": "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations… ", "references": ["605d738a39df3c5e596613ab0ca6925f0eecdf35", "605d738a39df3c5e596613ab0ca6925f0eecdf35", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "adcf1552e759f9cade8ef9e59ecf6159e25a055e", "c58dd287a476b4722c5b6b1316629e2874682219", "723e55901724533dc79c2255ce044fef858db9ae", "605d738a39df3c5e596613ab0ca6925f0eecdf35", "f52de7242e574b70410ca6fb70b79c811919fc00", "f52de7242e574b70410ca6fb70b79c811919fc00", "f52de7242e574b70410ca6fb70b79c811919fc00"]},{"id": "2063745d08868c928455f422202b72146a1960fb", "title": "Compositional Matrix-Space Models for Sentiment Analysis", "authors": ["Ainur Yessenalina", "Claire Cardie"], "date": "EMNLP", "abstract": "We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature.", "references": ["104fa9c3294df3a6713d5dccea790b5682037c31", "5f4a34af0f5a8ddca0cfa356d7fa67c72f539e81", "6fec21a78eb9279c87cc89ef7efa0acf22ff4abd", "e14609a3a6c6f8ef3269d3e0728f88da57826698", "da5cd00115f7ec108de8eebf071c5f3f19807df4", "e14609a3a6c6f8ef3269d3e0728f88da57826698", "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "5f4a34af0f5a8ddca0cfa356d7fa67c72f539e81", "da5cd00115f7ec108de8eebf071c5f3f19807df4", "104fa9c3294df3a6713d5dccea790b5682037c31"]},{"id": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "authors": ["Xuezhe Ma", "Eduard H. Hovy"], "date": "2016", "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing.", "references": ["d64561879a2fbd3d39a5e876a667ffa4561eed80", "f40bcba9593fc266cdebf35a107c85c30983173f", "d64561879a2fbd3d39a5e876a667ffa4561eed80", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "48e8e8085907192d501eb2bcc582035e90431a2f", "91a93f751f912b2f96d6771018d8f06c41e11152", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "eb42a490cf4f186d3383c92963817d100afd81e2", "eb42a490cf4f186d3383c92963817d100afd81e2", "f40bcba9593fc266cdebf35a107c85c30983173f"]},{"id": "27e38351e48fe4b7da2775bf94341738bc4da07e", "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "authors": ["Richard Socher", "Brody Huval", "Andrew Y. Ng"], "date": "EMNLP-CoNLL", "abstract": "Single-word vector space models have been very successful at learning lexical information.", "references": ["cfa2646776405d50533055ceb1b7f050e9014dcb", "3c5126da7ce388c64b796c80d15a3c3629d6ad58", "745d86adca56ec50761591733e157f84cfb19671", "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "2063745d08868c928455f422202b72146a1960fb", "509a2ca90a85c62d66a16b37e0de28715dd4e89f", "745d86adca56ec50761591733e157f84cfb19671", "3c5126da7ce388c64b796c80d15a3c3629d6ad58", "3c5126da7ce388c64b796c80d15a3c3629d6ad58"]},{"id": "bb6daf3bd95668ea9775d7e06d8b3f6994306cb7", "title": "I Know There Is No Answer: Modeling Answer Validation for Machine Reading Comprehension", "authors": ["Chuanqi Tan", "Furu Wei", "Ming Zhou"], "date": "NLPCC", "abstract": "Existing works on machine reading comprehension mostly focus on extracting text spans from passages with the assumption that the passage must contain the answer to the question. This assumption usually cannot be satisfied in real-life applications. In this paper, we study the reading comprehension task in which whether the given passage contains the answer is not specified in advance. The system needs to correctly refuse to give an answer when a passage does not contain the answer. We develop… ", "references": []},{"id": "57458bc1cffe5caa45a885af986d70f723f406b4", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "authors": ["Ronan Collobert", "Jason Weston"], "date": "ICML '08", "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language… ", "references": ["1f462943c8d0af69c12a09058251848324135e5a", "a16e484824b2580e092c985aa659e8680aeda5ee", "a16e484824b2580e092c985aa659e8680aeda5ee"]},{"id": "cfa2646776405d50533055ceb1b7f050e9014dcb", "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "authors": ["Richard Socher", "Jeffrey Pennington", "Christopher D. Manning"], "date": "EMNLP", "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions.", "references": ["167e1359943b96b9e92ee73db1df69a1f65d731d", "80922663aaa7b09e86276ab97210ab2372d3f61a", "dac72f2c509aee67524d3321f77e97e8eff51de6", "dac72f2c509aee67524d3321f77e97e8eff51de6", "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "b35e340a4356eb8953b04c17375ffb631da68916", "a9c15a179f3e4ad60eee6fbd0b731cc6ac17c8ac", "b35e340a4356eb8953b04c17375ffb631da68916", "da5cd00115f7ec108de8eebf071c5f3f19807df4", "da5cd00115f7ec108de8eebf071c5f3f19807df4"]},{"id": "c636a2dd242908fe2e598a1077c0c57bfdea8633", "title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "authors": ["Yelong Shen", "Po-Sen Huang", "Weizhu Chen"], "date": "2017", "abstract": "Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a… ", "references": ["35b91b365ceb016fb3e022577cec96fb9b445dc5", "35b91b365ceb016fb3e022577cec96fb9b445dc5"]},{"id": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "authors": ["Eric H. Huang", "Richard Socher", "Andrew Y. Ng"], "date": "ACL", "abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.", "references": ["5974441a0bebfb45579491a9a28bca4fff6bc256", "5db592bef4b5ff231e1de92588907808f00bfbb4", "83d787a13aca79c833d11718da9ab6243117cf47", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "3d6036af971c1f11ab712cc41487376a94e63673", "57458bc1cffe5caa45a885af986d70f723f406b4", "c62450a13bb6692385490dd4b371de9857761374", "68c03788224000794d5491ab459be0b2a2c38677", "c62450a13bb6692385490dd4b371de9857761374"]},{"id": "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "title": "Learning Recurrent Span Representations for Extractive Question Answering", "authors": ["Kenton Lee", "Tom Kwiatkowski", "Dipanjan Das"], "date": "2016", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction… ", "references": ["f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "b1e20420982a4f923c08652941666b189b11b7fe", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "05dd7254b632376973f3a1b4d39485da17814df5", "d1505c6123c102e53eb19dff312cb25cea840b72", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "05dd7254b632376973f3a1b4d39485da17814df5", "564257469fa44cdb57e4272f85253efb9acfd69d", "05dd7254b632376973f3a1b4d39485da17814df5"]},{"id": "fa025e5d117929361bcf798437957762eb5bb6d4", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "authors": ["Omer Levy", "Minjoon Seo", "Luke Zettlemoyer"], "date": "2017", "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot.", "references": ["822f1ed9a76a57cc19d8fda7745365b97130b97a", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "e94697b98b707f557436e025bdc8498fa261d3bc", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "e1b3a5622bc06510e8797e0c11593aa16d7b6523", "e94697b98b707f557436e025bdc8498fa261d3bc", "822f1ed9a76a57cc19d8fda7745365b97130b97a", "e1b3a5622bc06510e8797e0c11593aa16d7b6523", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "05dd7254b632376973f3a1b4d39485da17814df5"]},{"id": "ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "title": "Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference", "authors": ["Qian Chen", "Xiao-Dan Zhu", "Diana Inkpen"], "date": "RepEval@EMNLP", "abstract": "The RepEval 2017 Shared Task aims to evaluate natural language understanding models for sentence representation, in which a sentence is represented as a fixed-length vector with neural networks and the quality of the representation is tested with a natural language inference task. This paper describes our system (alpha) that is ranked among the top in the Shared Task, on both the in-domain test set (obtaining a 74.9% accuracy) and on the cross-domain test set (also attaining a 74.9% accuracy… ", "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "83b83ee4f27388445bdebb199cd75e5bf546dd85", "cff79255a94b9b05a4ce893eb403a522e0923f04", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "83b83ee4f27388445bdebb199cd75e5bf546dd85", "83e7654d545fbbaaf2328df365a781fb67b841b4", "62f88e8fc3b44c5627f2b4721b08498d78103893"]},{"id": "9a5ba9aee44ab873f3d60b05e2773c693707da88", "title": "Read + Verify: Machine Reading Comprehension with Unanswerable Questions", "authors": ["Minghao Hu", "Furu Wei", "Ming Zhou"], "date": "AAAI", "abstract": "Machine reading comprehension with unanswerable questions aims to abstain from answering when no answer can be inferred.", "references": ["a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "8490431f3a76fbd165d108eba938ead212a2a639", "8490431f3a76fbd165d108eba938ead212a2a639", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "8490431f3a76fbd165d108eba938ead212a2a639", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "05dd7254b632376973f3a1b4d39485da17814df5", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "8490431f3a76fbd165d108eba938ead212a2a639", "bb6daf3bd95668ea9775d7e06d8b3f6994306cb7"]},{"id": "5d833331b0e22ff359db05c62a8bca18c4f04b68", "title": "One billion word benchmark for measuring progress in statistical language modeling", "authors": ["Ciprian Chelba", "Tony Robinson"], "date": "2014", "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned… ", "references": ["c19fbefdeead6a4154a22a9c8551a18b1530033a", "9819b600a828a57e1cde047bbe710d3446b30da5", "3b56693f6fe6b82092c4adc756f20fb9b7710ac5", "ba786c46373892554b98df42df7af6f5da343c9d", "9819b600a828a57e1cde047bbe710d3446b30da5", "9819b600a828a57e1cde047bbe710d3446b30da5", "25eb5bb4eba859b1eaac200ec0c2e4638b7e83b5", "ba786c46373892554b98df42df7af6f5da343c9d", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "29053eab305c2b585bcfbb713243b05646e7d62d"]},{"id": "93b4cc549a1bc4bc112189da36c318193d05d806", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "authors": ["Matt Gardner", "Joel Grus", "Luke Zettlemoyer"], "date": "2018", "abstract": "This paper describes AllenNLP, a platform for research on deep learning methods in natural language understanding.", "references": ["3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "5b5cc77898a71a1386734584ceef4070263b8d03", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "5b5cc77898a71a1386734584ceef4070263b8d03", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "a4dd3beea286a20c4e4f66436875932d597190bc", "a4dd3beea286a20c4e4f66436875932d597190bc", "c34e41312b47f60986458759d5cc546c2b53f748", "2cd8e8f510c89c7c18268e8ad51c061e459ad321"]},{"id": "afc2850945a871e72c245818f9bc141bd659b453", "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "authors": ["Sandeep Subramanian", "Adam Trischler", "Christopher Joseph Pal"], "date": "2018", "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning… ", "references": ["3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "612598389b4349fef728c80ab4202fee32f3a536", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "d76c07211479e233f7c6a6f32d5346c983c5598f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "e44da7d8c71edcc6e575fa7faadd5e75785a7901", "3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "26e743d5bd465f49b9538deaf116c15e61b7951f", "612598389b4349fef728c80ab4202fee32f3a536"]},{"id": "8f1c9b656157b1d851563fb42129245701d83175", "title": "Transforming Question Answering Datasets Into Natural Language Inference Datasets", "authors": ["Dorottya Demszky", "Kelvin Guu", "Percy Liang"], "date": "2018", "abstract": "Existing datasets for natural language inference (NLI) have propelled research on language understanding. We propose a new method for automatically deriving NLI datasets from the growing abundance of large-scale question answering datasets. Our approach hinges on learning a sentence transformation model which converts question-answer pairs into their declarative forms. Despite being primarily trained on a single QA dataset, we show that it can be successfully applied to a variety of other QA… ", "references": ["7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "05dd7254b632376973f3a1b4d39485da17814df5", "3eda43078ae1f4741f09be08c4ecab6229046a5c", "05dd7254b632376973f3a1b4d39485da17814df5", "1a210410493fbc052f0b7a54e7bc89cee20e8d28", "1a210410493fbc052f0b7a54e7bc89cee20e8d28", "2dec6a802cbac1f640980b5106d88ae72c45ece4", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "3eda43078ae1f4741f09be08c4ecab6229046a5c"]},{"id": "14d9be7962a4ec5a6e55755f4c7588ea00793652", "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets", "authors": ["Ken Chatfield", "Karen Simonyan", "Andrew Zisserman"], "date": "2014", "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods.", "references": ["c08f5fa876181fc040d76c75fe2433eee3c9b001", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "6270baedeba28001cd1b563a199335720d6e0fe0", "7b7908f71188b89adf62ce9126a0466e1a34338f", "7b7908f71188b89adf62ce9126a0466e1a34338f"]},{"id": "d67175d17c450ab0ac9c256103828f9e9a0acb85", "title": "Some Improvements on Deep Convolutional Neural Network Based Image Classification", "authors": ["Andrew G. Howard"], "date": "2014", "abstract": "Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "398c296d0cc7f9d180f84969f8937e6d3a413796", "398c296d0cc7f9d180f84969f8937e6d3a413796", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "bd4318cd5129cf0d6268876888359f87b410d719"]},{"id": "2cc157afda51873c30b195fff56e917b9c06b853", "title": "High Performance Convolutional Neural Networks for Document Processing", "authors": ["Kumar Chellapilla", "Sidd Puri", "Patrice Y. Simard"], "date": "2006", "abstract": "Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both… ", "references": ["c681ebe49f96954bee87e131214a2921ac7c0f8e", "5562a56da3a96dae82add7de705e2bd841eb00fc", "5562a56da3a96dae82add7de705e2bd841eb00fc", "c681ebe49f96954bee87e131214a2921ac7c0f8e", "5562a56da3a96dae82add7de705e2bd841eb00fc", "c681ebe49f96954bee87e131214a2921ac7c0f8e", "464e8d981df7f326c3af6e9d7bd627f83e438816", "5562a56da3a96dae82add7de705e2bd841eb00fc", "c681ebe49f96954bee87e131214a2921ac7c0f8e", "464e8d981df7f326c3af6e9d7bd627f83e438816"]},{"id": "6270baedeba28001cd1b563a199335720d6e0fe0", "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition", "authors": ["Ali Sharif Razavian", "Hossein Azizpour", "Stefan Carlsson"], "date": "2014", "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle… ", "references": ["6286a82f72f632672c1890f3dd6bbb15b8e5168b", "9bc0295460089592d04e754a5fd427060b7bfa8c", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "a9ce496186120df8f9ed3367e76a4947419e992e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "189b1859f77ddc08027e1e0f92275341e5c0fdc6", "189b1859f77ddc08027e1e0f92275341e5c0fdc6", "25d0fa49ca846370ff4796a6ac6688a42cf50f77", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "authors": ["Kaiming He", "Xiangyu Zhang"], "date": "2015", "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks.", "references": ["64da1980714cfc130632c5b92b9d98c2f6763de6", "eb42cf88027de515750f230b23b1a057dc782108", "64da1980714cfc130632c5b92b9d98c2f6763de6", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "9f2efadf66817f1b38f58b3f50c7c8f34c69d89a", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "d67175d17c450ab0ac9c256103828f9e9a0acb85", "64da1980714cfc130632c5b92b9d98c2f6763de6", "8ad35df17ae4064dd174690efb04d347428f1117", "8ad35df17ae4064dd174690efb04d347428f1117"]},{"id": "f075f89b4f4026748cbf2fb9f989a9934c42ee8f", "title": "Object Detection Networks on Convolutional Feature Maps", "authors": ["Shaoqing Ren", "Kaiming He"], "date": "2017", "abstract": "Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important… ", "references": ["dcbf587642c39f495117552ca453a4f955ffa76a", "1109b663453e78a59e4f66446d71720ac58cec25", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "dcbf587642c39f495117552ca453a4f955ffa76a", "215cf528ed98049724969ceb82c66e78edda8d51", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "74b5a7014aaa00bcc40159bdd6f4b36d77ac1035", "74b5a7014aaa00bcc40159bdd6f4b36d77ac1035", "244bae85fda807361a51d4b26a14ecb2b2f8776b"]},{"id": "cbb19236820a96038d000dc629225d36e0b6294a", "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "authors": ["Kaiming He", "Xiangyu Zhang"], "date": "2015", "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\\times$ </tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq1-2389824.gif\"/></alternatives></inline-formula>224) input image.", "references": ["5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "a99add9d76d849a8d47b93532703e4ca0f683b92", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "eb42cf88027de515750f230b23b1a057dc782108", "eb42cf88027de515750f230b23b1a057dc782108", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16"]},{"id": "4dbc68cf2e14155edb6da0def30661aca8c96c22", "title": "Simplifying ConvNets for Fast Learning", "authors": ["Franck Mamalet", "Christophe Garcia"], "date": "ICANN", "abstract": "In this paper, we propose different strategies for simplifying filters, used as feature extractors, to be learnt in convolutional neural networks (ConvNets) in order to modify the hypothesis space, and to speed-up learning and processing times.", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "80e6390f0eacbcc33d410e9703941aef2471b10c", "28729bafdb929fa157d7a1a0ba721783e49803ae", "fd790b061082571e20be7892ce4a97e156497c9f", "fd790b061082571e20be7892ce4a97e156497c9f", "28729bafdb929fa157d7a1a0ba721783e49803ae", "fd790b061082571e20be7892ce4a97e156497c9f", "80e6390f0eacbcc33d410e9703941aef2471b10c", "fd790b061082571e20be7892ce4a97e156497c9f", "82bac0ae7d60f5bef57deb837de404b4472ee0a0"]},{"id": "1e80f755bcbf10479afd2338cec05211fdbd325c", "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "authors": ["Honglak Lee", "Roger B. Grosse", "Andrew Y. Ng"], "date": "ICML '09", "abstract": "There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max… ", "references": []},{"id": "b3d8dffb73bc93de239998548386c84177caa2ad", "title": "Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks", "authors": ["Ian J. Goodfellow", "Yaroslav Bulatov", "Vinay D. Shet"], "date": "2014", "abstract": "Abstract: Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural… ", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "162d958ff885f1462aeda91cd72582323fd6a1f4", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "162d958ff885f1462aeda91cd72582323fd6a1f4", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "02227c94dd41fe0b439e050d377b0beb5d427cda", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "162d958ff885f1462aeda91cd72582323fd6a1f4"]},{"id": "27a99c21a1324f087b2f144adc119f04137dfd87", "title": "Deep Fried Convnets", "authors": ["Zichao Yang", "Marcin Moczulski", "Ziyu Wang"], "date": "2015", "abstract": "The fully-connected layers of deep convolutional neural networks typically contain over 90% of the network parameters.", "references": ["6bdb186ec4726e00a8051119636d4df3b94043b5", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "6fe78db480995464bd97ba3b712ecc82129e6179", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "081651b38ff7533550a3adfc1c00da333a8fe86c", "6bdb186ec4726e00a8051119636d4df3b94043b5", "2a4117849c88d4728c33b1becaa9fb6ed7030725", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "5a434953b58c72fe2089531d6c4b4fc1325defcb", "title": "Learning Separable Filters", "authors": ["Amos Sironi", "Bugra Tekin", "Pascal Fua"], "date": "2013", "abstract": "Learning filters to produce sparse image representations in terms of over complete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of… ", "references": ["b1c23451334b0f9294a6a8de5be59d361547a946", "6d5b6e0ec6957a7e22245f5e9d2305f8fa46c189", "4dbc68cf2e14155edb6da0def30661aca8c96c22", "6436ab45779f90a4d9c473354672b86e465e3ff7", "bfd771fba9aed481cd3395e3e9ae3c284659f0d4", "bfd771fba9aed481cd3395e3e9ae3c284659f0d4", "498efaa51f5eda731dc6199c3547b9465717fa68", "498efaa51f5eda731dc6199c3547b9465717fa68", "498efaa51f5eda731dc6199c3547b9465717fa68", "6436ab45779f90a4d9c473354672b86e465e3ff7"]},{"id": "1840c15f0ce85a7f4756c185cd29508d6b53c66c", "title": "Janossy Densities. I. Determinantal Ensembles", "authors": ["Alexei Borodin", "Alexander Soshnikov"], "date": "2002", "abstract": "We derive an elementary formula for Janossy densities for determinantal point processes with a finite rank projection-type kernel. In particular, for β=2 polynomial ensembles of random matrices we show that the Janossy densities on an interval I ⊂ ℝ can be expressed in terms of the Christoffel–Darboux kernel for the orthogonal polynomials on the complement of I. ", "references": ["4cd691140606e7c79909a7a994e23d877b6890f8", "4cd691140606e7c79909a7a994e23d877b6890f8", "7422b726a35551eff938d0b4a4188f859c081181", "7422b726a35551eff938d0b4a4188f859c081181", "0a2051f480461301cbe0764fdbc970a42b2e8364", "d05f2f2b0e5d5fffb3f049650655cd9c734d1e79", "277647aad08aea0511a6185efe59473f4e4b4bf7", "66cbb3953c8740b10451043ebb52d411c5f85e4a", "7422b726a35551eff938d0b4a4188f859c081181", "277647aad08aea0511a6185efe59473f4e4b4bf7"]},{"id": "b91afd46236a9c9eda9056bf4e70fe9235867571", "title": "Determinantal Processes and Independence", "authors": ["John B. Hough", "Manjunath Krishnapur", "Bálint Virág"], "date": "2006", "abstract": "We give a probabilistic introduction to determinantal and per- manental point processes. Determinantal processes arise in physics (fermions, eigenvalues of random matrices) and in combinatorics (nonintersecting paths, random spanning trees). They have the striking property that the number of points in a region D is a sum of independent Bernoulli random variables, with parameters which are eigenvalues of the relevant operator on L 2 (D). Moreover, any determinantal process can be represented as… ", "references": ["fdfd269b2ce8256f0483b81d078ee652a7747987", "798ed9a1aa66862f7b7a92cceaeff4c43a5d4151", "ce72362402738b024c0ba7919c3a89c07b8d66d2", "ce72362402738b024c0ba7919c3a89c07b8d66d2", "ce72362402738b024c0ba7919c3a89c07b8d66d2", "798ed9a1aa66862f7b7a92cceaeff4c43a5d4151", "6f1d7cd30249e2858ac436791dd6f50fae9779d0", "798ed9a1aa66862f7b7a92cceaeff4c43a5d4151", "7099761d56368de3cc31d924b3c8b428e54e036a", "3e8bc9d077555e6ab0b338b9a46fafc809b3094b"]},{"id": "a22ac183c8b37824e32cae970db170b861a13438", "title": "Learning diverse rankings with multi-armed bandits", "authors": ["Filip Radlinski", "Robert D. Kleinberg", "Thorsten Joachims"], "date": "ICML '08", "abstract": "Algorithms for learning to rank Web documents usually assume a document's relevance is independent of other documents. This leads to learned ranking functions that produce rankings with redundant results. In contrast, user studies have shown that diversity at high ranks is often preferred. We present two online learning algorithms that directly learn a diverse ranking of documents based on users' clicking behavior. We show that these algorithms minimize abandonment, or alternatively, maximize… ", "references": ["1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3", "ec7018ef4a99a5a5ed6547eb16fe856e3f2e60c6", "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3", "0dec090d24ebe162650923f206979300cfaa847d", "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3", "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3", "c669c8cb28c6d219419e0e904a795164e8c6be05", "ec7018ef4a99a5a5ed6547eb16fe856e3f2e60c6", "63aaf12163fe9735dfe9a69114937c4fa34f303a", "d237072bae2bb7ab3c0e3ea10959c05df5384311"]},{"id": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images", "authors": ["Alex Krizhevsky"], "date": "2009", "abstract": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to… ", "references": ["73d6a26f407db77506959fdf3f7b853e44f3844a", "08d0ea90b53aba0008d25811268fe46562cfb38c", "68c03788224000794d5491ab459be0b2a2c38677", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "73d6a26f407db77506959fdf3f7b853e44f3844a", "43c8a545f7166659e9e21c88fe234e0323855216", "68c03788224000794d5491ab459be0b2a2c38677", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "08d0ea90b53aba0008d25811268fe46562cfb38c", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a"]},{"id": "fabe20a21465511a1d56d319e52eec17005b3452", "title": "Less is more: probabilistic models for retrieving fewer relevant documents", "authors": ["Harr Chen", "David R. Karger"], "date": "SIGIR '06", "abstract": "Traditionally, information retrieval systems aim to maximize thenumber of relevant documents returned to a user within some windowof the top. For that goal, the probability ranking principle, whichranks documents in decreasing order of probability of relevance, isprovably optimal. However, there are many scenarios in which thatranking does not optimize for the users information need. Oneexample is when the user would be satisfied with some limitednumber of relevant documents, rather than… ", "references": ["cd0702deabaa8b7ccfba077f89dcc24e48ae1d47", "e0be8d0485f0f71ff07930dd2356c497ac3e7fdb", "9b7769158ebcb1f6907980d68203c7d3fecc00e6", "ba2345dbc13c8e94b771ef9f09af57aa610fa0f3", "ba2345dbc13c8e94b771ef9f09af57aa610fa0f3", "9b7769158ebcb1f6907980d68203c7d3fecc00e6", "ba2345dbc13c8e94b771ef9f09af57aa610fa0f3", "223a80e2f3fe01e196d32e8edfe447a949a4bed6", "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47", "223a80e2f3fe01e196d32e8edfe447a949a4bed6"]},{"id": "9d94fc289d82738a4d1071470b16ba861ea12169", "title": "Building the gist of a scene: the role of global image features in recognition.", "authors": ["Aude Oliva", "Antonio Torralba"], "date": "2006", "abstract": "Humans can recognize the gist of a novel image in a single glance, independent of its complexity.", "references": ["a8cf3f0ea76961eca50bf26ab31e677037cab622", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "f24db70b4283b56f43a94edd48d1d35e20935ef4", "80c816d748c9ece1145e7a747de1aa99284e5c24", "a4c5dbecd638b12deecc3363df0f24f05b53b45d", "a4c5dbecd638b12deecc3363df0f24f05b53b45d", "9a21604e6db8020b9e76d89064f2b63fb875b67a", "80c816d748c9ece1145e7a747de1aa99284e5c24", "697b45e902c9dd2f31d205f0720e7079f71db200", "f24db70b4283b56f43a94edd48d1d35e20935ef4"]},{"id": "5f10ce4992742b7134d146d91af6a66077140f5f", "title": "Turning down the noise in the blogosphere", "authors": ["Khalid El-Arini", "Gaurav Veda", "Carlos Guestrin"], "date": "KDD", "abstract": "In recent years, the blogosphere has experienced a substantial increase in the number of posts published daily, forcing users to cope with information overload. The task of guiding users through this flood of information has thus become critical. To address this issue, we present a principled approach for picking a set of posts that best covers the important stories in the blogosphere.\n We define a simple and elegant notion of coverage and formalize it as a submodular optimization problem, for… ", "references": []},{"id": "f9f836d28f52ad260213d32224a6d227f8e8849a", "title": "Object recognition from local scale-invariant features", "authors": ["David G. Lowe"], "date": "1999", "abstract": "An object recognition system has been developed that uses a new class of local image features.", "references": ["50b77b2bc08199fe04150929d4df3d66727f96e8", "5b1e1696564e5a3021ac3a501c9deeb6c0fbc637", "9347996c6a11a8ea47d97df6e0c6c739ad68864a", "ef768a5c9bd0aaeddafea1d56b08b0c8180760c0", "49fcd806450d947e56c82ef2b438ad9c484069dc", "8735690a9e8f8884bf27717877ddf7f9071472e5", "9347996c6a11a8ea47d97df6e0c6c739ad68864a", "d11bf8b06cf96d90e1ee3dd6467c5c92ac53e9a1", "8735690a9e8f8884bf27717877ddf7f9071472e5", "5b1e1696564e5a3021ac3a501c9deeb6c0fbc637"]},{"id": "2fbeef5ca17328590cf74353ff5654dad7800d67", "title": "Spatio-temporal correlations and visual signalling in a complete neuronal population", "authors": ["Jonathan W. Pillow", "Jonathon Shlens", "Eero P. Simoncelli"], "date": "2008", "abstract": "Statistical dependencies in the responses of sensory neurons govern both the amount of stimulus information conveyed and the means by which downstream neurons can extract it. Although a variety of measurements indicate the existence of such dependencies, their origin and importance for neural coding are poorly understood. Here we analyse the functional significance of correlated firing in a complete population of macaque parasol retinal ganglion cells using a model of multi-neuron spike… ", "references": ["eb84341018bc6e5775815a37c74876e1e5a1e5fb", "0326e7569e05dfaabdc12357fe10f104bb7d55ab", "b5453a79263cbaf106e20aa9298abd985a4bc131", "88ecb565dad7d3b78de6e6fe86f17329d1bfc883", "bca010ee7633d5201e4d6cffa459a53c52cbe575", "eb84341018bc6e5775815a37c74876e1e5a1e5fb", "b5453a79263cbaf106e20aa9298abd985a4bc131", "eb84341018bc6e5775815a37c74876e1e5a1e5fb", "5b77c31297c27057a459000a027ff10a0a419d20", "bca010ee7633d5201e4d6cffa459a53c52cbe575"]},{"id": "8faad7901db9a73cacaf92ecdedbaece87d95f92", "title": "Kernel k-means: spectral clustering and normalized cuts", "authors": ["Inderjit S. Dhillon", "Yuqiang Guan", "Brian Kulis"], "date": "KDD '04", "abstract": "Kernel k-means and spectral clustering have both been used to identify clusters that are non-linearly separable in input space. Despite significant research, these methods have remained only loosely related. In this paper, we give an explicit theoretical connection between them. We show the generality of the weighted kernel k-means objective function, and derive the spectral clustering objective of normalized cut as a special case. Given a positive definite similarity matrix, our results lead… ", "references": ["d08824dd86424f7a81f3e8f463f68a817b43aabe", "d08824dd86424f7a81f3e8f463f68a817b43aabe", "c02dfd94b11933093c797c362e2f8f6a3b9b8012", "d08824dd86424f7a81f3e8f463f68a817b43aabe", "c02dfd94b11933093c797c362e2f8f6a3b9b8012", "d08824dd86424f7a81f3e8f463f68a817b43aabe", "d08824dd86424f7a81f3e8f463f68a817b43aabe", "d08824dd86424f7a81f3e8f463f68a817b43aabe", "d08824dd86424f7a81f3e8f463f68a817b43aabe", "d08824dd86424f7a81f3e8f463f68a817b43aabe"]},{"id": "5c8fe9a0412a078e30eb7e5eeb0068655b673e86", "title": "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise", "authors": ["Martin Ester", "Hans-Peter Kriegel", "Xiaowei Xu"], "date": "KDD", "abstract": "Clustering algorithms are attractive for the task of class identification in spatial databases.", "references": ["990b7bf28c99ccbb9c9384e5212e045258f78393", "990b7bf28c99ccbb9c9384e5212e045258f78393", "990b7bf28c99ccbb9c9384e5212e045258f78393", "54f940451ef18462253ab92f94f14b2ff1d7c333", "c56be85db5a65d206e5c01a51beda92e964397b9", "b5936b0786f1c7729ab5f12ef1b1daea0f6731d6", "c56be85db5a65d206e5c01a51beda92e964397b9", "b5936b0786f1c7729ab5f12ef1b1daea0f6731d6", "990b7bf28c99ccbb9c9384e5212e045258f78393", "990b7bf28c99ccbb9c9384e5212e045258f78393"]},{"id": "15acca25f75076b80b0bd24c5710c70733308c11", "title": "Near-Optimal MAP Inference for Determinantal Point Processes", "authors": ["Jennifer Gillenwater", "Alex Kulesza", "Ben Taskar"], "date": "NIPS", "abstract": "Determinantal point processes (DPPs) have recently been proposed as computationally efficient probabilistic models of diverse sets for a variety of applications, including document summarization, image search, and pose estimation. Many DPP inference operations, including normalization and sampling, are tractable; however, finding the most likely configuration (MAP), which is often required in practice for decoding, is NP-hard, so we must resort to approximate inference. This optimization… ", "references": ["cc555121cd1fc79e6d5f3bc240e520871721c2f4", "072d09c92805897ddc83116a77a2ee71b7ca2b36", "072d09c92805897ddc83116a77a2ee71b7ca2b36", "ec46bcbced500820521e9f65b0f9ffef5a83ae11", "6b0bc19ccd607dea219eebee331e6361855816cc", "cc555121cd1fc79e6d5f3bc240e520871721c2f4", "3d890c6b65ed6369a5443c9ea8515bdaeb3413b5", "6c4ab2b7bf202e621dcb722d2e7cf421415cc3ed", "6c4ab2b7bf202e621dcb722d2e7cf421415cc3ed", "672313a19e4fb38ac8eed59ecc1ea2a1976be050"]},{"id": "c76d84d779e28e3b236054a0f34c3e48910399d8", "title": "A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects.", "authors": ["Wilson Truccolo", "Uri T. Eden", "Emery N. Brown"], "date": "2005", "abstract": "Multiple factors simultaneously affect the spiking activity of individual neurons. Determining the effects and relative importance of these factors is a challenging problem in neurophysiology. We propose a statistical framework based on the point process likelihood function to relate a neuron's spiking probability to three typical covariates: the neuron's own spiking history, concurrent ensemble activity, and extrinsic covariates such as stimuli or behavior. The framework uses parametric models… ", "references": ["f46b5134322400e0b80e65cd2d308c982adfdc43", "fefd4dba2de33ca4792574734d27611c4da938c5", "64b3fed6224627f2f080f821391f3ff215850529", "92dc238ad532567eb9f1490811d5c5c6dee3b704", "52e32b822f705740abd2c0cd822536e8d1feae4d", "64b3fed6224627f2f080f821391f3ff215850529", "64b3fed6224627f2f080f821391f3ff215850529", "3586adda8efdd12c327659e59e7e47462728d3b1", "fefd4dba2de33ca4792574734d27611c4da938c5", "fefd4dba2de33ca4792574734d27611c4da938c5"]},{"id": "2b5db2ef319226e1a019c10bd17af0c283b56cf7", "title": "How Many Clusters? Which Clustering Method? Answers Via Model-Based Cluster Analysis", "authors": ["Chris Fraley", "Adrian E. Raftery"], "date": "1998", "abstract": "We consider the problem of determining the structure of clustered data, without prior knowledge of the number of clusters or any other information about their composition. Data are represented by a mixture model in which each component corresponds to a different cluster. Models with varying geometric properties are obtained through Gaussian components with different parametrizations and cross-cluster constraints. Noise and outliers can be modelled by adding a Poisson process component… ", "references": ["2e54ce4304164ac0e4d44fca2b8dd78424ed6dc5", "2e54ce4304164ac0e4d44fca2b8dd78424ed6dc5", "6aaab21d79b05130d8d0e7c4b05647d89f16b1d1", "46b50c1cef5c1bb3e95dd0a9d08715d5e40edda8", "0ad9f3c2baf70d1fdfdfa4688b46a0c660888dbe", "2e54ce4304164ac0e4d44fca2b8dd78424ed6dc5", "7a4e63b762e4046191cd4b818c3620228e3e700a", "5cb51d959c956c6cb326def3d8dbd261c4bb3a01", "305388319008f207aa121756da157f727f52f783", "46b50c1cef5c1bb3e95dd0a9d08715d5e40edda8"]},{"id": "ef7bb077881079f686715e2de711b905f6a18829", "title": "Course 14 – Theory of Point Processes for Neural Systems", "authors": ["Emery N. Brown"], "date": "2005", "abstract": "A point process is a stochastic process composed of a sequence of binary events that occur in continuous time. The theory of point processes is a highly developed subdiscipline in the field of stochastic processes. The chapter introduces the theory of univariate point processes for neural systems and focuses on results from the theory of univariate point processes that are relevant for modeling and data analysis in neuroscience. The derivation of interspike interval models from elementary… ", "references": []},{"id": "48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016", "title": "Determinantal Point Processes for Machine Learning", "authors": ["Alex Kulesza", "Ben Taskar"], "date": "2012", "abstract": "Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep… ", "references": ["96a5867d0b9b997108633ff3da314edf69b0122c", "96a5867d0b9b997108633ff3da314edf69b0122c", "325ea1f2022ee3886a5810df76dcfbe4010ad439", "c87f5836627a4ea7d928ff1aecf1b7cdebaf1302", "15acca25f75076b80b0bd24c5710c70733308c11", "702c2fde33ccb4328be06405c11e208a4b3ee347", "c87f5836627a4ea7d928ff1aecf1b7cdebaf1302", "6c4ab2b7bf202e621dcb722d2e7cf421415cc3ed", "96a5867d0b9b997108633ff3da314edf69b0122c", "325ea1f2022ee3886a5810df76dcfbe4010ad439"]},{"id": "0d92ce89894d50af18c902e6b89f85eb0d4fcde2", "title": "Maximum likelihood estimation of cascade point-process neural encoding models", "authors": ["Liam Paninski"], "date": "2004", "abstract": "Recent work has examined the estimation of models of stimulus-driven neural activity in which some linear filtering process is followed by a nonlinear, probabilistic spiking stage. We analyze the estimation of one such model for which this nonlinear step is implemented by a known parametric function; the assumption that this function is known speeds the estimation process considerably. We investigate the shape of the likelihood function for this type of model, give a simple condition on the… ", "references": ["7e7fc800af3f2f9d013925935bb05f5ab84b331b", "96fa3f8ef4d611b0ae81df42bfc82c266e12947c", "96fa3f8ef4d611b0ae81df42bfc82c266e12947c", "7e7fc800af3f2f9d013925935bb05f5ab84b331b", "96fa3f8ef4d611b0ae81df42bfc82c266e12947c", "20d8e96db2de8a815e961d23c51205251b611733", "04e3b181e93428e6561e08fdc5e22e5625d67b52", "508c7605cc6b4110dd97a288c38f99ce48a52db0", "04e3b181e93428e6561e08fdc5e22e5625d67b52", "04e3b181e93428e6561e08fdc5e22e5625d67b52"]},{"id": "8d52f379250a9755f463a9f1a86d555341329001", "title": "Deep vs. wide: depth on a budget for robust speech recognition", "authors": ["Oriol Vinyals", "Nelson Morgan"], "date": "INTERSPEECH", "abstract": "It has now been established that incorporating neural networks can be useful for speech recognition, and that machine learning methods can make it practical to incorporate a larger number of hidden layers in a “deep” structure. Here we incorporate the constraint of freezing the number of parameters for a given task, which in many applications corresponds to practical limitations on storage or computation. Given this constraint, we vary the size of each hidden layer as we change the number of… ", "references": ["f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "eb59dffc2f366518ea13a1399d76ce22c422ddf6", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "eb59dffc2f366518ea13a1399d76ce22c422ddf6", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "3d82e058a5c40954b8f5db170a298a889a254c37", "6658bbf68995731b2083195054ff45b4eca38b3a", "8f613e5481d8d567573b0ffa7b8e1c5b07d33a04"]},{"id": "3e568fb5a3ce754abe9f029aa80610f16b9f9f92", "title": "Theta Oscillations Provide Temporal Windows for Local Circuit Computation in the Entorhinal-Hippocampal Loop", "authors": ["Kenji Mizuseki", "Anton M. Sirota", "György Buzsáki"], "date": "2009", "abstract": "Theta oscillations are believed to play an important role in the coordination of neuronal firing in the entorhinal (EC)-hippocampal system but the underlying mechanisms are not known. We simultaneously recorded from neurons in multiple regions of the EC-hippocampal loop and examined their temporal relationships. Theta-coordinated synchronous spiking of EC neuronal populations predicted the timing of current sinks in target layers in the hippocampus. However, the temporal delays between… ", "references": ["6ac3b4840115b317dc3056b260ab4072132ce0bd", "f6bcea7b25e7dc7e156ebe611414e99e5e54ea67", "614a41f5f7a0a7d70ba802d53b8c8c16cb52aa83", "6cd38fa04c85bd68686d9137ac885c01b59a96d2", "efdfb01dd36feead24b9ed29a01aace59a6e51a0", "df40850beb0b7f573669b8e904c10afe80e98fba", "0a9d3e04e21c17335f1f3646a621c481657f0a00", "aa88e6a057f1b7019ee4d61f16d98b274337e2e6", "df40850beb0b7f573669b8e904c10afe80e98fba", "614a41f5f7a0a7d70ba802d53b8c8c16cb52aa83"]},{"id": "eb84341018bc6e5775815a37c74876e1e5a1e5fb", "title": "Organization of cell assemblies in the hippocampus", "authors": ["Kenneth D. Harris", "Jozsef Csicsvari", "György Buzsáki"], "date": "2003", "abstract": "Neurons can produce action potentials with high temporal precision. A fundamental issue is whether, and how, this capability is used in information processing. According to the ‘cell assembly’ hypothesis, transient synchrony of anatomically distributed groups of neurons underlies processing of both external sensory input and internal cognitive mechanisms. Accordingly, neuron populations should be arranged into groups whose synchrony exceeds that predicted by common modulation by sensory input… ", "references": ["bac8f8283ac2be30e202b78295ce7f4d00957ffb", "01eaf11a1b26eb44ef999e3a7057665d2b16aaa0", "2f21bea4510afce38895905c8d4ba5d5e1c0e2c4", "95dcac65492641d27c82f8d92d818c14536a15fd", "95dcac65492641d27c82f8d92d818c14536a15fd", "bf5a481c60a76041b132cda8f339366b5d306f83", "0169c679f05fb4c8efc0db267dc1878ed35af9db", "621fce5ca4d2480276f27567a9377c56766de048", "95dcac65492641d27c82f8d92d818c14536a15fd", "621fce5ca4d2480276f27567a9377c56766de048"]},{"id": "6d9429b96d9bb40e2d0d9c3f57d0b97f61db8503", "title": "Restructuring of deep neural network acoustic models with singular value decomposition", "authors": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "date": "INTERSPEECH", "abstract": "Recently proposed deep neural network (DNN) obtains significant accuracy improvements in many large vocabulary continuous speech recognition (LVCSR) tasks. However, DNN requires much more parameters than traditional systems, which brings huge cost during online evaluation, and also limits the application of DNN in a lot of scenarios. In this paper we present our new effort on DNN aiming at reducing the model size while keeping the accuracy improvements. We apply singular value decomposition… ", "references": ["ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "473f0739666af2791ad6592822118240ed968b70", "c25f3a963f62165a8fc46bc63865e6bec1477e59", "d7174b0cf599408fb723e6702504e27dc9d6c203", "d7174b0cf599408fb723e6702504e27dc9d6c203", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "473f0739666af2791ad6592822118240ed968b70", "008e9e2d3908c964d5b1c408c478215709dbea10", "23752f55103a1a0e94992c81075f31c9b6d170f5"]},{"id": "59d435c69a34efec487447c52f37cb96b36df570", "title": "Weak pairwise correlations imply strongly correlated network states in a neural population", "authors": ["Elad Schneidman", "Michael J. Berry", "William Bialek"], "date": "2006", "abstract": "Biological networks have so many possible states that exhaustive sampling is impossible. Successful analysis thus depends on simplifying hypotheses, but experiments on many systems hint that complicated, higher-order interactions among large groups of elements have an important role. Here we show, in the vertebrate retina, that weak correlations between pairs of neurons coexist with strongly collective behaviour in the responses of ten or more neurons. We find that this collective behaviour is… ", "references": ["df1ec1bfb66ffa141bca936e8dbf9226378c77d1", "f0fa267389e4dc801b93880ec46eb3e409980bdf", "cca4c343ea0b0d0dd76912d39d44d8aa42c3c8bf", "df1ec1bfb66ffa141bca936e8dbf9226378c77d1", "5a5989208e171ca1b94b973a544aedcfbfb3bde9", "14a5dd6c835641ad0337898874b65a8a2c443c5a", "d23c567f7f521bc49a67ebfd70a4ad2e795a5d23", "d23c567f7f521bc49a67ebfd70a4ad2e795a5d23", "d72dcb82bcc6c65f577793ddaf4fdfd031887669", "d23c567f7f521bc49a67ebfd70a4ad2e795a5d23"]},{"id": "5cea23330c76994cb626df20bed31cc2588033df", "title": "Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets", "authors": ["Tara N. Sainath", "Brian Kingsbury", "Bhuvana Ramabhadran"], "date": "2013", "abstract": "While Deep Neural Networks (DNNs) have achieved tremendous success for large vocabulary continuous speech recognition (LVCSR) tasks, training of these networks is slow. One reason is that DNNs are trained with a large number of training parameters (i.e., 10-50 million). Because networks are trained with a large number of output targets to achieve good performance, the majority of these parameters are in the final weight layer. In this paper, we propose a low-rank matrix factorization of the… ", "references": ["d3986480be64dce5ae55c1f64df660d7d698f8fb", "90b63e917d5737b06357d50aa729619e933d9614", "3d82e058a5c40954b8f5db170a298a889a254c37", "c25f3a963f62165a8fc46bc63865e6bec1477e59", "473f0739666af2791ad6592822118240ed968b70", "3d82e058a5c40954b8f5db170a298a889a254c37", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "a17745f1d7045636577bcd5d513620df5860e9e5", "0e4d042b668805e19f097b7eb0f223babec68f67", "e33cbb25a8c7390aec6a398e36381f4f7770c283"]},{"id": "851c27d7cdb74b0b21bd84a9333bca106f486713", "title": "Low precision storage for deep learning", "authors": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "date": "2014", "abstract": "We train a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct storing formats: floating point, fixed point and dynamic fixed point.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "4157ed3db4c656854e69931cb6089b64b08784b9", "771dc579bf86d269c9565b824f48f0666e84e58c", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "5d90f06bb70a0a3dced62413346235c02b1aa086", "5d90f06bb70a0a3dced62413346235c02b1aa086", "771dc579bf86d269c9565b824f48f0666e84e58c", "771dc579bf86d269c9565b824f48f0666e84e58c"]},{"id": "077b54784ffc0fc4b335392a0bdf630f595a12ce", "title": "Fault tolerance of pruned multilayer networks", "authors": ["Bruce E. Segee", "Michael J. Carter"], "date": "1991", "abstract": "Techniques for dynamically reducing the size of a neural network during learning have been found by some investigators to speed up learning convergence and improve network generalization. However, concern arises about the fault sensitivity of the pruned network relative to that of its parent. Work has been done to assess the tolerance of multilayer feedforward networks to the zeroing of individual weights, and to determine if network pruning during learning affects this tolerance. Multilayer… ", "references": []},{"id": "e354ec85b8287bf15ed596be16ef6e422ccc29e7", "title": "Creating artificial neural networks that generalize", "authors": ["Jocelyn Sietsma", "Robert J. F. Dow"], "date": "1991", "abstract": "Abstract We develop a technique to test the hypothesis that multilayered, feed-forward networks with few units on the first hidden layer generalize better than networks with many units in the first layer.", "references": ["386cbc45ceb59a7abb844b5078e5c944f17723b4"]},{"id": "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "title": "Sparse Feature Learning for Deep Belief Networks", "authors": ["Marc'Aurelio Ranzato", "Y-Lan Boureau", "Yann LeCun"], "date": "NIPS", "abstract": "Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation… ", "references": ["bbc18f70c3a85586ce90ef71bd9f2ada23f2df7f", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "bbc18f70c3a85586ce90ef71bd9f2ada23f2df7f", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "306ddd8b7ea3ead125491efc3e8a9f738ce65b89", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "29bae9472203546847ec1352a604566d0f602728", "355d44f53428b1ac4fb2ab468d593c720640e5bd"]},{"id": "7a6fd5573d2679506765d461ec4892fd4017b745", "title": "Learning Ordered Representations with Nested Dropout", "authors": ["Oren Rippel", "Michael A. Gelbart", "Ryan P. Adams"], "date": "ICML", "abstract": "In this paper, we present results on ordered representations of data in which different dimensions have different degrees of importance.", "references": ["1e05247708515d45166ef96a153f4e22811aa2c6", "2964d30862d0402b0d0ad4a427067f69e4a52130", "2964d30862d0402b0d0ad4a427067f69e4a52130", "be9a17321537d9289875fe475b71f4821457b435", "f5821548720901c89b3b7481f7500d7cd64e99bd", "be9a17321537d9289875fe475b71f4821457b435", "f5821548720901c89b3b7481f7500d7cd64e99bd", "54d2b5c64a67f65c5dd812b89e07973f97699552", "54d2b5c64a67f65c5dd812b89e07973f97699552", "be9a17321537d9289875fe475b71f4821457b435"]},{"id": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95", "title": "Deep learning with COTS HPC systems", "authors": ["Adam Coates", "Brody Huval", "Andrew Y. Ng"], "date": "ICML", "abstract": "Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU… ", "references": ["6348bb3b140c47ea29621d1dc5218db52433840b", "43c8a545f7166659e9e21c88fe234e0323855216", "5a47ba057a858f8c024d2518cc3731fc7eb40de1", "5a47ba057a858f8c024d2518cc3731fc7eb40de1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "6348bb3b140c47ea29621d1dc5218db52433840b", "3127190433230b3dc1abd0680bb58dced4bcd90e", "053912e76e50c9f923a1fc1c173f1365776060cc"]},{"id": "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "authors": ["Emily L. Denton", "Wojciech Zaremba", "Rob Fergus"], "date": "NIPS", "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to… ", "references": ["05cc38e249a6f642363b5a5cbd71cda67cea5893", "e8650503ab80ad7299f0845b1843abf3a97f313a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "e8650503ab80ad7299f0845b1843abf3a97f313a", "e8650503ab80ad7299f0845b1843abf3a97f313a", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "1366de5bb112746a555e9c0cd00de3ad8628aea8"]},{"id": "be9a17321537d9289875fe475b71f4821457b435", "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning", "authors": ["Adam Coates", "Andrew Y. Ng", "Honglak Lee"], "date": "AISTATS", "abstract": "A great deal of research has focused on algorithms for learning features from unlabeled data.", "references": ["932c2a02d462abd75af018125413b1ceaa1ee3f4", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "5d90f06bb70a0a3dced62413346235c02b1aa086", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f"]},{"id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "title": "Learning internal representations by error propagation", "authors": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "date": "1986", "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion ", "references": ["9db00dc5532e4c1b11404a6f957d806ca42c4e73", "9db00dc5532e4c1b11404a6f957d806ca42c4e73", "0120eefaf05bfad5293e87f56d2e787c05f78cf7", "9db00dc5532e4c1b11404a6f957d806ca42c4e73", "69e68bfaadf2dccff800158749f5a50fe82d173b", "65974be9cb2d147e44f93ca0ca0ab5f4c9e22cd7", "9db00dc5532e4c1b11404a6f957d806ca42c4e73", "65974be9cb2d147e44f93ca0ca0ab5f4c9e22cd7", "69e68bfaadf2dccff800158749f5a50fe82d173b", "bb4bad84a2fd896edfa4f5c22061b2913fec500d"]},{"id": "72e93aa6767ee683de7f001fa72f1314e40a8f35", "title": "Building high-level features using large scale unsupervised learning", "authors": ["Quoc V. Le", "Marc'Aurelio Ranzato", "Andrew Y. Ng"], "date": "2012", "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data.", "references": ["51e93552fe55be91a5711ff2aabc04b742503e68", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "51e93552fe55be91a5711ff2aabc04b742503e68", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "5d90f06bb70a0a3dced62413346235c02b1aa086", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "65d994fb778a8d9e0f632659fb33a082949a50d3", "1e80f755bcbf10479afd2338cec05211fdbd325c", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1"]},{"id": "6d02ef4ef79f3cb57dcf4eb20f890aa1be859219", "title": "Unified treatment of some inequalities among ratios of means", "authors": ["Emad El-Neweihi", "Frank Proschan"], "date": "1981", "abstract": "Abstract : Using majorization and Schur-functions, Marshall, Olkin, and Proschan obtained a result concerning monotonicity of the ratio of means. This note shows that a slight extension of their result provides a unified method for obtaining and extending inequalties between means due to Chan, Goldberg, and Gonek, as well as deriving additional inequalties of the same type. (Author) ", "references": ["a211a693915981acea30e9360b11e055baed8299", "a211a693915981acea30e9360b11e055baed8299", "a211a693915981acea30e9360b11e055baed8299"]},{"id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5", "title": "Modeling By Shortest Data Description*", "authors": ["Jorma Rissanen"], "date": "1978", "abstract": "The number of digits it takes to write down an observed sequence x1,...,xN of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the real-valued system parameters. ", "references": []},{"id": "a419456b897eb646d5c9b8f075ffd9d283fff26e", "title": "On the identric and logarithmic means", "authors": ["József Sándor"], "date": "1990", "abstract": "SummaryLeta, b > 0 be positive real numbers. The identric meanI(a, b) of a andb is defined byI = I(a, b) = (1/e)(bb/aa)1/(b−a), fora ≠ b, I(a, a) = a; while the logarithmic meanL(a, b) ofa andb isL = L(a, b) = (b − a)/(logb − loga), fora ≠ b, L(a, a) = a. Let us denote the arithmetic mean ofa andb byA = A(a, b) = (a + b)/2 and the geometric mean byG =G(a, b) =\n$$\\sqrt {ab}$$\n. In this paper we obtain some improvements of known results and new inequalities containing the identric and logarithmic… ", "references": []},{"id": "a1ea19d292460e3b7fca3df1313482f5abc0897e", "title": "Stolarsky-Tobey mean in n variables", "authors": ["Josip Pecaric", "V. Šimić"], "date": "1999", "abstract": "In this paper, an n –dimensional weighted Stolarsky–Tobey mean is defined via measure. This mean includes as special cases various generalizations of the logarithmic mean. Some elementary properties are listed and various inequalities derived. Attention is given to the case when the mean is specialized to Dirichlet measure. Relations to hypergeometric function are exhibited. An explicit form is given for the mean in the special case when all variables have equal weights. Mathematics subject… ", "references": []},{"id": "1295e0ec83fa4e5efbbfaf156d8525fdc8907929", "title": "Bounds for symmetric elliptic integrals", "authors": ["Edward Neuman"], "date": "2003", "abstract": "Lower and upper bounds for the four standard incomplete symmetric elliptic integrals are obtained. The bounding functions are expressed in terms of the elementary transcendental functions. Sharp bounds for the ratio of the complete elliptic integrals of the second kind and the first kind are also derived. These results can be used to obtain bounds for the product of these integrals. It is shown that an iterative numerical algorithm for computing the ratios and products of complete integrals has… ", "references": []},{"id": "94fae0f11be9f67bc6818553e10599ced559422d", "title": "Generalizations of the Logarithmic Mean", "authors": ["Kenneth B. Stolarsky"], "date": "1975", "abstract": "A tamper-resistant, identification device employing a myriad of modes of identifying indicia placement, wherein the device comprises a flexible band having opposed first and second ends wherein one of the ends has at least two apertures and the other end has corresponding rows of apertures wherein by placing the device in encircling configuration, so as to obtain alignment and coincidence of selected apertures, the two ends may be securely fastened together by a variety of fastening means… ", "references": []},{"id": "950bb724db08b009e6305157ac21b484cd2771fe", "title": "M", "authors": ["Axel M. Gressner", "Torsten Arndt"], "date": "2012", "abstract": "Synthese-Verteilung-Abbau-Elimination. MIF wurde ursprünglich als Zytokin der T-Zellen mit einer Vielzahl an immunostimulatorischen und proinflammatorischen Eigenschaften beschrieben, welches u. a. in die Vermittlung des Mitogen-aktivierten-Protein-Kinase (MAPK)Signals, in die Sekretion des Tumornekrosefaktors α (TNF-α) und die Aktivierung der Cyclooxygenase-2 (COX-2) involviert ist. Darüber hinaus wird MIF auch von verschiedenen parenchymalen und Tumorzellen sezerniert und spielt eine… ", "references": ["4bc6de42b9fb2143ab096025ce32400445483ee3", "a6cb14ffce1dda07281c4ae47813c43f33a85a27", "f8eb135fd75cb72409e2bdeec5ccc81c379f43d4", "ac102aecffdecc00f95ed7871f525dc802a3b0c0", "f2ea9a76819a952e6963e3df93b88b9f3b636c84", "46e0169f7d98db958ad45fde734e6e33939d1a96", "f8eb135fd75cb72409e2bdeec5ccc81c379f43d4", "f2ea9a76819a952e6963e3df93b88b9f3b636c84", "f8eb135fd75cb72409e2bdeec5ccc81c379f43d4", "050fd794879e685b1cfe243e7f107389ec37493e"]},{"id": "d779f5c56a7121bdb62d73c1894a1ab0d182cbc2", "title": "Application of Pretrained Deep Neural Networks to Large Vocabulary Conversational Speech Recognition", "authors": ["Navdeep Jaitly", "Patrick Nguyen", "Vincent Vanhoucke"], "date": "2012", "abstract": null, "references": ["843959ffdccf31c6694d135fad07425924f785b1", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "6658bbf68995731b2083195054ff45b4eca38b3a", "473f0739666af2791ad6592822118240ed968b70", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "6658bbf68995731b2083195054ff45b4eca38b3a", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "be53d4def5e0601f2416e9345babc7ef1b30a664"]},{"id": "db869fa192a3222ae4f2d766674a378e47013b1b", "title": "Bayesian learning for neural networks", "authors": ["Geoffrey E. Hinton", "Radford M. Neal"], "date": "1995", "abstract": "From the Publisher: \nArtificial \"neural networks\" are now widely used as flexible models for regression classification applications, but questions remain regarding what these models mean, and how they can safely be used when training data is limited. Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods. Insight into the nature of these… ", "references": ["1f462943c8d0af69c12a09058251848324135e5a", "1f462943c8d0af69c12a09058251848324135e5a", "cc278353721406a248bf733e40cdecbda8ff3a48", "1f462943c8d0af69c12a09058251848324135e5a", "906e33843520fa2395c72d71f8d20a1a5d9cd989", "cc278353721406a248bf733e40cdecbda8ff3a48", "d5ae04ca51e76d69f5ad15ba40a3eea520d3860d", "25c9f33aceac6dcff357727cbe2faf145b01d13c", "5104689e412832ea5c3af39e86321e93f298d849", "5104689e412832ea5c3af39e86321e93f298d849"]},{"id": "de996c32045df6f7b404dda2a753b6a9becf3c08", "title": "Parallel Networks that Learn to Pronounce English Text", "authors": ["Terrence J. Sejnowski", "Charles R. Rosenberg"], "date": "1987", "abstract": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech.", "references": ["6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "ab4aec5e0714b352e6c90d063fe830cbc70912bc", "4c51e3c0b4ed0f073d9bfd935b3a6824126336ab", "84cdfa79e6eb9bf9e625e3af38d9f968df18a880", "0cabc69aaf5dd7ff104bcec693a9ebe7bfb238c4", "0cabc69aaf5dd7ff104bcec693a9ebe7bfb238c4", "dc298a55900b5149b69bbd7708342cc91ecc940d", "4c51e3c0b4ed0f073d9bfd935b3a6824126336ab", "ab4aec5e0714b352e6c90d063fe830cbc70912bc", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7"]},{"id": "d2f6ac8b70044a437a7d5444252d6af6037b46c2", "title": "An Empirical Comparison of ID3 and Back-propagation", "authors": ["Douglas H. Fisher", "Kathleen B. McKusick"], "date": "IJCAI", "abstract": "AI and connectionist approaches to learning from examples differ in knowledge-base representation and inductive mechanisms. To explore these differences we experiment with a system from each paradigm: ID3 and back-propagation. We compare the systems on the basis of both prediction accuracy and length of training. The systems show distinct performance differences across a variety of domains. We identify aspects of each system that may account for these performance differences. Finally, we… ", "references": ["de996c32045df6f7b404dda2a753b6a9becf3c08", "58821c2fde1ec9f42feda075d5e034379870a7a7", "58821c2fde1ec9f42feda075d5e034379870a7a7", "7fc8100e73591ce8af1d553d4296ec38f939c248", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "6f70bb581325440cddbbcf4ba0e7120357d5c7d9", "6f70bb581325440cddbbcf4ba0e7120357d5c7d9", "7fc8100e73591ce8af1d553d4296ec38f939c248", "7fc8100e73591ce8af1d553d4296ec38f939c248", "58821c2fde1ec9f42feda075d5e034379870a7a7"]},{"id": "de75e4e15e22d4376300e5c968e2db44be29ac9e", "title": "Simplifying Neural Networks by Soft Weight-Sharing", "authors": ["Steven J. Nowlan", "Geoffrey E. Hinton"], "date": "1992", "abstract": "One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture… ", "references": ["82fa37d5be8e747131a5857992cc33bb95469ce3", "2cee043045b529fceda7964a70e626d45657245a", "6f3175b3930d0c71495a52a7bccb3889e5f33520", "59fa47fc237a0781b4bf1c84fedb728d20db26a1", "4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "f8830ea439ca695e7dd848275e534f1024c2fe8a", "82fa37d5be8e747131a5857992cc33bb95469ce3", "2cee043045b529fceda7964a70e626d45657245a", "f8830ea439ca695e7dd848275e534f1024c2fe8a", "e7297db245c3feb1897720b173a59fe7e36babb7"]},{"id": "fdfad550280c6a850d92424b6075e7fb58e8e415", "title": "Rule Learning by Searching on Adapted Nets", "authors": ["LiMin Fu"], "date": "AAAI", "abstract": "If the back propagation network can produce an inference structure with high and robust performance, then it is sensible to extract rules from it. The KT algonthm is a novel algonthm for generating rules from an adapted net efficiently. The algorithm is able to deal with both single-layer and multi-layer networks, and can learn both confirming and disconfirming rules. Empirically, the algorithm is demonstrated in the domain of wind shear detection by infrared sensors with success. ", "references": []},{"id": "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "title": "Deep Boltzmann Machines", "authors": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "date": "AISTATS", "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple… ", "references": ["9360e5ce9c98166bb179ad479a9d2919ff13d022", "39756c8a5ac11462e7df98ef7f7baf5b130ec5c9", "39756c8a5ac11462e7df98ef7f7baf5b130ec5c9", "0b718a3f9dae8abc741411aed5fe5d423079200f", "0b718a3f9dae8abc741411aed5fe5d423079200f", "0b718a3f9dae8abc741411aed5fe5d423079200f", "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "73d6a26f407db77506959fdf3f7b853e44f3844a", "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "9f87a11a523e4680e61966e36ea2eac516096f23"]},{"id": "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "title": "Acoustic Modeling Using Deep Belief Networks", "authors": ["Abdel-rahman Mohamed", "George E. Dahl", "Geoffrey E. Hinton"], "date": "2012", "abstract": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any… ", "references": ["a0125b014ff5171c74bd6d8365f4cffe3714c0b0", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "90b63e917d5737b06357d50aa729619e933d9614", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "a0125b014ff5171c74bd6d8365f4cffe3714c0b0", "df5b82595a29724467a98eed4d7e2a45e804579e"]},{"id": "8978cf7574ceb35f4c3096be768c7547b28a35d0", "title": "A Fast Learning Algorithm for Deep Belief Nets", "authors": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "date": "2006", "abstract": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive… ", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "709b4bfc5198336ba5d70da987889a157f695c1e", "2184fb6d32bc46f252b940035029273563c4fc82", "709b4bfc5198336ba5d70da987889a157f695c1e", "709b4bfc5198336ba5d70da987889a157f695c1e", "2184fb6d32bc46f252b940035029273563c4fc82", "9f87a11a523e4680e61966e36ea2eac516096f23", "9f87a11a523e4680e61966e36ea2eac516096f23", "2184fb6d32bc46f252b940035029273563c4fc82", "9360e5ce9c98166bb179ad479a9d2919ff13d022"]},{"id": "d14670a0c65a007912b37e2436ee2d7caf70fd76", "title": "Consensus patterns in DNA.", "authors": ["Gary D. Stormo"], "date": "1990", "abstract": "Matrices can provide realistic representations of protein/DNA specificity. In many cases simple mononucleotide-based matrices are adequate representations, but more complex matrices may be needed for other cases. Unlike simple consensus sequences, matrices allow for different penalties to be assessed for different changes to a binding site, a property that is essential for accurate description of a binding site pattern. When only a collection of binding site sequences is known, the best… ", "references": []},{"id": "d8c9a210221e3c925b4119a4ab90aa8b57bb31fc", "title": "Refinement of Approximately Correct Domain Theories by Knowledge-Based Neural Networks", "authors": ["Geoffrey G. Towell", "Jude W. Shavlik", "Michiel O. Noordewier"], "date": "AAAI", "abstract": null, "references": []},{"id": "a57c6d627ffc667ae3547073876c35d6420accff", "title": "Connectionist Learning Procedures", "authors": ["Geoffrey E. Hinton"], "date": "1989", "abstract": "A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered… ", "references": ["7257eacd80458e70c74494eb1b6759b52ff21399", "8a7acaf6469c06ae5876d92f013184db5897bb13", "97f7d20e1e82347d78cef335218692207b29d23f", "8a7acaf6469c06ae5876d92f013184db5897bb13", "3e6bea2649298c68d17b9421fc7dd19eeacc935e", "865787016949fefd4f0a31862a76db18077f2cf3", "8a7acaf6469c06ae5876d92f013184db5897bb13", "ab4aec5e0714b352e6c90d063fe830cbc70912bc", "3e6bea2649298c68d17b9421fc7dd19eeacc935e", "97f7d20e1e82347d78cef335218692207b29d23f"]},{"id": "6d77482b5e3478f4616f7467054ad50505207958", "title": "Deep Fisher Networks for Large-Scale Image Classification", "authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "date": "NIPS", "abstract": "As massively parallel computations have become broadly available with modern GPUs, deep architectures trained on very large datasets have risen in popularity. Discriminatively trained convolutional neural networks, in particular, were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as ImageNet. However, elements of these architectures are similar to standard hand-crafted representations used in computer vision. In this paper, we explore… ", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "398c296d0cc7f9d180f84969f8937e6d3a413796", "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "398c296d0cc7f9d180f84969f8937e6d3a413796"]},{"id": "a080a28ff7fb3b58fa8cd7123a473c5e75bf46e1", "title": "Training Knowledge-Based Neural Networks to Recognize Genes", "authors": ["Michiel O. Noordewier", "Geoffrey G. Towell", "Jude W. Shavlik"], "date": "NIPS", "abstract": "We describe the application of a hybrid symbolic/connectionist machine learning algorithm to the task of recognizing important genetic sequences. The symbolic portion of the KBANN system utilizes inference rules that provide a roughly-correct method for recognizing a class of DNA sequences known as eukaryotic splice-junctions. We then map this \"domain theory\" into a neural network and provide training examples. Using the samples, the neural network's learning algorithm adjusts the domain theory… ", "references": ["0926057bc5e996a7f071c7ef5057639d877a4b46", "3a1525d582936db10ee9a710581e5d47a0a78d19", "2ed814434625724b08b4dd4ec664e3d533f37efd", "995a3b11cc8a4751d8e167abc4aa937abc934df0", "0926057bc5e996a7f071c7ef5057639d877a4b46", "5fdadc3f070634f6cfb1be8e6bf984068fc8d676", "42b35bff4a707aa8f0e2f1db351b47696a3ccd1f", "ec8f2fcae2de3a904f58f6ab622a98c970dd7834", "3a1525d582936db10ee9a710581e5d47a0a78d19", "995a3b11cc8a4751d8e167abc4aa937abc934df0"]},{"id": "6cf7f474eb493b0e5aae74ccfd9cdc79e506060e", "title": "Learning the Structure of Deep Convolutional Networks", "authors": ["Jiashi Feng", "Trevor Darrell"], "date": "2015", "abstract": "In this work, we develop a novel method for automatically learning aspects of the structure of a deep model, in order to improve its performance, especially when labeled training data are scarce.", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "005b668ef278941f584df96f2aca1ca88f056470", "eb42cf88027de515750f230b23b1a057dc782108", "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "a9ce496186120df8f9ed3367e76a4947419e992e", "a9ce496186120df8f9ed3367e76a4947419e992e", "40bb155edc56515638f001fe6c35cc44c382fb86", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "a99add9d76d849a8d47b93532703e4ca0f683b92", "title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features", "authors": ["Yunchao Gong", "Liwei Wang", "Svetlana Lazebnik"], "date": "2014", "abstract": "Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition.", "references": ["6d77482b5e3478f4616f7467054ad50505207958", "b8de958fead0d8a9619b55c7299df3257c624a96", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5fc662287842e5cb2d23b5fa917354e957c573bf", "6d77482b5e3478f4616f7467054ad50505207958", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "b8de958fead0d8a9619b55c7299df3257c624a96", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b8de958fead0d8a9619b55c7299df3257c624a96", "6d77482b5e3478f4616f7467054ad50505207958"]},{"id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "authors": ["Shaoqing Ren", "Kaiming He", "Jian Sun"], "date": "2015", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.", "references": ["3ad998a9b2c071c4a1971048f8a2d754530f08e8", "f075f89b4f4026748cbf2fb9f989a9934c42ee8f", "cbb19236820a96038d000dc629225d36e0b6294a", "4328ec9d98eff5d7eb70997f76d81b27849f3220", "f075f89b4f4026748cbf2fb9f989a9934c42ee8f", "4328ec9d98eff5d7eb70997f76d81b27849f3220", "cbb19236820a96038d000dc629225d36e0b6294a", "1109b663453e78a59e4f66446d71720ac58cec25", "4328ec9d98eff5d7eb70997f76d81b27849f3220", "3ad998a9b2c071c4a1971048f8a2d754530f08e8"]},{"id": "d5b4721c8188269b120d3d06149a04435753e755", "title": "Convolutional neural networks with low-rank regularization", "authors": ["Cheng Tai", "E Weinan"], "date": "2016", "abstract": "Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the… ", "references": ["021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "e6f2f3a5cc7c7213835b9aede15715b5830520e1", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "eb42cf88027de515750f230b23b1a057dc782108", "e6f2f3a5cc7c7213835b9aede15715b5830520e1", "9716e4f69040f3f182714d7fb16ab9a65fb34ba6", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "6bdb186ec4726e00a8051119636d4df3b94043b5"]},{"id": "b64601d509711468f5d085261d463846f36785b2", "title": "Efficient and accurate approximations of nonlinear convolutional networks", "authors": ["Xiangyu Zhang", "Jianhua Zou"], "date": "2015", "abstract": "This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is… ", "references": ["14d9be7962a4ec5a6e55755f4c7588ea00793652", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "d67175d17c450ab0ac9c256103828f9e9a0acb85", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "eb42cf88027de515750f230b23b1a057dc782108"]},{"id": "d559dd84fc473fca7e91b9075675750823935afa", "title": "Sparse Convolutional Neural Networks", "authors": ["Bao-Yuan Liu", "Meitian Wang", "Marianna Pensky"], "date": "2015", "abstract": "Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity.", "references": ["021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "6bdb186ec4726e00a8051119636d4df3b94043b5", "6bdb186ec4726e00a8051119636d4df3b94043b5", "6bdb186ec4726e00a8051119636d4df3b94043b5", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "a9ce496186120df8f9ed3367e76a4947419e992e", "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "a9ce496186120df8f9ed3367e76a4947419e992e"]},{"id": "843959ffdccf31c6694d135fad07425924f785b1", "title": "Extracting and composing robust features with denoising autoencoders", "authors": ["Pascal Vincent", "Hugo Larochelle", "Pierre-Antoine Manzagol"], "date": "ICML '08", "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations.", "references": ["202cbbf671743aefd380d2f23987bd46b9caaf97", "43c8a545f7166659e9e21c88fe234e0323855216", "202cbbf671743aefd380d2f23987bd46b9caaf97", "202cbbf671743aefd380d2f23987bd46b9caaf97"]},{"id": "aa7bfd2304201afbb19971ebde87b17e40242e91", "title": "On the importance of initialization and momentum in deep learning", "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton"], "date": "ICML", "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were… ", "references": ["f2a0fbba89f0d18ea0abd29639d4e43babe59cf3", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "71ab580b72ec5a5b6d0884a54485cd69b9af21bb", "43c8a545f7166659e9e21c88fe234e0323855216", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "f2a0fbba89f0d18ea0abd29639d4e43babe59cf3", "d0be39ee052d246ae99c082a565aba25b811be2d", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "f2a0fbba89f0d18ea0abd29639d4e43babe59cf3"]},{"id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition", "authors": ["Kaiming He", "Xiangyu Zhang"], "date": "2016", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On… ", "references": ["8ad35df17ae4064dd174690efb04d347428f1117", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "eb42cf88027de515750f230b23b1a057dc782108", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "eb42cf88027de515750f230b23b1a057dc782108", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "8ad35df17ae4064dd174690efb04d347428f1117", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16"]},{"id": "82b9099ddf092463f497bd48bb112c46ca52c4d1", "title": "High-Performance Neural Networks for Visual Object Classification", "authors": ["Dan C. Ciresan", "Ueli Meier", "Jürgen Schmidhuber"], "date": "2011", "abstract": "We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better… ", "references": ["3fa5450f1c0795527939cfef5fbe3912c4dab3ab", "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd", "2cc157afda51873c30b195fff56e917b9c06b853", "be9a17321537d9289875fe475b71f4821457b435", "d46fd54609e09bcd135fd28750003185a5ee4125", "2cc157afda51873c30b195fff56e917b9c06b853", "581528b2215e017eba96ef4ee16d33a74645755f", "581528b2215e017eba96ef4ee16d33a74645755f", "2cc157afda51873c30b195fff56e917b9c06b853", "3fa5450f1c0795527939cfef5fbe3912c4dab3ab"]},{"id": "39f3b1804b8df5be645a1dcb4a876e128385d9be", "title": "Improving the Fisher Kernel for Large-Scale Image Classification", "authors": ["Florent Perronnin", "Jorge Sánchez", "Thomas Mensink"], "date": "ECCV", "abstract": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches.", "references": ["23694b6d61668e62bb11f17c1d75dde3b4951948", "23694b6d61668e62bb11f17c1d75dde3b4951948", "23694b6d61668e62bb11f17c1d75dde3b4951948", "3efe5e292d5356fefd5b239c431ebdd1cb4fe354", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "aa1fa18231b8c6b35a21796af446899fc681a107", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "fcbe764317d7ab97be0713038f772afe2e4ad7f9", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "5a5effa909cdeafaddbbb7855037e02f8e25d632"]},{"id": "dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71", "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "date": "2014", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets.", "references": ["860a9d55d87663ca88e74b3ca357396cd51733d0", "e952c51379567889753b2df005107520207ab337", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e952c51379567889753b2df005107520207ab337", "398c296d0cc7f9d180f84969f8937e6d3a413796", "860a9d55d87663ca88e74b3ca357396cd51733d0", "398c296d0cc7f9d180f84969f8937e6d3a413796", "e952c51379567889753b2df005107520207ab337", "39f3b1804b8df5be645a1dcb4a876e128385d9be"]},{"id": "398c296d0cc7f9d180f84969f8937e6d3a413796", "title": "Multi-column deep neural networks for image classification", "authors": ["Dan C. Ciresan", "Ueli Meier", "Jürgen Schmidhuber"], "date": "2012", "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs.", "references": ["9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "3fa5450f1c0795527939cfef5fbe3912c4dab3ab", "3fa5450f1c0795527939cfef5fbe3912c4dab3ab", "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "3fa5450f1c0795527939cfef5fbe3912c4dab3ab", "162d958ff885f1462aeda91cd72582323fd6a1f4", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "5a47ba057a858f8c024d2518cc3731fc7eb40de1"]},{"id": "c43025c429b1fbf6f1379f61801a1b40834d62e7", "title": "Convolutional networks and applications in vision", "authors": ["Yann LeCun", "Koray Kavukcuoglu", "Clément Farabet"], "date": "2010", "abstract": "Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or \"features\")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each… ", "references": ["f566b1f24e63151ddae652826638af054973a27f", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "1762498d7ef09cc706b551c54ce6894a7b2ee14d", "1762498d7ef09cc706b551c54ce6894a7b2ee14d", "b2af2a2f2d1be22ebf473f7e0f501f1f5c02f222", "a90998e0023db48b207cee3b39b0441b3935aaa7", "a90998e0023db48b207cee3b39b0441b3935aaa7", "688b6fbc3c5c06e254961f70de9d855d3d008d09", "f9e65fcb0e04174577f211d702d3f837e3624c5b"]},{"id": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts", "authors": ["Robert A. Jacobs", "Michael I. Jordan", "Geoffrey E. Hinton"], "date": "1991", "abstract": "We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into… ", "references": []},{"id": "3127190433230b3dc1abd0680bb58dced4bcd90e", "title": "Large Scale Distributed Deep Networks", "authors": ["Jeffrey Dean", "Gregory S. Corrado", "Andrew Y. Ng"], "date": "NIPS", "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance.", "references": ["72e93aa6767ee683de7f001fa72f1314e40a8f35", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "3bb6d5834bfb355553588e382ac5f9fa8a8d831d", "5352b7ca90cbe4938f8e71a25d49517e7f94670a", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "3bb6d5834bfb355553588e382ac5f9fa8a8d831d", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "371bd20afe88e73eafea2298b52beca7b9b5660a", "053912e76e50c9f923a1fc1c173f1365776060cc"]},{"id": "1109b663453e78a59e4f66446d71720ac58cec25", "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "authors": ["Pierre Sermanet", "David Eigen", "Yann LeCun"], "date": "2014", "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection.", "references": ["9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "398c296d0cc7f9d180f84969f8937e6d3a413796", "398c296d0cc7f9d180f84969f8937e6d3a413796", "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "140d2acd4cdbc30b102dac34f4c68f279ace6a26", "38b6540ddd5beebffd05047c78183f7575559fb2", "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "79ef1a3843a2dc01bde67c3a9a17c6deb352e285", "398c296d0cc7f9d180f84969f8937e6d3a413796", "fd790b061082571e20be7892ce4a97e156497c9f"]},{"id": "5562a56da3a96dae82add7de705e2bd841eb00fc", "title": "Best practices for convolutional neural networks applied to visual document analysis", "authors": ["Patrice Y. Simard", "David Steinkraus", "John C. Platt"], "date": "2003", "abstract": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.", "references": ["6e987ea8d60aace318f58c282ed30b50cfac958c", "437fce6c281031a9dc69db9c54027b531dcbeecc", "162d958ff885f1462aeda91cd72582323fd6a1f4", "e92cc6ecabaf1fbe904b35dd6183e24da01f66ec", "162d958ff885f1462aeda91cd72582323fd6a1f4", "e92cc6ecabaf1fbe904b35dd6183e24da01f66ec", "6e987ea8d60aace318f58c282ed30b50cfac958c", "37807e97c624fb846df7e559553b32539ba2ea5d", "37807e97c624fb846df7e559553b32539ba2ea5d", "661e5d7bd7454dc03a96f61fcbc7329ac13ed56c"]},{"id": "8d25d04051074be7590cbe5e4e34c45bb26674e1", "title": "Learning small-size DNN with output-distribution-based criteria", "authors": ["Jinyu Li", "Rui Zhao", "Yifan Gong"], "date": "INTERSPEECH", "abstract": "Deep neural network (DNN) obtains significant accuracy improvements on many speech recognition tasks and its power comes from the deep and wide network structure with a very large number of parameters. It becomes challenging when we deploy DNN on devices which have limited computational and storage resources. The common practice is to train a DNN with a small number of hidden nodes and a small senone set using the standard training process, leading to significant accuracy loss. In this study… ", "references": ["a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "d7174b0cf599408fb723e6702504e27dc9d6c203", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "d7174b0cf599408fb723e6702504e27dc9d6c203", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "7d1e282f1613c161585dfc9dd077282cb37b5b89", "23752f55103a1a0e94992c81075f31c9b6d170f5", "6658bbf68995731b2083195054ff45b4eca38b3a", "6658bbf68995731b2083195054ff45b4eca38b3a", "5cea23330c76994cb626df20bed31cc2588033df"]},{"id": "182015c5edff1956cbafbcb3e7bbe294aa54f9fc", "title": "Selecting Receptive Fields in Deep Networks", "authors": ["Adam Coates", "Andrew Y. Ng"], "date": "NIPS", "abstract": "Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer.", "references": ["31f04f8f83365fabf7ba9c9be1179c0da6815128", "265069b3670930fd884b02062d7e7b79ff2a49d5", "31f04f8f83365fabf7ba9c9be1179c0da6815128", "31f04f8f83365fabf7ba9c9be1179c0da6815128", "498efaa51f5eda731dc6199c3547b9465717fa68", "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "265069b3670930fd884b02062d7e7b79ff2a49d5", "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "d46fd54609e09bcd135fd28750003185a5ee4125"]},{"id": "72d32c986b47d6b880dad0c3f155fe23d2939038", "title": "Deep Learning of Representations: Looking Forward", "authors": ["Yoshua Bengio"], "date": "2013", "abstract": "Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and… ", "references": ["e60ff004dde5c13ec53087872cfcdd12e85beb57", "5c0fe8ba39bda83d6ca3b9705a780809d52a67b4", "843959ffdccf31c6694d135fad07425924f785b1", "f8c8619ea7d68e604e40b814b40c72888a755e95", "522e90b9fccfd3c1c0603359eb04757d770c1ab5", "843959ffdccf31c6694d135fad07425924f785b1", "5c0fe8ba39bda83d6ca3b9705a780809d52a67b4", "43c8a545f7166659e9e21c88fe234e0323855216", "d0965d8f9842f2db960b36b528107ca362c00d1a", "d0965d8f9842f2db960b36b528107ca362c00d1a"]},{"id": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "title": "Learning representations by back-propagating errors", "authors": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "date": "1986", "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured… ", "references": []},{"id": "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "title": "Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition", "authors": ["Navdeep Jaitly", "Patrick Nguyen", "Vincent Vanhoucke"], "date": "INTERSPEECH", "abstract": "The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artificial Neural Network Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems 5870 hours of Voice Search and 1400 hours of YouTube data. On the first dataset… ", "references": ["d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "be53d4def5e0601f2416e9345babc7ef1b30a664", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "8770b4a5ca7734c88e5755f9558f79e93229c023", "cd0568b4faa03910ae3c07d00c627666f404305d", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "26cf16673269bdb0979bc601a340083448e5ad44", "cd0568b4faa03910ae3c07d00c627666f404305d", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada"]},{"id": "51e93552fe55be91a5711ff2aabc04b742503e68", "title": "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning", "authors": ["Quoc V. Le", "Alexandre Karpenko", "Andrew Y. Ng"], "date": "NIPS", "abstract": "Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning.", "references": ["843959ffdccf31c6694d135fad07425924f785b1", "c1ba0d38d855af0f4a2c2cf0f4fa48e4477fc4ec", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "053912e76e50c9f923a1fc1c173f1365776060cc", "843959ffdccf31c6694d135fad07425924f785b1", "c1ba0d38d855af0f4a2c2cf0f4fa48e4477fc4ec", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "9287071f38b6700794a71cd7ba210a1a8cd21a6d", "c1ba0d38d855af0f4a2c2cf0f4fa48e4477fc4ec", "053912e76e50c9f923a1fc1c173f1365776060cc"]},{"id": "0ea90fac0958d84bcf4a2875c2b169478358b480", "title": "CUDAMat: a CUDA-based matrix class for Python", "authors": ["Volodymyr Mnih"], "date": "2009", "abstract": "CUDAMat is an open source software package that provides a CUDA-based matrix class for Python. The primary goal of CUDAMat is to make it easy to implement algorithms that are easily expressed in terms of dense matrix operations on a GPU. At present, the feature set of CUDAMat is biased towards providing functionality useful for implementing standard machine learning algorithms, however, it is general enough to be useful in other elds. We have used CUDAMat to implement several common machine… ", "references": ["e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1"]},{"id": "5352b7ca90cbe4938f8e71a25d49517e7f94670a", "title": "Scalable stacking and learning for building deep architectures", "authors": ["Li Deng", "Dong Yu", "John C. Platt"], "date": "2012", "abstract": "Deep Neural Networks (DNNs) have shown remarkable success in pattern recognition tasks.", "references": ["d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "b3275d20929462b051cc99a47383af9c7ca0ac0e", "6658bbf68995731b2083195054ff45b4eca38b3a", "e0acc24337501da9a68f03e8a9a5b42d52ffa927", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "995a3b11cc8a4751d8e167abc4aa937abc934df0", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "6658bbf68995731b2083195054ff45b4eca38b3a", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1"]},{"id": "80664dab16a1f18ce1998e38a03f080c5e98363a", "title": "GPU implementation of neural networks", "authors": ["Kyoung-Su Oh", "Keechul Jung"], "date": "2004", "abstract": "Abstract Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix… ", "references": []},{"id": "fa5cf89c59b834ec7573673657c99c77f53f7add", "title": "Neural Network Implementation Using CUDA and OpenMP", "authors": ["Honghoon Jang", "Anjin Park", "Keechul Jung"], "date": "2008", "abstract": "Many algorithms for image processing and pattern recognition have recently been implemented on GPU (graphic processing unit) for faster computational times.", "references": ["80664dab16a1f18ce1998e38a03f080c5e98363a", "80664dab16a1f18ce1998e38a03f080c5e98363a", "f4a25e36050132f2e695d936e15e88387e742ee1", "80664dab16a1f18ce1998e38a03f080c5e98363a", "f4a25e36050132f2e695d936e15e88387e742ee1", "d28328674da2c85ad9d2fcb9fdd262721f143393", "f4a25e36050132f2e695d936e15e88387e742ee1", "80664dab16a1f18ce1998e38a03f080c5e98363a", "561a39360b59879f73a7503675e304f856ca9a14", "80664dab16a1f18ce1998e38a03f080c5e98363a"]},{"id": "b7b915d508987b73b61eccd2b237e7ed099a2d29", "title": "Maxout Networks", "authors": ["Ian J. Goodfellow", "David Warde-Farley", "Yoshua Bengio"], "date": "ICML", "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these… ", "references": ["5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "2e2089ae76fe914706e6fa90081a79c8fe01611e", "5d90f06bb70a0a3dced62413346235c02b1aa086", "5d90f06bb70a0a3dced62413346235c02b1aa086", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "2e2089ae76fe914706e6fa90081a79c8fe01611e"]},{"id": "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "title": "Large-scale deep unsupervised learning using graphics processors", "authors": ["Rajat Raina", "Anand Madhavan", "Andrew Y. Ng"], "date": "ICML '09", "abstract": "The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large… ", "references": ["46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "ba786c46373892554b98df42df7af6f5da343c9d", "d02f832e2848a40d3ae9b62b7a245eb918a5b667", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e"]},{"id": "94fd94b7ebcdd80e47706376aa0540cbeb009262", "title": "Debunking the 100X GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU", "authors": ["Victor W. Lee", "Changkyu Kim", "Pradeep Dubey"], "date": "ISCA '10", "abstract": "Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver… ", "references": ["a205801dbd56f93f3b98fd6d9a535ed1961806fa"]},{"id": "67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6", "title": "Scalable Object Detection Using Deep Neural Networks", "authors": ["Dumitru Erhan", "Christian Szegedy", "Dragomir Anguelov"], "date": "2014", "abstract": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012.", "references": ["b5410a46dd09267b5d90eab26db897e9ab7a9e70", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "37e41557932cc0035eab23fd767bde68f6475c3a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e79272fe3d65197100eae8be9fec6469107969ae", "54b224478a63e33441c651175c522f3702062fc4", "37e41557932cc0035eab23fd767bde68f6475c3a", "37e41557932cc0035eab23fd767bde68f6475c3a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "82635fb63640ae95f90ee9bdc07832eb461ca881"]},{"id": "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "title": "Deep Neural Networks for Object Detection", "authors": ["Christian Szegedy", "Alexander Toshev", "Dumitru Erhan"], "date": "NIPS", "abstract": "Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14.", "references": ["120081166fc0d780c84e198622d638152a7cdf3e", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "5d777a71d36ec929f70c2f7b5eca47456c34a4e0", "120081166fc0d780c84e198622d638152a7cdf3e", "120081166fc0d780c84e198622d638152a7cdf3e", "82635fb63640ae95f90ee9bdc07832eb461ca881", "0f602c33b1762d57223c9f9656579f9d1dc2e30a", "648a65e31a56f54dd906d40872b1a8ac78309b0b", "0f602c33b1762d57223c9f9656579f9d1dc2e30a"]},{"id": "822f3b9a392a9abccdaa7ef5ae4183d2d4d3d6db", "title": "Discriminative Transfer Learning with Tree-based Priors", "authors": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "date": "NIPS", "abstract": "High capacity classifiers, such as deep neural networks, often struggle on classes that have very few training examples. We propose a method for improving classification performance for such classes by discovering similar classes and transferring knowledge among them. Our method learns to organize the classes into a tree hierarchy. This tree structure imposes a prior over the classifier's parameters. We show that the performance of deep neural networks can be improved by applying these priors… ", "references": ["e0f49caabbf79ffda35432219bb0ec9b41753dff", "5d90f06bb70a0a3dced62413346235c02b1aa086", "bff05119cd30c2c61323861a5e2a28094388427f", "89c808af926ecb20870b2521fbaa7dcbb85be106", "89c808af926ecb20870b2521fbaa7dcbb85be106", "5d90f06bb70a0a3dced62413346235c02b1aa086", "bff05119cd30c2c61323861a5e2a28094388427f", "bff05119cd30c2c61323861a5e2a28094388427f", "5d90f06bb70a0a3dced62413346235c02b1aa086", "e24a5e843d2ea999393b9f278f4b5c80f8a651d1"]},{"id": "523b12db4004b89284387f978c2af8ae0e79d54b", "title": "Knowledge Matters: Importance of Prior Information for Optimization", "authors": ["Çaglar Gülçehre", "Yoshua Bengio"], "date": "2016", "abstract": "We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered… ", "references": ["e60ff004dde5c13ec53087872cfcdd12e85beb57", "184ac0766262312ba76bbdece4e7ffad0aa8180b", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "184ac0766262312ba76bbdece4e7ffad0aa8180b", "184ac0766262312ba76bbdece4e7ffad0aa8180b", "0d2336389dff3031910bd21dd1c44d1b4cd51725", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "8de174ab5419b9d3127695405efd079808e956e8", "184ac0766262312ba76bbdece4e7ffad0aa8180b", "05fd1da7b2e34f86ec7f010bef068717ae964332"]},{"id": "0409187ebfce03de95677aaeb499e8f1953bdbaf", "title": "Faster matrix-vector multiplication on GeForce 8800GTX", "authors": ["N. Fujimoto"], "date": "2008", "abstract": "Recently a GPU has acquired programmability to perform general purpose computation fast by running ten thousands of threads concurrently. This paper presents a new algorithm for dense matrix-vector multiplication on NVIDIA CUDA architecture. The experimental results on GeForce 8800GTX show that the proposed algorithm runs maximum 15.69 (resp., 32.88) times faster than the sgemv routine in NVIDIA's BIAS library CUBLAS 1.1 (resp., Intel Math Kernel Library 9.1 on one-core of 2.0 GHz Intel Xeon… ", "references": ["ec88941835c88a2956fc84c9e7cfd6dedf1b36f6"]},{"id": "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "title": "What is the best multi-stage architecture for object recognition?", "authors": ["Kevin Jarrett", "Koray Kavukcuoglu", "Yann LeCun"], "date": "2009", "abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition… ", "references": ["54a9c2553138932426faebcaa67a63a84a56b55d", "0dadb25842ef596a0f676c04cbd3dad4e1876964", "f354310098e09c1e1dc88758fca36767fd9d084d", "0dadb25842ef596a0f676c04cbd3dad4e1876964", "54a9c2553138932426faebcaa67a63a84a56b55d", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "ceb0e1a86dc35e21ce5f0524c8476f15e1b08988", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "f354310098e09c1e1dc88758fca36767fd9d084d", "0dadb25842ef596a0f676c04cbd3dad4e1876964"]},{"id": "38f35dd624cd1cf827416e31ac5e0e0454028eca", "title": "Regularization of Neural Networks using DropConnect", "authors": ["Li Wan", "Matthew D. Zeiler", "Rob Fergus"], "date": "ICML", "abstract": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and… ", "references": ["1366de5bb112746a555e9c0cd00de3ad8628aea8", "398c296d0cc7f9d180f84969f8937e6d3a413796", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "162d958ff885f1462aeda91cd72582323fd6a1f4", "398c296d0cc7f9d180f84969f8937e6d3a413796", "2e2089ae76fe914706e6fa90081a79c8fe01611e", "02227c94dd41fe0b439e050d377b0beb5d427cda", "398c296d0cc7f9d180f84969f8937e6d3a413796", "398c296d0cc7f9d180f84969f8937e6d3a413796", "f707a81a278d1598cd0a4493ba73f22dcdf90639"]},{"id": "b8de958fead0d8a9619b55c7299df3257c624a96", "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "authors": ["Jeff Donahue", "Yangqing Jia", "Trevor Darrell"], "date": "ICML", "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional… ", "references": ["5c0fe8ba39bda83d6ca3b9705a780809d52a67b4", "5c0fe8ba39bda83d6ca3b9705a780809d52a67b4", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "5d9a3036181676e187c9c0ff995d8bed1db3557d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "42269d0438c0ae4ca892334946ed779999691074", "7de1d1612debcbde32cd588fa607a408df79c717", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "953e2cfa58679ff6ea8c0bb432afd641f15d3657"]},{"id": "f9def788d4ae040edb8bde18b8aeea635444a4d1", "title": "Learnable Pooling Regions for Image Classification", "authors": ["Mateusz Malinowski", "Mario Fritz"], "date": "2013", "abstract": "Biologically inspired, from the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper… ", "references": ["72e93aa6767ee683de7f001fa72f1314e40a8f35", "d2810567138cb8a17b73de8913013487300d4b89", "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "be9a17321537d9289875fe475b71f4821457b435", "398c296d0cc7f9d180f84969f8937e6d3a413796", "e2c04849a3802715d5a9d89179c9f161014d6c2a", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "72e93aa6767ee683de7f001fa72f1314e40a8f35"]},{"id": "0abb49fe138e8fb7332c26b148a48d0db39724fc", "title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks", "authors": ["Matthew D. Zeiler", "Rob Fergus"], "date": "2013", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks.", "references": ["5a47ba057a858f8c024d2518cc3731fc7eb40de1", "a538b05ebb01a40323997629e171c91aa28b8e2f", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "5562a56da3a96dae82add7de705e2bd841eb00fc", "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "a538b05ebb01a40323997629e171c91aa28b8e2f", "68447483e80991ca718cad40e73ac14c08da7413", "a538b05ebb01a40323997629e171c91aa28b8e2f", "02227c94dd41fe0b439e050d377b0beb5d427cda", "5a47ba057a858f8c024d2518cc3731fc7eb40de1"]},{"id": "ec92efde21707ddf4b81f301cd58e2051c1a2443", "title": "Fast dropout training", "authors": ["Sida I. Wang", "Christopher D. Manning"], "date": "ICML", "abstract": "Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do… ", "references": ["7abda1941534d3bb558dd959025d67f1df526303", "c6db1c92fc28fff14434b645861c0f4df5065e9e", "dc0975ae518a5b30e60fde23a41c74bafd7c6f8c", "05a8d8f1d2dadf01c35a363b1c37eed1dd27120f", "c6db1c92fc28fff14434b645861c0f4df5065e9e", "dc0975ae518a5b30e60fde23a41c74bafd7c6f8c", "03a460c8ca331d36e8f2e11edd49e7dbc35c7e43", "c6db1c92fc28fff14434b645861c0f4df5065e9e", "c6db1c92fc28fff14434b645861c0f4df5065e9e", "cfa2646776405d50533055ceb1b7f050e9014dcb"]},{"id": "de45a6f4473a321d3dd70b0f5e327a0783b57326", "title": "Agreement-Based Joint Training for Bidirectional Attention-Based Neural Machine Translation", "authors": ["Yong Cheng", "Shiqi Shen", "Yang Liu"], "date": "2016", "abstract": "The attentional mechanism has proven to be effective in improving end-to-end neural machine translation.", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "1956c239b3552e030db1b78951f64781101125ed", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "62c0ac04b5b4e9e67efb0027c983eb0f211671d0", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "62c0ac04b5b4e9e67efb0027c983eb0f211671d0", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "22358c1e6f371db45a0d237baff6052e0a50e498", "title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model", "authors": ["Shi Feng", "Shujie Liu", "Ming Zhou"], "date": "2016", "abstract": "Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and… ", "references": ["d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "e27d81521dc4e8b6ea93947c05ffccf06784f569", "8a756d4d25511d92a45d0f4545fa819de993851d", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "8a756d4d25511d92a45d0f4545fa819de993851d", "e27d81521dc4e8b6ea93947c05ffccf06784f569", "cea967b59209c6be22829699f05b8b1ac4dc092d", "8a756d4d25511d92a45d0f4545fa819de993851d", "8a756d4d25511d92a45d0f4545fa819de993851d"]},{"id": "8948bea1e2436e51316f131170923cb5b7d870db", "title": "Learning with Hierarchical-Deep Models", "authors": ["Ruslan Salakhutdinov", "Joshua B. Tenenbaum", "Antonio Torralba"], "date": "2013", "abstract": "We introduce HD (or “Hierarchical-Deep”) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that… ", "references": ["05fd1da7b2e34f86ec7f010bef068717ae964332", "100a038fdf29b4b20801887f0ec40e3f10d9a4f9", "1e80f755bcbf10479afd2338cec05211fdbd325c", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "05fd1da7b2e34f86ec7f010bef068717ae964332", "05fd1da7b2e34f86ec7f010bef068717ae964332", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "1e80f755bcbf10479afd2338cec05211fdbd325c", "843959ffdccf31c6694d135fad07425924f785b1"]},{"id": "3c20df69865df6a627cc45c524869ccc0297048f", "title": "Learning with Marginalized Corrupted Features", "authors": ["Laurens van der Maaten", "Minmin Chen", "Kilian Q. Weinberger"], "date": "ICML", "abstract": "The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples--which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential… ", "references": ["c3ecd8e19e016d15670c8953b4b9afaa5186b0f3", "843959ffdccf31c6694d135fad07425924f785b1", "c5ee421735abee2669a687dd8cad95376a4b7fee", "fcba51774867c77f491581d3625d375a0a8f473b", "be9a17321537d9289875fe475b71f4821457b435", "fcba51774867c77f491581d3625d375a0a8f473b", "c3ecd8e19e016d15670c8953b4b9afaa5186b0f3", "5ac5fbae8a7faf2e9bd49ad01106cec4a2d8f20a", "c5ee421735abee2669a687dd8cad95376a4b7fee", "3554953715a4d7af3d4c9201d4080899b84fbad7"]},{"id": "ada937c9f51316c6ac87f9d1d4509383d23e0c21", "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "authors": ["Trevor Cohn", "Cong Duy Vu Hoang", "Gholamreza Haffari"], "date": "2016", "abstract": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show… ", "references": ["4d8f2d14af5991d4f0d050d22216825cac3157bd", "93499a7c7f699b6630a86fad964536f9423bb6d0", "60cce28d1f56786930e86e5798d55e4a7948b0da", "9bd6cdae71506eb307507e44df7abe0c285b3ca7", "9bd6cdae71506eb307507e44df7abe0c285b3ca7", "cea967b59209c6be22829699f05b8b1ac4dc092d", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "60cce28d1f56786930e86e5798d55e4a7948b0da", "60cce28d1f56786930e86e5798d55e4a7948b0da"]},{"id": "9f2a8e923965b23c11066a2ead79658208f1fae1", "title": "Minimum Risk Training for Neural Machine Translation", "authors": ["Shiqi Shen", "Yong Cheng", "Yang Liu"], "date": "2016", "abstract": "We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to… ", "references": ["f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "f91b17e852774d80f4d11b9c7f5b99b1dd8aacf7", "25a4fb7025453ce73feef36eeaa45dbd0eb215e5", "1f12451245667a85d0ee225a80880fc93c71cc8b", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "25a4fb7025453ce73feef36eeaa45dbd0eb215e5"]},{"id": "7fc190b8f610e0168ab648c5544d3154b270d58c", "title": "Efficient inference in occlusion-aware generative models of images", "authors": ["Jonathan Huang", "Kevin Murphy"], "date": "2015", "abstract": "We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image --- and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a… ", "references": ["ae0ba9000628263f0f883fce3411fa435c8a9e53", "b40e631a1988ee4f70400d3830ecaa462414d058", "b40e631a1988ee4f70400d3830ecaa462414d058", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "687e80eb70c7bbad6001006d9269b202650a3354", "489d6e4cc55c6eb945f12d2813e06cb482294d06", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "9eb50914621312edf12fc96ee6d2fbe501388f67", "b40e631a1988ee4f70400d3830ecaa462414d058"]},{"id": "3e47c4c2dd98c49b7771c7228812d5fd9eee56a3", "title": "Importance Weighted Autoencoders", "authors": ["Yuri Burda", "Roger B. Grosse", "Ruslan Salakhutdinov"], "date": "2016", "abstract": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference.", "references": ["018300f5f0e679cee5241d9c69c8d88e00e8bf31", "c8b509be29721ee6b12c880b4d97ed6b60bad217", "018300f5f0e679cee5241d9c69c8d88e00e8bf31", "4e2f6b4bc889eed1afe5833d5190f6f02e501061", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "018300f5f0e679cee5241d9c69c8d88e00e8bf31", "08d0ea90b53aba0008d25811268fe46562cfb38c", "4e2f6b4bc889eed1afe5833d5190f6f02e501061", "7a24ec97e7f2881e245d20c46a56cbbfc734a4ff", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864"]},{"id": "6d72a0e83e772468c6084ae7c79e43a4f5989feb", "title": "A Local Learning Algorithm for Dynamic Feedforward and Recurrent Networks", "authors": ["Jürgen Schmidhuber"], "date": "1989", "abstract": "Abstract Most known learning algorithms for dynamic neural networks in non-stationary environments need global computations to perform credit assignment. These algorithms either are not local in time or not local in space. Those algorithms which are local in both time and space usually cannot deal sensibly with ‘hidden units’. In contrast, as far as we can judge, learning rules in biological systems with many ‘hidden units’ are local in both space and time. In this paper we propose a parallel… ", "references": []},{"id": "484ad17c926292fbe0d5211540832a8c8a8e958b", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "date": "ICML", "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.", "references": ["0ae2e4e974f7ee57f590a691aada75c27c4c5394", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "0ae2e4e974f7ee57f590a691aada75c27c4c5394", "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3", "00cf63a7926a826f7cf73c1d5edb117f98d70c2c", "32f078a7478d1ec2169599500a4507aceaccdda7", "5a9ef216bf11f222438fff130c778267d39a9564", "0ae2e4e974f7ee57f590a691aada75c27c4c5394", "0ae2e4e974f7ee57f590a691aada75c27c4c5394"]},{"id": "2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1", "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models", "authors": ["S. M. Ali Eslami", "Nicolas Manfred Otto Heess", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D… ", "references": ["a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "7fc190b8f610e0168ab648c5544d3154b270d58c", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "8a756d4d25511d92a45d0f4545fa819de993851d", "687e80eb70c7bbad6001006d9269b202650a3354", "7fc190b8f610e0168ab648c5544d3154b270d58c", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "fbd616a659e8412ba37f1bd54cfe8ed543a35eb6", "5c306ce578a8da634a4a64fce282a48d0eacfda1", "5f5dc5b9a2ba710937e2c413b37b053cd673df02"]},{"id": "32e97eef94beacace020e79322cef0e1e5a76ee0", "title": "Gradient calculations for dynamic recurrent neural networks: a survey", "authors": ["Barak A. Pearlmutter"], "date": "1995", "abstract": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also… ", "references": ["54e34d0053b71d78cec26e8c29f57a3b9e85de49", "006c42929dcd480490fdb367fd7478b2956dbc99", "54e34d0053b71d78cec26e8c29f57a3b9e85de49", "cccd3fd7a45e7643f26391bd539ffbede0690f36", "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "3a0de0ad4bf796e2506080d508f83205cf8d76fc", "976e3bb77b343d08f68063be5db2c1352458dbb1", "976e3bb77b343d08f68063be5db2c1352458dbb1", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "54e34d0053b71d78cec26e8c29f57a3b9e85de49"]},{"id": "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "title": "Auto-Encoding Variational Bayes", "authors": ["Diederik P. Kingma", "Max Welling"], "date": "2014", "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets.", "references": ["f87247fb37f6b48da0757d7a1acf38da44510cdb", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "f87247fb37f6b48da0757d7a1acf38da44510cdb", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "6a667700100e228cb30a5d884258a0db921603fe", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "bccb2f99a9d1c105699f5d88c479569085e2c7ba", "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3", "6a667700100e228cb30a5d884258a0db921603fe"]},{"id": "50c770b425a5bb25c77387f687a9910a9d130722", "title": "Learning Complex, Extended Sequences Using the Principle of History Compression", "authors": ["Jürgen Schmidhuber"], "date": "1992", "abstract": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe… ", "references": ["f8620fb17d7e0e41c44c1e87fe3693daad0d30bd", "26bc0449360d7016f684eafae5b5d2feded32041", "85597bc9c4724ad11f6d6d04627afab0ada2ecd7"]},{"id": "d0be39ee052d246ae99c082a565aba25b811be2d", "title": "Learning long-term dependencies with gradient descent is difficult", "authors": ["Yoshua Bengio", "Patrice Y. Simard", "Paolo Frasconi"], "date": "1994", "abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These… ", "references": ["d6deb1ddc764259fbdc7733ef80473081bff31d5", "d6deb1ddc764259fbdc7733ef80473081bff31d5", "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "d6deb1ddc764259fbdc7733ef80473081bff31d5", "e141d68065ce638f9fc4f006eab2f66711e89768", "fee5cc60c185e5d2942fd925bbde612f368ea7ef", "9a607334c1d963b4af29676578e1ef6aa11ba6e7", "e141d68065ce638f9fc4f006eab2f66711e89768"]},{"id": "2f7c4048a03281e976f28d35c2f9fef3a58346e6", "title": "Learning Unambiguous Reduced Sequence Descriptions", "authors": ["Jürgen Schmidhuber"], "date": "NIPS", "abstract": "Do you want your neural net algorithm to learn sequences? Do not limit yourself to conventional gradient descent (or approximations thereof). Instead, use your sequence learning algorithm (any will do) to implement the following method for history compression. No matter what your final goals are, train a network to predict its next input from the previous ones. Since only unpredictable inputs convey new information, ignore all predictable inputs but let all unexpected inputs (plus information… ", "references": ["50c770b425a5bb25c77387f687a9910a9d130722", "26bc0449360d7016f684eafae5b5d2feded32041", "424710825d726e10b016204ed2bc979e2a342d10", "424710825d726e10b016204ed2bc979e2a342d10", "26bc0449360d7016f684eafae5b5d2feded32041", "424710825d726e10b016204ed2bc979e2a342d10", "26bc0449360d7016f684eafae5b5d2feded32041", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "2245d7893aeebb06e9bbd990a1d21d4608951f5e", "26bc0449360d7016f684eafae5b5d2feded32041"]},{"id": "e141d68065ce638f9fc4f006eab2f66711e89768", "title": "Induction of Multiscale Temporal Structure", "authors": ["Michael C. Mozer"], "date": "NIPS", "abstract": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music… ", "references": []},{"id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "title": "Generalization of backpropagation with application to a recurrent gas market model", "authors": ["Paul J. Werbos"], "date": "1988", "abstract": "Abstract Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to… ", "references": ["9c790c8e2d3bc565d59a91600dac0d8b4d1eedc4"]},{"id": "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4", "title": "Learning Sequential Structure with the Real-Time Recurrent Learning Algorithm", "authors": ["Anthony W. Smith", "David Zipser"], "date": "1989", "abstract": "Recurrent connections in neural networks potentially allow information about events occurring in the past to be preserved and used in current computations. How effectively this potential is realized depends on the power of the learning algorithm used. As an example of a task requiring recurrency, Servan-Schreiber, Cleeremans, and McClelland1 have applied a simple recurrent learning algorithm to the task of recognizing finite-state grammars of increasing difficulty. These nets showed… ", "references": []},{"id": "f6e91c9e7e8f8a577a98ecfcfa998212a683195a", "title": "Learning long-term dependencies in NARX recurrent neural networks", "authors": ["Tsungnan Lin", "Bill G. Horne", "C. Lee Giles"], "date": "1996", "abstract": "It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We… ", "references": ["e141d68065ce638f9fc4f006eab2f66711e89768", "06778bd87125a28f0d045e0221ca1b8ad1d469b6", "d0be39ee052d246ae99c082a565aba25b811be2d", "59f884480d293672213ca315beec332943b64434", "9a607334c1d963b4af29676578e1ef6aa11ba6e7", "9a607334c1d963b4af29676578e1ef6aa11ba6e7", "f038237446e79e02d2960b41d61853f23737a7ac", "06778bd87125a28f0d045e0221ca1b8ad1d469b6", "e141d68065ce638f9fc4f006eab2f66711e89768", "f038237446e79e02d2960b41d61853f23737a7ac"]},{"id": "e27d81521dc4e8b6ea93947c05ffccf06784f569", "title": "Audio Chord Recognition with Recurrent Neural Networks", "authors": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "date": "ISMIR", "abstract": "In this paper, we present an audio chord recognition system based on a recurrent neural network.", "references": ["18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "99c3d399b14b2d3c8375706b048aedba46515a32", "86e951e190586b84c530f9f03504f9ad70cc650a", "365b3ff789a3433b1b66e7a257d9721e018734fb", "4de9647f1102dbe3718a9534fd51121d97389e78", "99c3d399b14b2d3c8375706b048aedba46515a32", "805aee7f3b50856df85e13e67b3034ac7ed23824", "805aee7f3b50856df85e13e67b3034ac7ed23824", "4de9647f1102dbe3718a9534fd51121d97389e78", "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8"]},{"id": "89eca547b1a2f6208ae529d45a65a51cf49adbff", "title": "BLEU Deconstructed: Designing a Better MT Evaluation Metric", "authors": ["Xingyi Song", "Trevor Cohn", "Lucia Specia"], "date": "2013", "abstract": "BLEU is the de facto standard automatic evaluation metric in machine translation. While BLEU is undeniably useful, it has a number of limitations. Although it works well for large documents and multiple references, it is unreliable at the sentence or sub-sentence levels, and with a single reference. In this paper, we propose new variants of BLEU which address these limitations, resulting in a more flexible metric which is not only more reliable, but also allows for more accurate discriminative… ", "references": ["acc4a0dc5f3a6c30e46f98b53fd688b3c9797aaa", "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5", "5dbba694b86cc3f18d2ce6045ada71e648f66d71", "5dbba694b86cc3f18d2ce6045ada71e648f66d71", "5dbba694b86cc3f18d2ce6045ada71e648f66d71", "be9bca1e9b0192fc49b316f2701242b50d98d456", "d7da009f457917aa381619facfa5ffae9329a6e9", "da6918ed87095d1313bd20606a934f899d4084b0", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7"]},{"id": "815c84ab906e43f3e6322f2ca3fd5e1360c64285", "title": "Human-level concept learning through probabilistic program induction", "authors": ["Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum"], "date": "2015", "abstract": "People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy.", "references": ["22e9f9e34055c66917a321cf8c9ed222cce82770"]},{"id": "89b1f4740ae37fd04f6ac007577bdd34621f0861", "title": "Generating Sequences With Recurrent Neural Networks", "authors": ["Alex Graves"], "date": "2013", "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time.", "references": ["1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "047655e733a9eed9a500afd916efa566915b9110", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "047655e733a9eed9a500afd916efa566915b9110", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "047655e733a9eed9a500afd916efa566915b9110"]},{"id": "c3c82b476162d2d006e02180530875a64af18154", "title": "Hardware accelerated convolutional neural networks for synthetic vision systems", "authors": ["Clément Farabet", "Berin Martini", "Eugenio Culurciello"], "date": "2010", "abstract": "In this paper we present a scalable hardware architecture to implement large-scale convolutional neural networks and state-of-the-art multi-layered artificial vision systems. This system is fully digital and is a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images. We present a performance comparison between a software, FPGA and ASIC implementation that shows a speed up in custom hardware implementations. ", "references": ["b8bc656a1935f07e894833b608cc4671b9fa828f", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "f354310098e09c1e1dc88758fca36767fd9d084d", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "162d958ff885f1462aeda91cd72582323fd6a1f4", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "f354310098e09c1e1dc88758fca36767fd9d084d", "f354310098e09c1e1dc88758fca36767fd9d084d", "f354310098e09c1e1dc88758fca36767fd9d084d"]},{"id": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition", "authors": ["Yann LeCun", "Léon Bottou", "Patrick Haffner"], "date": "1998", "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique.", "references": ["3aa4c691289f56f9af6cf543633cfb3917274281", "3aa4c691289f56f9af6cf543633cfb3917274281", "847d6ece37d22430a0d9e061b5dc1d1b8c679055", "86890c82b589e24007c56e1f40c5f928a0e04183", "0bdbc5a8a5ccb718007f0c1999ea7546deb0b473", "92a9311686e48d5d20fbfcdc21362251b121096c", "32315b101afe9bacb725cf80944884e7ac053245", "3aa4c691289f56f9af6cf543633cfb3917274281", "0bdbc5a8a5ccb718007f0c1999ea7546deb0b473", "0bdbc5a8a5ccb718007f0c1999ea7546deb0b473"]},{"id": "ebbd9e5fbc9c663d9dd60a08e1c3a09b15e65278", "title": "Forest Rescoring: Faster Decoding with Integrated Language Models", "authors": ["Liang Huang", "David Chiang"], "date": "ACL", "abstract": "Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the… ", "references": ["0b8f0e60a648880ddeaed371c339714f66f24624", "ad3d2f463916784d0c14a19936c1544309a0a440", "0b8f0e60a648880ddeaed371c339714f66f24624", "c6a83c4fcc99ba6753109301949c5b7cfa978079", "c6a83c4fcc99ba6753109301949c5b7cfa978079", "7e982f360b44094552264010781a476d85ac78a7", "d01737b617acc555153f4660417908bf3971b1a5", "53b4aaf51c6d1c164b19e8f9df5cfde560eeb6a7", "2a9d6137c95cc7c106bf84f18fb13449eded7826", "0b8f0e60a648880ddeaed371c339714f66f24624"]},{"id": "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "title": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition", "authors": ["Dominik Scherer", "Andreas C. Müller", "Sven Behnke"], "date": "ICANN", "abstract": "A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms… ", "references": ["ccd52aff02b0f902f4ce7247c4fee7273014c41c", "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "f9e65fcb0e04174577f211d702d3f837e3624c5b", "0dadb25842ef596a0f676c04cbd3dad4e1876964", "f354310098e09c1e1dc88758fca36767fd9d084d", "0dadb25842ef596a0f676c04cbd3dad4e1876964", "0dadb25842ef596a0f676c04cbd3dad4e1876964", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c"]},{"id": "0894b06cff1cd0903574acaa7fcf071b144ae775", "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", "authors": ["Jacob Devlin", "Rabih Zbib", "John Makhoul"], "date": "ACL", "abstract": "Recent work has shown success in using neural network language models (NNLMs) as features in MT systems.", "references": ["83b3eabf8780cc3ca4058d60eb11ca407fb12810", "5f08df805f14baa826dbddcb002277b15d3f1556", "9819b600a828a57e1cde047bbe710d3446b30da5", "71480da09af638260801af1db8eff6acb4e1122f", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "83b3eabf8780cc3ca4058d60eb11ca407fb12810", "5f08df805f14baa826dbddcb002277b15d3f1556", "c36d355e01e1ed90e57bffbbfc274d4d98952b96", "83b3eabf8780cc3ca4058d60eb11ca407fb12810"]},{"id": "fb4e8945040ff372be0bff598b1b8a9676b8e9f6", "title": "A novel dependency-to-string model for statistical machine translation", "authors": ["Jun Xie", "Haitao Mi", "Qun Liu"], "date": "EMNLP", "abstract": "Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents… ", "references": ["248d32911670e551db4835a5a5279d2d9673ee37", "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "7e982f360b44094552264010781a476d85ac78a7", "bbb31e16733c0e71b01baa11001738ab616c2393", "ebbd9e5fbc9c663d9dd60a08e1c3a09b15e65278", "bbb31e16733c0e71b01baa11001738ab616c2393", "ad3d2f463916784d0c14a19936c1544309a0a440", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "1f12451245667a85d0ee225a80880fc93c71cc8b", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d"]},{"id": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "title": "A Convolutional Neural Network for Modelling Sentences", "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "date": "ACL", "abstract": "The ability to accurately represent sentences is central to language understanding.", "references": ["dac72f2c509aee67524d3321f77e97e8eff51de6", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "dac72f2c509aee67524d3321f77e97e8eff51de6", "5f08df805f14baa826dbddcb002277b15d3f1556", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "dac72f2c509aee67524d3321f77e97e8eff51de6", "57458bc1cffe5caa45a885af986d70f723f406b4", "cd96a6e0b6bb099c515be8770764d2fd18e7b878", "57458bc1cffe5caa45a885af986d70f723f406b4", "dac72f2c509aee67524d3321f77e97e8eff51de6"]},{"id": "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "title": "Extensions of recurrent neural network language model", "authors": ["Stefan Kombrink", "Sanjeev Khudanpur"], "date": "2011", "abstract": "We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity.", "references": ["9819b600a828a57e1cde047bbe710d3446b30da5", "9819b600a828a57e1cde047bbe710d3446b30da5", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "9819b600a828a57e1cde047bbe710d3446b30da5", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "8a0e2e4dd10e1be7fd28474504cbdaa1237e4926", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "8a0e2e4dd10e1be7fd28474504cbdaa1237e4926", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "a73ae2ce1cfafae61238b3fdb1bbb61093962b4d"]},{"id": "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "title": "Deep Belief Networks for phone recognition", "authors": ["Abdel-rahman Mohamed", "George Dahl", "Geoffrey E. Hinton"], "date": "2009", "abstract": "Hidden Markov Models (HMMs) have been the state-of-the-art techniques for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. There are many proposals in the research community for deeper models that are capable of modeling the many types of variability present in the speech generation p r cess. Deep Belief Networks (DBNs) have recently proved to be very effective fo r a variety of machine learning problems… ", "references": ["71874b310bd6fff855086956bd5a1e7eb56d1739", "182c9ba291d97dc8d7482533044416869cb15f23", "71874b310bd6fff855086956bd5a1e7eb56d1739", "62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417", "d45b8dcc1f929a43f4dae4dbd69a12d163aa8ed8", "c427ef90813a3f324212d66ff0d02e6a49706ece", "71874b310bd6fff855086956bd5a1e7eb56d1739", "182c9ba291d97dc8d7482533044416869cb15f23", "182c9ba291d97dc8d7482533044416869cb15f23", "df5b82595a29724467a98eed4d7e2a45e804579e"]},{"id": "8b395470a57c48d174c4216ea21a7a58bc046917", "title": "Training Neural Network Language Models on Very Large Corpora", "authors": ["Holger Schwenk", "Jean-Luc Gauvain"], "date": "HLT/EMNLP", "abstract": "During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language… ", "references": ["a493a23b86192aa74e6f394061288082e1e7cdb7", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "e41498c05d4c68e4750fb84a380317a112d97b01", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "399da68d3b97218b6c80262df7963baa89dcc71b", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "8592a744942cabc7596c90dd6a4e13bdd233b677", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "e41498c05d4c68e4750fb84a380317a112d97b01"]},{"id": "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "title": "Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription", "authors": ["Frank Seide", "Gang Li", "Dong Yu"], "date": "2011", "abstract": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third—from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%—using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden… ", "references": ["7f58b2140534a391c2decb4ab09ab4cecdb548a4", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "7f58b2140534a391c2decb4ab09ab4cecdb548a4", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "c256a54a5f3f07a6dcf2dea3a220d0024cf3bfe5", "4e3ba28fb3493afd2c3db4bd8be6d8d41cf3647a", "473f0739666af2791ad6592822118240ed968b70", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656"]},{"id": "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "title": "Continuous space language models", "authors": ["Holger Schwenk"], "date": "2007", "abstract": "This paper describes the use of a neural network language model for large vocabulary continuous speech recognition.", "references": ["be1fed9544830df1137e72b1d2396c40d3e18365", "ed22171ce376d213bf64d998d594f876f6912cb7", "be1fed9544830df1137e72b1d2396c40d3e18365", "1ac8987534ad3be87d4b70195c1beb039b102409", "bbc659924151c767f907de9b412d9b7619d26690", "4ccfd99d94047c590899e19d4886d716fd1172c0", "bbc659924151c767f907de9b412d9b7619d26690", "bbc659924151c767f907de9b412d9b7619d26690", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "e41498c05d4c68e4750fb84a380317a112d97b01"]},{"id": "cb45e9217fe323fbc199d820e7735488fca2a9b3", "title": "Strategies for training large scale neural network language models", "authors": ["Anoop Deoras", "Jan Černocký"], "date": "2011", "abstract": "We describe how to effectively train neural network based language models on large data sets.", "references": ["3b56693f6fe6b82092c4adc756f20fb9b7710ac5", "a1a4d2f3621ce7c56d7c396bb59f4eaf0e2b14fa", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "26180488dac9be0d26eba8ab5e3cd9a0ba5213be", "a1a4d2f3621ce7c56d7c396bb59f4eaf0e2b14fa", "8b395470a57c48d174c4216ea21a7a58bc046917", "77dfe038a9bdab27c4505444931eaa976e9ec667", "3b56693f6fe6b82092c4adc756f20fb9b7710ac5", "bfab4ffa229c8af0174a683ff1eda524c4f59d00", "8b395470a57c48d174c4216ea21a7a58bc046917"]},{"id": "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "title": "Structured Output Layer neural network language model", "authors": ["Hai Son Le", "Ilya Oparin", "François Yvon"], "date": "2011", "abstract": "This paper introduces a new neural network language model (NNLM) based on word clustering to structure the output vocabulary: Structured Output Layer NNLM. This model is able to handle vocabularies of arbitrary size, hence dispensing with the design of short-lists that are commonly used in NNLMs. Several softmax layers replace the standard output layer in this model. The output structure depends on the word clustering which uses the continuous word representation induced by a NNLM. The GALE… ", "references": ["e41498c05d4c68e4750fb84a380317a112d97b01", "e41498c05d4c68e4750fb84a380317a112d97b01", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "ed23c461535afc492e80c63ee8d1ed55b8a176e1", "84b533115bfe031aaef4722bae8f54e1a39db01e", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "6769e78616bc3a332a1829d1a3c2220e5b94555d", "84b533115bfe031aaef4722bae8f54e1a39db01e"]},{"id": "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "title": "A Scalable Hierarchical Distributed Language Model", "authors": ["Andriy Mnih", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart… ", "references": ["e41498c05d4c68e4750fb84a380317a112d97b01", "bf32a271a17c9c3376127d287f746e4876779d49", "e41498c05d4c68e4750fb84a380317a112d97b01", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "3d6036af971c1f11ab712cc41487376a94e63673", "bf32a271a17c9c3376127d287f746e4876779d49"]},{"id": "52805ca2a7f5f6e73dc90ff20f1ca2f198dd031b", "title": "Measuring Word Alignment Quality for Statistical Machine Translation", "authors": ["A. Fraser", "Daniel Marcu"], "date": "2007", "abstract": "Automatic word alignment plays a critical role in statistical machine translation. Unfortunately, the relationship between alignment quality and statistical machine translation performance has not been well understood. In the recent literature, the alignment task has frequently been decoupled from the translation task and assumptions have been made about measuring alignment quality for machine translation which, it turns out, are not justified. In particular, none of the tens of papers… ", "references": ["8606671e9036a58c9e3fd96f8ed161edd4536e47", "38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1", "6c8f9b2b61e49c43e4639b3c1ce68f993fc2aa91", "6c8f9b2b61e49c43e4639b3c1ce68f993fc2aa91", "8606671e9036a58c9e3fd96f8ed161edd4536e47", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "94d6f1910680b46b44dd03de78b892e158381fa5", "38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1", "1f12451245667a85d0ee225a80880fc93c71cc8b", "cee30e5fe700b98bc408bc40ea9ec396520b473a"]},{"id": "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "title": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation", "authors": ["Holger Schwenk", "Anthony Rousseau", "Mohammed Attik"], "date": "WLM@NAACL-HLT", "abstract": "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity… ", "references": ["47e3d8a1f8e92923e739ca34bea17004a40514e9", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "e41498c05d4c68e4750fb84a380317a112d97b01", "f76b28565a0c677f14f802dca10d8d0db09a6cc3", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "3b56693f6fe6b82092c4adc756f20fb9b7710ac5", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "f76b28565a0c677f14f802dca10d8d0db09a6cc3"]},{"id": "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "title": "Continuous Space Translation Models with Neural Networks", "authors": ["Hai Son Le", "Alexandre Allauzen", "François Yvon"], "date": "HLT-NAACL", "abstract": "The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units… ", "references": ["057cb927b59c7cc16141fca2c825da1e3e3ef81a", "231a173bf9e0474c8eef8184d014507021ab2434", "0aee499c099b74d4bb2c860ab044a54becd08a4b", "057cb927b59c7cc16141fca2c825da1e3e3ef81a", "113c32635f3b83fbbc1c7af720dfc68d37a37848", "c36d355e01e1ed90e57bffbbfc274d4d98952b96", "231a173bf9e0474c8eef8184d014507021ab2434", "f73126f276c4cd1566d99bef9d996a2a6050a130", "f73126f276c4cd1566d99bef9d996a2a6050a130", "0aee499c099b74d4bb2c860ab044a54becd08a4b"]},{"id": "47e3d8a1f8e92923e739ca34bea17004a40514e9", "title": "Training Continuous Space Language Models: Some Practical Issues", "authors": ["Hai Son Le", "Alexandre Allauzen", "François Yvon"], "date": "EMNLP", "abstract": "Using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling, with applications in speech recognition and statistical machine translation. However, training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available. In this work, we study the performance and behavior of two neural statistical language models so as to highlight… ", "references": ["24653b3b33d48c409deb672f8d8ee0eff31cd418", "26080f0969a520ebd82f252dd060f5a4948bbd6e", "e41498c05d4c68e4750fb84a380317a112d97b01", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "bd7d93193aad6c4b71cc8942e808753019e87706", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "1f12451245667a85d0ee225a80880fc93c71cc8b", "57458bc1cffe5caa45a885af986d70f723f406b4", "e41498c05d4c68e4750fb84a380317a112d97b01", "1f12451245667a85d0ee225a80880fc93c71cc8b"]},{"id": "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "authors": ["Sébastien Jean", "Kyunghyun Cho", "Yoshua Bengio"], "date": "2015", "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on… ", "references": ["b3e89f05876d47b9bd6ece225aaeee457a6824e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "b3e89f05876d47b9bd6ece225aaeee457a6824e8", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "53ca064b9f1b92951c1997e90b776e95b0880e52", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7"]},{"id": "1f12451245667a85d0ee225a80880fc93c71cc8b", "title": "Minimum Error Rate Training in Statistical Machine Translation", "authors": ["Franz Josef Och"], "date": "ACL", "abstract": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.", "references": ["82e3794a2f7de37d60602681a25eef7711ec8ab8", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "fd0101dfbdd6768efe1e99a5ccd3ec0415fe723f", "e653891a9a9ac5585abb3348732d7e5a8f4e686c", "bbf24db4c7e1112188f931f0751944e8ebef68a0", "fd0101dfbdd6768efe1e99a5ccd3ec0415fe723f", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "d7da009f457917aa381619facfa5ffae9329a6e9", "dd85cca2c133835ea29069a6a4438c70185bd427"]},{"id": "e923e85b2d491c37bac08258deff0af485ab71b9", "title": "Discriminative Corpus Weight Estimation for Machine Translation", "authors": ["Spyridon Matsoukas", "Antti-Veikko I. Rosti", "Bing Zhang"], "date": "EMNLP", "abstract": "Current statistical machine translation (SMT) systems are trained on sentence-aligned and word-aligned parallel text collected from various sources. Translation model parameters are estimated from the word alignments, and the quality of the translations on a given test set depends on the parameter estimates. There are at least two factors affecting the parameter estimation: domain match and training data quality. This paper describes a novel approach for automatically detecting and down… ", "references": ["7e6d09530490561f1fc4dbfbd82fc4ff456f046c", "ade34bf617f7733bfa0676f2bd57fa5658d4e54c", "b281a9d0c728979f7ffccba1a61d0fc1d29530c1", "7e6d09530490561f1fc4dbfbd82fc4ff456f046c", "248d32911670e551db4835a5a5279d2d9673ee37", "81164d7f2d676b6044888219426cbb248a020930", "7e6d09530490561f1fc4dbfbd82fc4ff456f046c", "e7b1e437701b58081102a6799e95791d6405ff53", "b281a9d0c728979f7ffccba1a61d0fc1d29530c1", "51951073580f6995e55be873db9a7f6a9736ca86"]},{"id": "8a601640a709eb96e65d130ef8aac69e9ea3602d", "title": "Toward a unified approach to statistical language modeling for Chinese", "authors": ["Jianfeng Gao", "Joshua Goodman", "Kai-Fu Lee"], "date": "2002", "abstract": "This article presents a unified approach to Chinese statistical language modeling (SLM). Applying SLM techniques like trigram language models to Chinese is challenging because (1) there is no standard definition of words in Chinese; (2) word boundaries are not marked by spaces; and (3) there is a dearth of training data. Our unified approach automatically and consistently gathers a high-quality training data set from the Web, creates a high-quality lexicon, segments the training data using this… ", "references": ["974a522a4b09f65df68e1e0025d68f1c896e457c", "0f2a387a69a424f1e10998831dedd81abc2460c2", "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f", "29053eab305c2b585bcfbb713243b05646e7d62d", "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f", "29053eab305c2b585bcfbb713243b05646e7d62d", "974a522a4b09f65df68e1e0025d68f1c896e457c", "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f", "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f", "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f"]},{"id": "70600593f870f460624c56c2a57b9a03b94f94a5", "title": "An Autoencoder Approach to Learning Bilingual Word Representations", "authors": ["A. P. Sarath Chandar", "Stanislas Lauly", "Amrita Saha"], "date": "NIPS", "abstract": "Cross-language learning allows one to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are coherent between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of… ", "references": ["327c88dd06722a967be9c6b1176fbd79554967e7", "d1f37d9cab68eb8cda669cc949394732f33264b4", "d1f37d9cab68eb8cda669cc949394732f33264b4", "0157dcd6122c20b5afc359a799b2043453471f7f", "4b75d707eb3ffe4607c8cdd5436c8d7f8573fed9", "d1f37d9cab68eb8cda669cc949394732f33264b4", "26a6534f18879926ab0a921e6e93e246262e9066", "d1f37d9cab68eb8cda669cc949394732f33264b4", "343733a063e491d234a36d3e1090a739318b3566", "c19fbefdeead6a4154a22a9c8551a18b1530033a"]},{"id": "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "title": "Mixture-Model Adaptation for SMT", "authors": ["George F. Foster", "Roland Kuhn"], "date": "WMT@ACL", "abstract": "We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one… ", "references": ["404fffebcdb9b597489f62735d8ce59eff41f623", "0c67397ac4c61fd1b702d52a7bb6d42a5bf8d7aa", "6bbcfbd6d15ca72b7e5ef825b7ed8101da4798d8", "606df60d518db088986e74fad1f357ea6e5312f2", "d3e5c17d9a45b00d1923d256f3fd607553ce0231", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "0c67397ac4c61fd1b702d52a7bb6d42a5bf8d7aa", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "1f12451245667a85d0ee225a80880fc93c71cc8b", "d3e5c17d9a45b00d1923d256f3fd607553ce0231"]},{"id": "9b53e2a30f070a3bef3ca17a1872a70acfe478f9", "title": "Improving English-Spanish Statistical Machine Translation: Experiments in Domain Adaptation, Sentence Paraphrasing, Tokenization, and Recasing", "authors": ["Preslav Nakov"], "date": "WMT@ACL", "abstract": "We describe the experiments of the UC Berkeley team on improving English-Spanish machine translation of news text, as part of the WMT'08 Shared Translation Task. We experiment with domain adaptation, combining a small in-domain news bi-text and a large out-of-domain one from the Europarl corpus, building two separate phrase translation models and two separate language models. We further add a third phrase translation model trained on a version of the news bi-text augmented with monolingual… ", "references": ["d7da009f457917aa381619facfa5ffae9329a6e9", "5b9927bd8322b474360d364f295d56713f370485", "5b9927bd8322b474360d364f295d56713f370485", "195df0de3c4c181d26391dd73746c7aefe709ab6"]},{"id": "4d8f2d14af5991d4f0d050d22216825cac3157bd", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "authors": ["Kelvin Xu", "Jimmy Ba", "Yoshua Bengio"], "date": "2015", "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images.", "references": ["ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "5cb6700d94c6118ee13f4f4fecac99f111189812", "0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "5cb6700d94c6118ee13f4f4fecac99f111189812", "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "8a756d4d25511d92a45d0f4545fa819de993851d", "0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3"]},{"id": "3a7a536a17bd19587c05f3f8d1f4571c13388e9c", "title": "Application of Translation Knowledge Acquired by Hierarchical Phrase Alignment for Pattern-based MT", "authors": ["Kenji Imamura"], "date": "2002", "abstract": "Hierarchical phrase alignment is a method for extracting equivalent phrases from bilingual sentences, even though they belong to different language families. The method automatically extracts transfer knowledge from about 125K English and Japanese bilingual sentences and then applies it to a pattern-based MT system. The translation quality is then evaluated. The knowledge needs to be cleaned, since the corpus contains various translations and the phrase alignment contains errors. Various… ", "references": ["ca4497eafa13eca9df90f8de582efadc3b8c9d09", "38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1", "ca4497eafa13eca9df90f8de582efadc3b8c9d09", "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "ca4497eafa13eca9df90f8de582efadc3b8c9d09", "44254390973b3d3e6a98f53f1da961addb4d352b", "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "34c71b6177f2daf1d1d726dd05941a545a149928", "f30a129113961242c6279436d60df17e9043ad08", "34c71b6177f2daf1d1d726dd05941a545a149928"]},{"id": "d7b2656177ed5a35ee1d1dc7f1fa57da54ed14d6", "title": "Factored Language Models for Statistical Machine Translation", "authors": ["Amittai Axelrod"], "date": "2006", "abstract": "Machine translation systems, as a whole, are currently not able to use the output of linguistic tools, such as part-of-speech taggers, to effectively improve translation performance. However, a new language modeling technique, Factored Language Models can incorporate the additional linguistic information that is produced by these tools. In the field of automatic speech recognition, Factored Language Models smoothed with Generalized Parallel Backoff have been shown to significantly reduce… ", "references": ["c20b4068be640ebaffbc56382c3e4e0bcf62664e", "ad3d2f463916784d0c14a19936c1544309a0a440", "f3402fafad543cb094cd2707a73f07119f6125c9", "c20b4068be640ebaffbc56382c3e4e0bcf62664e", "6a5f9307b8b8473b233432b0e8aa0b4bef311996", "ad3d2f463916784d0c14a19936c1544309a0a440", "6a5f9307b8b8473b233432b0e8aa0b4bef311996", "399da68d3b97218b6c80262df7963baa89dcc71b", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "c20b4068be640ebaffbc56382c3e4e0bcf62664e"]},{"id": "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "title": "Improved Alignment Models for Statistical Machine Translation", "authors": ["Franz Josef Och", "Christoph Tillmann", "Hermann Ney"], "date": "EMNLP", "abstract": "In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words… ", "references": ["a19ceb1281b84d96abba03e973ba7274a8f0f8b0"]},{"id": "d49fc0b584012532e4fd7725149a29e25ac835bc", "title": "Learning Semantic Representations for the Phrase Translation Model", "authors": ["Jianfeng Gao", "Xiaodong He", "Li Deng"], "date": "2013", "abstract": "This paper presents a novel semantic-based phrase translation model. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent semantic space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a multi-layer neural network whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine… ", "references": ["4564666d8eb78a55d046144893407d312575979c", "4564666d8eb78a55d046144893407d312575979c", "057cb927b59c7cc16141fca2c825da1e3e3ef81a", "83e89037edfa113cf15b01a218cfcf12c6463bcb", "a6936964a235b30fd5970e7d7663c8a27a429411", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "a6936964a235b30fd5970e7d7663c8a27a429411", "25a4fb7025453ce73feef36eeaa45dbd0eb215e5", "a6936964a235b30fd5970e7d7663c8a27a429411", "25a4fb7025453ce73feef36eeaa45dbd0eb215e5"]},{"id": "c9214ebe91454e6369720136ab7dd990d52a07d4", "title": "Improved Statistical Alignment Models", "authors": ["Franz Josef Och", "Hermann Ney"], "date": "ACL", "abstract": "In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model… ", "references": []},{"id": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "authors": ["Kishore Papineni", "Salim Roukos", "Wei-Jing Zhu"], "date": "ACL", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick… ", "references": []},{"id": "0ffa423a5283396c88ff3d4033d541796bd039cc", "title": "Three Generative, Lexicalised Models for Statistical Parsing", "authors": ["Michael Collins"], "date": "1997", "abstract": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96). ", "references": ["59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b", "0ffa423a5283396c88ff3d4033d541796bd039cc", "59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b", "0ffa423a5283396c88ff3d4033d541796bd039cc", "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843", "0ffa423a5283396c88ff3d4033d541796bd039cc", "0ffa423a5283396c88ff3d4033d541796bd039cc", "0ffa423a5283396c88ff3d4033d541796bd039cc", "adfef97814b292a09520d8c78a141e7a4baf8726", "17ae3bda93abc40e758a1074c86baa041e977703"]},{"id": "62864f78fa4cb5f1ab45ebbe5a420b546b62d7a6", "title": "An Efficient A* Search Algorithm for Statistical Machine Translation", "authors": ["Franz Josef Och", "Nicola Ueffing", "Hermann Ney"], "date": "DDMMT@ACL", "abstract": "In this paper, we describe an efficient A* search algorithm for statistical machine translation. In contrary to beam-search or greedy approaches it is possible to guarantee the avoidance of search errors with A*. We develop various so-phisticated admissible and almost admissible heuristic functions. Especially our newly developped method to perform a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sentences. We compare the A* search… ", "references": ["d18af6780f9242ec988c89ed0b67dc7d05a7785a"]},{"id": "a739ae988ba0e3ff232f4507627dfc282ba7b3f4", "title": "Depth-Gated LSTM", "authors": ["Kaisheng Yao", "Trevor Cohn", "Chris Dyer"], "date": "2015", "abstract": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7"]},{"id": "c34e41312b47f60986458759d5cc546c2b53f748", "title": "End-to-end learning of semantic role labeling using recurrent neural networks", "authors": ["Wei Xu"], "date": "ACL", "abstract": "Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011). In this work, we propose to use deep… ", "references": ["8adb6fafa7b1b373f33fa95f1ab4006578bdf022", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "57458bc1cffe5caa45a885af986d70f723f406b4", "8adb6fafa7b1b373f33fa95f1ab4006578bdf022", "57458bc1cffe5caa45a885af986d70f723f406b4", "8a93cd1b6fbf7c8c86637bae18d979dafeb9a7c1", "c92970286c535992a86539b761357761e97a37ee", "8a93cd1b6fbf7c8c86637bae18d979dafeb9a7c1", "8adb6fafa7b1b373f33fa95f1ab4006578bdf022", "bc1022b031dc6c7019696492e8116598097a8c12"]},{"id": "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "authors": ["Junhua Mao", "Wei Xu", "Alan L. Yuille"], "date": "2015", "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the… ", "references": ["d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "0b544dfe355a5070b60986319a3f51fb45d1348e", "f01fc808592ea7c473a69a6e7484040a435f36d9", "f142c849ffef66f7520aff4e0b40ac964ccb8cc1", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f142c849ffef66f7520aff4e0b40ac964ccb8cc1", "82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9", "82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9"]},{"id": "dd5514876b7e1c09b6d2f931d90bb34aa3501441", "title": "Fast Decoding and Optimal Decoding for Machine Translation", "authors": ["Ulrich Germann", "Michael Jahr", "Kenji Yamada"], "date": "ACL", "abstract": "A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack… ", "references": ["4711ff01d8eff9b9d10deeb3b68f366f7944c208", "4711ff01d8eff9b9d10deeb3b68f366f7944c208", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "03b513b30b9d95e39285df1dc93be63e25f2744e", "03b513b30b9d95e39285df1dc93be63e25f2744e", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "ae9443b39a5abfbf3cc9776173c1ae4f94732408", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "ae9443b39a5abfbf3cc9776173c1ae4f94732408"]},{"id": "e94697b98b707f557436e025bdc8498fa261d3bc", "title": "Multi-Perspective Context Matching for Machine Comprehension", "authors": ["Zhiguo Wang", "Haitao Mi", "Radu Florian"], "date": "2016", "abstract": "Previous machine comprehension (MC) datasets are either too small to train end-to-end deep learning models, or not difficult enough to evaluate the ability of current MC techniques. The newly released SQuAD dataset alleviates these limitations, and gives us a chance to develop more realistic MC models. Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM) model, which is an end-to-end system that directly predicts the answer beginning and ending points in a passage. Our… ", "references": ["b1e20420982a4f923c08652941666b189b11b7fe", "f2e50e2ee4021f199877c8920f1f984481c723aa", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "05dd7254b632376973f3a1b4d39485da17814df5", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "564257469fa44cdb57e4272f85253efb9acfd69d", "b1e20420982a4f923c08652941666b189b11b7fe", "b1e20420982a4f923c08652941666b189b11b7fe", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19"]},{"id": "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "title": "Grid Long Short-Term Memory", "authors": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "date": "2016", "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images.", "references": ["a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "cea967b59209c6be22829699f05b8b1ac4dc092d", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "89b1f4740ae37fd04f6ac007577bdd34621f0861"]},{"id": "62f88e8fc3b44c5627f2b4721b08498d78103893", "title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "authors": ["Junbei Zhang", "Xiao-Dan Zhu", "Hui Jiang"], "date": "2017", "abstract": "The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA.", "references": ["3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "d1505c6123c102e53eb19dff312cb25cea840b72", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2", "05dd7254b632376973f3a1b4d39485da17814df5", "d1505c6123c102e53eb19dff312cb25cea840b72", "0680f04750b1e257ffdd161e85382031dc73ea7f", "0680f04750b1e257ffdd161e85382031dc73ea7f", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19"]},{"id": "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "title": "Gated-Attention Readers for Text Comprehension", "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "Ruslan Salakhutdinov"], "date": "2017", "abstract": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state… ", "references": ["75fa915984f1903cd8d0e1ea54b9d008d5a87fe5", "f2e50e2ee4021f199877c8920f1f984481c723aa", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5", "b1e20420982a4f923c08652941666b189b11b7fe", "f2e50e2ee4021f199877c8920f1f984481c723aa", "b1e20420982a4f923c08652941666b189b11b7fe", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "e86e81ad3fa4ab0b736f7fef721689e293ee788e", "f2e50e2ee4021f199877c8920f1f984481c723aa", "c4916a5fb50bcc73213b6f054c42ad10c68c52cd"]},{"id": "13fe71da009484f240c46f14d9330e932f8de210", "title": "Long Short-Term Memory-Networks for Machine Reading", "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "date": "2016", "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations… ", "references": ["4a197ce36461849bcaee565b510a8ef71b7dcae3", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "4a197ce36461849bcaee565b510a8ef71b7dcae3", "71ae756c75ac89e2d731c9c79649562b5768ff39", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "452059171226626718eb677358836328f884298e", "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "17de95a8ec3fe5917d91110b410ab64df33414bf", "452059171226626718eb677358836328f884298e", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa"]},{"id": "525f65936c331b0b766c7aea0eae64c595704c50", "title": "Mnemonic Reader for Machine Comprehension", "authors": ["Minghao Hu", "Yuxing Peng", "Xipeng Qiu"], "date": "2017", "abstract": "Recently, several end-to-end neural models have been proposed for machine comprehension tasks. Typically, these models use attention mechanisms to capture the complicated interaction between the context and the query and then point the boundary of answer. To better point the correct answer, we introduce the Mnemonic Reader for machine comprehension tasks, which enhance the attention reader in two aspects. Firstly, we use a self-alignment attention to model the long-distance dependency among… ", "references": ["c6e5df6322659276da6133f9b734a389d7a255e8", "c636a2dd242908fe2e598a1077c0c57bfdea8633", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "f314339651cb25e4234e0b96fe8bd87206847993", "0680f04750b1e257ffdd161e85382031dc73ea7f", "c50cd7df4271ef94a0a60894f0e2cf4ef89fb912", "f314339651cb25e4234e0b96fe8bd87206847993", "c6e5df6322659276da6133f9b734a389d7a255e8", "0680f04750b1e257ffdd161e85382031dc73ea7f"]},{"id": "f2e50e2ee4021f199877c8920f1f984481c723aa", "title": "Text Understanding with the Attention Sum Reader Network", "authors": ["Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst"], "date": "2016", "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation… ", "references": ["9653d5c2c7844347343d073bbedd96e05d52f69b", "71ae756c75ac89e2d731c9c79649562b5768ff39", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "86c2a7dc48445d75ec9bc71f7d9fdec622687e90", "564257469fa44cdb57e4272f85253efb9acfd69d", "9653d5c2c7844347343d073bbedd96e05d52f69b", "86c2a7dc48445d75ec9bc71f7d9fdec622687e90", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "d1505c6123c102e53eb19dff312cb25cea840b72", "b1e20420982a4f923c08652941666b189b11b7fe"]},{"id": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "title": "Bidirectional Attention Flow for Machine Comprehension", "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Hannaneh Hajishirzi"], "date": "2017", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query.", "references": ["05dd7254b632376973f3a1b4d39485da17814df5", "c636a2dd242908fe2e598a1077c0c57bfdea8633", "f2e50e2ee4021f199877c8920f1f984481c723aa", "f2e50e2ee4021f199877c8920f1f984481c723aa", "f314339651cb25e4234e0b96fe8bd87206847993", "c636a2dd242908fe2e598a1077c0c57bfdea8633", "2c1890864c1c2b750f48316dc8b650ba4772adc5", "e978d832a4d86571e1b52aa1685dc32ccb250f50", "c6e5df6322659276da6133f9b734a389d7a255e8", "f2e50e2ee4021f199877c8920f1f984481c723aa"]},{"id": "ab7b5917515c460b90451e67852171a531671ab8", "title": "The Mathematics of Statistical Machine Translation: Parameter Estimation", "authors": ["Peter F. Brown", "Stephen Della Pietra", "Robert L. Mercer"], "date": "1993", "abstract": "We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although… ", "references": ["a76563076016fb1cb813deba45db2409772a51da", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "0858eb565308ef214a0b7e51f0aa204185adf430", "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "2166fa493a8c6e40f7f8562d15712dd3c75f03df", "85b9eb556c211d954b31d9d58fed6891a07ab473", "3de5d40b60742e3dfa86b19e7f660962298492af", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "09091c767a756001bf87205df25f1505354469a2", "a1066659ec1afee9dce586f6f49b7d44527827e1"]},{"id": "35b91b365ceb016fb3e022577cec96fb9b445dc5", "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "authors": ["Felix Hill", "Antoine Bordes", "Jason Weston"], "date": "2016", "abstract": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state… ", "references": ["6e565308c8081e807709cb4a917443b737e6cdb4", "6e565308c8081e807709cb4a917443b737e6cdb4", "15de5528b04bf3d9cf741122677588140c25ebff", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "6e565308c8081e807709cb4a917443b737e6cdb4", "564257469fa44cdb57e4272f85253efb9acfd69d", "564257469fa44cdb57e4272f85253efb9acfd69d", "fac2ca048fdd7e848f0b9ba2f7be25bb49186770", "6e565308c8081e807709cb4a917443b737e6cdb4", "71ae756c75ac89e2d731c9c79649562b5768ff39"]},{"id": "cd96a6e0b6bb099c515be8770764d2fd18e7b878", "title": "Recurrent Convolutional Neural Networks for Discourse Compositionality", "authors": ["Nal Kalchbrenner", "Phil Blunsom"], "date": "CVSM@ACL", "abstract": "The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The… ", "references": ["9819b600a828a57e1cde047bbe710d3446b30da5", "9819b600a828a57e1cde047bbe710d3446b30da5", "79c0b2f44bbc2bc51de554b88ebe46204413f884", "27e38351e48fe4b7da2775bf94341738bc4da07e", "9819b600a828a57e1cde047bbe710d3446b30da5", "682b3dc0f4c46f96aa28358203c5013649e8dc62", "7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917", "9819b600a828a57e1cde047bbe710d3446b30da5", "7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917", "95174b77e4a5856fb0b0283bb0cb8acd3429d946"]},{"id": "596c882de006e4bb4a93f1fa08a5dd467bee060a", "title": "Learning Natural Language Inference with LSTM", "authors": ["Shuohang Wang", "Jing Jiang"], "date": "2016", "abstract": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural attention model for… ", "references": ["81bd1081df12c554f5b577677eb1a3975f728476", "81bd1081df12c554f5b577677eb1a3975f728476", "c333778104f648c385b4631f7b4a859787e9d3d3", "581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "f6d37a305fb900a56de5ddcd1095ec1ba5c8f157", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2"]},{"id": "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5", "title": "Natural Language Comprehension with the EpiReader", "authors": ["Adam Trischler", "Zheng Ye", "Kaheer Suleman"], "date": "EMNLP", "abstract": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a… ", "references": ["35b91b365ceb016fb3e022577cec96fb9b445dc5", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "564257469fa44cdb57e4272f85253efb9acfd69d", "46147f08468e873ff90d1d51e65493f262c7bb57", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "f2e50e2ee4021f199877c8920f1f984481c723aa", "564257469fa44cdb57e4272f85253efb9acfd69d", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "b1e20420982a4f923c08652941666b189b11b7fe"]},{"id": "d1275b2a2ab53013310e759e5c6878b96df643d4", "title": "Context dependent recurrent neural network language model", "authors": ["Geoffrey Zweig"], "date": "2012", "abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of… ", "references": ["9319ca5a532462f9f3515ac3d317668aa9650d5b", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "0ce317d84086b8885ddbc7923ec00bedb64ab6dc", "0ce317d84086b8885ddbc7923ec00bedb64ab6dc", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "9819b600a828a57e1cde047bbe710d3446b30da5", "0ce317d84086b8885ddbc7923ec00bedb64ab6dc", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97"]},{"id": "b1e20420982a4f923c08652941666b189b11b7fe", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "authors": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "date": "2016", "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP.", "references": ["35b91b365ceb016fb3e022577cec96fb9b445dc5", "6396ab37641d36be4c26420e58adeb8665914c3b", "f26e088bc4659a9b7fce28b6604d26de779bcf93", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "f26e088bc4659a9b7fce28b6604d26de779bcf93", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "452059171226626718eb677358836328f884298e", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "d1505c6123c102e53eb19dff312cb25cea840b72"]},{"id": "452059171226626718eb677358836328f884298e", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "authors": ["Ankit Kumar", "Ozan Irsoy", "Richard Socher"], "date": "ICML", "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a… ", "references": ["75ddc7ee15be14013a3462c01b38b0548486fbcb", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "687bac2d3320083eb4530bf18bb8f8f721477600", "af44f5db5b4396e1670cda07eff5ad84145ba843", "71ae756c75ac89e2d731c9c79649562b5768ff39", "d1275b2a2ab53013310e759e5c6878b96df643d4", "af44f5db5b4396e1670cda07eff5ad84145ba843", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "d53d878cf1a3f0bed5d9c68c925994cb72f47304"]},{"id": "e978d832a4d86571e1b52aa1685dc32ccb250f50", "title": "Dynamic Coattention Networks For Question Answering", "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "date": "2016", "abstract": "Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This… ", "references": ["ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "efbd381493bb9636f489b965a2034d529cd56bcd", "efbd381493bb9636f489b965a2034d529cd56bcd", "0680f04750b1e257ffdd161e85382031dc73ea7f", "f2e50e2ee4021f199877c8920f1f984481c723aa", "05dd7254b632376973f3a1b4d39485da17814df5", "05dd7254b632376973f3a1b4d39485da17814df5", "0680f04750b1e257ffdd161e85382031dc73ea7f", "564257469fa44cdb57e4272f85253efb9acfd69d", "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b"]},{"id": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend", "authors": ["Karl Moritz Hermann", "Tomás Kociský", "Phil Blunsom"], "date": "NIPS", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention… ", "references": ["445406b0d88ae965fa587cf5c167374ff1bbc09a", "564257469fa44cdb57e4272f85253efb9acfd69d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "bc1022b031dc6c7019696492e8116598097a8c12", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "564257469fa44cdb57e4272f85253efb9acfd69d", "a583af2696030bcf5f556edc74573fbee902be0b", "bc1022b031dc6c7019696492e8116598097a8c12", "445406b0d88ae965fa587cf5c167374ff1bbc09a", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},{"id": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "authors": ["Jason Weston", "Antoine Bordes"], "date": "2016", "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks… ", "references": ["c0be2ac2f45681f1852fc1d298af5dceb85834f4", "564257469fa44cdb57e4272f85253efb9acfd69d", "c1787db25af5614f41e56938aa594f2dbb1dca07", "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "b75329489baf067e6f7bbb74f16ffd49fba80dfa", "b75329489baf067e6f7bbb74f16ffd49fba80dfa", "b75329489baf067e6f7bbb74f16ffd49fba80dfa", "abba83b5747e98d10ab982df9ae94cc799668f16", "b75329489baf067e6f7bbb74f16ffd49fba80dfa"]},{"id": "46147f08468e873ff90d1d51e65493f262c7bb57", "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "authors": ["Adam Trischler", "Zheng Ye", "Philip Bachman"], "date": "2016", "abstract": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\\it MCTest} benchmark. Partly because of its limited size, prior work on {\\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel… ", "references": ["564257469fa44cdb57e4272f85253efb9acfd69d", "564257469fa44cdb57e4272f85253efb9acfd69d", "6d8994b90b9cc1df522bd56cceeaba4a4a6e9e1e", "f26e088bc4659a9b7fce28b6604d26de779bcf93", "71ae756c75ac89e2d731c9c79649562b5768ff39", "d1505c6123c102e53eb19dff312cb25cea840b72", "f26e088bc4659a9b7fce28b6604d26de779bcf93", "6d8994b90b9cc1df522bd56cceeaba4a4a6e9e1e", "452059171226626718eb677358836328f884298e", "6d8994b90b9cc1df522bd56cceeaba4a4a6e9e1e"]},{"id": "e4600ece1f09236d082eca4537ee9c1efe687f6c", "title": "FastQA: A Simple and Efficient Neural Architecture for Question Answering", "authors": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe"], "date": "2017", "abstract": "Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-toend neural architectures for QA.", "references": ["05dd7254b632376973f3a1b4d39485da17814df5", "05dd7254b632376973f3a1b4d39485da17814df5", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "b1e20420982a4f923c08652941666b189b11b7fe", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "b1e20420982a4f923c08652941666b189b11b7fe", "564257469fa44cdb57e4272f85253efb9acfd69d", "b1e20420982a4f923c08652941666b189b11b7fe"]},{"id": "564257469fa44cdb57e4272f85253efb9acfd69d", "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text", "authors": ["Matthew Richardson", "Christopher J. C. Burges", "Erin Renshaw"], "date": "EMNLP", "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text.", "references": ["5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "3c22c2dc4805aa8b94761b5ff48c434138bb9855", "c0e8b7b668d82afd5a7a90999d78c3a36d23d909", "e08acdc9aa026931ef75909cd5f75955755d8021", "eaf71a1e1a2b186db899558921616888ed2f26de", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "7ec3a89dc9b2f51c5224eb020e86c86c8498da00", "3c22c2dc4805aa8b94761b5ff48c434138bb9855", "d2dece97743aabd8ecfe549804d4c9ad6f3bc2af"]},{"id": "0680f04750b1e257ffdd161e85382031dc73ea7f", "title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "authors": ["Yang Yu", "Wei Zhang", "Bowen Zhou"], "date": "2017", "abstract": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question… ", "references": ["c4916a5fb50bcc73213b6f054c42ad10c68c52cd", "f92272e33b11a0d2f47b5b65446c0f1a913cfd17", "05dd7254b632376973f3a1b4d39485da17814df5", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "c4916a5fb50bcc73213b6f054c42ad10c68c52cd", "15454e478cc826e195cb15732ea0db57ad8bd38c", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5"]},{"id": "3eda43078ae1f4741f09be08c4ecab6229046a5c", "title": "NewsQA: A Machine Comprehension Dataset", "authors": ["Adam Trischler", "Tong Wang", "Kaheer Suleman"], "date": "Rep4NLP@ACL", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing… ", "references": ["f2e50e2ee4021f199877c8920f1f984481c723aa", "e4600ece1f09236d082eca4537ee9c1efe687f6c", "05dd7254b632376973f3a1b4d39485da17814df5", "b1e20420982a4f923c08652941666b189b11b7fe", "b1e20420982a4f923c08652941666b189b11b7fe", "e4600ece1f09236d082eca4537ee9c1efe687f6c", "46147f08468e873ff90d1d51e65493f262c7bb57", "e4600ece1f09236d082eca4537ee9c1efe687f6c", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5", "05dd7254b632376973f3a1b4d39485da17814df5"]},{"id": "6d8994b90b9cc1df522bd56cceeaba4a4a6e9e1e", "title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "authors": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Schütze"], "date": "2016", "abstract": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP.", "references": ["d1505c6123c102e53eb19dff312cb25cea840b72", "46147f08468e873ff90d1d51e65493f262c7bb57", "452059171226626718eb677358836328f884298e", "57458bc1cffe5caa45a885af986d70f723f406b4", "46147f08468e873ff90d1d51e65493f262c7bb57", "af44f5db5b4396e1670cda07eff5ad84145ba843", "564257469fa44cdb57e4272f85253efb9acfd69d", "452059171226626718eb677358836328f884298e", "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "d1505c6123c102e53eb19dff312cb25cea840b72"]},{"id": "a79ac27b270772c79b80d2235ca5ff2df2d2d370", "title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "authors": ["Tao Lei", "Regina Barzilay", "Tommi S. Jaakkola"], "date": "EMNLP", "abstract": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words… ", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cfa2646776405d50533055ceb1b7f050e9014dcb", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "cea967b59209c6be22829699f05b8b1ac4dc092d", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "d86227948b6000e5d7ed63cf2054ad600b7994a0", "57458bc1cffe5caa45a885af986d70f723f406b4", "cea967b59209c6be22829699f05b8b1ac4dc092d", "687bac2d3320083eb4530bf18bb8f8f721477600"]},{"id": "cff79255a94b9b05a4ce893eb403a522e0923f04", "title": "Neural Semantic Encoders", "authors": ["Tsendsuren Munkhdalai", "Hong Yu"], "date": "2017", "abstract": "We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "452059171226626718eb677358836328f884298e", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "cea967b59209c6be22829699f05b8b1ac4dc092d", "13fe71da009484f240c46f14d9330e932f8de210", "71ae756c75ac89e2d731c9c79649562b5768ff39", "71ae756c75ac89e2d731c9c79649562b5768ff39", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "13fe71da009484f240c46f14d9330e932f8de210", "452059171226626718eb677358836328f884298e"]},{"id": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "date": "NIPS", "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory.", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "71ae756c75ac89e2d731c9c79649562b5768ff39", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "30110856f45fde473f1903f686aa365cf70ed4c7", "30110856f45fde473f1903f686aa365cf70ed4c7", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "71ae756c75ac89e2d731c9c79649562b5768ff39", "9665247ea3421929f9b6ad721f139f11edb1dbb8"]},{"id": "03f34688ef4ee4239464633784235387e9bff4bb", "title": "Deep Learning of Representations", "authors": ["Yoshua Bengio", "Aaron C. Courville"], "date": "Handbook on Neural…", "abstract": "Unsupervised learning of representations has been found useful in many applications and benefits from several advantages, e.g., where there are many unlabeled examples and few labeled ones (semi-supervised learning), or where the unlabeled or labeled examples are from a distribution different but related to the one of interest (self-taught learning, multi-task learning, and domain adaptation). Some of these algorithms have successfully been used to learn a hierarchy of features, i.e., to build… ", "references": []},{"id": "e60ff004dde5c13ec53087872cfcdd12e85beb57", "title": "Learning Deep Architectures for AI", "authors": ["Yoshua Bengio"], "date": "2007", "abstract": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task… ", "references": ["9b30e7c50d8aa3d872a63d7ca2e18ebf6a23c031", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "9b30e7c50d8aa3d872a63d7ca2e18ebf6a23c031", "d5ddb30bf421bdfdf728b636993dc48b1e879176", "9b30e7c50d8aa3d872a63d7ca2e18ebf6a23c031", "d5ddb30bf421bdfdf728b636993dc48b1e879176", "5bf65452ae566a052b00d919404f462470869600", "d5ddb30bf421bdfdf728b636993dc48b1e879176", "c74e230a5a6fd5e2db6ace765ce38afe65f96214", "a24508e65e599b5b20c33af96dbe7017d5caca37"]},{"id": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100, 000+ Questions for Machine Comprehension of Text", "authors": ["Pranav Rajpurkar", "Jian Zhang", "Percy Liang"], "date": "EMNLP", "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.", "references": ["5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "b1e20420982a4f923c08652941666b189b11b7fe", "f26e088bc4659a9b7fce28b6604d26de779bcf93", "564257469fa44cdb57e4272f85253efb9acfd69d", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "b1e20420982a4f923c08652941666b189b11b7fe", "564257469fa44cdb57e4272f85253efb9acfd69d"]},{"id": "c965bac486a714d47a6362248f0a959c77622738", "title": "Techniques for Learning Binary Stochastic Feedforward Neural Networks", "authors": ["Tapani Raiko", "Mathias Berglund", "Laurent Dinh"], "date": "2015", "abstract": "Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic… ", "references": ["3832057ac487f43e885cdb485a6ca1462834bb8d", "162d958ff885f1462aeda91cd72582323fd6a1f4", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "f87247fb37f6b48da0757d7a1acf38da44510cdb", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "162d958ff885f1462aeda91cd72582323fd6a1f4", "512ea8d0c5b5de896129e76d4276f7b996fe88d8", "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "3832057ac487f43e885cdb485a6ca1462834bb8d"]},{"id": "f9f19bee621faf46f90b023f8de8248b57becbc4", "title": "Adaptive dropout for training deep neural networks", "authors": ["Jimmy Ba", "Brendan J. Frey"], "date": "NIPS", "abstract": "Recently, it was shown that deep neural networks can perform very well if the activities of hidden units are regularized during learning, e.g, by randomly dropping out 50% of their activities.", "references": ["43c8a545f7166659e9e21c88fe234e0323855216", "c3ecd8e19e016d15670c8953b4b9afaa5186b0f3", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "2e2089ae76fe914706e6fa90081a79c8fe01611e", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "e354ec85b8287bf15ed596be16ef6e422ccc29e7", "c3ecd8e19e016d15670c8953b4b9afaa5186b0f3", "c3ecd8e19e016d15670c8953b4b9afaa5186b0f3"]},{"id": "c19fbefdeead6a4154a22a9c8551a18b1530033a", "title": "Hierarchical Probabilistic Neural Network Language Model", "authors": ["Frederic Morin", "Yoshua Bengio"], "date": "AISTATS", "abstract": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers.", "references": ["d6fb7546a29320eadad868af66835059db93d99f", "a8ca92770bce439a207cc75fd28a749b51b5a516", "09c76da2361d46689825c4efc37ad862347ca577", "09c76da2361d46689825c4efc37ad862347ca577", "a8ca92770bce439a207cc75fd28a749b51b5a516", "a8ca92770bce439a207cc75fd28a749b51b5a516", "a8ca92770bce439a207cc75fd28a749b51b5a516", "3de5d40b60742e3dfa86b19e7f660962298492af", "4af41f4d838daa7ca6995aeb4918b61989d1ed80", "d6fb7546a29320eadad868af66835059db93d99f"]},{"id": "ac973bbfd62a902d073a85ca621fd297e8660a82", "title": "Scaling recurrent neural network language models", "authors": ["Will Williams", "Niranjani Prasad", "Tony Robinson"], "date": "2015", "abstract": "This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs.", "references": ["608045cac6ba523c3141b646c220d7736b1398a9", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "ded103d0613e1a8f51f586cc1678aee3ff26e811", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "ded103d0613e1a8f51f586cc1678aee3ff26e811", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85"]},{"id": "71480da09af638260801af1db8eff6acb4e1122f", "title": "Decoding with Large-Scale Neural Language Models Improves Translation", "authors": ["Ashish Vaswani", "Yinggong Zhao", "David Chiang"], "date": "EMNLP", "abstract": "We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality… ", "references": ["d6fb7546a29320eadad868af66835059db93d99f", "d36b19b4c5977dd2a2796a5ad3508a3d8a087809", "0db6eb46ca9941660acc775e3ca39bf4434c18be", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "0db6eb46ca9941660acc775e3ca39bf4434c18be", "d6fb7546a29320eadad868af66835059db93d99f", "d6fb7546a29320eadad868af66835059db93d99f", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff"]},{"id": "6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b", "title": "Larger-Context Language Modelling", "authors": ["Tian Wang", "Kyunghyun Cho"], "date": "2015", "abstract": "In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demon- strate that the proposed… ", "references": []},{"id": "e64a9960734215e2b1866ea3cb723ffa5585ac14", "title": "Efficient sparse coding algorithms", "authors": ["Honglak Lee", "Alexis Battle", "Andrew Y. Ng"], "date": "NIPS", "abstract": "Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data.", "references": ["2805537bec87a6177037b18f9a3a9d3f1038867b", "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1", "42d906c733f273109c0ed716a5ef6e2a379beb26", "9af121fbed84c3484ab86df8f17f1f198ed790a0", "1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5", "5ac5fbae8a7faf2e9bd49ad01106cec4a2d8f20a", "5ac5fbae8a7faf2e9bd49ad01106cec4a2d8f20a", "5ac5fbae8a7faf2e9bd49ad01106cec4a2d8f20a", "5ac5fbae8a7faf2e9bd49ad01106cec4a2d8f20a", "42d906c733f273109c0ed716a5ef6e2a379beb26"]},{"id": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation", "authors": ["Yoshua Bengio", "Nicholas Léonard", "Aaron C. Courville"], "date": "2013", "abstract": "Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance… ", "references": ["f8c8619ea7d68e604e40b814b40c72888a755e95", "843959ffdccf31c6694d135fad07425924f785b1", "f8c8619ea7d68e604e40b814b40c72888a755e95", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "c221cc946425d85f93c86e3be2c31d4feb00faa1", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "f8c8619ea7d68e604e40b814b40c72888a755e95"]},{"id": "5776d0fea69d826519ee3649f620e8755a490efe", "title": "Lifelong Machine Learning Systems: Beyond Learning Algorithms", "authors": ["Daniel L. Silver", "Qiang Yang", "Lianghao Li"], "date": "AAAI Spring Symposium…", "abstract": "Lifelong Machine Learning, or LML, considers systems that can learn many tasks from one or more domains over its lifetime.", "references": ["75e50717070e82cdf3945265a75def6960b55a9d", "55d9ae6dd0ce6f27e2b50d66d355bf6986f03c70", "75e50717070e82cdf3945265a75def6960b55a9d", "993d16e86ac8e263718a4269ed487bb6b027dddb", "5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c", "5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "a24508e65e599b5b20c33af96dbe7017d5caca37", "a24508e65e599b5b20c33af96dbe7017d5caca37", "5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c"]},{"id": "aba48504f4f9563eafa44e0cfb22e1345d767c80", "title": "Dynamic Filter Networks", "authors": ["Xu Jia", "Bert De Brabandere", "Luc Van Gool"], "date": "NIPS", "abstract": "In a traditional convolutional layer, the learned filters stay fixed after training.", "references": ["17fa1c2a24ba8f731c8b21f1244463bc4b465681", "73c8acd433c33e09916eaa1b0311c3ea7b2610d2", "90094dba07438121fb55220f241ab74e881f5cee", "73c8acd433c33e09916eaa1b0311c3ea7b2610d2", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "17fa1c2a24ba8f731c8b21f1244463bc4b465681", "b6e7d0ec83f2a40fadc99bb0f1ced8508f5cfee5", "c2fb5b39428818d7ec8cc78e152e19c21b7db568", "90094dba07438121fb55220f241ab74e881f5cee", "77f0a39b8e02686fd85b01971f8feb7f60971f80"]},{"id": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "title": "Character-Aware Neural Language Models", "authors": ["Yoon Kim", "Yacine Jernite", "Alexander M. Rush"], "date": "2016", "abstract": "We describe a simple neural language model that relies only on character-level inputs.", "references": ["b7cfccf123f86785476a06c8039889a2eb1e2d73", "b7cfccf123f86785476a06c8039889a2eb1e2d73", "53ab89807caead278d3deb7b6a4180b277d3cb77", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "d1275b2a2ab53013310e759e5c6878b96df643d4", "f9a1b3850dfd837793743565a8af95973d395a4e", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "b7cfccf123f86785476a06c8039889a2eb1e2d73", "750f26d613d3bda4ce043944aa3ef358b0c5de68", "53ab89807caead278d3deb7b6a4180b277d3cb77"]},{"id": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a", "title": "Learning without Forgetting", "authors": ["Zhizhong Li", "Derek Hoiem"], "date": "2018", "abstract": "When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task… ", "references": ["b8de958fead0d8a9619b55c7299df3257c624a96", "9213538b4f6f067e70bedc0709901b39481254c1", "5151d6cb3a4eaec14a56944d58338251fca344ab", "ed06a9685bc8e50866a7bbda49fc5033c4088276", "ea160ca21b2a8c4402b077d6338e6a679aa9d7b9", "0407b605b8f55db72e2545586bfe8e946b691b70", "f14325ec3041a73118bc4d819204cbbca07d5a71", "b8de958fead0d8a9619b55c7299df3257c624a96", "ea160ca21b2a8c4402b077d6338e6a679aa9d7b9", "83f200fdef3f1b1778a3b46eabd44d5e2b305e2e"]},{"id": "53c9443e4e667170acc60ca1b31a0ec7151fe753", "title": "Progressive Neural Networks", "authors": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Raia Hadsell"], "date": "2016", "abstract": "Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and… ", "references": ["69e76e16740ed69f4dc55361a3d319ac2f1293dd", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "f82e4ff4f003581330338aaae71f60316e58dd26", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "5776d0fea69d826519ee3649f620e8755a490efe", "082b1f5c791cadef18c4920ecc1396615a3fe7cb", "1c4927af526d5c28f7c2cfa492ece192d80a61d4", "1def5d3711ebd1d86787b1ed57c91832c5ddc90b", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd"]},{"id": "49899bd9e5a7f59aa14e6d21ed501e3c3acd5852", "title": "LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition", "authors": ["Kazuki Irie", "Zoltán Tüske", "Hermann Ney"], "date": "INTERSPEECH", "abstract": "Popularized by the long short-term memory (LSTM), multiplicative gates have become a standard means to design artificial neural networks with intentionally organized information flow.", "references": ["4a197ce36461849bcaee565b510a8ef71b7dcae3", "084a54c5b76e10648e1d15985641baa5433ff893", "878ba5458e9e51f0b341fd9117fa0b43ef4096d3", "084a54c5b76e10648e1d15985641baa5433ff893", "4a197ce36461849bcaee565b510a8ef71b7dcae3", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "878ba5458e9e51f0b341fd9117fa0b43ef4096d3", "084a54c5b76e10648e1d15985641baa5433ff893", "4a197ce36461849bcaee565b510a8ef71b7dcae3", "4a197ce36461849bcaee565b510a8ef71b7dcae3"]},{"id": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models", "authors": ["Stephen Merity", "Caiming Xiong", "Richard Socher"], "date": "2017", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies.", "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "e957747f4f8600940be4c5bb001aa70c84e53a53", "452059171226626718eb677358836328f884298e", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "2a76c2121eee30af82a24058b4e149f05bcda911", "e44da7d8c71edcc6e575fa7faadd5e75785a7901", "d1275b2a2ab53013310e759e5c6878b96df643d4", "9819b600a828a57e1cde047bbe710d3446b30da5", "d1275b2a2ab53013310e759e5c6878b96df643d4", "e44da7d8c71edcc6e575fa7faadd5e75785a7901"]},{"id": "5762b7deff7e95febe193196d548379ff34b34f1", "title": "Improved Learning through Augmenting the Loss", "authors": ["Hakan Inan", "Khashayar Khosravi"], "date": "2016", "abstract": "We present two improvements to the well-known Recurrent Neural Network Language Models(RNNLM). First, we use the word embedding matrix to project the RNN output onto the output space and already achieve a large reduction in the number of free parameters while still improving performance. Second, instead of merely minimizing the standard cross entropy loss between the prediction distribution and the ”one-hot” target distribution, we minimize an additional loss term which takes into account the… ", "references": ["07ca885cb5cc4328895bfaec9ab752d5801b14cd", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "9819b600a828a57e1cde047bbe710d3446b30da5", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7"]},{"id": "5bf65452ae566a052b00d919404f462470869600", "title": "Products of experts", "authors": ["Geoffrey E. Hinton"], "date": "1999", "abstract": "It is possible to combine multiple probabilistic models of the same data by multiplying the probabilities together and then renormalizing. This is a very efficient way to model high-dimensional data which simultaneously satisfies many different low dimensional constraints. Each individual expert model can focus on giving high probability to data vectors that satisfy just one of the constraints. Data vectors that satisfy this one constraint but violate other constraints will be ruled out by… ", "references": []},{"id": "428818a9edfb547431be6d7ec165c6af576c83d5", "title": "Recurrent Topic-Transition GAN for Visual Paragraph Generation", "authors": ["Xiaodan Liang", "Zhiting Hu", "Eric P. Xing"], "date": "2017", "abstract": "A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The… ", "references": ["e8cd37fbd8bd5e690eef5861cf92af8e002d4533", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "3a7011346ce939e3251915e92ae2f252e4c7f777", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "f678a0041f2c6f931168010e7418c500c3f14cdb", "d7ce5665a72c0b607f484c1b448875f02ddfac3b", "efbc200feab74e5087c4005d8759e5dadb3a3077", "efbc200feab74e5087c4005d8759e5dadb3a3077", "b21c78a62fbb945a19ae9a8935933711647e7d70", "a72b8bbd039989db39769da836cdb287737deb92"]},{"id": "f6d8a7fc2e2d53923832f9404376512068ca2a57", "title": "Hierarchical mixtures of experts and the EM algorithm", "authors": ["Michael I. Jordan", "Robert A. Jacobs"], "date": "1993", "abstract": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIMs). Learning is treated as a maximum likelihood problem; in particular, we present an expectation-maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an online learning algorithm in which the parameters are… ", "references": ["1f462943c8d0af69c12a09058251848324135e5a"]},{"id": "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c", "title": "Conditional Generative Adversarial Nets", "authors": ["Mehdi Mirza", "Simon Osindero"], "date": "2014", "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an… ", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5726c7b40fcc454b77d989656c085520bf6c15fa", "5656fa5aa6e1beeb98703fc53ec112ad227c49ca", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "5656fa5aa6e1beeb98703fc53ec112ad227c49ca", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "0d24a0695c9fc669e643bad51d4e14f056329dec", "title": "An Actor-Critic Algorithm for Sequence Prediction", "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Yoshua Bengio"], "date": "2016", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL.", "references": ["df137487e20ba7c6e1e2b9a1e749f2a578b5ad99", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "cea967b59209c6be22829699f05b8b1ac4dc092d", "b624504240fa52ab76167acfe3156150ca01cf3b", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "b624504240fa52ab76167acfe3156150ca01cf3b", "b624504240fa52ab76167acfe3156150ca01cf3b", "5507dc32b368c8afd3b9507e9b5888da7bd7d7cd", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},{"id": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "title": "Speech recognition with deep recurrent neural networks", "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "date": "2013", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data.", "references": ["96494e722f58705fa20302fe6179d483f52705b4", "c1bca434074c447a31fa227059baccee66b48387", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "96494e722f58705fa20302fe6179d483f52705b4", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "3d82e058a5c40954b8f5db170a298a889a254c37", "dbee79ac1865cd42780215d8fb2da4bc2ab7f381", "9a9f4bf3bfe133e1c70f6b60654c238b677c66d0", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1"]},{"id": "042116e805aa3b5171efaf0c822dc142310ceefe", "title": "Boundary-Seeking Generative Adversarial Networks", "authors": ["R. Devon Hjelm", "Athul Paul Jacob", "Yoshua Bengio"], "date": "2017", "abstract": "Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance… ", "references": ["a6cb366736791bcccc5c8639de5a8f9636bf87e8", "8388f1be26329fa45e5807e968a641ce170ea078", "176f1d608b918eec8dc4b75e7b6e0acaba84a447", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "2966ecd82505ecd55ead0e6a327a304c8f9868e3", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "8388f1be26329fa45e5807e968a641ce170ea078", "8388f1be26329fa45e5807e968a641ce170ea078", "9360e5ce9c98166bb179ad479a9d2919ff13d022"]},{"id": "339c6e6d46836c173fb6a23b493c724896d4cc70", "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses", "authors": ["Ryan Lowe", "Michael Noseworthy", "Joelle Pineau"], "date": "2017", "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue… ", "references": ["404a7bb70a535de4ac3edf79543f00293523d486", "17f5c7411eeeeedf25b0db99a9130aa353aee4ba", "7a35e306999fc619e7ecff1ad9c9b693df0ef65c", "990237c429e93250bab66e3ac91b4c9e4b5df633", "26046351a69ce6d21ac34fd9d065672a35441397", "129cbad01be98ee88a930e31898cb76be79c41c1", "26046351a69ce6d21ac34fd9d065672a35441397", "404a7bb70a535de4ac3edf79543f00293523d486", "916441619914101258c71669b5ccc36424b54a6c", "609e0f0e60ddfe83fdc71bf5397205323888289d"]},{"id": "c3b38c2fd30adb316d0bdb32e983804be5595c30", "title": "Domain-Adversarial Neural Networks", "authors": ["Hana Ajakan", "Pascal Germain", "Mario Marchand"], "date": "2014", "abstract": "We introduce a new neural network learning algorithm suited to the context of domain adaptation, in which data at training and test time come from similar but different distributions. Our algorithm is inspired by theo ry on domain adaptation suggesting that, for effective domain transfer to be achiev ed, predictions must be made based on a data representation that cannot discriminate between the training (source) and test (target) domains. We propose a training objective that implements this… ", "references": ["6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "843959ffdccf31c6694d135fad07425924f785b1", "96c6bc559b79d8fd518f431c707e8b44ce3bc4de", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "8db26a22942404bd435909a16bb3a50cd67b4318", "6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde", "d4d8ef65132928e7bbdf0945cf98ef192dd1aed4", "a3cbb2a295dba31d9c9af77cc177fe538de94d5e"]},{"id": "b624504240fa52ab76167acfe3156150ca01cf3b", "title": "Attention-Based Models for Speech Recognition", "authors": ["Jan Chorowski", "Dzmitry Bahdanau", "Yoshua Bengio"], "date": "NIPS", "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3.", "references": ["4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "24741d280869ad9c60321f5ab6e5f01b7852507d", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "8dc1d5c47b8af57cbff36632318b4302706df6a3", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "8dc1d5c47b8af57cbff36632318b4302706df6a3", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},{"id": "4fc0ea6db600850908264652e1a5d7904f66ca58", "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks", "authors": ["Tong Che", "Yanran Li", "Yoshua Bengio"], "date": "2017", "abstract": "Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted.", "references": ["f0fb306a3ea4e31f59b18ffeb497054ea934ba6a", "9a700c7a7e7468e436f00c34551fbe3e0f70e42f", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "1b40fe1a9d25d5694c7ea40a57d0aaa2e2cd5dd1", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "f0fb306a3ea4e31f59b18ffeb497054ea934ba6a", "571b0750085ae3d939525e62af510ee2cee9d5ea", "8388f1be26329fa45e5807e968a641ce170ea078", "042116e805aa3b5171efaf0c822dc142310ceefe", "9a700c7a7e7468e436f00c34551fbe3e0f70e42f"]},{"id": "609e0f0e60ddfe83fdc71bf5397205323888289d", "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "authors": ["Iulian Serban", "Alessandro Sordoni", "Yoshua Bengio"], "date": "AAAI", "abstract": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model… ", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "129cbad01be98ee88a930e31898cb76be79c41c1", "916441619914101258c71669b5ccc36424b54a6c", "0c3b69b5247ef18fd5bab1109d87a04184ea8f4b", "cea967b59209c6be22829699f05b8b1ac4dc092d", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "651e5bcc14f14605a879303e97572a27ea8c7956", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b544dfe355a5070b60986319a3f51fb45d1348e", "d82b55c35c8673774a708353838918346f6c006f"]},{"id": "7f9135f3584e4e1715b2990a4f389c94af0313a5", "title": "Generating Long and Diverse Responses with Neural Conversation Models", "authors": ["Louis Shao", "Stephan Gouws", "Ray Kurzweil"], "date": "2017", "abstract": "Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models -- purely data-driven systems trained end-to-end on dialogue corpora -- have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using… ", "references": ["9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "1298dae5751fb06184f6b067d1503bde8037bdb7", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "651e5bcc14f14605a879303e97572a27ea8c7956", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "1298dae5751fb06184f6b067d1503bde8037bdb7", "74ec753c27a01e93380c148ba886f8e0317c61ee", "74ec753c27a01e93380c148ba886f8e0317c61ee"]},{"id": "2966ecd82505ecd55ead0e6a327a304c8f9868e3", "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "authors": ["Lantao Yu", "Weinan Zhang", "Yong Yu"], "date": "2017", "abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data.", "references": ["469a7d19c074c0d2df699340cbd5b105bdd0f7e6", "469a7d19c074c0d2df699340cbd5b105bdd0f7e6", "469a7d19c074c0d2df699340cbd5b105bdd0f7e6", "0d24a0695c9fc669e643bad51d4e14f056329dec", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0", "469a7d19c074c0d2df699340cbd5b105bdd0f7e6", "cea967b59209c6be22829699f05b8b1ac4dc092d", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99"]},{"id": "1298dae5751fb06184f6b067d1503bde8037bdb7", "title": "Deep Reinforcement Learning for Dialogue Generation", "authors": ["Jiwei Li", "Will Monroe", "Jianfeng Gao"], "date": "EMNLP", "abstract": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep… ", "references": ["3385397d3be400c3f4a6f79f9c47e67e50333b45", "208c94412e618afb08e760318caaa4526eec8d6d", "4b7dcca3de306591b54b6cba36cd4a4982860630", "17f5c7411eeeeedf25b0db99a9130aa353aee4ba", "09eb7642b733dfcf1d9ac14a9435d3bc96d18f0c", "6719ef93142d64a69b52c916f9ee132b5339d9d1", "839f1b61803e65f20f067b361d0ebf6db337172c", "1e0ede6f60b1106070e041211133c634a2e4f991", "839f1b61803e65f20f067b361d0ebf6db337172c", "4b7dcca3de306591b54b6cba36cd4a4982860630"]},{"id": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning", "authors": ["Jonas Gehring", "Michael Auli", "Yann Dauphin"], "date": "2017", "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks.", "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "510e26733aaff585d65701b9f1be7ca9d5afc586", "3d2c6941a9b4608ba52b328369a3352db2092ae0", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "0b544dfe355a5070b60986319a3f51fb45d1348e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0936352b78a52bc5d2b5e3f04233efc56664af51", "b60abe57bc195616063be10638c6437358c81d1e", "f958d4921951e394057a1c4ec33bad9a34e5dad1"]},{"id": "17f5c7411eeeeedf25b0db99a9130aa353aee4ba", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "authors": ["Iulian Serban", "Alessandro Sordoni", "Joelle Pineau"], "date": "AAAI", "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art… ", "references": ["eb53b7c13156e3acacb47c1e51d93cefeabfaeb0", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "167ad306d84cca2455bc50eb833454de9f2dcd02", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "651e5bcc14f14605a879303e97572a27ea8c7956", "167ad306d84cca2455bc50eb833454de9f2dcd02", "0b544dfe355a5070b60986319a3f51fb45d1348e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d"]},{"id": "032274e57f7d8b456bd255fe76b909b2c1d7458e", "title": "A Deep Reinforced Model for Abstractive Summarization", "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "date": "2018", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences.", "references": ["7a67159fc7bc76d0b37930b55005a69b51241635", "489955574c435169abd72285cfe2f055f538a401", "5ab72d44237533534de8402e30f3ccce25ce30de", "2f160ce71f01ac2043de67536ff0e413ff6f58c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "489955574c435169abd72285cfe2f055f538a401", "7a67159fc7bc76d0b37930b55005a69b51241635", "489955574c435169abd72285cfe2f055f538a401", "f77a604410d88307ec5c6331c8b6133272fbaa10", "f77a604410d88307ec5c6331c8b6133272fbaa10"]},{"id": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks", "authors": ["Yoon Kim", "Carl Denton", "Alexander M. Rush"], "date": "2017", "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow… ", "references": ["e06a68b26bde368883761c9dceb547914b2ecca8", "654a3e53fb41d8168798ee0ee61dfab73739b1ed", "e06a68b26bde368883761c9dceb547914b2ecca8", "04d1a26c2516dc14a765112a63ec60dc3cb3de72", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "bc82b4f9f202062857958f0336fc28327a75563b", "bc82b4f9f202062857958f0336fc28327a75563b", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e"]},{"id": "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "authors": ["Yonghui Wu", "Mike Schuster", "Jeffrey Dean"], "date": "2016", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems.", "references": ["93499a7c7f699b6630a86fad964536f9423bb6d0", "acec46ffd3f6046af97529127d98f1d623816ea4", "1af68821518f03568f913ab03fc02080247a27ff", "1956c239b3552e030db1b78951f64781101125ed", "93499a7c7f699b6630a86fad964536f9423bb6d0", "4d070993cb75407b285e14cb8aac0077624ef4d9", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "733b821faeebe49b6efcf5369e3b9902b476529e", "93499a7c7f699b6630a86fad964536f9423bb6d0", "1af68821518f03568f913ab03fc02080247a27ff"]},{"id": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "title": "Recurrent Highway Networks", "authors": ["Julian G. Zilly", "Rupesh Kumar Srivastava", "Jürgen Schmidhuber"], "date": "ICML", "abstract": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose… ", "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "9cee45ef1212ebbc7d468f9b1d7df24f5005e64d", "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "5762b7deff7e95febe193196d548379ff34b34f1"]},{"id": "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "title": "On the State of the Art of Evaluation in Neural Language Models", "authors": ["Gábor Melis", "Chris Dyer", "Phil Blunsom"], "date": "2018", "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks.", "references": ["0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "9819b600a828a57e1cde047bbe710d3446b30da5", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "401d68e1a930b0f7e02030cab4c185fb1839cb11", "67d968c7450878190e45ac7886746de867bf673d", "efbd381493bb9636f489b965a2034d529cd56bcd", "67d968c7450878190e45ac7886746de867bf673d", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "401d68e1a930b0f7e02030cab4c185fb1839cb11"]},{"id": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models", "authors": ["Ofir Press", "Lior Wolf"], "date": "2017", "abstract": "We study the topmost weight matrix of neural network language models.", "references": ["f9a1b3850dfd837793743565a8af95973d395a4e", "58001259d2f6442b07cc0d716ff99899abbb2bc7", "2012f32199adc88747d5a1b47c7b4ba1cb3cb995", "53ca064b9f1b92951c1997e90b776e95b0880e52", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "2012f32199adc88747d5a1b47c7b4ba1cb3cb995", "53ca064b9f1b92951c1997e90b776e95b0880e52", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "fc18e99f918d8906ec44be3f7d90d8f9ebabae96", "title": "Revisiting Activation Regularization for Language RNNs", "authors": ["Stephen Merity", "Bryan McCann", "Richard Socher"], "date": "2017", "abstract": "Recurrent neural networks (RNNs) serve as a fundamental building block for many sequence tasks across natural language processing. Recent research has focused on recurrent dropout techniques or custom RNN cells in order to improve performance. Both of these can require substantial modifications to the machine learning model or to the underlying RNN configurations. We revisit traditional regularization techniques, specifically L2 regularization on RNN activations and slowness regularization over… ", "references": ["67d968c7450878190e45ac7886746de867bf673d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "9819b600a828a57e1cde047bbe710d3446b30da5", "e9c771197a6564762754e48c1daafb066f449f2e", "424aef7340ee618132cc3314669400e23ad910ba", "cf76789618f5db929393c1187514ce6c3502c3cd", "67d968c7450878190e45ac7886746de867bf673d", "9819b600a828a57e1cde047bbe710d3446b30da5", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2"]},{"id": "25a4fb7025453ce73feef36eeaa45dbd0eb215e5", "title": "Maximum Expected BLEU Training of Phrase and Lexicon Translation Models", "authors": ["Xiaodong He", "Li Deng"], "date": "ACL", "abstract": "This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the… ", "references": ["10d21ca7728cb3dd15731accedda9ea711d8a0f4", "396aabd694da04cdb846cb724ca9f866f345cbd5", "1f12451245667a85d0ee225a80880fc93c71cc8b", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "2887e349875dfc30a6e7666424d83abc40c7fd53", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "2e74e29298f0f71694ac21958996d147191fe4b0", "a4b828609b60b06e61bea7a4029cc9e1cad5df87"]},{"id": "67d968c7450878190e45ac7886746de867bf673d", "title": "Neural Architecture Search with Reinforcement Learning", "authors": ["Barret Zoph", "Quoc V. Le"], "date": "2016", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding.", "references": ["35c1668dc64d24a28c6041978e5fcca754eb2f4b", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "bf85a0cd645ad68919c0706741ab568a60a58af2", "d1275b2a2ab53013310e759e5c6878b96df643d4", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "bf85a0cd645ad68919c0706741ab568a60a58af2", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "d1275b2a2ab53013310e759e5c6878b96df643d4", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250"]},{"id": "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "title": "An End-to-End Discriminative Approach to Machine Translation", "authors": ["Percy Liang", "Alexandre Bouchard-Côté", "Ben Taskar"], "date": "ACL", "abstract": "We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We first discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We find that making frequent but smaller updates is preferable to making fewer but… ", "references": ["1f12451245667a85d0ee225a80880fc93c71cc8b", "9c9548ac1705a48cc565c238c6102b7aa69101dc", "9c9548ac1705a48cc565c238c6102b7aa69101dc", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "567dc4e26ece98e96c2e798ae8acafa5883945a9", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "c3fdc954fa36b123da63a3d35a8eecfdaf1b298b", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab"]},{"id": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks", "authors": ["Alex Graves"], "date": "2016", "abstract": "This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output.", "references": ["9653d5c2c7844347343d073bbedd96e05d52f69b", "d01379ebb53c66a4ccf5f4959d904dcf9e161e41", "9653d5c2c7844347343d073bbedd96e05d52f69b", "d01379ebb53c66a4ccf5f4959d904dcf9e161e41", "8829e3873846c6bbad5aca111e64f9d2c1b24299", "b59d91e0699d4e1896a15bae13fd180bdaf77ea5", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e"]},{"id": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "title": "LSTM: A Search Space Odyssey", "authors": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jürgen Schmidhuber"], "date": "2017", "abstract": "Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative… ", "references": ["754d6ad9c36406bfb7f48e0a7b3cac430edc0648", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "754d6ad9c36406bfb7f48e0a7b3cac430edc0648", "067e07b725ab012c80aa2f87857f6791c1407f6d", "2f83f6e1afadf0963153974968af6b8342775d82", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "11540131eae85b2e11d53df7f1360eeb6476e7f4"]},{"id": "5082a1a13daea5c7026706738f8528391a1e6d59", "title": "A Neural Attention Model for Abstractive Sentence Summarization", "authors": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "date": "2015", "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build.", "references": ["1ec86811a79fb02a1c551b8f418314a00f5f5a99", "92ba9c288cbd0089cf6e9d988c9672f095a67109", "0be949cc24188ef7205bdaaeb7df2508344b8d5a", "9c818fe59a76b242dcca62579bd353fe9cf01c0d", "9d08213ede54c4e205d18b4400288831af918ec8", "50dfb7358cc85cc7ab0eda68c517164ebd205d42", "cea967b59209c6be22829699f05b8b1ac4dc092d", "9d08213ede54c4e205d18b4400288831af918ec8", "92ba9c288cbd0089cf6e9d988c9672f095a67109", "1ec86811a79fb02a1c551b8f418314a00f5f5a99"]},{"id": "1596e3cb20ba3b9aefe440e30660c2a9f035f683", "title": "Learning Sub-Word Units for Open Vocabulary Speech Recognition", "authors": ["Carolina Parada", "Mark Dredze", "Ariya Rastrow"], "date": "ACL", "abstract": "Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of sub-word units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone… ", "references": ["c0787db6b91bf8d7c2b175eccaf6ab27be031c81", "27c63939daf043f810942296bd98f1cac3510846", "c0787db6b91bf8d7c2b175eccaf6ab27be031c81", "ec5f929b57cf12b4d624ab125f337c14ad642ab1", "bf7d940ea625da995e6f577d5249379f82e1c004", "9b470d0b7a3d0417fb44983071ce38628d406fa3", "ec5f929b57cf12b4d624ab125f337c14ad642ab1", "27c63939daf043f810942296bd98f1cac3510846", "27c63939daf043f810942296bd98f1cac3510846", "ec5f929b57cf12b4d624ab125f337c14ad642ab1"]},{"id": "472bd5b90c289b715340708536ade437d20b237e", "title": "Hybrid Language Models Using Mixed Types of Sub-Lexical Units for Open Vocabulary German LVCSR", "authors": ["M. Ali Basha Shaik", "Amr El-Desoky Mousa", "Hermann Ney"], "date": "INTERSPEECH", "abstract": "German is a highly inflected language with a large number of words derived from the same root. It makes use of a high degree of word compounding leading to high Out-of-vocabulary (OOV) rates, and Language Model (LM) perplexities. For such languages the use of sub-lexical units for Large Vocabulary Continuous Speech Recognition (LVCSR) becomes a natural choice. In this paper, we investigate the use of mixed types of sub-lexical units in the same recognition lexicon. Namely, morphemic or syllabic… ", "references": ["1ad72930186136b693799f07f7d605bebfa26752", "1ad72930186136b693799f07f7d605bebfa26752", "df040243cadd17c8c5ef1e0edc8e5aabb9613688", "9d87ca8779b968b5fe08e074c6aa5c2bc6e5a4cc", "16e987de3d97c8fe93a10cee9bb4df3ce7b72696", "94c904bb29ba60b00a06ca92ce47acc9b3e6a4fe", "16e987de3d97c8fe93a10cee9bb4df3ce7b72696", "db3c3008ced47d307337b66d73d7f61ff8ab6a47", "df040243cadd17c8c5ef1e0edc8e5aabb9613688", "94c904bb29ba60b00a06ca92ce47acc9b3e6a4fe"]},{"id": "adfcf065e15fd3bc9badf6145034c84dfb08f204", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "authors": ["Junyoung Chung", "Çaglar Gülçehre", "Yoshua Bengio"], "date": "2014", "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "d0be39ee052d246ae99c082a565aba25b811be2d", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "d0be39ee052d246ae99c082a565aba25b811be2d", "d0be39ee052d246ae99c082a565aba25b811be2d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "a97b5db17acc731ef67321832dbbaf5766153135", "d0be39ee052d246ae99c082a565aba25b811be2d", "0d6203718c15f137fda2f295c96269bc2b254644", "d0be39ee052d246ae99c082a565aba25b811be2d"]},{"id": "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks", "authors": ["Samy Bengio", "Oriol Vinyals", "Noam Shazeer"], "date": "2015", "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors… ", "references": ["d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "47d2dc34e1d02a8109f5c04bb6939725de23716d", "d5d46991c7e92352865dbf442be7c74d0d560dd8", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "d5d46991c7e92352865dbf442be7c74d0d560dd8", "79ab3c49903ec8cb339437ccf5cf998607fc313e", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "4d376d6978dad0374edfa6709c9556b42d3594d3"]},{"id": "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "title": "Generating Text with Recurrent Neural Networks", "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton"], "date": "ICML", "abstract": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character… ", "references": ["44d2abe2175df8153f465f6c39b68b76a0d40ab9", "c6629770cb6a00ad585918e71fe6dbad829ad0d1", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "c6629770cb6a00ad585918e71fe6dbad829ad0d1", "d0be39ee052d246ae99c082a565aba25b811be2d", "346fbcffe4237aa60e8bcb3d4294a8b99436f1d0", "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "0d6203718c15f137fda2f295c96269bc2b254644", "0d6203718c15f137fda2f295c96269bc2b254644"]},{"id": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification", "authors": ["Yoon Kim"], "date": "EMNLP", "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks.", "references": ["687bac2d3320083eb4530bf18bb8f8f721477600", "941f318e41147773ae69d9da4f8de9b8dbea70f4", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "66ff19af88c81e0f3582ac65359a0543a16e1ac8", "66ff19af88c81e0f3582ac65359a0543a16e1ac8", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "941f318e41147773ae69d9da4f8de9b8dbea70f4", "dc0975ae518a5b30e60fde23a41c74bafd7c6f8c", "66ff19af88c81e0f3582ac65359a0543a16e1ac8", "cfa2646776405d50533055ceb1b7f050e9014dcb"]},{"id": "5999fd9b9712fee3184989d043bff899935b4208", "title": "Mandarin Word-Character Hybrid-Input Neural Network Language Model", "authors": ["Moonyoung Kang", "Tim Ng", "Long H. Nguyen"], "date": "INTERSPEECH", "abstract": "We applied neural network language model (NNLM) on Chinese by training and testing it on 2011 GALE Mandarin evaluation task. Exploiting the fact that there are no word boundaries in written Chinese, we trained various NNLMs using either word, character, or both, including a wordcharacter hybrid-input NNLM which accepts both word and character as input. Our best result showed up to 0.6% absolute (6.3% relative) Character Error Rate (CER) reduction compared to an un-pruned 4-gram standard… ", "references": ["4052d4c2e36c0ddf78a1a47eee19452bb287a427", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "4052d4c2e36c0ddf78a1a47eee19452bb287a427", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "8f3e1bbbdc1190ae6320c0520f539337a5ca5927", "a5f8135cf356a80e13241b5b36a5836eaad85fd1", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "0861015b3c89d68749548c19c6f056eee34eafc6", "377110ffbaf595badd15943a3c214b32c91d934e"]},{"id": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification", "authors": ["Xiang Zhang", "Junbo Jake Zhao", "Yann LeCun"], "date": "NIPS", "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks. ", "references": ["28b98600e363025d66d5b9a529330a808acc6c6f", "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "7e8d5a108c28cdfb92f419ce919fbf7993dfebfc", "b0aca3e7877c3c20958b0fae5cbf2dd602104859", "fbf417c83ae5b895fc645346e4efbf3a0aabeac9", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "20b5c6af6182cf078b10fd30be9548cf36e9ed83", "b0aca3e7877c3c20958b0fae5cbf2dd602104859", "2f83f6e1afadf0963153974968af6b8342775d82", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba"]},{"id": "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "title": "An Empirical Exploration of Recurrent Network Architectures", "authors": ["Rafal Józefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "date": "ICML", "abstract": "The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train.", "references": ["aa7bfd2304201afbb19971ebde87b17e40242e91", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "d0be39ee052d246ae99c082a565aba25b811be2d", "aa7bfd2304201afbb19971ebde87b17e40242e91", "0d6203718c15f137fda2f295c96269bc2b254644", "0d6203718c15f137fda2f295c96269bc2b254644", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "7dded890956b37df5ac4c42b8ffbc142725f2801", "title": "Recurrent Memory Array Structures", "authors": ["Kamil Rocki"], "date": "2016", "abstract": "The following report introduces ideas augmenting standard Long Short Term Memory (LSTM) architecture with multiple memory cells per hidden unit in order to improve its generalization capabilities. It considers both deterministic and stochastic variants of memory operation. It is shown that the nondeterministic Array-LSTM approach improves state-of-the-art performance on character level text prediction achieving 1.402 BPC on enwik8 dataset. Furthermore, this report estabilishes baseline neural… ", "references": ["5b791cd374c7109693aaddee2c12d659ae4e3ec0", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "d0be39ee052d246ae99c082a565aba25b811be2d", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "40be3888daa5c2e5af4d36ae22f690bcc8caf600", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "cf76789618f5db929393c1187514ce6c3502c3cd", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "d0be39ee052d246ae99c082a565aba25b811be2d", "56172b6fe2613c37d9790bde8ab6ccda14b35678"]},{"id": "4ef03716945bd3907458efbe1bbf8928dafc1efc", "title": "Regularization and nonlinearities for neural language models: when are they needed?", "authors": ["Marius Pachitariu", "Maneesh Sahani"], "date": "2013", "abstract": "Neural language models (LMs) based on recurrent neural networks (RNN) are some of the most successful word and character-level LMs. Why do they work so well, in particular better than linear neural LMs? Possible explanations are that RNNs have an implicitly better regularization or that RNNs have a higher capacity for storing patterns due to their nonlinearities or both. Here we argue for the first explanation in the limit of little training data and the second explanation for large amounts of… ", "references": ["d0be39ee052d246ae99c082a565aba25b811be2d", "d1b78d136e9e6be0aeb814027f0f3fd843606155", "d0be39ee052d246ae99c082a565aba25b811be2d", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "d0be39ee052d246ae99c082a565aba25b811be2d", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "aa7bfd2304201afbb19971ebde87b17e40242e91", "d1b78d136e9e6be0aeb814027f0f3fd843606155", "d0be39ee052d246ae99c082a565aba25b811be2d"]},{"id": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch", "authors": ["Ronan Collobert", "Jason Weston", "Pavel P. Kuksa"], "date": "2011", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis… ", "references": ["31b4c03d721dc10b87c178277c1d369f91db8f0e", "7ece4e8d31f872d928369ac2cf58a616a7182112", "dac72f2c509aee67524d3321f77e97e8eff51de6", "790ecefeaf2b471b439743a772ccce026131bef5", "7ece4e8d31f872d928369ac2cf58a616a7182112", "48b4524a3b1207157b1b2f87885c434c96fc7a19", "6503a3d9fb204c2a08ecfcfe6ba5b815fc65a030", "790ecefeaf2b471b439743a772ccce026131bef5", "a16e484824b2580e092c985aa659e8680aeda5ee", "7ece4e8d31f872d928369ac2cf58a616a7182112"]},{"id": "952454718139dba3aafc6b3b67c4f514ac3964af", "title": "Recurrent Batch Normalization", "authors": ["Tim Cooijmans", "Nicolas Ballas", "Aaron C. Courville"], "date": "2016", "abstract": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks.", "references": ["84069287da0a6b488b8c933f3cb5be759cb6237e", "4d376d6978dad0374edfa6709c9556b42d3594d3", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "d0be39ee052d246ae99c082a565aba25b811be2d", "f95adc1d8daaa07a0c956826ec274ca9e2515ddc", "84069287da0a6b488b8c933f3cb5be759cb6237e", "d0be39ee052d246ae99c082a565aba25b811be2d", "d0be39ee052d246ae99c082a565aba25b811be2d", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "84069287da0a6b488b8c933f3cb5be759cb6237e"]},{"id": "759956bb98689dbcc891528636d8994e54318f85", "title": "Strategies for Training Large Vocabulary Neural Language Models", "authors": ["Wenlin Chen", "David Grangier", "Michael Auli"], "date": "2016", "abstract": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive… ", "references": ["71480da09af638260801af1db8eff6acb4e1122f", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "a17745f1d7045636577bcd5d513620df5860e9e5"]},{"id": "12a5b7190b981bf478b4c9c04d3c0d41f13b9023", "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies", "authors": ["Shihao Ji", "S. V. N. Vishwanathan", "Pradeep Dubey"], "date": "2016", "abstract": "We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies.", "references": ["5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "8250ecbaef057bdb5390ef4e4be798f1523a23f6", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "0cea6b034ee949d10604ba163270b699e711ded8", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "8250ecbaef057bdb5390ef4e4be798f1523a23f6", "ac973bbfd62a902d073a85ca621fd297e8660a82", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "5d833331b0e22ff359db05c62a8bca18c4f04b68"]},{"id": "0dc9eb7d17f2def56ad930945f2521653f04c3fa", "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation", "authors": ["Noam Shazeer", "Joris Pelemans", "Ciprian Chelba"], "date": "2014", "abstract": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the On e Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best… ", "references": ["4af41f4d838daa7ca6995aeb4918b61989d1ed80", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "3b56693f6fe6b82092c4adc756f20fb9b7710ac5", "3b56693f6fe6b82092c4adc756f20fb9b7710ac5", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "f9a1b3850dfd837793743565a8af95973d395a4e", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab"]},{"id": "b7cfccf123f86785476a06c8039889a2eb1e2d73", "title": "genCNN: A Convolutional Architecture for Word Sequence Prediction", "authors": ["Mingxuan Wang", "Zhengdong Lu", "Qun Liu"], "date": "2015", "abstract": "We propose a novel convolutional architecture, named $gen$CNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can… ", "references": ["9f08b01251cb99f4ffae8c7b3e4468d3af9c98d3", "396aabd694da04cdb846cb724ca9f866f345cbd5", "396aabd694da04cdb846cb724ca9f866f345cbd5", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "167ad306d84cca2455bc50eb833454de9f2dcd02", "cea967b59209c6be22829699f05b8b1ac4dc092d", "167ad306d84cca2455bc50eb833454de9f2dcd02", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1a3c74c7b11ad5635570932577cdde2a3f7a6a5c", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},{"id": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache", "authors": ["Edouard Grave", "Armand Joulin", "Nicolas Usunier"], "date": "2017", "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We… ", "references": ["e837b79de602c69395498c1fbbe39bbb4e6f75ad", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "efbd381493bb9636f489b965a2034d529cd56bcd", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "be1fed9544830df1137e72b1d2396c40d3e18365", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a"]},{"id": "6a4007e60346e4501acc936b49b7a476e73afa1e", "title": "Learning Multilingual Word Representations using a Bag-of-Words Autoencoder", "authors": ["Stanislas Lauly", "Alex Boulanger", "Hugo Larochelle"], "date": "2014", "abstract": "Recent work on learning multilingual word representations usually relies on the use of word-level alignements (e.g. infered with the help of GIZA++) between translated sentences, in order to align the word embeddings in different languages. In this workshop paper, we investigate an autoencoder model for learning multilingual word representations that does without such word-level alignements. The autoencoder is trained to reconstruct the bag-of-word representation of given sentence from an… ", "references": ["d1f37d9cab68eb8cda669cc949394732f33264b4", "0d3233d858660aff451a6c2561a05378ed09725a", "d1b78d136e9e6be0aeb814027f0f3fd843606155", "d49fc0b584012532e4fd7725149a29e25ac835bc", "bc1022b031dc6c7019696492e8116598097a8c12", "0d3233d858660aff451a6c2561a05378ed09725a", "d49fc0b584012532e4fd7725149a29e25ac835bc", "aa1762a629b31d254450e37ce8baa235d729d82b", "dac72f2c509aee67524d3321f77e97e8eff51de6", "d1f37d9cab68eb8cda669cc949394732f33264b4"]},{"id": "3c5126da7ce388c64b796c80d15a3c3629d6ad58", "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "authors": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "date": "2011", "abstract": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive… ", "references": ["f5e23b61597f3d2b51515c557a213be1f0e6ed6f", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "4471e3117cdac2fae74d305d54b237bb3addd749", "73e897104540642698321c106cc9c35af369fe12", "73e897104540642698321c106cc9c35af369fe12", "73e897104540642698321c106cc9c35af369fe12", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c"]},{"id": "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "title": "A Structured Vector Space Model for Word Meaning in Context", "authors": ["Katrin Erk", "Sebastian Padó"], "date": "EMNLP", "abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. \n \nWe present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument… ", "references": ["d3cf28ab36ff7f7601a55c1e832736b2473a07f0", "00162f43964fd457a9158408c1ac0e8990489782", "d3cf28ab36ff7f7601a55c1e832736b2473a07f0", "00162f43964fd457a9158408c1ac0e8990489782", "d3cf28ab36ff7f7601a55c1e832736b2473a07f0", "2f55b6959997e8464c19140644c6e3d6781a55b4", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "fd1901f34cc3673072264104885d70555b1a4cdc", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4"]},{"id": "d1f37d9cab68eb8cda669cc949394732f33264b4", "title": "Inducing Crosslingual Distributed Representations of Words", "authors": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai"], "date": "COLING", "abstract": "Distributed representations of words have proven extremely useful in numerous natural language processing tasks.", "references": ["fc9344147e3ddd8dd8ec86452961745df50e5c1e", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "e0f5c8b7925ab500714634ba74dfd47a047ac279", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "78d515c5b01fb34f5013e453f3ed852badfa31eb", "6c6c5cfa01c57f2015c8c923b1404727ad3330fc", "6c6c5cfa01c57f2015c8c923b1404727ad3330fc", "e0f5c8b7925ab500714634ba74dfd47a047ac279", "ae5e6c6f5513613a161b2c85563f9708bf2e9178"]},{"id": "330da625c15427c6e42ccfa3b747fb29e5835bf0", "title": "Efficient Estimation of Word Representations in Vector Space", "authors": ["Kai Chen", "Jeffrey Dean"], "date": "2013", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets.", "references": ["c19fbefdeead6a4154a22a9c8551a18b1530033a", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "cb45e9217fe323fbc199d820e7735488fca2a9b3", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068"]},{"id": "fcfb39e64678fe9cb681f11b9a3314becec82bb2", "title": "Comparison of feedforward and recurrent neural network language models", "authors": ["Martin Sundermeyer", "Ilya Oparin", "Hermann Ney"], "date": "2013", "abstract": "Research on language modeling for speech recognition has increasingly focused on the application of neural networks. Two competing concepts have been developed: On the one hand, feedforward neural networks representing an n-gram approach, on the other hand recurrent neural networks that may learn context dependencies spanning more than a fixed number of predecessor words. To the best of our knowledge, no comparison has been carried out between feedforward and state-of-the-art recurrent networks… ", "references": ["c19fbefdeead6a4154a22a9c8551a18b1530033a", "a17745f1d7045636577bcd5d513620df5860e9e5", "d4f00f97581d30e7104d1fe924085248093aa472", "9819b600a828a57e1cde047bbe710d3446b30da5", "a17745f1d7045636577bcd5d513620df5860e9e5", "e41498c05d4c68e4750fb84a380317a112d97b01", "d4f00f97581d30e7104d1fe924085248093aa472", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "a17745f1d7045636577bcd5d513620df5860e9e5", "d4f00f97581d30e7104d1fe924085248093aa472"]},{"id": "0d6203718c15f137fda2f295c96269bc2b254644", "title": "Learning Recurrent Neural Networks with Hessian-Free Optimization", "authors": ["James Martens", "Ilya Sutskever"], "date": "ICML", "abstract": "In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for… ", "references": ["c6867b6b564462d6b902f68e0bfa58f4717ca1cc", "f29bee88bcf08c805bbef6e43bcf9ac022a3e713", "f29bee88bcf08c805bbef6e43bcf9ac022a3e713", "4c46347fbc272b21468efe3d9af34b4b2bad6684", "aed054834e2c696807cc8b227ac7a4197196e211", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "030ba5a03666bf4c3a17c64699f8de8ec13d623b", "4c46347fbc272b21468efe3d9af34b4b2bad6684", "f29bee88bcf08c805bbef6e43bcf9ac022a3e713"]},{"id": "3b6b08fc7709016a6444389f129323fa8fce66b1", "title": "Multilingual Deep Learning", "authors": ["Sarath Chandar"], "date": "2013", "abstract": "Resource fortunate languages such as English, French, Chinese, etc. clearly overshadow many not-so-fortunate languages in terms of both (i) the number of NLP tools available and (ii) the quality of these tools. One solution to alleviate this problem is to collect more annotated resources for these languages, but this is often not feasible due to the cost, time, and effort involved. A more feasible option is to use cross language learning which aims to use annotated resources available in some… ", "references": ["6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b", "80e9e3fc3670482c1fee16b2542061b779f47c4f", "327c88dd06722a967be9c6b1176fbd79554967e7", "c45d9d930543934efef0b7501596c5effd1857dd", "343733a063e491d234a36d3e1090a739318b3566", "14cb95cdc52b783fb8972a13b48081d8cdf81453", "343733a063e491d234a36d3e1090a739318b3566", "70f89905a695b7a3bde394f1ebac4a28e5458f8b", "14cb95cdc52b783fb8972a13b48081d8cdf81453", "c45d9d930543934efef0b7501596c5effd1857dd"]},{"id": "e4a94d6eef25cdebdde2c91fb3c45a737d5e3141", "title": "Performance analysis of Neural Networks in combination with n-gram language models", "authors": ["Ilya Oparin", "Martin Sundermeyer", "Jean-Luc Gauvain"], "date": "2012", "abstract": "Neural Network language models (NNLMs) have recently become an important complement to conventional n-gram language models (LMs) in speech-to-text systems. However, little is known about the behavior of NNLMs. The analysis presented in this paper aims to understand which types of events are better modeled by NNLMs as compared to n-gram LMs, in what cases improvements are most substantial and why this is the case. Such an analysis is important to take further benefit from NNLMs used in… ", "references": ["668087f0ae7ce1de6e0bd0965dbb480c08103260", "382c5a5f938d426fb7734994933804b5b8b3fddd", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "884689d18aea6ac49f2fa13361637eb9092cca00", "5999fd9b9712fee3184989d043bff899935b4208", "5999fd9b9712fee3184989d043bff899935b4208", "382c5a5f938d426fb7734994933804b5b8b3fddd", "382c5a5f938d426fb7734994933804b5b8b3fddd", "382c5a5f938d426fb7734994933804b5b8b3fddd", "884689d18aea6ac49f2fa13361637eb9092cca00"]},{"id": "2f83f6e1afadf0963153974968af6b8342775d82", "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "authors": ["Alex Graves", "Jürgen Schmidhuber"], "date": "2005", "abstract": "In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm.", "references": ["4d0050f220b755fe3319fe1e7011b6796c7ad2ea", "8a51dc0b5694af3e54393e20e05f42fc3cbe476b", "8a51dc0b5694af3e54393e20e05f42fc3cbe476b", "685d42a668413422615519a52ac75d66fded4611", "93c1268cc00bf0fe4ed7a7a5e2d2f272988baadf", "4c7fc0ca5bec117b75c7f4fc9c8b45579569abda", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "047655e733a9eed9a500afd916efa566915b9110", "047655e733a9eed9a500afd916efa566915b9110", "93c1268cc00bf0fe4ed7a7a5e2d2f272988baadf"]},{"id": "047655e733a9eed9a500afd916efa566915b9110", "title": "Learning Precise Timing with LSTM Recurrent Networks", "authors": ["Felix A. Gers", "Nicol N. Schraudolph", "Jürgen Schmidhuber"], "date": "2002", "abstract": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection.", "references": ["850e994c086e95c8d8c2ba3c90e53104a0fa709e", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "bec62424fdb3633f3478216638671a2c78f6d833", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "bec62424fdb3633f3478216638671a2c78f6d833", "f828b401c86e0f8fddd8e77774e332dfd226cb05", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "bec62424fdb3633f3478216638671a2c78f6d833", "f6cde498cfaf7b862da3fa358caee9749e65ed3d", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"]},{"id": "c36d355e01e1ed90e57bffbbfc274d4d98952b96", "title": "Factored bilingual n-gram language models for statistical machine translation", "authors": ["Josep Maria Crego", "François Yvon"], "date": "2010", "abstract": "In this work, we present an extension of n-gram-based translation models based on factored language models (FLMs). Translation units employed in the n-gram-based approach to statistical machine translation (SMT) are based on mappings of sequences of raw words, while translation model probabilities are estimated through standard language modeling of such bilingual units. Therefore, similar to other translation model approaches (phrase-based or hierarchical), the sparseness problem of the units… ", "references": ["da336ed15bd24c268cb2a09efe2aa4f298cda3ba", "fc43ce300875906274ae9f40a1b437374703d0cf", "223dcd0e44532fc02444709e61327432c74fe46d", "24653b3b33d48c409deb672f8d8ee0eff31cd418", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "4b7ec490154397c2691d3404eccd412665fa5e6a", "113c32635f3b83fbbc1c7af720dfc68d37a37848", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "24653b3b33d48c409deb672f8d8ee0eff31cd418", "4b7ec490154397c2691d3404eccd412665fa5e6a"]},{"id": "b4fc91e543ec868658cde6170f1e59c33292e595", "title": "Recurrent Neural Network Based Language Modeling in Meeting Recognition", "authors": ["Stefan Kombrink", "Lukás Burget"], "date": "INTERSPEECH", "abstract": "We use recurrent neural network (RNN) based language models to improve the BUT English meeting recognizer. On the baseline setup using the original language models we decrease word error rate (WER) more than 1% absolute by n-best list rescoring and language model adaptation. When n-gram language models are trained on the same moderately sized data set as the RNN models, improvements are higher yielding a system which performs comparable to the baseline. A noticeable improvement was observed… ", "references": ["8a0e2e4dd10e1be7fd28474504cbdaa1237e4926", "8592a744942cabc7596c90dd6a4e13bdd233b677", "368a08ef108193ebe12f568b79594e4840a47444", "9819b600a828a57e1cde047bbe710d3446b30da5", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "b158a006bebb619e2ea7bf0a22c27d45c5d19004", "9819b600a828a57e1cde047bbe710d3446b30da5", "8a0e2e4dd10e1be7fd28474504cbdaa1237e4926", "d4e81f4fd59723dcb08b1ec6ef30cb115eafda1c", "b158a006bebb619e2ea7bf0a22c27d45c5d19004"]},{"id": "9319ca5a532462f9f3515ac3d317668aa9650d5b", "title": "Exact training of a neural syntactic language model", "authors": ["Ahmad Emami", "Frederick Jelinek"], "date": "2004", "abstract": "The structured language model (SLM) aims at predicting the next word in a given word string by making a syntactical analysis of the preceding words. However, it faces the data sparseness problem because of the large dimensionality and diversity of the information available in the syntactic parsing. Previously, we proposed using neural network models for the SLM (Emami, A. et al., Proc. ICASSP, 2003; Emami, Proc. EUROSPEECH'03., 2003). The neural network model is better suited to tackle the data… ", "references": ["a1c3748820d6b5ab4e7334524815df9bb6d20aed"]},{"id": "ff80a400198f0ce26887672407d8872825e663bf", "title": "Random forests and the data sparseness problem in language modeling", "authors": ["Peng Xu", "Frederick Jelinek"], "date": "2007", "abstract": "Abstract Language modeling is the problem of predicting words based on histories containing words already hypothesized. Two key aspects of language modeling are effective history equivalence classification and robust probability estimation. The solution of these aspects is hindered by the data sparseness problem. Application of random forests (RFs) to language modeling deals with the two aspects simultaneously. We develop a new smoothing technique based on randomly grown decision trees (DTs… ", "references": ["5a03eea43e128f49218ed95b909da1136c757e57", "09c76da2361d46689825c4efc37ad862347ca577", "3bb45466dfb9770e706d1e63205e266e7761f915", "0ccb664faaf4221dfe20c5b321d017ce33af3fec", "567dc4e26ece98e96c2e798ae8acafa5883945a9", "29053eab305c2b585bcfbb713243b05646e7d62d", "567dc4e26ece98e96c2e798ae8acafa5883945a9", "0ccb664faaf4221dfe20c5b321d017ce33af3fec", "567dc4e26ece98e96c2e798ae8acafa5883945a9", "29053eab305c2b585bcfbb713243b05646e7d62d"]},{"id": "0687165a9f0360bde0469fd401d966540e0897c3", "title": "A Dynamic Language Model for Speech Recognition", "authors": ["Frederick Jelinek", "Bernard Mérialdo", "M. Strauss"], "date": "HLT", "abstract": "In the case of a trigram language model, the probability of the next word conditioned on the previous two words is estimated from a large corpus of text.", "references": ["343c8af478f7703459b0e390e888efe723f15e31", "491566891addc26134c617ab026f5548de39401a", "491566891addc26134c617ab026f5548de39401a", "360899707ca19389e1d33d9499591c371c755286", "491566891addc26134c617ab026f5548de39401a", "491566891addc26134c617ab026f5548de39401a", "491566891addc26134c617ab026f5548de39401a", "491566891addc26134c617ab026f5548de39401a", "360899707ca19389e1d33d9499591c371c755286", "491566891addc26134c617ab026f5548de39401a"]},{"id": "f93a0a3e8a3e6001b4482430254595cf737697fa", "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention", "authors": ["Yang P. Liu", "Chengjie Sun", "Xiaolong Wang"], "date": "2016", "abstract": "In this paper, we proposed a sentence encoding-based model for recognizing text entailment.", "references": ["bfccb2d6e3d9f9b6bd8b14b2d4c6efa36c79341b", "455afd748e8834ef521e4b67c7c056d3c33429e2", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "455afd748e8834ef521e4b67c7c056d3c33429e2", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "36c097a225a95735271960e2b63a2cb9e98bff83", "ea2563467c1c472a346d165b7f97c86317d63ca4", "455afd748e8834ef521e4b67c7c056d3c33429e2", "bfccb2d6e3d9f9b6bd8b14b2d4c6efa36c79341b"]},{"id": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "title": "A large annotated corpus for learning natural language inference", "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher D. Manning"], "date": "2015", "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing… ", "references": ["abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "11ec56898a9e7f401a2affe776b5297bd4e25025", "5feb2c61b04532869e44d1ca4e48c7108aee5fd3", "287d5571dbf255a7ccbdc2bcfe9211fd8f0b2a7c", "287d5571dbf255a7ccbdc2bcfe9211fd8f0b2a7c", "11ec56898a9e7f401a2affe776b5297bd4e25025", "687bac2d3320083eb4530bf18bb8f8f721477600", "7c05a4ffee7e159e34b2efea7e44d994333ec628", "ea2563467c1c472a346d165b7f97c86317d63ca4", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6"]},{"id": "395044a2e3f5624b2471fb28826e7dbb1009356e", "title": "Towards Universal Paraphrastic Sentence Embeddings", "authors": ["John Wieting", "Mohit Bansal", "Karen Livescu"], "date": "2016", "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database (Ganitkevitch et al., 2013). We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribution as the training data and from a wide range of other domains. We find that the most complex architectures, such as long short-term memory (LSTM) recurrent neural networks, perform best on the in… ", "references": ["687bac2d3320083eb4530bf18bb8f8f721477600", "d86227948b6000e5d7ed63cf2054ad600b7994a0", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "d86227948b6000e5d7ed63cf2054ad600b7994a0", "d86227948b6000e5d7ed63cf2054ad600b7994a0", "b21c78a62fbb945a19ae9a8935933711647e7d70", "687bac2d3320083eb4530bf18bb8f8f721477600", "f9f91e7bac46b13444eddeb2438b01089e73b786", "60dda7f5efd67758bde1ee7f45e6d3ef86445495"]},{"id": "3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings", "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "date": "ICLR", "abstract": "The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs.", "references": []},{"id": "119b5e8927f98e3fea76cbb57d7e053c24ac5c18", "title": "Fast Text Compression with Neural Networks", "authors": ["Matthew V. Mahoney"], "date": "FLAIRS Conference", "abstract": "Neural networks have the potential to extend data compression algorithms beyond the character level n-gram models now in use, but have usually been avoided because they are too slow to be practical. We introduce a model that produces better compression than popular Limpel-Ziv compressors (zip, gzip, compress), and is competitive in time, space, and compression ratio with PPM and BurrowsWheeler algorithms, currently the best known. The compressor, a bit-level predictive arithmetic encoder using… ", "references": ["ab4aec5e0714b352e6c90d063fe830cbc70912bc", "084c55d6432265785e3ff86a2e900a49d501c00a", "af56e6d4901dcd0f589bf969e604663d40f1be5d", "af56e6d4901dcd0f589bf969e604663d40f1be5d", "1f5d21625f8264f455591b3c7cbdac18b983b3c0", "af56e6d4901dcd0f589bf969e604663d40f1be5d", "79521a6d8814f9162ed1f7028e9e007c4df7181a", "076fa8d095c37c657f2aff39cf90bc2ea883b7cb"]},{"id": "fc999072ce188ee1d57b6bb744cb276b09a491bb", "title": "The 2005 AMI System for the Transcription of Speech in Meetings", "authors": ["Thomas Hain", "Lukás Burget", "Steve Renals"], "date": "MLMI", "abstract": "In this paper we describe the 2005 AMI system for the transcription of speech in meetings used in the 2005 NIST RT evaluations. The system was designed for participation in the speech to text part of the evaluations, in particular for transcription of speech recorded with multiple distant microphones and independent headset microphones. System performance was tested on both conference room and lecture style meetings. Although input sources are processed using different front-ends, the… ", "references": ["ff7804a13efc26bf23ce813319641db69ddbb969", "d98343b130f6a63b6276b3a3bdfa60dfdd17e859", "1564b8ff799983f335dbf3bb7a5c92b689c4a19f", "2c5eec22c4dabb124b2bbf85966d88dd38e62193", "f54aa90f53d81b43c1b8dd8ff38300cbd47cfac1", "22f8b93d37d4b620fa13ed1a50a28b2d3eb5cc3a", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "2c5eec22c4dabb124b2bbf85966d88dd38e62193"]},{"id": "43e5b43965aa4099e75110e4fd8e5458efb82fd8", "title": "Text Compression as a Test for Artificial Intelligence", "authors": ["Matthew V. Mahoney"], "date": "AAAI/IAAI", "abstract": "It is shown that optimal text compression is a harder problem than artificial intelligence as defined by Turing’s (1950) imitation game; thus compression ratio on a standard benchmark corpus could be used as an objective and quantitative alternative test for AI (Mahoney, 1999). Specifically, let L, M, and J be the probability distributions of responses chosen by a human, machine, and human judge respectively to the judge’s questions in the imitation game. The goal of AI is M = L, the machine is… ", "references": ["d812b8b86470f5cc6b7c0f7554007e9459abd089", "e34aa3e43470489964e1ec71631485a57ba0d2eb", "8941fe8899d17ae1dd9626f6945516cc68a3c43d", "d7d07b2b2ca6e28561fa142fd6f2fd020bcb40a7", "b080dd6ce069c7eac8b89e9f50eb121ef6f06b90"]},{"id": "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "title": "Distributed Representations of Sentences and Documents", "authors": ["Quoc V. Le"], "date": "2014", "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector.", "references": ["c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "4aa4069693bee00d1b0759ca3df35e59284e9845", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "4aa4069693bee00d1b0759ca3df35e59284e9845", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "2e12a485325776d3c23eae2b488d4812d86b4052", "649d03490ef72c5274e3bccd03d7a299d2f8da91"]},{"id": "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "title": "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model", "authors": ["Yoshua Bengio", "Jean-Sébastien Senecal"], "date": "2008", "abstract": "Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to… ", "references": ["b0130277677e5b915d5cd86b3afafd77fd08eb2e", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "e41498c05d4c68e4750fb84a380317a112d97b01", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "5bf65452ae566a052b00d919404f462470869600", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "5bf65452ae566a052b00d919404f462470869600"]},{"id": "8a0e2e4dd10e1be7fd28474504cbdaa1237e4926", "title": "A guide to recurrent neural networks and backpropagation", "authors": ["Mikael Bodén"], "date": "2001", "abstract": "This paper provides guidance to some of the concepts surrounding recurrent neural networks. Contrary to feedforward networks, recurrent networks can be sensitive, and be adapted to past inputs. Backpropagation learning is described for feedforward networks, adapted to suit our (probabilistic) modeling needs, and extended to cover recurrent networks. The aim of this brief paper is to set the scene for applying and understanding recurrent neural networks. ", "references": ["d0be39ee052d246ae99c082a565aba25b811be2d", "57dbba9281cbd1b4dd5d1932f0dd605b8f498322", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "872cdc269f3cb59f8a227818f35041415091545f", "d0be39ee052d246ae99c082a565aba25b811be2d", "872cdc269f3cb59f8a227818f35041415091545f", "57dbba9281cbd1b4dd5d1932f0dd605b8f498322", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "bd46c1b5948abe04e565a8bae6454da63a1b021e"]},{"id": "f6223a7549a6f35de2e9dfa5a27a9f93de304f47", "title": "Attending to Characters in Neural Sequence Labeling Models", "authors": ["Marek Rei", "Gamal K. O. Crichton", "Sampo Pyysalo"], "date": "2016", "abstract": "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words.", "references": ["330da625c15427c6e42ccfa3b747fb29e5835bf0", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "2ebc224d52761d5b76704c7b8f51369247a73d5f", "4d070993cb75407b285e14cb8aac0077624ef4d9", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "51a55df1f023571a7e07e338ee45a3e3d66ef73e"]},{"id": "f8f92f791d67883f77525c5aecdb7b10b3488b29", "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Christopher Bryant"], "date": "CoNLL Shared Task", "abstract": "The CoNLL-2014 shared task was devoted to grammatical error correction of all error types. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results. Compared to the CoNLL2013 shared task, we have introduced the following changes in CoNLL-2014: (1) A participating system is expected to detect and… ", "references": ["9a8ebbfc26eb959e111c5c4ad8159aad142cccbc", "9a8ebbfc26eb959e111c5c4ad8159aad142cccbc", "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc", "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc", "a563c6f1564b1e6d1a643c544379abf5069f8000", "9a8ebbfc26eb959e111c5c4ad8159aad142cccbc", "a563c6f1564b1e6d1a643c544379abf5069f8000", "f1e8f81a776ddefb621007370c56454f47c0cdce", "794e03473140760db0681fda938fad93e6d159bd", "71efd9633bef36d837ecc6753e8c6e0c5f87c839"]},{"id": "944e1a7b2c5c62e952418d7684e3cade89c76f87", "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "authors": ["Rie Kubota Ando", "Tong Zhang"], "date": "2005", "abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data.", "references": ["278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "9755f9993553131e5cc796d34ecaa624fe0ddffa", "e2de29049d62de925cf709024b92774cd82b0a5a", "161ffb54a3fdf0715b198bb57bd22f910242eb49", "ed3c324be93f30797e0f71d5f5fb5417cdd790bc", "df3520f52fcf42b4f10ed4b35b3b3f9cd050f290", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "9755f9993553131e5cc796d34ecaa624fe0ddffa", "b4299baa815ca5a815a70fba94a9f6f2b42fff19", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4"]},{"id": "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "title": "A Neural Conversational Model", "authors": ["Oriol Vinyals", "Quoc V. Le"], "date": "2015", "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence.", "references": ["9819b600a828a57e1cde047bbe710d3446b30da5", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "9819b600a828a57e1cde047bbe710d3446b30da5", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "title": "Unsupervised Learning of Video Representations using LSTMs", "authors": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "date": "ICML", "abstract": "We use Long Short Term Memory (LSTM) networks to learn representations of video sequences.", "references": ["67dccc9a856b60bdc4d058d83657a089b8ad4486", "bd243d77076b3b8fe046bd3dc6e8a02aa9b38d62", "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c", "cea967b59209c6be22829699f05b8b1ac4dc092d", "67dccc9a856b60bdc4d058d83657a089b8ad4486", "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c", "0b544dfe355a5070b60986319a3f51fb45d1348e", "bd243d77076b3b8fe046bd3dc6e8a02aa9b38d62", "0b544dfe355a5070b60986319a3f51fb45d1348e", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "622a40854f79fb385b61f1a3de1cdce4999e9f4b", "title": "Deep-structured hidden conditional random fields for phonetic recognition", "authors": ["Dong Yu", "Li Deng"], "date": "INTERSPEECH", "abstract": "We extend our earlier work on deep-structured conditional random field (DCRF) and develop deep-structured hidden conditional random field (DHCRF). We investigate the use of this new sequential deep-learning model for phonetic recognition. DHCRF is a hierarchical model in which the final layer is a hidden conditional random field (HCRF) and the intermediate layers are zero-th-order conditional random fields (CRFs). Parameter estimation and sequence inference in the DHCRF are developed in this… ", "references": ["8142355a9b964fc270f16e5111ba3d969b01c3db", "43cfb0a633ebd33d5a67a6f8103b754b94d72ec9", "64595fd3db21e4e07252d8a2a5d640d2d7dd916d", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "8142355a9b964fc270f16e5111ba3d969b01c3db", "43cfb0a633ebd33d5a67a6f8103b754b94d72ec9", "f30da12f78987cd18e007a1a5312605081c5ac62", "930bde26f600dade443e88af0e81c9695a96294e", "f30da12f78987cd18e007a1a5312605081c5ac62", "ad4cfdb0c0bb62bf1fa99cb3d0519cadd9cbc160"]},{"id": "dcab6c0b486633f2f7722cde18371a18ca9da3bd", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "authors": ["Isabelle Augenstein", "Anders Søgaard"], "date": "2017", "abstract": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi… ", "references": ["ac17cfa150d802750b46220084d850cfdb64d1c1", "652a7e6090a6b10cbeb0883ddac2002620aa8e69", "652a7e6090a6b10cbeb0883ddac2002620aa8e69", "9405d0388f90ba1432ef13c21309d8363860e22e", "652a7e6090a6b10cbeb0883ddac2002620aa8e69", "ac17cfa150d802750b46220084d850cfdb64d1c1", "03ad06583c9721855ccd82c3d969a01360218d86", "652a7e6090a6b10cbeb0883ddac2002620aa8e69", "ac17cfa150d802750b46220084d850cfdb64d1c1", "2f83f6e1afadf0963153974968af6b8342775d82"]},{"id": "c1116b32168ca91607d81e8aa6be64ee7b539449", "title": "CDNN: a context dependent neural network for continuous speech recognition", "authors": ["Hervé Bourlard", "Nelson Morgan", "Steve Renals"], "date": "1992", "abstract": "A series of theoretical and experimental results have suggested that multilayer perceptrons (MLPs) are an effective family of algorithms for the smooth estimate of highly dimensioned probability density functions that are useful in continuous speech recognition. All of these systems have exclusively used context-independent phonetic models, in the sense that the probabilities or costs are estimated for simple speech units such as phonemes or words, rather than biphones or triphones. Numerous… ", "references": []},{"id": "1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6", "title": "Transfer Learning for Low-Resource Neural Machine Translation", "authors": ["Barret Zoph", "Deniz Yuret", "Kevin Knight"], "date": "2016", "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training… ", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "d01737b617acc555153f4660417908bf3971b1a5", "d01737b617acc555153f4660417908bf3971b1a5", "d01737b617acc555153f4660417908bf3971b1a5", "83cf4b2f39bcc802b09fd59b69e23068447b26b7", "1956c239b3552e030db1b78951f64781101125ed", "d01737b617acc555153f4660417908bf3971b1a5", "1956c239b3552e030db1b78951f64781101125ed", "1956c239b3552e030db1b78951f64781101125ed", "71480da09af638260801af1db8eff6acb4e1122f"]},{"id": "acd87e4f672f0b92ea4164414c213560c23bee52", "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "authors": ["Barbara Plank", "Anders Søgaard", "Yoav Goldberg"], "date": "2016", "abstract": "Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the… ", "references": ["891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "af88ce6116c2cd2927a4198745e99e5465173783", "af88ce6116c2cd2927a4198745e99e5465173783", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "cea967b59209c6be22829699f05b8b1ac4dc092d", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "318fb72e53eb43667a78f32ec22f7e3135036e1b", "c34e41312b47f60986458759d5cc546c2b53f748", "c34e41312b47f60986458759d5cc546c2b53f748"]},{"id": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "title": "Show and tell: A neural image caption generator", "authors": ["Oriol Vinyals", "Alexander Toshev", "Dumitru Erhan"], "date": "2015", "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.", "references": ["2e36ea91a3c8fbff92be2989325531b4002e2afc", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "b8de958fead0d8a9619b55c7299df3257c624a96", "fbdbe747c6aa8b35b981d21e475ff1506a1bae66", "cea967b59209c6be22829699f05b8b1ac4dc092d", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "fbdbe747c6aa8b35b981d21e475ff1506a1bae66", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141"]},{"id": "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "title": "Investigation of full-sequence training of deep belief networks for speech recognition", "authors": ["Abdel-rahman Mohamed", "Dong Yu", "Li Deng"], "date": "INTERSPEECH", "abstract": "Recently, Deep Belief Networks (DBNs) have been proposed for phone recognition and were found to achieve highly competitive performance.", "references": ["2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "b6c94cc324f585bd6c004f2b99b5589568643e45", "64595fd3db21e4e07252d8a2a5d640d2d7dd916d", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "d9d2ba2003d7324ae3d5ff7423a13f13efc79ca5", "1603a40b7bb56d563d9401f0d24c67d428e509f2", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "930bde26f600dade443e88af0e81c9695a96294e", "d9d2ba2003d7324ae3d5ff7423a13f13efc79ca5", "e3c1bf806c325f306e5084c3bd332b83d2077e2a"]},{"id": "90b63e917d5737b06357d50aa729619e933d9614", "title": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine", "authors": ["George E. Dahl", "Marc'Aurelio Ranzato", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally… ", "references": ["8f72052f0d67a63756b79fe12d7e36ad338b616c", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "e084bbb9cbbce7c0d282df263cf70cba4042f067", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "8f72052f0d67a63756b79fe12d7e36ad338b616c", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "28c322dc17c8d1dcf1a868f210e0276b288aaa34", "8f613e5481d8d567573b0ffa7b8e1c5b07d33a04", "e084bbb9cbbce7c0d282df263cf70cba4042f067"]},{"id": "df5b82595a29724467a98eed4d7e2a45e804579e", "title": "Speech Recognition Using Augmented Conditional Random Fields", "authors": ["Yasser Hifny", "Steve Renals"], "date": "2009", "abstract": "Acoustic modeling based on hidden Markov models (HMMs) is employed by state-of-the-art stochastic speech recognition systems. Although HMMs are a natural choice to warp the time axis and model the temporal phenomena in the speech signal, their conditional independence properties limit their ability to model spectral phenomena well. In this paper, a new acoustic modeling paradigm based on augmented conditional random fields (ACRFs) is investigated and developed. This paradigm addresses some… ", "references": ["df53e0dc66eb13bb51c6e4803ceae56d3ebe6f23", "5bc06dbc8715b0348192bc545ac9037f98608ed0", "5bc06dbc8715b0348192bc545ac9037f98608ed0", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "ecb6e57ae893cb1b6eda8908940f23787415eaa8", "8644ef2fd2cb0ab150cc13eb601f8353c6a306bf", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "ecb6e57ae893cb1b6eda8908940f23787415eaa8", "aa321cbc2482116f32eaa54a2f393229afa48398", "a27fc84fb14188f7149bd5cae9b24b7eaed3e588"]},{"id": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec", "title": "Multi-Scale Context Aggregation by Dilated Convolutions", "authors": ["Fisher Yu", "Vladlen Koltun"], "date": "2016", "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The… ", "references": ["6f9f143ec602aac743e07d092165b708fa8f1473", "39ad6c911f3351a3b390130a6e4265355b4d593b", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "39ad6c911f3351a3b390130a6e4265355b4d593b", "39ad6c911f3351a3b390130a6e4265355b4d593b", "9f48616039cb21903132528c0be5348b3019db50", "9f48616039cb21903132528c0be5348b3019db50", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "9f48616039cb21903132528c0be5348b3019db50"]},{"id": "51239b320c73f3f2219286bf62f24d6763379328", "title": "Associating neural word embeddings with deep image representations using Fisher Vectors", "authors": ["Benjamin Klein", "Guy Lev", "Lior Wolf"], "date": "2015", "abstract": "In recent years, the problem of associating a sentence with an image has gained a lot of attention. This work continues to push the envelope and makes further progress in the performance of image annotation and image search by a sentence tasks. In this work, we are using the Fisher Vector as a sentence representation by pooling the word2vec embedding of each word in the sentence. The Fisher Vector is typically taken as the gradients of the log-likelihood of descriptors, with respect to the… ", "references": ["34f2f2dbaca68eb05426b51620673e71b69e1b37", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "d4cede3acfd94fccc927519e04384a8debfec705", "23694b6d61668e62bb11f17c1d75dde3b4951948", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54"]},{"id": "d41cfe9b2ada4e09d53262bc75c473d8043936fc", "title": "Self-Adaptive Hierarchical Sentence Model", "authors": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart"], "date": "IJCAI", "abstract": "The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to allow the representations of… ", "references": ["81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "687bac2d3320083eb4530bf18bb8f8f721477600", "687bac2d3320083eb4530bf18bb8f8f721477600", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "27e38351e48fe4b7da2775bf94341738bc4da07e", "687bac2d3320083eb4530bf18bb8f8f721477600", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee"]},{"id": "39ad6c911f3351a3b390130a6e4265355b4d593b", "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs", "authors": ["Liang-Chieh Chen", "George Papandreou", "Alan L. Yuille"], "date": "2015", "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection.", "references": ["a9ce496186120df8f9ed3367e76a4947419e992e", "342786659379879f58bf5c4ff43c84c83a6a7389", "0959ef8fefe9e7041f508c2448fc026bc9e08393", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "a9ce496186120df8f9ed3367e76a4947419e992e", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "b49c25e6e40fffb10d3e90d54b876a54e6725962", "b49c25e6e40fffb10d3e90d54b876a54e6725962", "e56bb892c581f682052ddd3896c65a2b29e64612", "317aee7fc081f2b137a85c4f20129007fd8e717e"]},{"id": "10a4db59e81d26b2e0e896d3186ef81b4458b93f", "title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "authors": ["Jason P. C. Chiu", "Eric Nichols"], "date": "2016", "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance.", "references": ["91a93f751f912b2f96d6771018d8f06c41e11152", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "aa9efc8b2737eac0675ba5abb5feab8305482c12", "aa9efc8b2737eac0675ba5abb5feab8305482c12", "9533de7fb7c0cbc78b00c5f6181a8378acd9ddc1", "91a93f751f912b2f96d6771018d8f06c41e11152", "f8cdf754fb7c08caf6e2f82b176819230910be5b", "00cf902b27676cdc376e26567e70298b96c672a1", "aa9efc8b2737eac0675ba5abb5feab8305482c12"]},{"id": "7647a06965d868a4f6451bef0818994100a142e8", "title": "Empower Sequence Labeling with Task-Aware Neural Language Model", "authors": ["Liyuan Liu", "Jingbo Shang", "Jiawei Han"], "date": "AAAI", "abstract": "Linguistic sequence labeling is a general modeling approach that encompasses a variety of problems, such as part-of-speech tagging and named entity recognition.", "references": ["8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "189e6bb7523733c4e524214b9e6ae92d4ed50dac", "10a4db59e81d26b2e0e896d3186ef81b4458b93f", "4bb8107199208080123d1bf5d5ddf233cf530adf", "bc1022b031dc6c7019696492e8116598097a8c12", "ade0c116120b54b57a91da51235108b75c28375a", "ade0c116120b54b57a91da51235108b75c28375a", "10a4db59e81d26b2e0e896d3186ef81b4458b93f"]},{"id": "d7db74be6cda0ec2bd28ec187563def85ccef78f", "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings", "authors": ["Rie Johnson", "Tong Zhang"], "date": "ICML", "abstract": "One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson & Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region… ", "references": ["2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "ecb5336bf7b54a62109f325e7152bb74c4c7f527", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "ecb5336bf7b54a62109f325e7152bb74c4c7f527", "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc", "cea967b59209c6be22829699f05b8b1ac4dc092d", "a925892c520f2bda9d274bd64789130106392242", "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc", "ecb5336bf7b54a62109f325e7152bb74c4c7f527"]},{"id": "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "title": "Deep visual-semantic alignments for generating image descriptions", "authors": ["Andrej Karpathy", "Fei-Fei Li"], "date": "CVPR", "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding… ", "references": []},{"id": "f83d712e1afb83fd45628261ede23fb112ec1666", "title": "OxLM: A Neural Language Modelling Framework for Machine Translation", "authors": ["Baltescu Paul", "Blunsom Phil", "Hoang Trung Hieu"], "date": "2014", "abstract": "Abstract This paper presents an open source implementation1 of a neural language model for machine translation. Neural language models deal with the problem of data sparsity by learning distributed representations for words in a continuous vector space. The language modelling probabilities are estimated by projecting a word's context in the same space as the word representations and by assigning probabilities proportional to the distance between the words and the context's projection. Neural… ", "references": ["86151fd48b2578ac1232bd927e07a8815144496a", "7b5e31257f01aba987f16e175a3e49e00a5bd3bb", "6053d1693c0fab0c21ebbea0ba5408b441a3542b", "46f418bf6fab132f193661226c5c27d67f870ea5", "6053d1693c0fab0c21ebbea0ba5408b441a3542b", "6053d1693c0fab0c21ebbea0ba5408b441a3542b", "86151fd48b2578ac1232bd927e07a8815144496a", "71480da09af638260801af1db8eff6acb4e1122f", "7b5e31257f01aba987f16e175a3e49e00a5bd3bb", "46f418bf6fab132f193661226c5c27d67f870ea5"]},{"id": "3090262c765a95fe9cd975fa12b4ec15391ece6d", "title": "Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation", "authors": ["Rui Wang", "Zhao Hai", "Eiichiro Sumita"], "date": "EMNLP", "abstract": "Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary. In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT. The results show that our method can improve both the… ", "references": ["6aaa677da0e22fefae29182c2562de790902631f", "71480da09af638260801af1db8eff6acb4e1122f", "5d43224147a5bb8b17b6a6fc77bf86490e86991a", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "47e3d8a1f8e92923e739ca34bea17004a40514e9", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "6aaa677da0e22fefae29182c2562de790902631f", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "6aaa677da0e22fefae29182c2562de790902631f", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97"]},{"id": "ade34bf617f7733bfa0676f2bd57fa5658d4e54c", "title": "Efficient data selection for machine translation", "authors": ["Arindam Mandal", "Dimitra Vergyri", "Necip Fazil Ayan"], "date": "2008", "abstract": "Performance of statistical machine translation (SMT) systems relies on the availability of a large parallel corpus which is used to estimate translation probabilities. However, the generation of such corpus is a long and expensive process. In this paper, we introduce two methods for efficient selection of training data to be translated by humans. Our methods are motivated by active learning and aim to choose new data that adds maximal information to the currently available data pool. The first… ", "references": ["bcf4ced6b5a253c7aec557c95e32d802eac9c414", "81164d7f2d676b6044888219426cbb248a020930", "eddc57c88fcef195538035eb205355db656cba98", "ad351d3e221882950ece7eca9f13fbcf6973f29d", "81164d7f2d676b6044888219426cbb248a020930", "81164d7f2d676b6044888219426cbb248a020930", "ad351d3e221882950ece7eca9f13fbcf6973f29d", "7e6d09530490561f1fc4dbfbd82fc4ff456f046c", "81164d7f2d676b6044888219426cbb248a020930", "15c28b0edbd2296324c07a0f218de83033781831"]},{"id": "d36b19b4c5977dd2a2796a5ad3508a3d8a087809", "title": "CSLM - a modular open-source continuous space language modeling toolkit", "authors": ["Holger Schwenk"], "date": "INTERSPEECH", "abstract": "Language models play a very important role in many natural language processing applications, in particular large vocabulary speech recognition and statistical machine translation.", "references": ["d4a258df43cc14e46988de9a4a7b2f0ea817529b", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "d4f00f97581d30e7104d1fe924085248093aa472", "028a35d6835b2717c02a7acd5be1c71e0749df26", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "8b395470a57c48d174c4216ea21a7a58bc046917", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "028a35d6835b2717c02a7acd5be1c71e0749df26", "d4a258df43cc14e46988de9a4a7b2f0ea817529b"]},{"id": "03a460c8ca331d36e8f2e11edd49e7dbc35c7e43", "title": "Reducing Weight Undertraining in Structured Discriminative Learning", "authors": ["Charles A. Sutton", "Michael Sindelar", "Andrew McCallum"], "date": "HLT-NAACL", "abstract": "Discriminative probabilistic models are very popular in NLP because of the latitude they afford in designing features. But training involves complex trade-offs among weights, which can be dangerous: a few highly-indicative features can swamp the contribution of many individually weaker features, causing their weights to be undertrained. Such a model is less robust, for the highly-indicative features may be noisy or missing in the test data. To ameliorate this weight undertraining, we introduce… ", "references": ["61a559a5ab77b449758795c86c6ff8a42b389987", "b4299baa815ca5a815a70fba94a9f6f2b42fff19", "eb42a490cf4f186d3383c92963817d100afd81e2", "07b27e79099f00a8d50f9e529e6b325ed827ead2", "6a07215a1790f7e0656e5d4ebedb4ba8570a6c36", "b4299baa815ca5a815a70fba94a9f6f2b42fff19", "6a07215a1790f7e0656e5d4ebedb4ba8570a6c36", "eb42a490cf4f186d3383c92963817d100afd81e2", "fd0060b30c2741375ed45e8a29f22931c7f42d75", "1e19a94d547ee023837c14c361139185e2353fc0"]},{"id": "e2d4df61a787210c67041929f6a43203bee99edf", "title": "Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning", "authors": ["Mengqiu Wang", "Christopher D. Manning"], "date": "2013", "abstract": "We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a… ", "references": ["0cae1bcefe1edb5c7ecfe04b94bb2b21285d3c6e", "3dd63c9b5ea5df6ec020a34bf8ea7c1bf15997c7", "c70550f81e3d582da97f82777ac502cf4652d6e1", "7744a4e9e59f8e43a01f464ec452bd216fd99688", "3dd63c9b5ea5df6ec020a34bf8ea7c1bf15997c7", "343733a063e491d234a36d3e1090a739318b3566", "327c88dd06722a967be9c6b1176fbd79554967e7", "a3d3f7f8bbe1ebf0eeb68b85ca34b34791d3a23e", "c70550f81e3d582da97f82777ac502cf4652d6e1", "7744a4e9e59f8e43a01f464ec452bd216fd99688"]},{"id": "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "title": "Shallow Parsing with Conditional Random Fields", "authors": ["Fei Sha", "Fernando C Pereira"], "date": "HLT-NAACL", "abstract": "Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.", "references": ["d9c71db75046473f0e3d3229950d7c84c09afd5e", "02c8a0bc8bab9920e6615cfacf1df2ab3f2b1f68", "7f8f8f33187e20768ae0177780ac5ef78b77feca", "ba93f0b457de350e5bb8fcc9e647c85eaa046991", "d9c71db75046473f0e3d3229950d7c84c09afd5e", "cb0690094be9d21334745f917b0adc4d87e0e898", "cb0690094be9d21334745f917b0adc4d87e0e898", "ba93f0b457de350e5bb8fcc9e647c85eaa046991", "d9c71db75046473f0e3d3229950d7c84c09afd5e", "54c846ee00c6132d70429cc279e8577f63ed05e4"]},{"id": "1956c239b3552e030db1b78951f64781101125ed", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "authors": ["Thang Luong", "Ilya Sutskever", "Wojciech Zaremba"], "date": "ACL", "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We… ", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "97cedf99252026f58e8154bc61d49cf885d42030", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "97cedf99252026f58e8154bc61d49cf885d42030", "a4b828609b60b06e61bea7a4029cc9e1cad5df87"]},{"id": "0d1e90fa2457d34e6efead247a1a75ef76a48160", "title": "Semi-Supervised Structured Output Learning Based on a Hybrid Generative and Discriminative Approach", "authors": ["Jun Suzuki", "Akinori Fujino", "Hideki Isozaki"], "date": "EMNLP-CoNLL", "abstract": "This paper proposes a framework for semi-supervised structured output learning (SOL), specifically for sequence labeling, based on a hybrid generative and discriminative approach. We define the objective function of our hybrid model, which is written in log-linear form, by discriminatively combining discriminative structured predictor(s) with generative model(s) that incorporate unlabeled data. Then, unlabeled data is used in a generative manner to increase the sum of the discriminant functions… ", "references": ["125842668eab7decac136db8a59d392dc5e4e395", "1e834157de8b7a090747842359fbb7060680c45a", "2648926c90f6cc9c9fa2e9590bc9b3df88ac9cd2", "d5f5110d65eda0d2df7329582a232a86bf9a3a65", "125842668eab7decac136db8a59d392dc5e4e395", "2648926c90f6cc9c9fa2e9590bc9b3df88ac9cd2", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "d5f5110d65eda0d2df7329582a232a86bf9a3a65", "125842668eab7decac136db8a59d392dc5e4e395", "ad67ccee45b801b0138016e2f44a566344e77320"]},{"id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "title": "Combining labeled and unlabeled data with co-training", "authors": ["Avrim Blum", "Tom Mitchell"], "date": "COLT' 98", "abstract": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point… ", "references": []},{"id": "e2de29049d62de925cf709024b92774cd82b0a5a", "title": "Text Classification from Labeled and Unlabeled Documents using EM", "authors": ["Kamal Nigam", "Andrew McCallum", "Tom Michael Mitchell"], "date": "2004", "abstract": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive… ", "references": ["80ef14d2a1b8c7efbf45bedae9d001fe5446c7de", "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33", "5f78d6f79b3ef103cb2d8d170632eb74d9496412", "bc8e59e4c7c2cbb6695ee5488aa569780449b212", "c608ec27a937361122d178b38b6b7387440b58eb", "f2671b151fad7e176176b35d425b2b6356ff4595", "890c16ca29a781a7b793c603822ffd57aee9f57f", "bc8e59e4c7c2cbb6695ee5488aa569780449b212", "5f78d6f79b3ef103cb2d8d170632eb74d9496412", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4"]},{"id": "ad945987071d3c5b4b915b85e09ae3488b2212c0", "title": "New Transfer Learning Techniques for Disparate Label Sets", "authors": ["Young-Bum Kim", "Karl Stratos", "Minwoo Jeong"], "date": "ACL", "abstract": "In natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar). Standard domain adaptation techniques are not directly applicable to take advantage of the existing annotations because they assume that the label set is invariant. We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a… ", "references": ["8c4c181abcfd6ed7672a4782f7af3489b01490b4", "f83f4c72cd43eb99bde196ba33d034380fec3789", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "8c4c181abcfd6ed7672a4782f7af3489b01490b4", "f83f4c72cd43eb99bde196ba33d034380fec3789", "8c4c181abcfd6ed7672a4782f7af3489b01490b4", "6de6d6b35d1b7ac21c6c7252a1cfb445379e6e72", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "b672ef69f60aea81220d658963445c41e60bb0e3", "855530bd8af356b5d12c1ac66ee056e03c81c124"]},{"id": "4dabd6182ce2681c758f654561d351739e8df7bf", "title": "Multilingual Language Processing From Bytes", "authors": ["Daniel Gillick", "Cliff Brunk", "Amarnag Subramanya"], "date": "2016", "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar… ", "references": ["93a9694b6a4149e815c30a360347593b75860761", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "bc1022b031dc6c7019696492e8116598097a8c12", "93a9694b6a4149e815c30a360347593b75860761", "93a9694b6a4149e815c30a360347593b75860761", "bc1022b031dc6c7019696492e8116598097a8c12", "91a93f751f912b2f96d6771018d8f06c41e11152", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "bc1022b031dc6c7019696492e8116598097a8c12"]},{"id": "c1088267b10e19d565865c2dcf0bc0f94696bf2e", "title": "Learning to Understand Phrases by Embedding the Dictionary", "authors": ["Felix Hill", "Kyunghyun Cho", "Yoshua Bengio"], "date": "2016", "abstract": "Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words… ", "references": ["d1f37d9cab68eb8cda669cc949394732f33264b4", "3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4", "3f3d110cf78f759760c354bfde1b2ceb9883c544", "3f3d110cf78f759760c354bfde1b2ceb9883c544", "af44f5db5b4396e1670cda07eff5ad84145ba843", "fc303b3b0be5b476ae5f3d8414b685e91d0378c6", "fc303b3b0be5b476ae5f3d8414b685e91d0378c6", "d1f37d9cab68eb8cda669cc949394732f33264b4", "70600593f870f460624c56c2a57b9a03b94f94a5", "3f3d110cf78f759760c354bfde1b2ceb9883c544"]},{"id": "6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2", "title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "authors": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "date": "2015", "abstract": "Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks.", "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "5079c42cb329f70a4f2b94fd429ea8d35a043f56", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f4c018bcc8ea707b83247866bdc8ccb87cd9f5da", "3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "53ab89807caead278d3deb7b6a4180b277d3cb77", "5079c42cb329f70a4f2b94fd429ea8d35a043f56", "fae0dd3a8350fad208089a1b3b7ac3a366939d68"]},{"id": "d86227948b6000e5d7ed63cf2054ad600b7994a0", "title": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification", "authors": ["Mohit Iyyer", "Varun Manjunatha", "Hal Daumé"], "date": "ACL", "abstract": "Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their inputs, which requires many expensive computations.", "references": ["687bac2d3320083eb4530bf18bb8f8f721477600", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "c75b30bc3f3282b378f6f48071c7d26c1ee88f37", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "687bac2d3320083eb4530bf18bb8f8f721477600"]},{"id": "03ad06583c9721855ccd82c3d969a01360218d86", "title": "Deep multi-task learning with low level tasks supervised at lower layers", "authors": ["Anders Søgaard", "Yoav Goldberg"], "date": "ACL", "abstract": "In all previous work on deep multi-task learning we are aware of, all task supervisions are on the same (outermost) layer.", "references": ["e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "7ece4e8d31f872d928369ac2cf58a616a7182112", "8334df0103443b78546a3cec9284f668106774fc", "7ece4e8d31f872d928369ac2cf58a616a7182112", "71e90c4015e3dea597dbd583f3d3d08cdc0077fb", "8334df0103443b78546a3cec9284f668106774fc", "71e90c4015e3dea597dbd583f3d3d08cdc0077fb", "bc1022b031dc6c7019696492e8116598097a8c12", "8334df0103443b78546a3cec9284f668106774fc", "8334df0103443b78546a3cec9284f668106774fc"]},{"id": "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2", "title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference", "authors": ["Qian Chen", "Xiao-Dan Zhu", "Hui Jiang"], "date": "2016", "abstract": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is notoriously challenging but is fundamental to natural language understanding and many applications. With the availability of large annotated data, neural network models have recently advanced the field significantly. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.3% on the standard benchmark, the Stanford Natural Language Inference dataset… ", "references": ["36c097a225a95735271960e2b63a2cb9e98bff83", "523f420cb55d8070f565c87a50099a9a5b0b9206", "cff79255a94b9b05a4ce893eb403a522e0923f04", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "8dbb0b9ca61e2753c6759446c6909acda616095a", "f93a0a3e8a3e6001b4482430254595cf737697fa", "705dcc8eadba137834e4b0359e2d696d4b209f5b", "705dcc8eadba137834e4b0359e2d696d4b209f5b", "36c097a225a95735271960e2b63a2cb9e98bff83", "705dcc8eadba137834e4b0359e2d696d4b209f5b"]},{"id": "602af317fe1ff52f254a824df5880505e086c76d", "title": "The First International Chinese Word Segmentation Bakeoff", "authors": ["Richard Sproat", "Thomas Emerson"], "date": "SIGHAN", "abstract": "This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the… ", "references": ["a8307715a3dd8cb4857b88e93d358438a9467762", "f5433dfc8ced247847e97c5f270fd3b91ae8c0fa", "f5433dfc8ced247847e97c5f270fd3b91ae8c0fa"]},{"id": "d76c07211479e233f7c6a6f32d5346c983c5598f", "title": "Multi-task Sequence to Sequence Learning", "authors": ["Minh-Thang Luong", "Quoc V. Le", "Lukasz Kaiser"], "date": "2016", "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder… ", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "c3b8367a80181e28c95630b9b63060d895de08ff", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "93499a7c7f699b6630a86fad964536f9423bb6d0", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "2826f9dccdcceb113b33ccf2841d488f1419bb30", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "c3b8367a80181e28c95630b9b63060d895de08ff"]},{"id": "b40719006ef3d7d90a9b826341bc463a8c9c1fd8", "title": "A Note on Semi-Supervised Learning using Markov Random Fields", "authors": ["Wei Li", "Andrew McCallum"], "date": "2004", "abstract": "This paper describes conditional-probability training of Markov random elds using combinations of labeled and unlabeled data. We capture the similarities between instances learning the appropriate distance metric from the data. The likelihood model and several training procedures are presented. ", "references": ["278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "0eedbab3ae55fd6a4e7bbc75fcc261293384f883", "6bbc0c752570c46a772f2982728f9ad4191f25dd", "e2de29049d62de925cf709024b92774cd82b0a5a", "138b6767d572e84147da34dd38573b0eff5171b7", "50c56af1eb05cfb6ec81e84a6924fb46cb202747", "8e6779bb55f7fbed5684ded55df51747ea678a84", "50c56af1eb05cfb6ec81e84a6924fb46cb202747", "125842668eab7decac136db8a59d392dc5e4e395", "34dc22dcbdf1e09fb48691ee1fc6fe4bb8f834c3"]},{"id": "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "date": "2015", "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of… ", "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "27e38351e48fe4b7da2775bf94341738bc4da07e", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "687bac2d3320083eb4530bf18bb8f8f721477600", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "687bac2d3320083eb4530bf18bb8f8f721477600", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "687bac2d3320083eb4530bf18bb8f8f721477600"]},{"id": "125842668eab7decac136db8a59d392dc5e4e395", "title": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions", "authors": ["Xiaojin Zhu", "Zoubin Ghahramani", "John D. Lafferty"], "date": "ICML", "abstract": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model.", "references": ["1150f9289c6151506e3f7cf0e6ebbcfd49f1dace", "8f79332b1361e9eaa9da3327f83f57dcac5cd11d", "1150f9289c6151506e3f7cf0e6ebbcfd49f1dace", "01dfd7b78017ea4059f02081680a9fd4b2bb2a34", "6f17768a9fe231a2fd38708be90f98db3890c986", "6f17768a9fe231a2fd38708be90f98db3890c986", "01dfd7b78017ea4059f02081680a9fd4b2bb2a34", "8f79332b1361e9eaa9da3327f83f57dcac5cd11d", "a8797f1d253c75669d96e6fcceda2be3f8534e1d"]},{"id": "0ecc5ffeae38689dd2fe6ed4c32a6745744d7641", "title": "Integrating Topics and Syntax", "authors": ["Thomas L. Griffiths", "Mark Steyvers", "Joshua B. Tenenbaum"], "date": "NIPS", "abstract": "Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that… ", "references": ["f198043a866e9187925a8d8db9a55e3bfdd47f2c"]},{"id": "3de5d40b60742e3dfa86b19e7f660962298492af", "title": "Class-Based n-gram Models of Natural Language", "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Robert L. Mercer"], "date": "1992", "abstract": "We address the problem of predicting a word from previous words in a sample of text.", "references": []},{"id": "c76c62c5ab6c076a80f925d277ef04dd36f6bf9c", "title": "The information bottleneck method", "authors": ["Naftali Tishby", "Fernando C Pereira", "William Bialek"], "date": "1999", "abstract": "We define the relevant information in a signal $x\\in X$ as being the information that this signal provides about another signal $y\\in \\Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\\X$ play a role in the prediction. We formalize this problem as that of… ", "references": ["6e50f382197a590765e6d6ee9d6ec899434097e6", "6e50f382197a590765e6d6ee9d6ec899434097e6", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "90507f43e958c59ccea41aafd06ab2728749429c", "6e50f382197a590765e6d6ee9d6ec899434097e6", "26dbfca1155d823b72db68663af3262b3da08da6"]},{"id": "ad269ba941949a1d66b6649a71d752784c576dc3", "title": "Name Tagging with Word Clusters and Discriminative Training", "authors": ["Scott Miller", "Jethran Guinness", "Alex Zamanian"], "date": "HLT-NAACL", "abstract": "We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. Cluster membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to… ", "references": ["278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "3de5d40b60742e3dfa86b19e7f660962298492af", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "41e936981f5a2d55bfec0143e9a15e23ad96436b", "944cba683d10d8c1a902e05cd68e32a9f47b372e", "944cba683d10d8c1a902e05cd68e32a9f47b372e", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4"]},{"id": "83d9fec6bee4cf6e01445d6ff51aae58b6d96d5a", "title": "Expressing Implicit Semantic Relations without Supervision", "authors": ["Peter D. Turney"], "date": "2006", "abstract": "We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations. For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns (P1,..., Pm) is ranked according to how well each pattern Pi expresses the relations between X and Y. For example, given X = ostrich and Y = bird, the two highest ranking output patterns are \"X is the largest Y\" and \"Y such as the X\". The output… ", "references": ["0565c6903f86e2c649c326ee7607ecc5ccef3a0f", "2ebfd4a8f0ac488a1bc6b86fb4e2e38071adf841", "0565c6903f86e2c649c326ee7607ecc5ccef3a0f", "e02837ac075543fe1b04f3003133a6015564d443", "dbfd191afbbc8317577cbc44afe7156df546e143", "212d2715aee9fbefe140685b088b789d6c8277b0", "0565c6903f86e2c649c326ee7607ecc5ccef3a0f", "212d2715aee9fbefe140685b088b789d6c8277b0", "68c03788224000794d5491ab459be0b2a2c38677", "212d2715aee9fbefe140685b088b789d6c8277b0"]},{"id": "4b88329392a75287942d85f42012f47f356a2714", "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations", "authors": ["Peter D. Turney"], "date": "COLING", "abstract": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under… ", "references": ["be3f6b91b0eae23e37b3fb877b6cc7fc7dfcef5a", "212d2715aee9fbefe140685b088b789d6c8277b0", "e517e1645708e7b050787bb4734002ea194a1958", "e46c547dcbf65d26e85ff723bae34067cceb3477", "e46c547dcbf65d26e85ff723bae34067cceb3477", "f3844a65398d3ef22e3c6323e2ec94388e033bb4", "1755f2ac7eb8a091f3613be47da844803df86fc1", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "e02837ac075543fe1b04f3003133a6015564d443", "be3f6b91b0eae23e37b3fb877b6cc7fc7dfcef5a"]},{"id": "509a2ca90a85c62d66a16b37e0de28715dd4e89f", "title": "Topics in semantic representation.", "authors": ["Thomas L. Griffiths", "Mark Steyvers", "Joshua B. Tenenbaum"], "date": "2007", "abstract": "Processing language requires the retrieval of concepts from memory in response to an ongoing stream of information. This retrieval is facilitated if one can infer the gist of a sentence, conversation, or document and use that gist to predict related concepts and disambiguate words. This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads to a novel approach to semantic… ", "references": ["06cb835bda3420186e2c6f6fa2dbc1613a9b2d75", "74afa62f0f35a3153230c488ab223a5afb4e8b3e", "1d8d5b866e19b4072d5cd1a2c0659468db300b57", "c84f76770b820d69a6a1f3914a1c84e7c20a8271", "06cb835bda3420186e2c6f6fa2dbc1613a9b2d75", "06cb835bda3420186e2c6f6fa2dbc1613a9b2d75", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "c84f76770b820d69a6a1f3914a1c84e7c20a8271", "c84f76770b820d69a6a1f3914a1c84e7c20a8271"]},{"id": "e02837ac075543fe1b04f3003133a6015564d443", "title": "Corpus-based Learning of Analogies and Semantic Relations", "authors": ["Peter D. Turney", "Michael L. Littman"], "date": "2005", "abstract": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning “A is to B as C is to D”; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly… ", "references": ["dbfd191afbbc8317577cbc44afe7156df546e143", "4966f2d75734b4abd4ad105b85eff675cb781b5d", "54c2d1d5b8855a7e1b1e87995215c1cf4662f654", "f932caac89709a716a7d3e6632caf9f34d709518", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "4656da2393dc4dc5935989483a176a07beb59dc1", "f932caac89709a716a7d3e6632caf9f34d709518", "fbd0e0ad4e06902b10b6a157b9db92df577720f1", "fbd0e0ad4e06902b10b6a157b9db92df577720f1", "4656da2393dc4dc5935989483a176a07beb59dc1"]},{"id": "afd5da111e80fd0ee783b1ecf1bcb1be8cbff8f4", "title": "Modelling Word Similarity: an Evaluation of Automatic Synonymy Extraction Algorithms", "authors": ["Kris Heylen", "Yves Peirsman", "Dirk Speelman"], "date": "LREC", "abstract": "Vector-based models of lexical semantics retrieve semantically related words automatically from large corpora by exploiting the property that words with a similar meaning tend to occur in similar contexts. Despite their increasing popularity, it is unclear which kind of semantic similarity they actually capture and for which kind of words. In this paper, we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic… ", "references": ["3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "fd1901f34cc3673072264104885d70555b1a4cdc", "fd1901f34cc3673072264104885d70555b1a4cdc", "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb", "fd1901f34cc3673072264104885d70555b1a4cdc", "46d946344e5ed8871b113af3e498c4640fb31d9b", "b791d488eef45ef79da812f7569fc2cc83196aa5", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "46d946344e5ed8871b113af3e498c4640fb31d9b", "46d946344e5ed8871b113af3e498c4640fb31d9b"]},{"id": "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "title": "Dependency-Based Construction of Semantic Space Models", "authors": ["Sebastian Padó", "Mirella Lapata"], "date": "2007", "abstract": "Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic… ", "references": ["384720361a8cae1f2bbb87e05215cc58fef74ac1", "084c55d6432265785e3ff86a2e900a49d501c00a", "1521ddb27860cc8834f8a82e62665bf983c8ad2c", "3751d4d5e9332f6e98824a6fd814e2ce8f497daf", "6b64e068a8face2540fc436af40dbcd2b0912bbf", "1521ddb27860cc8834f8a82e62665bf983c8ad2c", "3751d4d5e9332f6e98824a6fd814e2ce8f497daf", "084c55d6432265785e3ff86a2e900a49d501c00a", "e8066c5522ebfa8f0f08589dcbe5f315bfec90c1", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4"]},{"id": "2cafda848586a9a4061a11227adbaf70720644c8", "title": "Automatic Verb Classification Based on Statistical Distributions of Argument Structure", "authors": ["Paola Merlo", "Suzanne Stevenson"], "date": "2001", "abstract": "Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks. Especially important is knowledge about verbs, which are the primary source of relational information in a sentence-the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure… ", "references": ["a4a1b6b2580609e5a64fd62ea4f1ff6bcc2ba8bd", "d61b4f56253d65b586a5553768c40168e897dff7", "d61b4f56253d65b586a5553768c40168e897dff7", "19093faac77eb132b2c8ce21a20f66718e60f2b6", "42053bb5bc617a3c75c56f61cc6be1e8e53f4e29", "8b6ce9178000cf7304694e56bbe804e2674d4a7f", "d61b4f56253d65b586a5553768c40168e897dff7", "6e986f302a0b0b0fd700c89b94ec54585c5e45a7", "19093faac77eb132b2c8ce21a20f66718e60f2b6", "45de5bfc4895b1cdb8177cf312327c60ca513099"]},{"id": "2cda689911f9dcf7e1555e5cb0a25ae93b95ad0b", "title": "Concepts and properties in word spaces", "authors": ["Marco Baroni", "Alessandro Lenci"], "date": "2008", "abstract": "Properties play a central role in most theories of conceptual knowledge. Since computational models derived from word co-occurrence statistics have been claimed to provide a natural basis for semantic representations, the question arises of whether such models are capable of producing reasonable property-based descriptions of concepts, and whether these descriptions are similar to those elicited from humans. This article presents a qualitative analysis of the properties generated by humans in… ", "references": ["68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "b2c04cc369b8f08f399d5fb95ddc884d52cfebd2", "40f616fe271de6908b87f04a4d39f75aba41c3bc", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "40f616fe271de6908b87f04a4d39f75aba41c3bc", "94a461dd1bd2c70c562fc0791a1cb7d933c83e01", "dbfd191afbbc8317577cbc44afe7156df546e143", "94a461dd1bd2c70c562fc0791a1cb7d933c83e01", "b2c04cc369b8f08f399d5fb95ddc884d52cfebd2"]},{"id": "605d738a39df3c5e596613ab0ca6925f0eecdf35", "title": "Distributed representations, simple recurrent networks, and grammatical structure", "authors": ["Jeffrey L. Elman"], "date": "1991", "abstract": "AbstractIn this paper three problems for a connectionist account of language are considered1.What is the nature of linguistic representations?2.How can complex structural relationships such as constituent be represented?3.How can the apparently open-ended nature of language be accommodated by a fixed-resource system?\nUsing a prediction task, a simple recurrent network (SRN) is trained on multiclausal sentences which contain multiply-embedded relative clauses. Principal component analysis of the… ", "references": ["3107651f0f2ec7a6e6b713d820f5e083b6446c8d", "de996c32045df6f7b404dda2a753b6a9becf3c08", "4fa569625b5ab35e955a8d5be11a4aa9f59ca424", "a801005c1eb6e293f0ea571c9080b131db9e6992", "88e190feb57cf046abca09a0569bab50d824d9d5", "de996c32045df6f7b404dda2a753b6a9becf3c08", "1e554939c3788b70d0e7b316aa7f051d889dd493", "4fa569625b5ab35e955a8d5be11a4aa9f59ca424", "4fa569625b5ab35e955a8d5be11a4aa9f59ca424", "1e554939c3788b70d0e7b316aa7f051d889dd493"]},{"id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "title": "Head-Driven Statistical Models for Natural Language Parsing", "authors": ["Michael Collins"], "date": "2003", "abstract": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies… ", "references": ["d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "a4c0e02d99de82149efd719260e5a5549a13854a", "a4c0e02d99de82149efd719260e5a5549a13854a", "a4c0e02d99de82149efd719260e5a5549a13854a", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3", "a4c0e02d99de82149efd719260e5a5549a13854a", "436772d9a916f0382800cf18581cfdfd4f83c457", "844db702be4bc149b06b822b47247e15f5894cc3", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036"]},{"id": "723e55901724533dc79c2255ce044fef858db9ae", "title": "Wide coverage natural language processing using kernel methods and neural networks for structured data", "authors": ["Sauro Menchetti", "Fabrizio Costa", "Massimiliano Pontil"], "date": "2005", "abstract": "Convolution kernels and recursive neural networks are both suitable approaches for supervised learning when the input is a discrete structure like a labeled tree or graph. We compare these techniques in two natural language problems. In both problems, the learning task consists in choosing the best alternative tree in a set of candidates. We report about an empirical evaluation between the two methods on a large corpus of parsed sentences and speculate on the role played by the representation… ", "references": ["a5eb96540ef53b49eac2246d6b13635fe6e54451", "e6c7adc28e20d361d5c35aa9808094b10f6a34d1", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "a5eb96540ef53b49eac2246d6b13635fe6e54451", "fe638b5610475d4524684fb2c2b7b08c119c8700", "d4ab5a707aeab5d52f162136bab86f3d2b1ff8ad", "d4ab5a707aeab5d52f162136bab86f3d2b1ff8ad", "f330f1f472f860212b980bb9be81eff884f7f0e1", "bd5a55b27310ad9ccb8b37fa59c028e2149a8ccd", "d73a70359f568ab32943a74f7891a27257847b3e"]},{"id": "adcf1552e759f9cade8ef9e59ecf6159e25a055e", "title": "Neural Network Probability Estimation for Broad Coverage Parsing", "authors": ["James Henderson"], "date": "EACL", "abstract": "We present a neural-network-based statistical parser, trained and tested on the Penn Treebank. The neural network is used to estimate the parameters of a generative model of left-corner parsing, and these parameters are used to search for the most probable parse. The parser's performance (88.8% F-measure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs). Crucial to this success is the neural network architecture's ability to induce a… ", "references": ["8ac9089b40346c3b6c962798f929789595a21e4f", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "8ac9089b40346c3b6c962798f929789595a21e4f", "6c9f553e723a40a6713453b734b552c1928bf52b", "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "cecef246b1cb60c0c03b5ba229701987f3c3088b", "81926eab7d1eb37b0c72c8aeb04420617568e965", "844db702be4bc149b06b822b47247e15f5894cc3", "8ac9089b40346c3b6c962798f929789595a21e4f", "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a"]},{"id": "104fa9c3294df3a6713d5dccea790b5682037c31", "title": "Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus", "authors": ["Saif M. Mohammad", "Cody Dunne", "Bonnie J. Dorr"], "date": "EMNLP", "abstract": "Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words.", "references": ["5c8d05e27e36ebd64ee43fe1670262cdcc2123ba", "8ec5c6abf7b8c64d5939285bfe97b56d9bd2c6f4", "c95055d5f16fcba89bad2e182a6e5a10ddb64367", "9de3d523623899badd57cc6996fde336aed545e7", "157d40d3e7fba266544cbbfd5f84852ddd944f8b", "9e7c7853a16a378cc24a082153b282257a9675b7", "d2d8b52f59945b4a3ef9d20ab44e108319eead6f", "8ec5c6abf7b8c64d5939285bfe97b56d9bd2c6f4", "c95055d5f16fcba89bad2e182a6e5a10ddb64367", "96761dc3d8adaeff44eb9c07501eb3109802ee4b"]},{"id": "5f4a34af0f5a8ddca0cfa356d7fa67c72f539e81", "title": "Assessing Sentiment of Text by Semantic Dependency and Contextual Valence Analysis", "authors": ["Shaikh Mostafa Al Masum", "Helmut Prendinger", "Mitsuru Ishizuka"], "date": "ACII", "abstract": "Text is not only an important medium to describe facts and events, but also to effectively communicate information about the writer's (positive or negative) sentiment underlying an opinion, and an affect or emotion (e.g. happy, fearful, surprised etc.). We consider sentiment assessment and emotion sensing from text as two different problems, whereby sentiment assessment is a prior task to emotion sensing. This paper presents an approach to sentiment assessment, i.e. the recognition of negative… ", "references": ["de00b6d04d7d4c7d554b404f54b4362c9ba3df92", "1cff7cc15555c38607016aaba24059e76b160adb", "de00b6d04d7d4c7d554b404f54b4362c9ba3df92", "836f5bc6b13404790d01c337e71511c1eed96b53", "1cff7cc15555c38607016aaba24059e76b160adb", "8a26ded61b67cce241352ba8742cd8fa2541d605", "836f5bc6b13404790d01c337e71511c1eed96b53", "9b4876f7313b111074e79a01f570e6e9e02c0dce", "836f5bc6b13404790d01c337e71511c1eed96b53", "6af58c061f2e4f130c3b795c21ff0c7e3903278f"]},{"id": "6fec21a78eb9279c87cc89ef7efa0acf22ff4abd", "title": "Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization", "authors": ["Andrew B. Goldberg", "Xiaojin Zhu"], "date": "TextGraphs Workshop On Graph…", "abstract": "We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., \"4 stars\"), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that… ", "references": ["125842668eab7decac136db8a59d392dc5e4e395", "a3b3aad58ecc6aed599c7567d4fe07ad3480a866", "125842668eab7decac136db8a59d392dc5e4e395", "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "28633b18c2b3bad80b1edb552146cc200b90f0e7", "28633b18c2b3bad80b1edb552146cc200b90f0e7", "ec7c68427a26f812532b1c913c68fcf84b7de58e", "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7", "0eedbab3ae55fd6a4e7bbc75fcc261293384f883"]},{"id": "c58dd287a476b4722c5b6b1316629e2874682219", "title": "Learning task-dependent distributed representations by backpropagation through structure", "authors": ["Christoph Goller", "Andreas Küchler"], "date": "1996", "abstract": "While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs.", "references": ["26affdaceca32cd4a5fde0db61ffef02a59baa13", "6a835df43fdc2f79126319f6fa033bb42147c6f6", "14a9a814a54dbab99388fafbd96a1c5fe249e376", "6a835df43fdc2f79126319f6fa033bb42147c6f6", "f03db7ef9cf309561eb02eb317b875deb8817c01", "f48d6233238c9da9cf36cbcdc9e5d00cf6e1b4b0", "f03db7ef9cf309561eb02eb317b875deb8817c01", "bd210021bf4a559c1849351d90c20ce7fcd34dea", "4add0dd0041403f585cf85b455aae487e37dfd3a", "4add0dd0041403f585cf85b455aae487e37dfd3a"]},{"id": "da5cd00115f7ec108de8eebf071c5f3f19807df4", "title": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables", "authors": ["Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi"], "date": "HLT-NAACL", "abstract": "In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective… ", "references": ["ffd77cd376a8ba679e01300f0eb7518892701f62", "9b4876f7313b111074e79a01f570e6e9e02c0dce", "b35e340a4356eb8953b04c17375ffb631da68916", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "a9c15a179f3e4ad60eee6fbd0b731cc6ac17c8ac", "b35e340a4356eb8953b04c17375ffb631da68916", "23c93e0bb33ed2cf4432003d2ceedb2dbc658cb7", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "d9b0190b06ac7270e9052895f8592beb4959ccfd", "d9b0190b06ac7270e9052895f8592beb4959ccfd"]},{"id": "f52de7242e574b70410ca6fb70b79c811919fc00", "title": "Learning Accurate, Compact, and Interpretable Tree Annotation", "authors": ["Slav Petrov", "Leon Barrett", "Dan Klein"], "date": "ACL", "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic… ", "references": ["a600850ac0120cb09a0b7de7da80bb6a7a76de06", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "b8f896caa1226713ed2731101cb8de21195dbf0b", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "0bf69a49c2baed67fa9a044daa24b9e199e73093", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "b8f896caa1226713ed2731101cb8de21195dbf0b", "93231398214275e4316aa19ced49a508ace56ffa", "0bf69a49c2baed67fa9a044daa24b9e199e73093", "93231398214275e4316aa19ced49a508ace56ffa"]},{"id": "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "title": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales", "authors": ["Bo Pang", "Lillian Lee"], "date": "ACL", "abstract": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four… ", "references": ["a3b3aad58ecc6aed599c7567d4fe07ad3480a866", "338a891907dce447da9a0fa2f27221bd35164163", "0650df86ad901fb9aadd9033a83c328a6f595666", "a3b3aad58ecc6aed599c7567d4fe07ad3480a866", "0650df86ad901fb9aadd9033a83c328a6f595666", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "3bc4736f9b8512043ed47357a81f26b93a1204b6", "0650df86ad901fb9aadd9033a83c328a6f595666", "9e7c7853a16a378cc24a082153b282257a9675b7", "338a891907dce447da9a0fa2f27221bd35164163"]},{"id": "e14609a3a6c6f8ef3269d3e0728f88da57826698", "title": "Sentiment Classification of Movie Reviews Using Contextual Valence Shifters", "authors": ["Alistair Kennedy", "Diana Inkpen"], "date": "2006", "abstract": "We present two methods for determining the sentiment expressed by a movie review. The semantic orientation of a review can be positive, negative, or neutral. We examine the effect of valence shifters on classifying the reviews. We examine three types of valence shifters: negations, intensifiers, and diminishers. Negations are used to reverse the semantic polarity of a particular term, while intensifiers and diminishers are used to increase and decrease, respectively, the degree to which a term… ", "references": ["9e7c7853a16a378cc24a082153b282257a9675b7", "ce20678cfbffded477f43df156f6ab37f6edc6a0", "45472c24919b1a7a008f84131dcb7b9f729a72ed", "d8f07fe5d91e267978bb2f14e019115990eb227b", "a88a61933c3c8807021b29196a9f6db8cd05a7a2", "45472c24919b1a7a008f84131dcb7b9f729a72ed", "d8f07fe5d91e267978bb2f14e019115990eb227b", "9b4876f7313b111074e79a01f570e6e9e02c0dce", "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "5c8d05e27e36ebd64ee43fe1670262cdcc2123ba"]},{"id": "d64561879a2fbd3d39a5e876a667ffa4561eed80", "title": "Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings", "authors": ["Nanyun Peng", "Mark Dredze"], "date": "EMNLP", "abstract": "We consider the task of named entity recognition for Chinese social media.", "references": ["b52fe0b796e4c899624ed3e9d9ea566453156844", "da0cc33fac4d926eaa61f86566572a4653b3e990", "14935c3ffb1cafd53a23d84bec66388a77422435", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "14935c3ffb1cafd53a23d84bec66388a77422435", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "a07f56a919687e432e92c403d371a9e2d564ef3f", "a07f56a919687e432e92c403d371a9e2d564ef3f", "a07f56a919687e432e92c403d371a9e2d564ef3f", "5920903e1c2c7ea165ad84a8fe6dbafdd586cbe7"]},{"id": "f40bcba9593fc266cdebf35a107c85c30983173f", "title": "Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning", "authors": ["Nanyun Peng", "Mark Dredze"], "date": "2016", "abstract": "Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation… ", "references": ["8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "a07f56a919687e432e92c403d371a9e2d564ef3f", "00a2a05f44a790b7886cd25192f4b91d3b81dde2", "a07f56a919687e432e92c403d371a9e2d564ef3f", "12510c9659c71ce821fe671de5ed7033ba0af31c", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "5ee9bed324a4f0716554409dc367df3beeb27b44", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "00a2a05f44a790b7886cd25192f4b91d3b81dde2", "00a2a05f44a790b7886cd25192f4b91d3b81dde2"]},{"id": "48e8e8085907192d501eb2bcc582035e90431a2f", "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen"], "date": "2016", "abstract": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters… ", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "4dabd6182ce2681c758f654561d351739e8df7bf", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "eb42a490cf4f186d3383c92963817d100afd81e2", "9c72719fa829a3ab5686bdd5c6f4f2538a1a92f5", "91a93f751f912b2f96d6771018d8f06c41e11152", "cea967b59209c6be22829699f05b8b1ac4dc092d", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "bc1022b031dc6c7019696492e8116598097a8c12"]},{"id": "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "title": "From distributional to semantic similarity", "authors": ["James Richard Curran"], "date": "2004", "abstract": "Lexical-semantic resources, including thesauri and WORDNET, have been successfully incorporated into a wide range of applications in Natural Language Processing. However they are very difficult and expensive to create and maintain, and their usefulness has been severely hampered by their limited coverage, bias and inconsistency. Automated and semi-automated methods for developing such resources are therefore crucial for further resource development and improved application performance. Systems… ", "references": ["0285f18f1642c3684e6abb7d5162348278c41abf", "0285f18f1642c3684e6abb7d5162348278c41abf", "b3e9130ecab419f8267fccadf80c1ee2489be793", "c2d95e890ee904f70701fa27326d31980424d5dd", "7d37dff2d8e65764e7293750051d519359d8835d", "dbfd191afbbc8317577cbc44afe7156df546e143", "7d37dff2d8e65764e7293750051d519359d8835d", "b3e9130ecab419f8267fccadf80c1ee2489be793", "e4b66a159672c613f9d4c7cdb8e6c6f85d871bf5", "0285f18f1642c3684e6abb7d5162348278c41abf"]},{"id": "91a93f751f912b2f96d6771018d8f06c41e11152", "title": "Boosting Named Entity Recognition with Neural Character Embeddings", "authors": ["Cícero Nogueira dos Santos", "Victor Guimarães"], "date": "NEWS@ACL", "abstract": "Most state-of-the-art named entity recognition (NER) systems rely on handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking. In this work we propose a language-independent NER system that uses automatically learned features only. Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classification. We perform an extensive number of… ", "references": ["5c2cf68e49df90bff81f2a7c457ce38c73f9cb98", "0617dd6924df7a3491c299772b70e90507b195dc", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "afacd9d2048902a8faf82c4f79584d6a05170ba6", "afacd9d2048902a8faf82c4f79584d6a05170ba6", "0617dd6924df7a3491c299772b70e90507b195dc", "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "5c2cf68e49df90bff81f2a7c457ce38c73f9cb98", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "523f255f90be3f1ae0f151c60cac50467e965ed0"]},{"id": "d53d878cf1a3f0bed5d9c68c925994cb72f47304", "title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", "authors": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum"], "date": "CoNLL", "abstract": "Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the… ", "references": ["330da625c15427c6e42ccfa3b747fb29e5835bf0", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "7ece4e8d31f872d928369ac2cf58a616a7182112", "57458bc1cffe5caa45a885af986d70f723f406b4", "57458bc1cffe5caa45a885af986d70f723f406b4", "7ece4e8d31f872d928369ac2cf58a616a7182112", "57458bc1cffe5caa45a885af986d70f723f406b4", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "b5e12de7d3f2a66bfcf10a510f9ac80afb3541c1", "b4299baa815ca5a815a70fba94a9f6f2b42fff19"]},{"id": "745d86adca56ec50761591733e157f84cfb19671", "title": "Composition in Distributional Models of Semantics", "authors": ["Jeff Mitchell", "Mirella Lapata"], "date": "2010", "abstract": "Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little… ", "references": ["f932caac89709a716a7d3e6632caf9f34d709518", "f932caac89709a716a7d3e6632caf9f34d709518", "1ad9b8e92f303cebfd4c9a97ca99ac09ace82fcc", "d3cf28ab36ff7f7601a55c1e832736b2473a07f0", "d3cf28ab36ff7f7601a55c1e832736b2473a07f0", "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e", "1ad9b8e92f303cebfd4c9a97ca99ac09ace82fcc", "509a2ca90a85c62d66a16b37e0de28715dd4e89f", "1ad9b8e92f303cebfd4c9a97ca99ac09ace82fcc", "6d8018bd8b288baca0c55522877efd1b49258747"]},{"id": "a16e484824b2580e092c985aa659e8680aeda5ee", "title": "Shallow Semantic Parsing using Support Vector Machines", "authors": ["Sameer Pradhan", "Wayne H. Ward", "Dan Jurafsky"], "date": "HLT-NAACL", "abstract": "In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus. ", "references": ["fd1901f34cc3673072264104885d70555b1a4cdc", "6ffea7929f0e4bbee9e98755eb3d8fc09e89cf4e", "9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce", "7eff3ef2c978dcbc8b5a4ab99fa4f0a187afa5ed", "6ffea7929f0e4bbee9e98755eb3d8fc09e89cf4e", "3da63687558e077ab2ef0b4a24985c2614602c25", "3da63687558e077ab2ef0b4a24985c2614602c25", "c07c690601169dc1155b2dcf90941b32f606a9d4", "c07c690601169dc1155b2dcf90941b32f606a9d4", "923db0aeb26a6dc1cb42069c9db04e5dd2d2200a"]},{"id": "1f462943c8d0af69c12a09058251848324135e5a", "title": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition", "authors": ["John S. Bridle"], "date": "NATO Neurocomputing", "abstract": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a… ", "references": []},{"id": "eb42a490cf4f186d3383c92963817d100afd81e2", "title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network", "authors": ["Kristina Toutanova", "Dan Klein", "Yoram Singer"], "date": "HLT-NAACL", "abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ… ", "references": ["205b9f4891a2ead886604f161a44b3aed483609a", "a574e320d899e7e82e341eb64baef7dfe8a24642", "ba8ae7c8261a77ec93727f741d50001b9353f65d", "8e824aaf67f4f4f068455c6dbb7a6ed877794bd6", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b", "a5aa95289383a3fd91dd68a314b1031d3e165c3b", "a574e320d899e7e82e341eb64baef7dfe8a24642", "a5aa95289383a3fd91dd68a314b1031d3e165c3b", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10"]},{"id": "b35e340a4356eb8953b04c17375ffb631da68916", "title": "Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis", "authors": ["Yejin Choi", "Claire Cardie"], "date": "EMNLP", "abstract": "Determining the polarity of a sentiment-bearing expression requires more than a simple bag-of-words approach. In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity. In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our experiments show… ", "references": ["03380e7083807d3264472871dc0582036cf79479", "dea301ddc616ed7da7568b91a6627699fcbd07cd", "97bdc5522ae46b281389654199d656eb67728a32", "03380e7083807d3264472871dc0582036cf79479", "0f26d6ce9e7dae5276ce1d2b2a578151a45887b5", "68c03788224000794d5491ab459be0b2a2c38677", "68c03788224000794d5491ab459be0b2a2c38677", "97bdc5522ae46b281389654199d656eb67728a32", "03380e7083807d3264472871dc0582036cf79479", "9b4876f7313b111074e79a01f570e6e9e02c0dce"]},{"id": "a9c15a179f3e4ad60eee6fbd0b731cc6ac17c8ac", "title": "Learning to Shift the Polarity of Words for Sentiment Classification", "authors": ["Daisuke Ikeda", "Hiroya Takamura", "Manabu Okumura"], "date": "IJCNLP", "abstract": "We propose a machine learning based method of sentiment classification of sentences using word-level polarity. The polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions. The proposed method models the polarity-shifters. Our model can be trained in two different ways: word-wise and sentence-wise learning. In sentence-wise learning, the model can be trained so that the prediction of sentence… ", "references": ["5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac", "40fd86ed795b17e6abf00e81b4b13d955813a4c6", "64b8553bd62997d332910078bcd5ee74a43f9350", "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "40fd86ed795b17e6abf00e81b4b13d955813a4c6", "bc7308a97ec2d3f7985d48671abe7a8942a5b9f8", "64b8553bd62997d332910078bcd5ee74a43f9350", "bc7308a97ec2d3f7985d48671abe7a8942a5b9f8", "bc7308a97ec2d3f7985d48671abe7a8942a5b9f8", "bc7308a97ec2d3f7985d48671abe7a8942a5b9f8"]},{"id": "167e1359943b96b9e92ee73db1df69a1f65d731d", "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts", "authors": ["Bo Pang", "Lillian Lee"], "date": "2004", "abstract": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\".", "references": ["ab3eec0a211e16751effa729281dd448001203db", "ab3eec0a211e16751effa729281dd448001203db", "9e7c7853a16a378cc24a082153b282257a9675b7", "f0456ee92718b04e8ea51f47c9486455fafcf7d3", "5ccef5423f3cb428121eb6fef2224d803c136806", "338a891907dce447da9a0fa2f27221bd35164163", "ab3eec0a211e16751effa729281dd448001203db", "310b72fbc3d384ca88ca994b33476b8a2be2e27f", "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "25be54c49920d7d3520135c341817b1172eb079f"]},{"id": "5db592bef4b5ff231e1de92588907808f00bfbb4", "title": "Multi-View Learning of Word Embeddings via CCA", "authors": ["Paramveer S. Dhillon", "Dean P. Foster", "Lyle H. Ungar"], "date": "NIPS", "abstract": "Recently, there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for NLP tasks. However, most current approaches are slow to train, do not model the context of the word, and lack theoretical grounding. In this paper, we present a new learning method, Low Rank Multi-View Learning (LR-MVL) which uses a fast spectral method to estimate low dimensional context-specific word… ", "references": ["3de5d40b60742e3dfa86b19e7f660962298492af", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "57458bc1cffe5caa45a885af986d70f723f406b4", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "57458bc1cffe5caa45a885af986d70f723f406b4", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "3de5d40b60742e3dfa86b19e7f660962298492af", "dac72f2c509aee67524d3321f77e97e8eff51de6", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "3de5d40b60742e3dfa86b19e7f660962298492af"]},{"id": "80922663aaa7b09e86276ab97210ab2372d3f61a", "title": "Dynamic auto-encoders for semantic indexing", "authors": ["Piotr W. Mirowski", "Marc'Aurelio Ranzato", "Yann LeCun"], "date": "2010", "abstract": "We present a new algorithm for topic modeling, text classification and retrieval, tailored to sequences of time-stamped documents. Based on the auto-encoder architecture, our nonlinear multi-layer model is trained stage-wise to produce increasingly more compact representations of bags-of-words at the document or paragraph level, thus performing a semantic analysis. It also incorporates simple temporal dynamics on the latent representations, to take advantage of the inherent structure of… ", "references": ["f198043a866e9187925a8d8db9a55e3bfdd47f2c", "e99f196cf21e0781ef1e119d14e6db45cd71bf3b", "97a5bdf0ed22a5688abef32b282e922da362e7b5", "0f0fb40889d16cb8b99bb7bcb4f3c2d39beb0be3", "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a", "e99f196cf21e0781ef1e119d14e6db45cd71bf3b", "97a5bdf0ed22a5688abef32b282e922da362e7b5", "c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e", "0f0fb40889d16cb8b99bb7bcb4f3c2d39beb0be3", "c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e"]},{"id": "5974441a0bebfb45579491a9a28bca4fff6bc256", "title": "Measuring Distributional Similarity in Context", "authors": ["Georgiana Dinu", "Mirella Lapata"], "date": "EMNLP", "abstract": "The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment. Vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context. In his paper we propose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition… ", "references": ["107da5e5023b6caf546790ede6d6a370bb4c8e68", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "38c5a728c57baa59907ee0890137b75a5269f4a7", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "38c5a728c57baa59907ee0890137b75a5269f4a7", "107da5e5023b6caf546790ede6d6a370bb4c8e68", "4c891b14e91e89797dac565e788b7e3e23813ec1", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78"]},{"id": "3d6036af971c1f11ab712cc41487376a94e63673", "title": "Using a connectionist model in a syntactical based language model", "authors": ["Ahmad Emami", "Peng Xu", "Frederick Jelinek"], "date": "2003", "abstract": "We investigate the performance of the Structured Language Model when one of its components is modeled by a connectionist model.", "references": ["e41498c05d4c68e4750fb84a380317a112d97b01", "693a614718a96d61968ec573b2932a3301092c9a", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "e41498c05d4c68e4750fb84a380317a112d97b01", "693a614718a96d61968ec573b2932a3301092c9a", "693a614718a96d61968ec573b2932a3301092c9a", "5a03eea43e128f49218ed95b909da1136c757e57", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "5a03eea43e128f49218ed95b909da1136c757e57"]},{"id": "83d787a13aca79c833d11718da9ab6243117cf47", "title": "Word Meaning in Context: A Simple and Effective Vector Model", "authors": ["Stefan Thater", "Hagen Fürstenau", "Manfred Pinkal"], "date": "IJCNLP", "abstract": "We present a model that represents word meaning in context by vectors which are modified according to the words in the target’s syntactic context. Contextualization of a vector is realized by reweighting its components, based on distributional information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of… ", "references": ["107da5e5023b6caf546790ede6d6a370bb4c8e68", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "00162f43964fd457a9158408c1ac0e8990489782", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "8dabfccd40dad42c0740f592f34ba048cb1730e3", "00162f43964fd457a9158408c1ac0e8990489782", "4c891b14e91e89797dac565e788b7e3e23813ec1", "c62450a13bb6692385490dd4b371de9857761374", "c1e48526eddd68b5bf98739a578ab69a009f570d", "00162f43964fd457a9158408c1ac0e8990489782"]},{"id": "dac72f2c509aee67524d3321f77e97e8eff51de6", "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "authors": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "date": "ACL", "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining… ", "references": ["ae29b936d437a93ad259ee008ba56fe82ab4db61", "9474fe3f566863ab7410e74b66ac848eac7cb4e2", "9474fe3f566863ab7410e74b66ac848eac7cb4e2", "57458bc1cffe5caa45a885af986d70f723f406b4", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "b4299baa815ca5a815a70fba94a9f6f2b42fff19", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "ae29b936d437a93ad259ee008ba56fe82ab4db61"]},{"id": "c62450a13bb6692385490dd4b371de9857761374", "title": "Multi-Prototype Vector-Space Models of Word Meaning", "authors": ["Joseph Reisinger", "Raymond J. Mooney"], "date": "HLT-NAACL", "abstract": "Current vector-space models of lexical semantics create a single \"prototype\" vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple \"sense-specific\" vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human… ", "references": ["df3fd99704ab829157062bb44fb4929f9cba9217", "cbbf322dd1d97b70ffdd61e5afbad8d45b951efc", "477804ac9dbf871b4fe5e5ac80467413dd619a63", "0a32b3d027064798fb31ce42894fec31e834f7db", "cbbf322dd1d97b70ffdd61e5afbad8d45b951efc", "477804ac9dbf871b4fe5e5ac80467413dd619a63", "3317f2788b2b07d9ba4cb4335e29316fcf8a971a", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "477804ac9dbf871b4fe5e5ac80467413dd619a63", "a9fee459ed211f53bfadef22e3ab774d0e927358"]},{"id": "68c03788224000794d5491ab459be0b2a2c38677", "title": "WordNet: a lexical database for English", "authors": ["George A. Miller"], "date": "1995", "abstract": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern… ", "references": []},{"id": "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering", "authors": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "date": "EMNLP", "abstract": "We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering.", "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "46be284f1e1ece64465af6fe3a69ce544e0c7e33", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "228920ddc0d376c376ae534ceed589005f51867a", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e"]},{"id": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference", "authors": ["Ankur P. Parikh", "Oscar Täckström", "Jakob Uszkoreit"], "date": "2016", "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order… ", "references": ["581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "ea2563467c1c472a346d165b7f97c86317d63ca4", "46b8cbcdff87b842c2c1d4a003c831f845096ba7", "ea2563467c1c472a346d165b7f97c86317d63ca4", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "36c097a225a95735271960e2b63a2cb9e98bff83", "46b8cbcdff87b842c2c1d4a003c831f845096ba7", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "46b8cbcdff87b842c2c1d4a003c831f845096ba7"]},{"id": "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "title": "Automatic Word Sense Discrimination", "authors": ["Hinrich Schütze"], "date": "1998", "abstract": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the… ", "references": ["8bd6e85e7f6a9880f81a54fe2d049cacc82fc427", "d6520da982493225779ef2cac3411d10de50f5a7", "b53b6b7ffd1435c2c6a1b6684f9975b73648d131", "d6520da982493225779ef2cac3411d10de50f5a7", "1d922631a6bf8361d7602e12cafb9e15d421c827", "04305cc88e1b55365d9bcb5039d26ba5d4595cfd", "59407446503d49a8cf5f5643b17502835b62f139", "b53b6b7ffd1435c2c6a1b6684f9975b73648d131", "04305cc88e1b55365d9bcb5039d26ba5d4595cfd", "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0"]},{"id": "822f1ed9a76a57cc19d8fda7745365b97130b97a", "title": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction", "authors": ["Tim Rocktäschel", "Sameer Singh", "Sebastian Riedel"], "date": "HLT-NAACL", "abstract": "Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data.", "references": ["498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "3fbc710e6584187e143582c5be20ebcdb4ff363a", "311eb232e4bd3ed53b1ef3381d75b65615d4e29c", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "d48edf9e81653f4c3da716b037b0b50d54c5b034"]},{"id": "e1b3a5622bc06510e8797e0c11593aa16d7b6523", "title": "Effective Crowd Annotation for Relation Extraction", "authors": ["Angli Liu", "Stephen Soderland", "Daniel S. Weld"], "date": "HLT-NAACL", "abstract": "Can crowdsourced annotation of training data boost performance for relation extraction over methods based solely on distant supervision? While crowdsourcing has been shown effective for many NLP tasks, previous researchers found only minimal improvement when applying the method to relation extraction. This paper demonstrates that a much larger boost is possible, e.g., raising F1 from 0.40 to 0.60. Furthermore, the gains are due to a simple, generalizable technique, Gated Instruction, which… ", "references": ["9ce464683fa0653245ac4c28e295d35758b955d1", "60bc22ff917ff9ca92f17e0a2d0973a066be9096", "05369d319bc1811fbfc57abfcef00273b325128e", "8dff21517f7ac744089a260dbc3e2f48649e3119", "4521f5e30024dee07de088288aa5607bdeb38ad5", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "4521f5e30024dee07de088288aa5607bdeb38ad5", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "60bc22ff917ff9ca92f17e0a2d0973a066be9096", "5670f4e460ccb7e6021b15d50d879a98a7a7b01c"]},{"id": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "authors": ["Adina Williams", "Nikita Nangia", "Samuel R. Bowman"], "date": "NAACL-HLT", "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity… ", "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "17ad4237ed84911c421bf00bea6313faf66e103a", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "83b83ee4f27388445bdebb199cd75e5bf546dd85", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "17ad4237ed84911c421bf00bea6313faf66e103a", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "11ec56898a9e7f401a2affe776b5297bd4e25025"]},{"id": "83b83ee4f27388445bdebb199cd75e5bf546dd85", "title": "The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations", "authors": ["Nikita Nangia", "Adina Williams", "Samuel R. Bowman"], "date": "2017", "abstract": "This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. (2017). All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al.. The best single model used stacked BiLSTMs with residual connections to extract sentence features and reached 74… ", "references": ["891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "ea0bc259e6e3abdc72c2fc58887e71bbab4e9c2b", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "1d9a6ff3e8b345ca39236dc8fa8236b1902b9265", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "f04df4e20a18358ea2f689b4c129781628ef7fc1"]},{"id": "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "authors": ["Daniel Fernando Campos", "Tri Nguyen", "Li Deng"], "date": "2016", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from… ", "references": ["3adff57fd09965224506a1bacc0579d9d3c8c11e", "995b7affd684b910d5a1c520c3af00fd20cc39b0", "f2e50e2ee4021f199877c8920f1f984481c723aa", "88bb0a28bb58d847183ec505dda89b63771bb495", "636a79420d838eabe4af7fb25d6437de45ab64e8", "34365ffe1ecf19bc642e4d33fd7ba57f16ef4b8a", "05dd7254b632376973f3a1b4d39485da17814df5", "995b7affd684b910d5a1c520c3af00fd20cc39b0", "05dd7254b632376973f3a1b4d39485da17814df5", "05dd7254b632376973f3a1b4d39485da17814df5"]},{"id": "83e7654d545fbbaaf2328df365a781fb67b841b4", "title": "Enhanced LSTM for Natural Language Inference", "authors": ["Qian Chen", "Xiao-Dan Zhu", "Diana Inkpen"], "date": "ACL", "abstract": "Reasoning and inference are central to human and artificial intelligence.", "references": ["8314f8eef3b64054bfc00607507a92de92fb7c85", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "8314f8eef3b64054bfc00607507a92de92fb7c85", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "523f420cb55d8070f565c87a50099a9a5b0b9206", "8dbb0b9ca61e2753c6759446c6909acda616095a", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "cff79255a94b9b05a4ce893eb403a522e0923f04", "523f420cb55d8070f565c87a50099a9a5b0b9206"]},{"id": "3b56693f6fe6b82092c4adc756f20fb9b7710ac5", "title": "Efficient Subsampling for Training Complex Language Models", "authors": ["Puyang Xu", "Asela Gunawardana", "Sanjeev Khudanpur"], "date": "EMNLP", "abstract": "We propose an efficient way to train maximum entropy language models (MELM) and neural network language models (NNLM). The advantage of the proposed method comes from a more robust and efficient subsampling technique. The original multi-class language modeling problem is transformed into a set of binary problems where each binary classifier predicts whether or not a particular word will occur. We show that the binarized model is as powerful as the standard model and allows us to aggressively… ", "references": ["fb486e03369a64de2d5b0df86ec0a7b55d3907db", "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "ba786c46373892554b98df42df7af6f5da343c9d", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "07ca885cb5cc4328895bfaec9ab752d5801b14cd"]},{"id": "8490431f3a76fbd165d108eba938ead212a2a639", "title": "Stochastic Answer Networks for Machine Reading Comprehension", "authors": ["Xiaodong Liu", "Yelong Shen", "Jianfeng Gao"], "date": "2018", "abstract": "We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neural network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD). ", "references": ["b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "c636a2dd242908fe2e598a1077c0c57bfdea8633", "05dd7254b632376973f3a1b4d39485da17814df5", "104715e1097b7ebee436058bfd9f45540f269845", "c636a2dd242908fe2e598a1077c0c57bfdea8633", "05dd7254b632376973f3a1b4d39485da17814df5", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "f2bf2377db2a93472edb916cddd19523c2bb907e"]},{"id": "29053eab305c2b585bcfbb713243b05646e7d62d", "title": "Entropy-based Pruning of Backoff Language Models", "authors": ["Andreas Stolcke"], "date": "1998", "abstract": "A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold… ", "references": ["837fcdfe8fdcc9c7f2f8a8c58b2afd7e64b43ee0", "1506c6fb9f4fed076ab4bfaaffa210e1ffa16c23", "1506c6fb9f4fed076ab4bfaaffa210e1ffa16c23", "bdc3d618db015b2f17cd76224a942bfdfc36dc73"]},{"id": "25eb5bb4eba859b1eaac200ec0c2e4638b7e83b5", "title": "Speed regularization and optimality in word classing", "authors": ["Geoffrey Zweig", "Konstantin Makarychev"], "date": "2013", "abstract": "Word-classing has been used in language modeling for two distinct purposes: to improve the likelihood of the language model, and to improve the runtime speed. In particular, frequency-based heuristics have been proposed to improve the speed of recurrent neural network language models (RNN-LMs). In this paper, we present a dynamic programming algorithm for determining classes in a way that provably minimizes the runtime of the resulting class-based language models. However, we also find that the… ", "references": ["4af41f4d838daa7ca6995aeb4918b61989d1ed80", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "e41498c05d4c68e4750fb84a380317a112d97b01", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "ed23c461535afc492e80c63ee8d1ed55b8a176e1", "0b89397e6dd9eb617a8b860fc091c87b356a255f", "4af41f4d838daa7ca6995aeb4918b61989d1ed80", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "ed23c461535afc492e80c63ee8d1ed55b8a176e1", "4af41f4d838daa7ca6995aeb4918b61989d1ed80"]},{"id": "ba786c46373892554b98df42df7af6f5da343c9d", "title": "Large Language Models in Machine Translation", "authors": ["Thorsten Brants", "Ashok C. Popat", "Jeffrey Dean"], "date": "EMNLP-CoNLL", "abstract": "This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing… ", "references": ["cb826a3899752b796f14df1c50378c64954a6b0a", "cb826a3899752b796f14df1c50378c64954a6b0a", "c6a83c4fcc99ba6753109301949c5b7cfa978079", "0afe29a9a8871faab23cbf90cf499c735a3b0c51", "7424d48786b23ffdfe6d938133b89830420afc9b", "ab7b5917515c460b90451e67852171a531671ab8", "c6a83c4fcc99ba6753109301949c5b7cfa978079", "d7da009f457917aa381619facfa5ffae9329a6e9", "ab7b5917515c460b90451e67852171a531671ab8", "591080c335daba2494f18cc04c9f7071501af0eb"]},{"id": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "authors": ["Mandar Joshi", "Eunsol Choi", "Luke Zettlemoyer"], "date": "ACL", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples.", "references": ["3eda43078ae1f4741f09be08c4ecab6229046a5c", "3adff57fd09965224506a1bacc0579d9d3c8c11e", "636a79420d838eabe4af7fb25d6437de45ab64e8", "05dd7254b632376973f3a1b4d39485da17814df5", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "3adff57fd09965224506a1bacc0579d9d3c8c11e", "564257469fa44cdb57e4272f85253efb9acfd69d", "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "b1e20420982a4f923c08652941666b189b11b7fe"]},{"id": "5b5cc77898a71a1386734584ceef4070263b8d03", "title": "ParlAI: A Dialog Research Software Platform", "authors": ["Alexander H. Miller", "Will Feng", "Jason Weston"], "date": "2017", "abstract": "We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform for dialog research implemented in Python, available at this http URL. Its goal is to provide a unified framework for sharing, training and testing of dialog models, integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning; and a repository of machine learning models for comparing with others' models, and improving upon existing architectures. Over 20 tasks are… ", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "4bf7edee5a4c4cfdbdd43a607c402420129fa277", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "c18df1edc0a45891806d44896a8f666944e93d01", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "cea967b59209c6be22829699f05b8b1ac4dc092d", "4bf7edee5a4c4cfdbdd43a607c402420129fa277"]},{"id": "a4dd3beea286a20c4e4f66436875932d597190bc", "title": "Deep Semantic Role Labeling: What Works and What's Next", "authors": ["Luheng He", "Kenton Lee", "Luke Zettlemoyer"], "date": "ACL", "abstract": "We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations.", "references": ["7ed7a41c275f2870b840a5e6c3eaec8888c9480c", "70d3d2e0a8f34d6c3cb7890e249e2ed6a574ce50", "7ed7a41c275f2870b840a5e6c3eaec8888c9480c", "70d3d2e0a8f34d6c3cb7890e249e2ed6a574ce50", "70d3d2e0a8f34d6c3cb7890e249e2ed6a574ce50", "70d3d2e0a8f34d6c3cb7890e249e2ed6a574ce50", "8495259ca47c938fbfc6a0a71633b27e907d998b", "8495259ca47c938fbfc6a0a71633b27e907d998b", "c6b8c1728e6d2572b16ca2bfa5c3c82bb0fd8be6", "b5c6f0d18fd783536b4e6c2205d75b7c4477c6d2"]},{"id": "612598389b4349fef728c80ab4202fee32f3a536", "title": "Unsupervised Learning of Sentence Representations using Convolutional Neural Networks", "authors": ["Zhe Gan", "Yunchen Pu", "Lawrence Carin"], "date": "2016", "abstract": "We propose a new encoder-decoder approach to learn distributed sentence representations from unlabeled sentences.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "cea967b59209c6be22829699f05b8b1ac4dc092d", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "b21c78a62fbb945a19ae9a8935933711647e7d70", "b21c78a62fbb945a19ae9a8935933711647e7d70", "b21c78a62fbb945a19ae9a8935933711647e7d70", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "6e795c6e9916174ae12349f5dc3f516570c17ce8"]},{"id": "7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "title": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language", "authors": ["Luheng He", "Mike Lewis", "Luke Zettlemoyer"], "date": "EMNLP", "abstract": "This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure.", "references": ["b1d7f596fc34fd6cc6bbfc22a083bca8d2d38f14", "c4100faa2cc35bc72e61dcbb173f1fee5e8e8840", "b1d7f596fc34fd6cc6bbfc22a083bca8d2d38f14", "b29cdc426784534ad5c83ccf646fde67c4695607", "b1d7f596fc34fd6cc6bbfc22a083bca8d2d38f14", "b29cdc426784534ad5c83ccf646fde67c4695607", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "1de2f2d45e505eab0bc520ee554353c6d509f32a", "b29cdc426784534ad5c83ccf646fde67c4695607", "c7d3f610b528226f1c862c4f9cd6b37623f7390f"]},{"id": "e44da7d8c71edcc6e575fa7faadd5e75785a7901", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "authors": ["Yossi Adi", "Einat Kermany", "Yoav Goldberg"], "date": "2017", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties… ", "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "b21c78a62fbb945a19ae9a8935933711647e7d70", "b21c78a62fbb945a19ae9a8935933711647e7d70", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "9462eee3e5eff15df5e97c38e24072c65e581cee", "b21c78a62fbb945a19ae9a8935933711647e7d70", "40be3888daa5c2e5af4d36ae22f690bcc8caf600", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "9462eee3e5eff15df5e97c38e24072c65e581cee", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "2dec6a802cbac1f640980b5106d88ae72c45ece4", "title": "Generating Natural Language Inference Chains", "authors": ["Vladyslav Kolesnyk", "Tim Rocktäschel", "Sebastian Riedel"], "date": "2016", "abstract": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can… ", "references": ["5082a1a13daea5c7026706738f8528391a1e6d59", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "36c097a225a95735271960e2b63a2cb9e98bff83", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "cea967b59209c6be22829699f05b8b1ac4dc092d", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "36c097a225a95735271960e2b63a2cb9e98bff83", "5082a1a13daea5c7026706738f8528391a1e6d59", "596c882de006e4bb4a93f1fa08a5dd467bee060a"]},{"id": "1a210410493fbc052f0b7a54e7bc89cee20e8d28", "title": "Crowdsourcing Question-Answer Meaning Representations", "authors": ["Julian Michael", "Gabriel Stanovsky", "Luke Zettlemoyer"], "date": "2017", "abstract": "We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We also develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A detailed qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing… ", "references": ["97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "d8259bcbe9cb0cf5bad6ea25645f4407fc544a1c", "7bc7d7e9f16c9a6e669224f3bdd6e95401493c45", "05dd7254b632376973f3a1b4d39485da17814df5", "7bc7d7e9f16c9a6e669224f3bdd6e95401493c45", "d8259bcbe9cb0cf5bad6ea25645f4407fc544a1c", "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "fda558136b2d2b812a608b21fe22959d48db1078"]},{"id": "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "title": "Annotation Artifacts in Natural Language Inference Data", "authors": ["Suchin Gururangan", "Swabha Swayamdipta", "Noah A. Smith"], "date": "NAACL-HLT", "abstract": "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to.", "references": ["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "c333778104f648c385b4631f7b4a859787e9d3d3", "72d7c465ef199a9670b3da7a318b0227f5cc3229", "72d7c465ef199a9670b3da7a318b0227f5cc3229", "1778e32c18bd611169e64c1805a51abff341ca53", "3c092128a2c98e5e3be5f8872cf05c635430cd60", "c333778104f648c385b4631f7b4a859787e9d3d3", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c"]},{"id": "bd4318cd5129cf0d6268876888359f87b410d719", "title": "Lessons from the Netflix prize challenge", "authors": ["Robert M. Bell", "Yehuda Koren"], "date": "2007", "abstract": "This article outlines the overall strategy and summarizes a few key innovations of the team that won the first Netflix progress prize. ", "references": []},{"id": "c08f5fa876181fc040d76c75fe2433eee3c9b001", "title": "Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks", "authors": ["Maxime Oquab", "Léon Bottou", "Josef Sivic"], "date": "2014", "abstract": "Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large- scale visual recognition challenge (ILSVRC2012). The success of CNNs is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification methods. Learning CNNs, however, amounts to estimating millions of parameters and requires a very large number of annotated image samples. This property… ", "references": ["b8de958fead0d8a9619b55c7299df3257c624a96", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "1109b663453e78a59e4f66446d71720ac58cec25", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "d743430cb2329caa5d446c17fc9ec07f5e916ab0"]},{"id": "7b7908f71188b89adf62ce9126a0466e1a34338f", "title": "The devil is in the details: an evaluation of recent feature encoding methods", "authors": ["Ken Chatfield", "Victor S. Lempitsky", "Andrew Zisserman"], "date": "BMVC", "abstract": "A large number of novel encodings for bag of visual words models have been proposed in the past two years to improve on the standard histogram of quantized local features. Examples include locality-constrained linear encoding [23], improved Fisher encoding [17], super vector encoding [27], and kernel codebook encoding [20]. While several authors have reported very good results on the challenging PASCAL VOC classification data by means of these new techniques, differences in the feature… ", "references": ["498efaa51f5eda731dc6199c3547b9465717fa68", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "b6371f8c70c2684faefd99fffcc556c3a75dd7f4", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "c2d3746a1f755928b5011932285d686eb5a9127b", "2f7713dcc35e7c05becf3be5522f36c9546b0364", "c65be1f97642510843667d36e399de58837d3419", "c2d3746a1f755928b5011932285d686eb5a9127b", "4e65c9f0a64b6a4333b12e2adc3861ad75aca83b", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d"]},{"id": "c681ebe49f96954bee87e131214a2921ac7c0f8e", "title": "Linear Operators for GPU Implementation of Numerical Algorithms", "authors": ["J. L. Kruger", "Rüdiger Westermann"], "date": "SIGGRAPH", "abstract": "A frame for facial identification system in which changeable component segments are assembled in partially overlapping, layered fashion, has a base wth a central reservoir therein in which the assembled segments are disposed. A transparent cover plate is hinged at one side of the base, and top hinge plates rest on a ledge of the base, with the cover plate against a cover rim. Clamps on the base are pivoted to engage the cover plate against the rim. ", "references": []},{"id": "1a2a770d23b4a171fa81de62a78a3deb0588f238", "title": "Visualizing and Understanding Convolutional Networks", "authors": ["Matthew D. Zeiler", "Rob Fergus"], "date": "ECCV", "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18.", "references": ["b8de958fead0d8a9619b55c7299df3257c624a96", "b8de958fead0d8a9619b55c7299df3257c624a96", "38594b15593dce53ae1888b072b417013f1830ba", "65d994fb778a8d9e0f632659fb33a082949a50d3", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "b8de958fead0d8a9619b55c7299df3257c624a96", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "d67175d17c450ab0ac9c256103828f9e9a0acb85", "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "d67175d17c450ab0ac9c256103828f9e9a0acb85"]},{"id": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database", "authors": ["Jia Deng", "Wei Dong", "Fei-Fei Li"], "date": "2009", "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an… ", "references": ["5a5effa909cdeafaddbbb7855037e02f8e25d632", "530b99c3819bd4c4f1884fa89c9a0d6e024156bd", "dc22b98797333881542ccacea3179ba1cec522ae", "092c275005ae49dc1303214f6d02d134457c7053", "530b99c3819bd4c4f1884fa89c9a0d6e024156bd", "b3e7d3e37e67af7f4546b46051063bea1b62dbae", "54d2b5c64a67f65c5dd812b89e07973f97699552", "dc22b98797333881542ccacea3179ba1cec522ae", "804d14a9033b4ff532ba39c2f7d0aeac8a5548f7", "7c6f0c1917bb0f7e23c4c35b553045fa39663211"]},{"id": "464e8d981df7f326c3af6e9d7bd627f83e438816", "title": "Multi-Digit Recognition Using a Space Displacement Neural Network", "authors": ["Ofer Matan", "Christopher J. C. Burges", "John S. Denker"], "date": "NIPS", "abstract": "We present a feed-forward network architecture for recognizing an unconstrained handwritten multi-digit string.", "references": ["f866ac085771f5676800db2d9b102975b2a1b2d7", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "1f462943c8d0af69c12a09058251848324135e5a", "24ca5231df7cbd31e11a154c0c63ad48295f0398", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "a4bfe622ab32e6c645eb4be2079734355b22d304", "52a3dca70cc21b540ad5d7b7b6e23744c20095f7", "847d6ece37d22430a0d9e061b5dc1d1b8c679055", "a4bfe622ab32e6c645eb4be2079734355b22d304", "52a3dca70cc21b540ad5d7b7b6e23744c20095f7"]},{"id": "189b1859f77ddc08027e1e0f92275341e5c0fdc6", "title": "Sparse Representations and Distance Learning for Attribute Based Category Recognition", "authors": ["Grigorios Tsagkatakis", "Andreas E. Savakis"], "date": "ECCV Workshops", "abstract": "While traditional approaches in object recognition require the specification of training examples from each class and the application of class specific classifiers, in real world situations, the immensity of the number of image classes makes this task daunting. A novel approach in object recognition is attribute based classification, where instead of training classifiers for the recognition of specific object class instances, classifiers are trained on attributes of the object images and these… ", "references": ["6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "d5eec41043d91964879c4c745c7165f823967f29", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "d5eec41043d91964879c4c745c7165f823967f29", "82635fb63640ae95f90ee9bdc07832eb461ca881", "bb4362bd6f0bc5bb467fc8f169723243caa97d1d"]},{"id": "6286a82f72f632672c1890f3dd6bbb15b8e5168b", "title": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification", "authors": ["Li-Jia Li", "Hao Su", "Fei-Fei Li"], "date": "NIPS", "abstract": "Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a… ", "references": ["e79272fe3d65197100eae8be9fec6469107969ae", "cec734d7097ab6b1e60d95228ffd64248eb89d66", "d65b8a4c07146e617a0029f98fcf5317a0a68a39", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "e79272fe3d65197100eae8be9fec6469107969ae", "f9f836d28f52ad260213d32224a6d227f8e8849a", "e264e1e55433f158bf8aa8b260bf430d76d5fa28", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af"]},{"id": "a9ce496186120df8f9ed3367e76a4947419e992e", "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "authors": ["Ross B. Girshick", "Jeff Donahue", "Jagannath Malik"], "date": "2014", "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights… ", "references": ["0e19e69403501be0c4e9cb19bd5a11632721ba58", "cb2115a6765e8484830865b8ad5e6cc5dd29b48d", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "1395f0561db13cad21a519e18be111cbe1e6d818", "2eb6caace8296fd4dfd4947efa4fe911c8e133b2", "06b2739f4daba0382870f7e34ab653fa444993c0", "2eb6caace8296fd4dfd4947efa4fe911c8e133b2", "f99408de2ae6c5c036e1825bdadf7b193c3ba734", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "1109b663453e78a59e4f66446d71720ac58cec25"]},{"id": "9bc0295460089592d04e754a5fd427060b7bfa8c", "title": "Attribute-Based Classification for Zero-Shot Visual Object Categorization", "authors": ["Christoph H. Lampert", "Hannes Nickisch", "Stefan Harmeling"], "date": "2014", "abstract": "We study the problem of object recognition for categories for which we have no training examples, a task also called zero--data or zero-shot learning.", "references": ["fd1da46066aa0c13646c453a22d1da055254874d", "759d9a6c9206c366a8d94a06f4eb05659c2bb7f2", "f6908334853988faf987be40024ba88480170441", "23e568fcf0192e4ff5e6bed7507ee5b9e6c43598", "f6908334853988faf987be40024ba88480170441", "3958db5769c927cfc2a9e4d1ee33ecfba86fe054", "f6908334853988faf987be40024ba88480170441", "759d9a6c9206c366a8d94a06f4eb05659c2bb7f2", "a1dd806b8f4f418d01960e22fb950fe7a56c18f1", "dd626564bd47e9fc67a5b276301282ba2fe3d833"]},{"id": "64da1980714cfc130632c5b92b9d98c2f6763de6", "title": "On rectified linear units for speech processing", "authors": ["Matthew D. Zeiler", "Marc'Aurelio Ranzato", "Geoffrey E. Hinton"], "date": "2013", "abstract": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero… ", "references": ["3127190433230b3dc1abd0680bb58dced4bcd90e", "3127190433230b3dc1abd0680bb58dced4bcd90e", "b0383b56a5275819c95cef5af11fdba72c5afacb", "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "a538b05ebb01a40323997629e171c91aa28b8e2f", "b0383b56a5275819c95cef5af11fdba72c5afacb", "e8f811399746c059bf4d4c3d43334045e0222209", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "375d7b8a70277d5d7b5e0cc999b03ba395c42901", "86cf4c859d34c84fefdd8d36e5ea8ab691948512"]},{"id": "25d0fa49ca846370ff4796a6ac6688a42cf50f77", "title": "All About VLAD", "authors": ["Relja Arandjelovic", "Andrew Zisserman"], "date": "2013", "abstract": "The objective of this paper is large scale object instance retrieval, given a query image.", "references": ["21e2b9539a680af2a3d02c94c5315fa5ded4e3ad", "b6371f8c70c2684faefd99fffcc556c3a75dd7f4", "09a35fbc5d0a002102a00dad3cf16b67f7d6d694", "21e2b9539a680af2a3d02c94c5315fa5ded4e3ad", "21e2b9539a680af2a3d02c94c5315fa5ded4e3ad", "09a35fbc5d0a002102a00dad3cf16b67f7d6d694", "09a35fbc5d0a002102a00dad3cf16b67f7d6d694", "cbe0eb0db59f55d0686b9310b265527e1ce860ad", "cbe0eb0db59f55d0686b9310b265527e1ce860ad", "09a35fbc5d0a002102a00dad3cf16b67f7d6d694"]},{"id": "dcbf587642c39f495117552ca453a4f955ffa76a", "title": "Analyzing the Performance of Multilayer Neural Networks for Object Recognition", "authors": ["Pulkit Agrawal", "Ross B. Girshick", "Jitendra Malik"], "date": "2014", "abstract": "In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful… ", "references": ["1a2a770d23b4a171fa81de62a78a3deb0588f238", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b8de958fead0d8a9619b55c7299df3257c624a96", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "cec734d7097ab6b1e60d95228ffd64248eb89d66", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "9f2efadf66817f1b38f58b3f50c7c8f34c69d89a", "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "authors": ["Yaniv Taigman", "Ming Yang", "Lior Wolf"], "date": "2014", "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify.", "references": ["cfaae9b6857b834043606df3342d8dc97524aa9d", "72ed7aa31abf66fdd8175046f92bdde461fb2b49", "57ebeff9273dea933e2a75c306849baf43081a8c", "224d0eee53c2aa5d426d2c9b7fa5d843a47cf1db", "72ed7aa31abf66fdd8175046f92bdde461fb2b49", "f61a7a7cd13e2702f0fbacc05e13b355c1e297e2", "5f14a9595b0796ce6e5338f157b763326c1f632f", "5f14a9595b0796ce6e5338f157b763326c1f632f", "57ebeff9273dea933e2a75c306849baf43081a8c", "316d51aaa37891d730ffded7b9d42946abea837f"]},{"id": "215cf528ed98049724969ceb82c66e78edda8d51", "title": "Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction", "authors": ["Yuting Zhang", "Kihyuk Sohn", "Honglak Lee"], "date": "2015", "abstract": "Object detection systems based on the deep convolutional neural network (CNN) have recently made ground-breaking advances on several object detection benchmarks. While the features learned by these high-capacity neural networks are discriminative for categorization, inaccurate localization is still a major source of error for detection. Building upon high-capacity CNN architectures, we address the localization problem by 1) using a search algorithm based on Bayesian optimization that… ", "references": ["aa1c888d43f1d254e9fece485c3d6fd2454b894f", "1109b663453e78a59e4f66446d71720ac58cec25", "531e412155cde56382906aeee2a1b28ec61259c5", "a9ce496186120df8f9ed3367e76a4947419e992e", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "1109b663453e78a59e4f66446d71720ac58cec25", "1109b663453e78a59e4f66446d71720ac58cec25", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "aa1c888d43f1d254e9fece485c3d6fd2454b894f", "1109b663453e78a59e4f66446d71720ac58cec25"]},{"id": "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "title": "Deeply-Supervised Nets", "authors": ["Chen-Yu Lee", "Saining Xie", "Zhuowen Tu"], "date": "2015", "abstract": "Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent.", "references": ["1a2a770d23b4a171fa81de62a78a3deb0588f238", "4417f78b31546227784941bbd6f6532a177e60b8", "eb42cf88027de515750f230b23b1a057dc782108", "e2d894584986b44710f634b696db371f8aff92e0", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "e2d894584986b44710f634b696db371f8aff92e0", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "b8de958fead0d8a9619b55c7299df3257c624a96", "1a2a770d23b4a171fa81de62a78a3deb0588f238"]},{"id": "80e6390f0eacbcc33d410e9703941aef2471b10c", "title": "Hybrid convolutional neural networks", "authors": ["Iveta Mrázová", "Marek Kukacka"], "date": "2008", "abstract": "Convolutional neural networks are known to outperform all other neural network models when classifying a wide variety of 2D-shapes. This type of networks supports a massively parallel extraction of low-level features in the processed images. Especially this characteristic is assumed to impact the performance of convolutional networks in character recognition tasks - and in particular when considering scaled, rotated, translated or otherwise deformed patterns. Yet training of convolutional… ", "references": ["f2df0c1026ffa474f603a535e48e5c115d3d8629", "ee30014d4915ffa88578e475a445ae3597b88346", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "cd3d49f089f25c1dbb884378361b378a901032f8", "f70ae50828e3b6166628f5e8edb239b1cca6b471", "ee30014d4915ffa88578e475a445ae3597b88346", "f2df0c1026ffa474f603a535e48e5c115d3d8629", "ee30014d4915ffa88578e475a445ae3597b88346", "cd3d49f089f25c1dbb884378361b378a901032f8", "f70ae50828e3b6166628f5e8edb239b1cca6b471"]},{"id": "74b5a7014aaa00bcc40159bdd6f4b36d77ac1035", "title": "Generic Object Detection with Dense Neural Patterns and Regionlets", "authors": ["Will Y. Zou", "Xiaoyu Wang", "Yuanqing Lin"], "date": "2014", "abstract": "This paper addresses the challenge of establishing a bridge between deep convolutional neural networks and conventional object detection frameworks for accurate and efficient generic object detection. We introduce Dense Neural Patterns , short for DNPs, which are dense local features derived from discriminatively trained deep convolutional neural networks. DNPs can be easily plugged into conventional detection frameworks in the same way as other dense local features(like HOG or LBP). The… ", "references": ["a9ce496186120df8f9ed3367e76a4947419e992e", "f99408de2ae6c5c036e1825bdadf7b193c3ba734", "f354310098e09c1e1dc88758fca36767fd9d084d", "a1306ce652f556fbb9e794d91084a29294298e6d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "a1306ce652f556fbb9e794d91084a29294298e6d", "e79272fe3d65197100eae8be9fec6469107969ae", "8547b76e77995286c41148bf3a9626f75dc323a6", "f99408de2ae6c5c036e1825bdadf7b193c3ba734"]},{"id": "244bae85fda807361a51d4b26a14ecb2b2f8776b", "title": "Deformable part models are convolutional neural networks", "authors": ["Ross B. Girshick", "Forrest N. Iandola", "Jitendra Malik"], "date": "2015", "abstract": "Deformable part models (DPMs) and convolutional neural networks (CNNs) are two widely used tools for visual recognition.", "references": ["8d32768d7cb1c5f363d2cace7343da5d28882edc", "cbb19236820a96038d000dc629225d36e0b6294a", "8d32768d7cb1c5f363d2cace7343da5d28882edc", "8d32768d7cb1c5f363d2cace7343da5d28882edc", "f0dc6279760b3cc9b8d103d1922fb5595317feea", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "a9ce496186120df8f9ed3367e76a4947419e992e", "f0dc6279760b3cc9b8d103d1922fb5595317feea", "89d5b41b7fb0a122f811be270e6d5f72fc59d680", "a9ce496186120df8f9ed3367e76a4947419e992e"]},{"id": "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "title": "Rapid object detection using a boosted cascade of simple features", "authors": ["Paul A. Viola", "Michael J. Jones"], "date": "2001", "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates.", "references": ["8930f62a4b5eb1cbabf224cf84aa009ea798cfee", "8930f62a4b5eb1cbabf224cf84aa009ea798cfee", "ccf5208521cb8c35f50ee8873df89294b8ed7292", "3565c5a65842f26091578b9d71d496cc1561239d", "76f560991d56ad689ec32f9e9d13291e0193f4cf", "3565c5a65842f26091578b9d71d496cc1561239d", "9b535f4edc4cbf8d4fb6182ec6b5c54db3c1cccb", "ccf5208521cb8c35f50ee8873df89294b8ed7292", "9b535f4edc4cbf8d4fb6182ec6b5c54db3c1cccb", "76f560991d56ad689ec32f9e9d13291e0193f4cf"]},{"id": "28729bafdb929fa157d7a1a0ba721783e49803ae", "title": "Embedded facial image processing with Convolutional Neural Networks", "authors": ["Franck Mamalet", "Sébastien Roux", "Christophe Garcia"], "date": "2010", "abstract": "This paper presents an embedded facial image analysis framework based on Convolutional Neural Networks (ConvNets). This robust framework has been proposed by Garcia, Delakis and Duffner on general purpose workstations without any constraints on computational and memory resources. We show that ConvNets, which consist of a pipeline of convolution and subsampling operations followed by a Multi Layer Perceptron, are particularly well suited for implementation on embedded processors. We present a… ", "references": ["887567782cb859ecd339693589056903b0071353", "ba86d894dd909634243a28d8c52711c8237ddd6e", "e787b59c7c4ff69c107eefa6870ad5c95022a739", "ec2a4f726a45c7ff8a53ed35ef5ebe719d168646", "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "162d958ff885f1462aeda91cd72582323fd6a1f4", "8426b3379a20ec8ba028ecc796f0b1c97a54673e", "ba86d894dd909634243a28d8c52711c8237ddd6e", "06719154ab53d3a57041b2099167e3619f1677bc", "ec2a4f726a45c7ff8a53ed35ef5ebe719d168646"]},{"id": "fd790b061082571e20be7892ce4a97e156497c9f", "title": "text Detection with Convolutional Neural Networks", "authors": ["Manolis Delakis", "Christophe Garcia"], "date": "VISAPP", "abstract": "Text detection is an important preliminary step before text can be recognized in unconstrained image environments. We present an approach based on convolutional neural networks to detect and localize horizontal text lines from raw color pixels. The network learns to extract and combine its own set of features through learning instead of using hand-crafted ones. Learning was also used in order to precisely localize the text lines by simply training the network to reject badly-cut text and… ", "references": ["42a8ff86566538103c6116f9047a4c3128e1542c", "f8f5c282dc11937d29183b955dc3e4fbb677571b", "f8f5c282dc11937d29183b955dc3e4fbb677571b", "162d958ff885f1462aeda91cd72582323fd6a1f4", "57930a675de539c59bc33f56d9894c999d264f72", "162d958ff885f1462aeda91cd72582323fd6a1f4", "162d958ff885f1462aeda91cd72582323fd6a1f4", "162d958ff885f1462aeda91cd72582323fd6a1f4", "93c8a2963be6088dd55959bfe8cd92916891fb66", "68a859142ef42196e6a56305b8c6ac4cb2c9326e"]},{"id": "82bac0ae7d60f5bef57deb837de404b4472ee0a0", "title": "Automatic Scene Text Recognition using a Convolutional Neural Network", "authors": ["Zohra Saidane"], "date": "2007", "abstract": "This paper presents an automatic recognition method for color text characters extracted from scene images, which is robust to strong distortions, complex background, low resolution and non uniform lightning. Based on a specific architecture of convolutional neural networks, the proposed system automatically learns how to recognize characters without making any assumptions, without applying any preprocessing or post-processing and without using tunable parameters. For this purpose, we use a… ", "references": ["7f3e0bd21ead365af8dd302ab7d5b1ab53e7d437", "cf7edca25496335f180851111f446879d0b534d1", "1d4816c612e38dac86f2149af667a5581686cdef", "bd568cc328d34c43af90e10f6b133c03994bee09", "1d4816c612e38dac86f2149af667a5581686cdef", "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "162d958ff885f1462aeda91cd72582323fd6a1f4", "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "7f3e0bd21ead365af8dd302ab7d5b1ab53e7d437", "0b8e87e9d7f1edebc8cb6992ed4eac17b4a8682c"]},{"id": "6fe78db480995464bd97ba3b712ecc82129e6179", "title": "Learning by Stretching Deep Networks", "authors": ["Gaurav Pandey", "Ambedkar Dukkipati"], "date": "ICML", "abstract": "In recent years, deep architectures have gained a lot of prominence for learning complex AI tasks because of their capability to incorporate complex variations in data within the model. However, these models often need to be trained for a long time in order to obtain good results. In this paper, we propose a technique, called 'stretching', that allows the same models to perform considerably better with very little training. We show that learning can be done tractably, even when the weight… ", "references": ["398c296d0cc7f9d180f84969f8937e6d3a413796", "16849f9ef7a750ce85675592d283f1c0330ab2e4", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "16849f9ef7a750ce85675592d283f1c0330ab2e4", "162d958ff885f1462aeda91cd72582323fd6a1f4", "16849f9ef7a750ce85675592d283f1c0330ab2e4", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "162d958ff885f1462aeda91cd72582323fd6a1f4", "398c296d0cc7f9d180f84969f8937e6d3a413796", "0abb49fe138e8fb7332c26b148a48d0db39724fc"]},{"id": "6436ab45779f90a4d9c473354672b86e465e3ff7", "title": "Learning Separable Filters", "authors": ["Roberto Rigamonti", "Amos Sironi", "Pascal Fua"], "date": "CVPR", "abstract": "Learning filters to produce sparse image representations in terms of over complete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of… ", "references": []},{"id": "b1c23451334b0f9294a6a8de5be59d361547a946", "title": "Are sparse representations really relevant for image classification?", "authors": ["Roberto Rigamonti", "Matthew A. Brown", "Vincent Lepetit"], "date": "2011", "abstract": "Recent years have seen an increasing interest in sparse representations for image classification and object recognition, probably motivated by evidence from the analysis of the primate visual cortex. It is still unclear, however, whether or not sparsity helps classification. In this paper we evaluate its impact on the recognition rate using a shallow modular architecture, adopting both standard filter banks and filter banks learned in an unsupervised way. In our experiments on the CIFAR-IO and… ", "references": ["54d2b5c64a67f65c5dd812b89e07973f97699552", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "54d2b5c64a67f65c5dd812b89e07973f97699552", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "54d2b5c64a67f65c5dd812b89e07973f97699552", "74227090d23c958f601ad05369fad587e3b546f1"]},{"id": "4cd691140606e7c79909a7a994e23d877b6890f8", "title": "Correlation Functions, Cluster Functions, and Spacing Distributions for Random Matrices", "authors": ["Craig A. Tracy", "Harold Widom"], "date": "1998", "abstract": "The usual formulas for the correlation functions in orthogonal and symplectic matrix models express them as quaternion determinants. From this representation one can deduce formulas for spacing probabilities in terms of Fredholm determinants of matrix-valued kernels. The derivations of the various formulas are somewhat involved. In this article we present a direct approach which leads immediately to scalar kernels for the unitary ensembles and matrix kernels for the orthogonal and symplectic… ", "references": []},{"id": "bfd771fba9aed481cd3395e3e9ae3c284659f0d4", "title": "Filter learning for linear structure segmentation", "authors": ["Roberto Rigamonti", "Engin Türetken", "Vincent Lepetit"], "date": "2011", "abstract": "We introduce an approach to learning convolution filters whose joint output can be fed to a classifier that labels them as belonging to linear structures or not. The filters are learned using sparse synthesis techniques but we show that enforcing constraints is not required at run-time to achieve good classification performance. In practice, this is important as it drastically reduces the computational cost. We show that our approach outperforms the state-of-the-art on difficult, and very… ", "references": ["498efaa51f5eda731dc6199c3547b9465717fa68", "74a10cbbde0ad2a3d6b84f48811bb3fea3d7676f", "74a10cbbde0ad2a3d6b84f48811bb3fea3d7676f", "74a10cbbde0ad2a3d6b84f48811bb3fea3d7676f", "8c8c9c0ff996da9a2e8f8387788903e21a4c3668", "be9a17321537d9289875fe475b71f4821457b435", "be9a17321537d9289875fe475b71f4821457b435", "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec", "8c8c9c0ff996da9a2e8f8387788903e21a4c3668", "4b3d5bb0f9597fff321d3d48e0d22b2cae7e648a"]},{"id": "02227c94dd41fe0b439e050d377b0beb5d427cda", "title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "authors": ["Yuval Netzer", "Tiejie Wang", "Andrew Y. Ng"], "date": "2011", "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications.", "references": ["bf38dfb13352449b965c08282b66d3ffc5a0539f", "cec734d7097ab6b1e60d95228ffd64248eb89d66", "54d2b5c64a67f65c5dd812b89e07973f97699552", "54d2b5c64a67f65c5dd812b89e07973f97699552", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "12244deb997152492d96c6246ec21b2b9804800d", "12244deb997152492d96c6246ec21b2b9804800d", "32b8f58a038df83138435b12a499c8bf0de13811", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "bf38dfb13352449b965c08282b66d3ffc5a0539f"]},{"id": "7422b726a35551eff938d0b4a4188f859c081181", "title": "Random matrix models with additional interactions", "authors": ["Khandker A. Muttalib"], "date": "1995", "abstract": "It has been argued that, despite remarkable success, existing random matrix theories are not adequate at describing disordered conductors in the metallic regime, due to the presence of certain two-body interactions in the effective Hamiltonian for the eigenvalues, in addition to the standard logarithmic interaction that arises entirely from symmetry considerations. We present a new method that allows exact solution of random matrix models with such additional two-body interactions. This should… ", "references": []},{"id": "6d5b6e0ec6957a7e22245f5e9d2305f8fa46c189", "title": "Separable Dictionary Learning", "authors": ["Simon Hawe", "Matthias Seibert", "Martin Kleinsteuber"], "date": "2013", "abstract": "Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In… ", "references": ["054bbbab7d8324d381903b2f33d4e8a17b54eff0", "19de9e4850a208800db50615afec2b08b25d4f99", "19de9e4850a208800db50615afec2b08b25d4f99", "2f88b7d26f08c25ee336168fba0e37772c06ca6e", "19de9e4850a208800db50615afec2b08b25d4f99", "019de3bca6f53fddb12a52b626b428e18de854f0", "17e7cca7e795d8ba1fa9d2c88bf2675c2d6ddfe8", "306de9c553695822ae9e6de044b6856baf0cce7d", "a8b515f2e5d065ed9e8c25710356014262dc0c6e", "054bbbab7d8324d381903b2f33d4e8a17b54eff0"]},{"id": "498efaa51f5eda731dc6199c3547b9465717fa68", "title": "Learning mid-level features for recognition", "authors": ["Y-Lan Boureau", "Francis R. Bach", "Jean Ponce"], "date": "2010", "abstract": "Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter responses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be broken down into two steps: (1) a coding step, which performs a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pooling step, which summarizes the coded features over larger neighborhoods. Several combinations of coding… ", "references": ["03a073589eaf8ce3440464d020e0d0b26df5869b", "ba0548583a5ab3dca551f60e30f85ea42b2a4873", "65cccb5b4aec5dd9a7161ddebcb0213b39f53439", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "ba0548583a5ab3dca551f60e30f85ea42b2a4873", "d5715f6b7f2bb482ae45b50899128590afb8ec41", "ba0548583a5ab3dca551f60e30f85ea42b2a4873", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "03a073589eaf8ce3440464d020e0d0b26df5869b"]},{"id": "0a2051f480461301cbe0764fdbc970a42b2e8364", "title": "Harmonic analysis on the infinite-dimensional unitary group and determinantal point processes", "authors": ["Alexei Borodin", "Grigori Olshanski"], "date": "2001", "abstract": "The infinite-dimensional unitary group U(∞) is the inductive limit of growing compact unitary groups U(N). In this paper we solve a problem of harmonic analysis on U(∞) stated in [Ol3]. The problem consists in computing spectral decomposition for a remarkable 4-parameter family of characters of U(∞). These characters generate representations which should be viewed as analogs of nonexisting regular representation of U(∞). \n \nThe spectral decomposition of a character of U(∞) is described by the… ", "references": ["49feb3e86487595adcdb6365c29788a18db04042", "c79d0e8193fdc26a5920c8128db7dd9b1f17b62e", "d48f35d1d37c58b79901c82c5d905be1cdb14842", "8cc47caf8eaf223d1628ce161a6f823054b62e8c", "e13109dbb390e815a2e7ac328fbe4b6cb1113946", "d48f35d1d37c58b79901c82c5d905be1cdb14842", "e13109dbb390e815a2e7ac328fbe4b6cb1113946", "00b0e2735ddac0c8e4911d40a72fbb7b631385b4", "d48f35d1d37c58b79901c82c5d905be1cdb14842", "0da5140de01361460b8d72d4ecbb4a964c07ea8b"]},{"id": "277647aad08aea0511a6185efe59473f4e4b4bf7", "title": "Fredholm determinants, Jimbo‐Miwa‐Ueno τ‐functions, and representation theory", "authors": ["Alexei Borodin", "Percy Deift"], "date": "2001", "abstract": "The authors show that a wide class of Fredholm determinants arising in the representation theory of “big” groups, such as the infinite-dimensional unitary group, solve Painleve equations. Their methods are based on the theory of integrable operators and the theory of Riemann-Hilbert problems. © 2002 Wiley Periodicals, Inc. ", "references": []},{"id": "fdfd269b2ce8256f0483b81d078ee652a7747987", "title": "Random point fields associated with certain Fredholm determinants I: fermion, Poisson and boson point processes", "authors": ["Tomoyuki Shirai", "Yoichiro Takahashi"], "date": "2003", "abstract": "We introduce certain classes of random point fields, including fermion and boson point processes, which are associated with Fredholm determinants of certain integral operators and study some of their basic properties: limit theorems, correlation functions, Palm measures etc. Also we propose a conjecture on an α-analogue of the determinant and permanent. ", "references": []},{"id": "798ed9a1aa66862f7b7a92cceaeff4c43a5d4151", "title": "Local Characteristics, Entropy and Limit Theorems for Spanning Trees and Domino Tilings Via Transfer-Impedances", "authors": ["Robert Burton", "Robin Pemantle"], "date": "1993", "abstract": "Let G be a finite graph or an infinite graph on which Z^d acts with finite fundamental domain. If G is finite, let T be a random spanning tree chosen uniformly from all spanning trees of G; if G is infinite, known methods show that this still makes sense, producing a random essential spanning forest of G. A method for calculating local characteristics (i.e. finite-dimensional marginals) of T from the transfer-impedance matrix is presented. This differs from the classical matrix-tree theorem in… ", "references": []},{"id": "d05f2f2b0e5d5fffb3f049650655cd9c734d1e79", "title": "Orthogonal Polynomials and Random Matrices: A Riemann-Hilbert Approach", "authors": ["Percy Deift"], "date": "2000", "abstract": "Riemann-Hilbert problems Jacobi operators Orthogonal polynomials Continued fractions Random matrix theory Equilibrium measures Asymptotics for orthogonal polynomials Universality Bibliography. ", "references": []},{"id": "ce72362402738b024c0ba7919c3a89c07b8d66d2", "title": "THE COINCIDENCE APPROACH TO STOCHASTIC POINT PROCESSES", "authors": ["Odile Macchi"], "date": "1975", "abstract": "The structure of the probability space associated with a general point process, when regarded as a counting process, is reviewed using the coincidence formalism. The rest of the paper is devoted to the class of regular point processes for which all coincidence probabilities admit densities. It is shown that their distribution is completely specified by the system of coincidence densities. The specification formalism is stressed for 'completely' regular point processes. A construction theorem… ", "references": []},{"id": "6f1d7cd30249e2858ac436791dd6f50fae9779d0", "title": "Determinantal Processes with Number Variance Saturation", "authors": ["Kurt Johansson"], "date": "2004", "abstract": "Consider Dyson’s Hermitian Brownian motion model after a finite time S, where the process is started at N equidistant points on the real line. These N points after time S form a determinantal process and has a limit as N→∞. This limting determinantal process has the interesting feature that it shows number variance saturation. The variance of the number of particles in an interval converges to a limiting value as the length of the interval goes to infinity. Number variance saturation is also… ", "references": ["c79e704512c235cca5bd7ce60efe9b4bca1c3c4a", "689ba933fcee050f97f0c2aaecc1c4175396ef3c", "fef27316c2a2aab78ef8bc7e9368e3b124ef10e2", "c79e704512c235cca5bd7ce60efe9b4bca1c3c4a", "fef27316c2a2aab78ef8bc7e9368e3b124ef10e2", "a7f63c00882b7c5462005d63f57069579c5d41d1", "ddfc25e3a2f53eea1086a070f97d557a830fea1e", "8b9c4015bba287acd0db6c9239a18e6edfca816f", "8b9c4015bba287acd0db6c9239a18e6edfca816f", "85bf74cdad3d5b7acbc86bb5acc8b6935b7d1308"]},{"id": "7099761d56368de3cc31d924b3c8b428e54e036a", "title": "Zeros of the i.i.d. Gaussian power series: a conformally invariant determinantal process", "authors": ["Yuval Peres", "Bálint Virág"], "date": "2005", "abstract": "Consider the zero set of the random power series f(z) = P anz n with i.i.d. complex Gaussian coefficientsan. We show that these zeros form a determinantal process: more precisely, their joint intensity can be written as a minor of the Bergman kernel. We show that the number of zeros of f in a disk of radius r about the origin has the same distribution as the sum of independent {0,1}-valued random variables Xk, where P(Xk = 1) = r 2k . Moreover, the set of absolute values of the zeros of f has… ", "references": ["a97581760cd0f6bbab5218dcd2ed5893712f9650", "18c760a9012e33b61c62b5709f00b7b0fb7e4f6b", "507fb17c4271aec03be1ec9467fd7ad88c3bc5d7", "c4eb502ceedce63371ffb828608c989e84866f9b", "18c760a9012e33b61c62b5709f00b7b0fb7e4f6b", "4a314e5f6da3f1215db575bea9caa98652aaa7f4", "46274063ceac6f65a6625753e5def7209a145b41", "a97581760cd0f6bbab5218dcd2ed5893712f9650", "ddcc048fbf590853aeec150932993435f96d6ce9", "a97581760cd0f6bbab5218dcd2ed5893712f9650"]},{"id": "66cbb3953c8740b10451043ebb52d411c5f85e4a", "title": "Level spacing distributions and the Bessel kernel", "authors": ["Craig A. Tracy", "Harold Widom"], "date": "1994", "abstract": "AbstractScaling models of randomN×N hermitian matrices and passing to the limitN→∞ leads to integral operators whose Fredholm determinants describe the statistics of the spacing of the eigenvalues of hermitian matrices of large order. For the Gaussian Unitary Ensemble, and for many others'as well, the kernel one obtains by scaling in the “bulk” of the spectrum is the “sine kernel”\n$$\\frac{{\\sin \\pi (x - y)}}{{\\pi (x - y)}}$$\n. Rescaling the GUE at the “edge” of the spectrum leads to the kernel… ", "references": ["36597c0d5103c95d2436dab19621b9613fb2db92", "fc9135a1e14823a2733d2f341b0722819ad10c00", "fc9135a1e14823a2733d2f341b0722819ad10c00", "d3a4bdea291a79280d1e1d1c075a27feff417e64", "2f6fc2f2ef282f6a710b517ee701c3341e2c6e82", "598b1a0a6451f58298c38955ca2acebf84437211", "cf49d57962bb2b67c001258e5f96f2b624cd169f", "36597c0d5103c95d2436dab19621b9613fb2db92", "cf49d57962bb2b67c001258e5f96f2b624cd169f", "337effdabae73d7964179b4ab7c704b9c4a6293c"]},{"id": "3e8bc9d077555e6ab0b338b9a46fafc809b3094b", "title": "High powers of random elements of compact Lie groups", "authors": ["Eric M. Rains"], "date": "1997", "abstract": "Summary.If a random unitary matrix $U$ is raised to a sufficiently high power, its eigenvalues are exactly distributed as independent, uniform phases. We prove this result, and apply it to give exact asymptotics of the variance of the number of eigenvalues of $U$ falling in a given arc, as the dimension of $U$ tends to infinity. The independence result, it turns out, extends to arbitrary representations of arbitrary compact Lie groups. We state and prove this more general theorem, paying… ", "references": []},{"id": "ec7018ef4a99a5a5ed6547eb16fe856e3f2e60c6", "title": "A Markov random field model for term dependencies", "authors": ["Donald Metzler", "W. Bruce Croft"], "date": "SIGIR '05", "abstract": "This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision… ", "references": ["9ea50374077a506b86dce4796c683abcd98e18d7", "3add8c6c36af4755cecaf994bda0e5a202035811", "9ea50374077a506b86dce4796c683abcd98e18d7", "dd90df6350c6245434655636f6e809e7b7a721d2", "3add8c6c36af4755cecaf994bda0e5a202035811", "dd90df6350c6245434655636f6e809e7b7a721d2", "dd90df6350c6245434655636f6e809e7b7a721d2", "dd90df6350c6245434655636f6e809e7b7a721d2", "3add8c6c36af4755cecaf994bda0e5a202035811", "9ea50374077a506b86dce4796c683abcd98e18d7"]},{"id": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3", "title": "Finite-time Analysis of the Multiarmed Bandit Problem", "authors": ["Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer"], "date": "2002", "abstract": "Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi… ", "references": ["eaec01700f5ea63af311cfd7a70a3869460ce080", "305d689afb6574ffec7b01e24431d541d0ce6f5d", "97b5d2379c86d1d58ba53d7e670c2f570fb3d2c3", "abba12ca7c82e73cee7204e0f694cb0072d25cc8", "936fcbdbf013b0308a68d0c35c6f1a89d461184b", "8597ed8596c08ba2ba7bc8da3e9546749d6f4f7b", "abba12ca7c82e73cee7204e0f694cb0072d25cc8", "12d1d070a53d4084d88a77b8b143bad51c40c38f", "df511a5d9d12bff681438e2dbe2ecef70268c9c9", "12d1d070a53d4084d88a77b8b143bad51c40c38f"]},{"id": "0dec090d24ebe162650923f206979300cfaa847d", "title": "Learning to Rank with Nonsmooth Cost Functions", "authors": ["Bernhard Schölkopf", "John Platt"], "date": "2007", "abstract": "The quality measures used in information retrieval are particularly difficult to optimize directly, since they depend on the model scores only through the sorted order of the documents returned for a given query. Thus, the derivatives of the cost with respect to the model parameters are either zero, or are undefined. In this paper, we propose a class of simple, flexible algorithms, called LambdaRank, which avoids these difficulties by working with implicit cost functions. We describe LambdaRank… ", "references": []},{"id": "c669c8cb28c6d219419e0e904a795164e8c6be05", "title": "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries", "authors": ["Jaime Carbinell", "Jade Goldstein-Stewart"], "date": "2017", "abstract": "This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by… ", "references": []},{"id": "73d6a26f407db77506959fdf3f7b853e44f3844a", "title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "authors": ["Tijmen Tieleman"], "date": "ICML '08", "abstract": "A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other… ", "references": ["8978cf7574ceb35f4c3096be768c7547b28a35d0", "883b8a189fe1bb97b9ad2a382e057ba7e2a2e56f", "4f7476037408ac3d993f5088544aab427bc319c1", "883b8a189fe1bb97b9ad2a382e057ba7e2a2e56f", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "4f7476037408ac3d993f5088544aab427bc319c1", "883b8a189fe1bb97b9ad2a382e057ba7e2a2e56f", "4f7476037408ac3d993f5088544aab427bc319c1", "883b8a189fe1bb97b9ad2a382e057ba7e2a2e56f", "4f7476037408ac3d993f5088544aab427bc319c1"]},{"id": "d237072bae2bb7ab3c0e3ea10959c05df5384311", "title": "Improving Web Search Ranking by Incorporating User Behavior Information", "authors": ["Eugene Agichtein", "Eric Brill", "Susan T. Dumais"], "date": "2006", "abstract": "We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. We show that incorporating implicit feedback can augment other features… ", "references": ["63aaf12163fe9735dfe9a69114937c4fa34f303a", "63aaf12163fe9735dfe9a69114937c4fa34f303a", "63aaf12163fe9735dfe9a69114937c4fa34f303a", "63aaf12163fe9735dfe9a69114937c4fa34f303a", "9e7c4c8fc3bb8cf5353f2c3cac6a1c5fa95bae65", "9e7c4c8fc3bb8cf5353f2c3cac6a1c5fa95bae65", "9e7c4c8fc3bb8cf5353f2c3cac6a1c5fa95bae65", "9e7c4c8fc3bb8cf5353f2c3cac6a1c5fa95bae65", "9e7c4c8fc3bb8cf5353f2c3cac6a1c5fa95bae65", "9e7c4c8fc3bb8cf5353f2c3cac6a1c5fa95bae65"]},{"id": "43c8a545f7166659e9e21c88fe234e0323855216", "title": "Greedy Layer-Wise Training of Deep Networks", "authors": ["Bernhard Schölkopf", "John Platt"], "date": "2007", "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more ef cient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting… ", "references": []},{"id": "63aaf12163fe9735dfe9a69114937c4fa34f303a", "title": "Learning to rank using gradient descent", "authors": ["Chris Burges", "Tal Shaked", "Greg Hullender"], "date": "ICML '05", "abstract": "We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function.", "references": ["75e85c2e90b0abb17ae6445516a49ac05c1dbf0f", "958f001b6f348f7c353260b289bed185fffac847", "b1a5961609c623fc816aaa77565ba38b25531a8e", "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f", "b1a5961609c623fc816aaa77565ba38b25531a8e", "958f001b6f348f7c353260b289bed185fffac847", "81b8bb14524665e3dbaa772f297cd5e5e79ba0d6", "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f", "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f", "b1a5961609c623fc816aaa77565ba38b25531a8e"]},{"id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "title": "Reducing the dimensionality of data with neural networks.", "authors": ["Geoffrey E. Hinton", "Ruslan Salakhutdinov"], "date": "2006", "abstract": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than… ", "references": ["df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9", "df648ec7958e04f93835df0808734e065f9a46e9"]},{"id": "08d0ea90b53aba0008d25811268fe46562cfb38c", "title": "On the quantitative analysis of deep belief networks", "authors": ["Ruslan Salakhutdinov", "Iain Murray"], "date": "ICML '08", "abstract": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's… ", "references": []},{"id": "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47", "title": "Beyond Independent Relevance", "authors": ["ChengXiang Zhai", "William W. Cohen", "John D. Lafferty"], "date": "2015", "abstract": "We present a non-traditional retrieval problem we call subtopic retrieval. The subtopic retrieval problem is concerned with finding documents that cover many different subtopics of a query topic. In such a problem, the utility of a document in a ranking is dependent on other documents in the ranking, violating the assumption of independent relevance which is assumed in most traditional retrieval methods. Subtopic retrieval poses challenges for evaluating performance, as well as for developing… ", "references": ["624d8241c597ee12477053cf1970d017a7651417", "624d8241c597ee12477053cf1970d017a7651417"]},{"id": "9b7769158ebcb1f6907980d68203c7d3fecc00e6", "title": "Evaluating high accuracy retrieval techniques", "authors": ["Chirag Shah", "W. Bruce Croft"], "date": "SIGIR '04", "abstract": "Although information retrieval research has always been concerned with improving the effectiveness of search, in some applications, such as information analysis, a more specific requirement exists for high accuracy retrieval. This means that achieving high precision in the top document ranks is paramount. In this paper we present work aimed at achieving high accuracy in ad-hoc document retrieval by incorporating approaches from question answering(QA). We focus on getting the first relevant… ", "references": ["646d4888871aca2a25111eb2520e4c47e253b014", "646d4888871aca2a25111eb2520e4c47e253b014", "1250f6d90ba07b242f8548e39bb30164b4b8b3aa", "520028c1b1b6d47093d3e74ce35cd9c4fb4234d8", "520028c1b1b6d47093d3e74ce35cd9c4fb4234d8", "1250f6d90ba07b242f8548e39bb30164b4b8b3aa", "646d4888871aca2a25111eb2520e4c47e253b014", "520028c1b1b6d47093d3e74ce35cd9c4fb4234d8", "1250f6d90ba07b242f8548e39bb30164b4b8b3aa", "646d4888871aca2a25111eb2520e4c47e253b014"]},{"id": "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "title": "Robust Object Recognition with Cortex-Like Mechanisms", "authors": ["Thomas Serre", "Lior Wolf", "Tomaso A. Poggio"], "date": "2007", "abstract": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass… ", "references": ["56f4906d8fcc4097299d9644ee54fa4534b8e594", "f9e65fcb0e04174577f211d702d3f837e3624c5b", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "7aa7c0df2c2e154d7bddb873168ebc8446472425", "e03bda45248b4169e2a20cb9124ae60440cad2de", "0b8af3a7db6ff26f73f69c15cfa6635d607d58ec", "6bb4de9c81b221f97d541b81134aab9029c42b91", "f9e65fcb0e04174577f211d702d3f837e3624c5b", "e03bda45248b4169e2a20cb9124ae60440cad2de"]},{"id": "ba2345dbc13c8e94b771ef9f09af57aa610fa0f3", "title": "Measuring ineffectiveness", "authors": ["Ellen M. Voorhees"], "date": "SIGIR '04", "abstract": "An evaluation methodology that targets ineffective topics is needed to support research on obtaining more consistent retrieval across topics. Using average values of traditional evaluation measures is not an appropriate methodology because it emphasizes effective topics: poorly performing topics' scores are by definition small, and they are therefore difficult to distinguish from the noise inherent in retrieval evaluation. We examine two new measures that emphasize a system's worst topics… ", "references": []},{"id": "e0be8d0485f0f71ff07930dd2356c497ac3e7fdb", "title": "A risk minimization framework for information retrieval", "authors": ["ChengXiang Zhai", "John D. Lafferty"], "date": "2006", "abstract": "This paper presents a probabilistic information retrieval framework in which the retrieval problem is formally treated as a statistical decision problem. In this framework, queries and documents are modeled using statistical language models, user preferences are modeled through loss functions, and retrieval is cast as a risk minimization problem. We discuss how this framework can unify existing retrieval models and accommodate systematic development of new retrieval models. As an example of… ", "references": ["5ce8078bce4dea312a2c28d34af4dd82667bbffe", "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47", "03a8cb23b78ae1e8662b226d96e4a0ac2bf5d3fd", "03a8cb23b78ae1e8662b226d96e4a0ac2bf5d3fd", "7883dd07bc13a024f018ef50e9f4313218cf1718", "7883dd07bc13a024f018ef50e9f4313218cf1718", "5ce8078bce4dea312a2c28d34af4dd82667bbffe", "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47", "5ce8078bce4dea312a2c28d34af4dd82667bbffe", "68c9e02b77a5f46677d87f6efe86aafa8fb1772c"]},{"id": "223a80e2f3fe01e196d32e8edfe447a949a4bed6", "title": "Expected search length: A single measure of retrieval effectiveness based on the weak ordering action of retrieval systems", "authors": ["William S. Cooper"], "date": "1968", "abstract": "A measure of document retrieval system performance called the “expected search length reduction factor” is defined and compared with indicators, such as precision and recall, that have been suggested by other workers. The new measure is based on calculations of the expected number of irrelevant documents in the collection which would have to be searched through before the desired number of relevant documents could be found. Its advantages are: (1) it provides a single index for the property it… ", "references": []},{"id": "a8cf3f0ea76961eca50bf26ab31e677037cab622", "title": "Configuration based scene classification and image indexing", "authors": ["Pamela Lipson", "W. Eric L. Grimson", "Pawan Sinha"], "date": "1997", "abstract": "Scene classification is a major open challenge in machine vision. Most solutions proposed so far such as those based on color histograms and local texture statistics cannot capture a scene's global configuration, which is critical in perceptual judgments of scene similarity. We present a novel approach, \"configural recognition\", for encoding scene class structure. The approach's main feature is its use of qualitative spatial and photometric relationships within and across regions in low… ", "references": ["836bfe1994a174b41c46e7244845b44f8702eeab", "87cc226aa060db976fbf6ac3a07969b33b544b96", "836bfe1994a174b41c46e7244845b44f8702eeab", "b8bede7bcffa3358a0bd76c63137230fa856e0cc", "f33ab691655bbefea841eabaac34903ae9755ab7"]},{"id": "f24db70b4283b56f43a94edd48d1d35e20935ef4", "title": "Diagnostic Colors Mediate Scene Recognition", "authors": ["Aude Oliva", "Philippe G. Schyns"], "date": "2000", "abstract": "In this research, we aim to ground scene recognition on information other than the identity of component objects. Specifically we seek to understand the structure of color cues that allows the express recognition of scene gists. Using the L*a*b* color space we examined the conditions under which chromatic cues concur with brightness to allow a viewer to recognize scenes at a glance. Using different methods, Experiments 1 and 2 tested the hypothesis that colors do contribute when they are… ", "references": ["bf85123feb2cf0ec42930ce324501fd2cd9c049c", "293f77164e068cb62ca5bf76ff4116333df7de4b", "91f2cb5a7ba9da57c140e6f2303cdd16a4c77ecb", "91f2cb5a7ba9da57c140e6f2303cdd16a4c77ecb", "aff99b6538c2a6b89478a7e7901fe31878e63f03", "2196b6fcf0a65872c70cf9852af29a6a839a55fa", "aff99b6538c2a6b89478a7e7901fe31878e63f03", "2196b6fcf0a65872c70cf9852af29a6a839a55fa", "48f0d6211dc70a56202c1a0c95124c65a51f721f", "2196b6fcf0a65872c70cf9852af29a6a839a55fa"]},{"id": "80c816d748c9ece1145e7a747de1aa99284e5c24", "title": "Gist of a scene", "authors": ["Abel Martin Gonzalez Oliva"], "date": "2005", "abstract": "ABSTRACT Studies in scene perception have shown that observers recognize a real-world scene at a single glance. During this expeditious process of seeing, the visual system forms a spatial representation of the outside world that is rich enough to grasp the meaning of the scene, to recognize a few objects and other salient information in the image, and to facilitate object detection and the deployment of attention. This representation refers to the gist of a scene, which includes all levels of… ", "references": ["f24db70b4283b56f43a94edd48d1d35e20935ef4", "acf0b6745708457a53d5327eea345c0bcf466e97", "3cc6c4b2881c5607df9d3d6bb25ed94fd0add236", "acf0b6745708457a53d5327eea345c0bcf466e97", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "48f0d6211dc70a56202c1a0c95124c65a51f721f", "f24db70b4283b56f43a94edd48d1d35e20935ef4", "f24db70b4283b56f43a94edd48d1d35e20935ef4", "697b45e902c9dd2f31d205f0720e7079f71db200", "3cc6c4b2881c5607df9d3d6bb25ed94fd0add236"]},{"id": "9a21604e6db8020b9e76d89064f2b63fb875b67a", "title": "Scene-Centered Description from Spatial Envelope Properties", "authors": ["Aude Oliva", "Antonio Torralba"], "date": "Biologically Motivated…", "abstract": "In this paper, we propose a scene-centered representation able to provide a meaningful description of real world images at multiple levels of categorization (from superordinate to subordinate levels). The scene-centered representation is based upon the estimation of spatial envelope properties describing the shape of a scene (e.g. size, perspective, mean depth) and the nature of its content. The approach is holistic and free of segmentation phase, grouping mechanisms, 3D construction and object… ", "references": ["f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "acf0b6745708457a53d5327eea345c0bcf466e97", "f24db70b4283b56f43a94edd48d1d35e20935ef4", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "f05f91c03fc7497162f754fdbfcddc5fcf4179fb", "f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "bf85123feb2cf0ec42930ce324501fd2cd9c049c"]},{"id": "869171b2f56cfeaa9b81b2626cb4956fea590a57", "title": "Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope", "authors": ["Aude Oliva", "Antonio Torralba"], "date": "2001", "abstract": "In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably… ", "references": ["913beb8d70383bacaf7f0133d9a88ca592542af7", "a8cf3f0ea76961eca50bf26ab31e677037cab622", "364c60c92a77e2eca1b7faeea763aea3a7d2070a", "913beb8d70383bacaf7f0133d9a88ca592542af7", "33f65bf55cf25bb474aa0e593b7cc4a128c7f1bf", "364c60c92a77e2eca1b7faeea763aea3a7d2070a", "f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "f24db70b4283b56f43a94edd48d1d35e20935ef4", "bd580fad7a14f93d6d59765a5fe91974e2653281", "913beb8d70383bacaf7f0133d9a88ca592542af7"]},{"id": "50b77b2bc08199fe04150929d4df3d66727f96e8", "title": "Large-scale tests of a keyed, appearance-based 3-D object recognition system", "authors": ["Randal C. Nelson", "Andrea Selinger"], "date": "1998", "abstract": "We describe and analyze an appearance-based 3-D object recognition system that avoids some of the problems of previous appearance-based schemes. We describe various large-scale performance tests and report good performance for full-sphere/hemisphere recognition of up to 24 complex, curved objects, robustness against clutter and occlusion, and some intriguing generic recognition behavior. We also establish a protocol that permits performance in the presence of quantifiable amounts of clutter and… ", "references": ["50899b2355d6908a304bacb5e406f800f3dde558", "9191de6f4059469b54cb576322e331c51312dc6f", "3b900851aca0872dd0d87c22ed32335eb38f5e36", "9191de6f4059469b54cb576322e331c51312dc6f", "c5648d1f511a5180cc0bf7af80a42d3dea3a4680", "3b900851aca0872dd0d87c22ed32335eb38f5e36", "50899b2355d6908a304bacb5e406f800f3dde558", "27ce5a120a86632dd56f869ee65656b7d7312a3a", "8735690a9e8f8884bf27717877ddf7f9071472e5", "27ce5a120a86632dd56f869ee65656b7d7312a3a"]},{"id": "a4c5dbecd638b12deecc3363df0f24f05b53b45d", "title": "Behavioral and Neuroimaging Evidence for a Contribution of Color and Texture Information to Scene Classification in a Patient with Visual Form Agnosia", "authors": ["Jennifer K. E. Steeves", "G. Keith Humphrey", "Melvyn A. Goodale"], "date": "2004", "abstract": "A common notion is that object perception is a necessary precursor to scene perception. Behavioral evidence suggests, however, that scene perception can operate independently of object perception. Further, neuroimaging has revealed a specialized human cortical area for viewing scenes that is anatomically distinct from areas activated by viewing objects. Here we show that an individual with visual form agnosia, D.F., who has a profound deficit in object recognition but spared color and visual… ", "references": ["48f0d6211dc70a56202c1a0c95124c65a51f721f", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "2196b6fcf0a65872c70cf9852af29a6a839a55fa", "d06fc7272b8baf936a6468bd8b8a14771c3f853f", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "e22e79780108f6562a62d6c74036b325a771fedb", "b61f99b452b9d213f0d850b85287f2a31ceb5676", "2740b47a162ea190d3faf80bdb9a2f6916791a19", "48f0d6211dc70a56202c1a0c95124c65a51f721f", "f24db70b4283b56f43a94edd48d1d35e20935ef4"]},{"id": "697b45e902c9dd2f31d205f0720e7079f71db200", "title": "Statistics of natural image categories", "authors": ["Antonio Torralba", "Aude Oliva"], "date": "2003", "abstract": "In this paper we study the statistical properties of natural images belonging to different categories and their relevance for scene and object categorization tasks. We discuss how second-order statistics are correlated with image categories, scene scale and objects. We propose how scene categorization could be computed in a feedforward manner in order to provide top-down and contextual information very early in the visual processing chain. Results show how visual categorization based directly… ", "references": ["0e9a0da864277b24c635f4bea918fffe2bc3f865", "aeb37769d72999bcbfb0582b73607fd8d23f4545", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "aeb37769d72999bcbfb0582b73607fd8d23f4545", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "110c6d754d5767eb534151052f990a200da34f53", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "364c60c92a77e2eca1b7faeea763aea3a7d2070a", "9a21604e6db8020b9e76d89064f2b63fb875b67a"]},{"id": "5b1e1696564e5a3021ac3a501c9deeb6c0fbc637", "title": "Color indexing", "authors": ["Michael J. Swain", "Dana H. Ballard"], "date": "2004", "abstract": "Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the location of a known object. Color can be successfully used for both tasks.This article demonstrates that color histograms of multicolored objects provide a… ", "references": []},{"id": "9347996c6a11a8ea47d97df6e0c6c739ad68864a", "title": "Complex cells and Object Recognition", "authors": ["Shimon Edelman", "Nathan Intrator", "Tomaso A. Poggio"], "date": "1997", "abstract": "Nearest-neighbor correlation-based similarity computation in the space of outputs of complex-type receptive fields can support robust recognition of 3D objects.", "references": ["5a30589d274c2867425ead17780a0d22c69fc672", "8b37258659bcdbc380b1e6c4e22cce9ea06397a1", "7fa7606947a9b2f4ff7efb39d3c04c29fc4a7a17", "dd35d4ed68f2bc7a29b78adac63d4ce9139fdda7", "5a30589d274c2867425ead17780a0d22c69fc672", "aa1eea18e569a13bb262e3e6b266509b36bf6bb4", "aa1eea18e569a13bb262e3e6b266509b36bf6bb4", "7fa7606947a9b2f4ff7efb39d3c04c29fc4a7a17", "8b37258659bcdbc380b1e6c4e22cce9ea06397a1", "7fa7606947a9b2f4ff7efb39d3c04c29fc4a7a17"]},{"id": "ef768a5c9bd0aaeddafea1d56b08b0c8180760c0", "title": "Visual learning and recognition of 3-d objects from appearance", "authors": ["Hiroshi Murase", "Shree K. Nayar"], "date": "1995", "abstract": "The problem of automatically learning object models for recognition and pose estimation is addressed. In contrast to the traditional approach, the recognition problem is formulated as one of matching appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illumination conditions. While shape and reflectance are intrinsic properties and constant for a rigid object, pose and illumination vary… ", "references": ["90c6694f16f4104f1bc261f2485f5c4ddc05eeaa", "5ff14f5eddeaa425ab15504a1bc45f328dc9221e", "5e0ce7719dcb315145284fea50fd7c96df3599ab", "a9561071b5902c0d3df0dc6eb23f819f4abd8cac", "5e0ce7719dcb315145284fea50fd7c96df3599ab", "6c47e23b8ad2833df6346afab4b3151af3ae2399", "5eb5243a0de130c96abeb33032e406c195dc103a", "d781d5e651e12bf666cf993ae307db785113b9ae", "5e0ce7719dcb315145284fea50fd7c96df3599ab", "5eb5243a0de130c96abeb33032e406c195dc103a"]},{"id": "49fcd806450d947e56c82ef2b438ad9c484069dc", "title": "Local Grayvalue Invariants for Image Retrieval", "authors": ["Cordelia Schmid", "Roger Mohr"], "date": "1997", "abstract": "This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective… ", "references": ["c5648d1f511a5180cc0bf7af80a42d3dea3a4680", "7d8bb8044d9a41fa45c50e5e3fee21cda8ba9cec", "5a30589d274c2867425ead17780a0d22c69fc672", "8560e9c39c50ea928eef7c115d99fdae5acfdfa3", "6818668fb895d95861a2eb9673ddc3a41e27b3b3", "6818668fb895d95861a2eb9673ddc3a41e27b3b3", "6818668fb895d95861a2eb9673ddc3a41e27b3b3", "c5648d1f511a5180cc0bf7af80a42d3dea3a4680", "5a30589d274c2867425ead17780a0d22c69fc672", "8ccc8ab68720396b6646e649d6d1d12ebcce9a7a"]},{"id": "0326e7569e05dfaabdc12357fe10f104bb7d55ab", "title": "Bayesian modelling and analysis of spatio-temporal neuronal networks", "authors": ["Fabio Rigat", "M.C.M. de Gunst", "Jaap van Pelt"], "date": "2005", "abstract": "This paper illustrates a novel dynamic Bayesian network for stochastic integrate-andfire neuronal systems. We adopt a Bayesian hierarchical perspective to introduce at different model stages the parameters characterizing the neuronal spiking process over a discrete time grid, the unknown structure of functional connectivities and its dependence on the spatial arrangement of the neurons. We compute estimates for all the model parameters and predictions for future spiking states through the… ", "references": ["28cd57afa9089d1cc5274f97434cde8565e01524", "7b1107899e6379d620ff8efa7056cd10acf09d8f", "421fcb636a5867a3b1a1f50779a9fe63b3f0c2d6", "7b1107899e6379d620ff8efa7056cd10acf09d8f", "4fbb4c3605e4829285ab931ca40f81d2a7e20234", "bca010ee7633d5201e4d6cffa459a53c52cbe575", "52e32b822f705740abd2c0cd822536e8d1feae4d", "bca010ee7633d5201e4d6cffa459a53c52cbe575", "28cd57afa9089d1cc5274f97434cde8565e01524", "4fbb4c3605e4829285ab931ca40f81d2a7e20234"]},{"id": "88ecb565dad7d3b78de6e6fe86f17329d1bfc883", "title": "Coding and transmission of information by neural ensembles", "authors": ["Bruno B. Averbeck", "Daeyeol Lee"], "date": "2004", "abstract": "The brain processes information about sensory stimuli and motor intentions using a massive ensemble of neurons arrayed in parallel. Individual neurons receive convergent inputs from thousands of other neurons, leading to the possibility that patterns of spikes across the input neurons might be crucial components of the neural code. Recently, advances in multielectrode recording techniques have allowed several laboratories to investigate the nature of the interactions between neurons, and their… ", "references": []},{"id": "8735690a9e8f8884bf27717877ddf7f9071472e5", "title": "Three-Dimensional Object Recognition from Single Two-Dimensional Images", "authors": ["David G. Lowe"], "date": "1987", "abstract": "Abstract A computer vision system has been implemented that can recognize three-dimensional objects from unknown viewpoints in single gray-scale images. Unlike most other approaches, the recognition is accomplished without any attempt to reconstruct depth information bottom-up from the visual input. Instead, three other mechanisms are used that can bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization is used to… ", "references": ["9ee1e8a9cc139282b1786d8c45c94399f150dd5d", "b8831f3095686131a98731d6806abdc75c6baebd", "debdc594d1fcefbdad7a914eab463615123582c2", "9ee1e8a9cc139282b1786d8c45c94399f150dd5d", "9009c9685754346deb93f316144a9da1f70ffcd8", "9ee1e8a9cc139282b1786d8c45c94399f150dd5d", "9ee1e8a9cc139282b1786d8c45c94399f150dd5d", "880e17cd35abe8a602db11ecd3b58703a82084c0", "aa4b60b5847999c2f778e3e67ca1f2201e396abb", "b8831f3095686131a98731d6806abdc75c6baebd"]},{"id": "d11bf8b06cf96d90e1ee3dd6467c5c92ac53e9a1", "title": "Fitting Parameterized Three-Dimensional Models to Images", "authors": ["David G. Lowe"], "date": "1991", "abstract": "Model-based recognition and motion tracking depend upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. The author extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image… ", "references": ["339149a85a558018c321e21d8446127f4fe65b06", "339149a85a558018c321e21d8446127f4fe65b06", "e06d2241036f4699826da161ca8d336509c32680", "e06d2241036f4699826da161ca8d336509c32680", "339149a85a558018c321e21d8446127f4fe65b06", "339149a85a558018c321e21d8446127f4fe65b06", "85edabd5dbaefd061a10fffd7349623cc81a80ac", "85fe26c3178b0fedab2b510217edcf107a37aae3", "4f37468a95ccc62debb9e5a4cb0d73489ca61190", "85fe26c3178b0fedab2b510217edcf107a37aae3"]},{"id": "b5453a79263cbaf106e20aa9298abd985a4bc131", "title": "Decoding visual information from a population of retinal ganglion cells.", "authors": ["David K. Warland", "Pamela Reinagel", "Markus Meister"], "date": "1997", "abstract": "Decoding visual information from a population of retinal ganglion cells. J. Neurophysiol. 78: 2336-2350, 1997. This work investigates how a time-dependent visual stimulus is encoded by the collective activity of many retinal ganglion cells. Multiple ganglion cell spike trains were recorded simultaneously from the isolated retina of the tiger salamander using a multielectrode array. The stimulus consisted of photopic, spatially uniform, temporally broadband flicker. From the recorded spike… ", "references": ["270019fc26a2f37e29bb48055c0c42ee461dafc5", "87378cc06871a3812a0543be46455597eb108443", "270019fc26a2f37e29bb48055c0c42ee461dafc5", "168a233e6476001475488a284f294bff983c41b8", "72dafa24626298cf09f8c96c8aeb6508ff1ff27c", "72dafa24626298cf09f8c96c8aeb6508ff1ff27c", "ee7b14beac17161aa0131b540f939e23dcd53b70", "270019fc26a2f37e29bb48055c0c42ee461dafc5", "87378cc06871a3812a0543be46455597eb108443", "052b1d8ce63b07fec3de9dbb583772d860b7c769"]},{"id": "990b7bf28c99ccbb9c9384e5212e045258f78393", "title": "Efficient Multi-Step Processing of Spatial Joins", "authors": ["Thomas Brinkhoff", "Hans-Peter Kriegel", "Bernhard Seeger"], "date": "SIGMOD", "abstract": null, "references": []},{"id": "bca010ee7633d5201e4d6cffa459a53c52cbe575", "title": "Analyzing Functional Connectivity Using a Network Likelihood Model of Ensemble Neural Spiking Activity", "authors": ["Murat Okatan", "Matthew A. Wilson", "Emery N. Brown"], "date": "2005", "abstract": "Analyzing the dependencies between spike trains is an important step in understanding how neurons work in concert to represent biological signals. Usually this is done for pairs of neurons at a time using correlation-based techniques. Chornoboy, Schramm, and Karr (1988) proposed maximum likelihood methods for the simultaneous analysis of multiple pair-wise interactions among an ensemble of neurons. One of these methods is an iterative, continuous-time estimation algorithm for a network… ", "references": ["f46b5134322400e0b80e65cd2d308c982adfdc43", "006a34dcdfc63c5b7a2cf4d0b76cecedbb4b93d9", "1bf7fe11a7d82cbb85de50e9eda2a0a5f9ca05a2", "d4c1c13f05d350d34b3db3fea823a2f756db4f95", "cd7441a94d159c7cc17be2e426d4f626c9ad5773", "f46b5134322400e0b80e65cd2d308c982adfdc43", "1bf7fe11a7d82cbb85de50e9eda2a0a5f9ca05a2", "04e3b181e93428e6561e08fdc5e22e5625d67b52", "1ec97625c015931cecd69fffcb262ad7335f8c67", "bda0f6c8996720c843a86451858176b4b7a4217c"]},{"id": "b5936b0786f1c7729ab5f12ef1b1daea0f6731d6", "title": "The Sequoia 2000 storage benchmark", "authors": ["Michael Stonebraker", "James Frew", "Jeff Meredith"], "date": "1994", "abstract": null, "references": []},{"id": "5b77c31297c27057a459000a027ff10a0a419d20", "title": "Decoding neuronal spike trains: How important are correlations?", "authors": ["Sheila Nirenberg", "Peter E. Latham"], "date": "2003", "abstract": "It has been known for >30 years that neuronal spike trains exhibit correlations, that is, the occurrence of a spike at one time is not independent of the occurrence of spikes at other times, both within spike trains from single neurons and across spike trains from multiple neurons. The presence of these correlations has led to the proposal that they might form a key element of the neural code. Specifically, they might act as an extra channel for information, carrying messages about events in… ", "references": []},{"id": "c56be85db5a65d206e5c01a51beda92e964397b9", "title": "The r*-tree: An efficient and robust access method for points and rectangles", "authors": ["Norio Katayama", "Shin'ichi Satoh"], "date": "SIGMOD", "abstract": "An optical fibre connector comprises separately formed bodies of substantially elongate form, each having an axial bore in which an optical fibre can be fitted and, associated with the two bodies or with each pair of adjacent bodies, means for connecting said bodies together in substantially axial alignment. One of said bodies has an end, e.g. a flared socket, of such a configuration having regard to the configuration of an end, e.g. a conical plug, of the other of said bodies that as said… ", "references": []},{"id": "d08824dd86424f7a81f3e8f463f68a817b43aabe", "title": "Multiclass spectral clustering", "authors": ["Stella X. Yu", "Jianbo Shi"], "date": "2003", "abstract": "We propose a principled account on multiclass spectral clustering. Given a discrete clustering formulation, we first solve a relaxed continuous optimization problem by eigen-decomposition. We clarify the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms. We then solve an optimal discretization problem, which seeks a discrete solution closest to the continuous optima. The discretization is efficiently computed in an iterative fashion using singular value… ", "references": ["34d4eb4666a20f7e3fad689d7862959bd128130b", "4e892deef85c4cb62716776657d66dc417574bcc", "02a93481fb6fbc88e4ecdb3c59b3d875139f10ef", "c8d09c226e7d120066656300b818c683ce8c8b8d", "9a1ed876196ec9733acb1daa6d65e35ff0414291", "02a93481fb6fbc88e4ecdb3c59b3d875139f10ef", "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "34d4eb4666a20f7e3fad689d7862959bd128130b", "9a1ed876196ec9733acb1daa6d65e35ff0414291", "c8d09c226e7d120066656300b818c683ce8c8b8d"]},{"id": "c02dfd94b11933093c797c362e2f8f6a3b9b8012", "title": "On Spectral Clustering: Analysis and an algorithm", "authors": ["Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss"], "date": "NIPS", "abstract": "Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues.", "references": ["a36b175da3a91795906193efc257d3a31333f008", "34d4eb4666a20f7e3fad689d7862959bd128130b", "f36bfb04f8ff4d10aaa204b5338d2234f05c97d2", "3868293dee5b1dff83650bea83054b35c2e2ade8", "3868293dee5b1dff83650bea83054b35c2e2ade8", "a36b175da3a91795906193efc257d3a31333f008", "3868293dee5b1dff83650bea83054b35c2e2ade8", "e9d7f589a3d368a3701832e28d90ca09ec9e5577", "f36bfb04f8ff4d10aaa204b5338d2234f05c97d2", "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9"]},{"id": "072d09c92805897ddc83116a77a2ee71b7ca2b36", "title": "Constrained Non-monotone Submodular Maximization: Offline and Secretary Algorithms", "authors": ["Anupam Gupta", "Aaron Roth", "Kunal Talwar"], "date": "2010", "abstract": "Constrained submodular maximization problems have long been studied, most recently in the context of auctions and computational advertising, with near-optimal results known under a variety of constraints when the submodular function is monotone. In this paper, we give constant approximation algorithms for the non-monotone case that work for p-independence systems (which generalize constraints given by the intersection of p matroids that had been studied previously), where the running time is… ", "references": ["b799ebad02a60be7631203a943ab466e450a79b6", "3a43c0c79f3fdb2490eb87efd299a0c7e8480a91", "148d30f6b936c92c3479654890d5d53b1bdf91c7", "e4fa6b463cd08d144999cfe7c31873eb4496e807", "b0e4485d39eb2cb734bb48e29425381fad679ce5", "e7fbc3a15e8d4a56bd9fd9577e937def48a83879", "e7fbc3a15e8d4a56bd9fd9577e937def48a83879", "3a43c0c79f3fdb2490eb87efd299a0c7e8480a91", "b799ebad02a60be7631203a943ab466e450a79b6", "3a43c0c79f3fdb2490eb87efd299a0c7e8480a91"]},{"id": "cc555121cd1fc79e6d5f3bc240e520871721c2f4", "title": "A Unified Continuous Greedy Algorithm for Submodular Maximization", "authors": ["Moran Feldman", "Joseph Naor"], "date": "2011", "abstract": "The study of combinatorial problems with a submodular objective function has attracted much attention in recent years, and is partly motivated by the importance of such problems to economics, algorithmic game theory and combinatorial optimization. Classical works on these problems are mostly combinatorial in nature. Recently, however, many results based on continuous algorithmic tools have emerged. The main bottleneck of such continuous techniques is how to approximately solve a non-convex… ", "references": ["3a43c0c79f3fdb2490eb87efd299a0c7e8480a91", "3a43c0c79f3fdb2490eb87efd299a0c7e8480a91", "496a848e569568a80f6c00e3f621bab5403212cf", "22caeed46a0a12fbe9b4d3661d7a33692fc0a950", "672313a19e4fb38ac8eed59ecc1ea2a1976be050", "f8259211451ce416db41af9201b4735d2bdbb2f4", "b0e4485d39eb2cb734bb48e29425381fad679ce5", "672313a19e4fb38ac8eed59ecc1ea2a1976be050", "672313a19e4fb38ac8eed59ecc1ea2a1976be050", "496a848e569568a80f6c00e3f621bab5403212cf"]},{"id": "54f940451ef18462253ab92f94f14b2ff1d7c333", "title": "A dynamic approach for clustering data", "authors": ["Jose A. García", "Joaquín Fernández-Valdivia", "Rafael Molina"], "date": "1995", "abstract": "Abstract This paper introduces a new method for clustering data using a dynamic scheme. An appropriate partitioning is obtained based on both a dissimilarity measure between pairs of entities as well as a dynamic procedure of splitting. A dissimilarity function is defined by using the cost of the optimum path from a datum to each entity on a graph, with the cost of a path being defined as the greatest distance between two successive vertices on the path. The procedure of clustering is dynamic… ", "references": ["f64f69245b33fb21eeb18d9c73cf082c79b89cb6", "9f9a680dfb2a1a7f39d68f9bed4575d4a3075ac2", "43dd784b27863e9314aa331f4ee483310f20a674", "9f9a680dfb2a1a7f39d68f9bed4575d4a3075ac2", "4ae384fb530f986e4f80d03ada0abd3acf89dbc0", "9f9a680dfb2a1a7f39d68f9bed4575d4a3075ac2", "43dd784b27863e9314aa331f4ee483310f20a674", "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed", "142ea3e53037cb8d72849380dab8dcd1a6bb910c", "142ea3e53037cb8d72849380dab8dcd1a6bb910c"]},{"id": "3d890c6b65ed6369a5443c9ea8515bdaeb3413b5", "title": "Submodular maximization by simulated annealing", "authors": ["Shayan Oveis Gharan", "Jan Vondrák"], "date": "2011", "abstract": "We consider the problem of maximizing a nonnegative (possibly non-monotone) submodular set function with or without constraints. Feige et al. [9] showed a 2/5-approximation for the unconstrained problem and also proved that no approximation better than 1/2 is possible in the value oracle model. Constant-factor approximation has been also known for submodular maximization subject to a matroid independence constraint (a factor of 0.309 [33]) and for submodular maximization subject to a matroid… ", "references": ["c8fc71e82d35d9e7197fc09abb862eb4d88cc17a", "1f0caecf4f61ea8baf5f32eeb69d85659c13ec7d", "1f0caecf4f61ea8baf5f32eeb69d85659c13ec7d", "1f0caecf4f61ea8baf5f32eeb69d85659c13ec7d", "1f0caecf4f61ea8baf5f32eeb69d85659c13ec7d", "1f0caecf4f61ea8baf5f32eeb69d85659c13ec7d", "1f0caecf4f61ea8baf5f32eeb69d85659c13ec7d", "c8fc71e82d35d9e7197fc09abb862eb4d88cc17a", "1f0caecf4f61ea8baf5f32eeb69d85659c13ec7d", "c8fc71e82d35d9e7197fc09abb862eb4d88cc17a"]},{"id": "6b0bc19ccd607dea219eebee331e6361855816cc", "title": "Nonmonotone Submodular Maximization via a Structural Continuous Greedy Algorithm - (Extended Abstract)", "authors": ["Moran Feldman", "Joseph Naor"], "date": "ICALP", "abstract": "Consider a suboptimal solution S for a maximization problem.", "references": ["a420fef967a45ac6163cc76b2e1e47685520e1a3", "3d890c6b65ed6369a5443c9ea8515bdaeb3413b5", "f8259211451ce416db41af9201b4735d2bdbb2f4", "a420fef967a45ac6163cc76b2e1e47685520e1a3", "b0e4485d39eb2cb734bb48e29425381fad679ce5", "f8259211451ce416db41af9201b4735d2bdbb2f4", "672313a19e4fb38ac8eed59ecc1ea2a1976be050", "3d890c6b65ed6369a5443c9ea8515bdaeb3413b5", "b0e4485d39eb2cb734bb48e29425381fad679ce5", "f8259211451ce416db41af9201b4735d2bdbb2f4"]},{"id": "672313a19e4fb38ac8eed59ecc1ea2a1976be050", "title": "Submodular function maximization via the multilinear relaxation and contention resolution schemes", "authors": ["Chandra Chekuri", "Jan Vondrák", "Rico Zenklusen"], "date": "STOC '11", "abstract": "We consider the problem of maximizing a non-negative submodular set function f:2N -> RR+ over a ground set N subject to a variety of packing type constraints including (multiple) matroid constraints, knapsack constraints, and their intersections. In this paper we develop a general framework that allows us to derive a number of new results, in particular when f may be a non-monotone function. Our algorithms are based on (approximately) solving the multilinear extension F of f [5] over a polytope… ", "references": ["950bb724db08b009e6305157ac21b484cd2771fe", "b06f4a6384fc6871776e73d8d9692946f9fa2117"]},{"id": "fefd4dba2de33ca4792574734d27611c4da938c5", "title": "What Causes a Neuron to Spike?", "authors": ["Blaise Agüera y Arcas", "Adrienne L. Fairhall"], "date": "2003", "abstract": "The computation performed by a neuron can be formulated as a combination of dimensional reduction in stimulus space and the nonlinearity inherent in a spiking output. White noise stimulus and reverse correlation (the spike-triggered average and spike-triggered covariance) are often used in experimental neuroscience to ask neurons which dimensions in stimulus space they are sensitive to and to characterize the nonlinearity of the response. In this article, we apply reverse correlation to the… ", "references": ["87378cc06871a3812a0543be46455597eb108443", "92dc238ad532567eb9f1490811d5c5c6dee3b704", "7e4d2a32c8b88c9c6a2d5da4952612afd33231c3", "4afea1dbd2a4b011f212bb3741ecb25a22a82402", "5293dcf320c93d80a4832b1c225293e16d55de61", "5293dcf320c93d80a4832b1c225293e16d55de61", "15f41a2a42fa681e65a175fa78dbb83e0c6e7f41", "7e4d2a32c8b88c9c6a2d5da4952612afd33231c3", "4afea1dbd2a4b011f212bb3741ecb25a22a82402", "ee3bae9a988ad9144265d5aadc54248eedd70de8"]},{"id": "6c4ab2b7bf202e621dcb722d2e7cf421415cc3ed", "title": "Structured Determinantal Point Processes", "authors": ["Alex Kulesza", "Ben Taskar"], "date": "NIPS", "abstract": "We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random fields and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We… ", "references": ["787827850b614135f6b432603afc90b58a8cc665", "394c6c50445ab4ccd1c79fbc2db8f35994ef9f15", "787827850b614135f6b432603afc90b58a8cc665", "787827850b614135f6b432603afc90b58a8cc665", "0206ebe8a7d587548ce8f4507ab919c43a369014", "b91afd46236a9c9eda9056bf4e70fe9235867571", "d0a9b181fc252108de45720d4645ac245e1ba463", "b60ea458dc5d571c7bc028d9fd98fb545c0070d1", "ce72362402738b024c0ba7919c3a89c07b8d66d2", "d4dcfabc9877d614b0aa472ba8597d8d7053adb6"]},{"id": "92dc238ad532567eb9f1490811d5c5c6dee3b704", "title": "Computation in a Single Neuron: Hodgkin and Huxley Revisited", "authors": ["Blaise Agüera y Arcas", "Adrienne L. Fairhall", "William Bialek"], "date": "2002", "abstract": "A spiking neuron computes by transforming a complex dynamical input into a train of action potentials, or spikes. The computation performed by the neuron can be formulated as dimensional reduction, or feature detection, followed by a nonlinear decision function over the low-dimensional space. Generalizations of the reverse correlation technique with white noise input provide a numerical strategy for extracting the relevant low-dimensional features from experimental data, and information theory… ", "references": ["fefd4dba2de33ca4792574734d27611c4da938c5", "f5f401f073559e6a741bbf28a3d67af96bb02460", "406844893bba22c7488093ee91bcd63a18cda40e", "f5f401f073559e6a741bbf28a3d67af96bb02460", "88ddb7205b70a2f53b4972366d1d8634712c064e", "7a0fd770cbf1fc0ba82ed683642c670cb5a1e7c7", "f5f401f073559e6a741bbf28a3d67af96bb02460", "22b44517447c13eb0806ad3597eeac0922a31ae4", "ceea7a2e5b61ff315b3847a8d3dd8270dc22217e", "406844893bba22c7488093ee91bcd63a18cda40e"]},{"id": "3586adda8efdd12c327659e59e7e47462728d3b1", "title": "Multivariate receptive field mapping in marmoset auditory cortex", "authors": ["Artur Luczak", "Troy A Hackett", "Mark Laubach"], "date": "2004", "abstract": "We describe a novel method for estimation of multivariate neuronal receptive fields that is based on least-squares (LS) regression. The method is shown to account for the relationship between the spike train of a given neuron, the activity of other neurons that are recorded simultaneously, and a variety of time-varying features of acoustic stimuli, e.g. spectral content, amplitude, and sound source direction. Vocalization-evoked neuronal responses from the marmoset auditory cortex are used to… ", "references": ["345fbe20a6d95559feb7b6ed236f68b808fec6c7", "36c4fd9537aaeb18cb1e344736720452d53c4d93", "4f80e9ec2223b2269c5a023999e88873785c4d75", "1d9ee8b0d5edd0fe13ce53c2442b47a837571547", "1d9ee8b0d5edd0fe13ce53c2442b47a837571547", "1587c55cd33c952c56be4993209dc65622e2f799", "1587c55cd33c952c56be4993209dc65622e2f799", "1587c55cd33c952c56be4993209dc65622e2f799", "0749bc6daf1b5459f6be9fb472390a99fc088b74", "1587c55cd33c952c56be4993209dc65622e2f799"]},{"id": "2e54ce4304164ac0e4d44fca2b8dd78424ed6dc5", "title": "Inference in model-based cluster analysis", "authors": ["Halima Bensmail", "Gilles Celeux", "Christian P. Robert"], "date": "1997", "abstract": "A new approach to cluster analysis has been introduced based on parsimonious geometric modelling of the within-group covariance matrices in a mixture of multivariate normal distributions, using hierarchical agglomeration and iterative relocation. It works well and is widely used via the MCLUST software available in S-PLUS and StatLib. However, it has several limitations: there is no assessment of the uncertainty about the classification, the partition can be suboptimal, parameter estimates are… ", "references": ["7b96da96d6995ce1e178cc3d92e6ee971e35cddf", "c9b8d80234615ab20594387b0de4b075a9158fcb", "0ad9f3c2baf70d1fdfdfa4688b46a0c660888dbe", "de6c9990b0003fe4fd8c0f3492ec6cdd25528ce3", "7b96da96d6995ce1e178cc3d92e6ee971e35cddf", "c760e0246bdfd660f681eecc7dd06ae82e6761ba", "c43d352c1463f58f9271e0bc8724eaee2e1ad7f0", "c9b8d80234615ab20594387b0de4b075a9158fcb", "de6c9990b0003fe4fd8c0f3492ec6cdd25528ce3", "de6c9990b0003fe4fd8c0f3492ec6cdd25528ce3"]},{"id": "52e32b822f705740abd2c0cd822536e8d1feae4d", "title": "A Spike-Train Probability Model", "authors": ["Robert E. Kass", "Valérie Ventura"], "date": "2001", "abstract": "Poisson processes usually provide adequate descriptions of the irregularity in neuron spike times after pooling the data across large numbers of trials, as is done in constructing the peristimulus time histogram. When probabilities are needed to describe the behavior of neurons within individual trials, however, Poisson process models are often inadequate. In principle, an explicit formula gives the probability density of a single spike train in great generality, but without additional… ", "references": ["f46b5134322400e0b80e65cd2d308c982adfdc43", "5384ac32e10917bb56af35856804f3cdbd88a0d7", "b0b85df88899ec6e1b4451a7dc4453603ab89ca2", "b71660c257c625360a4919757857587c016374c8", "5384ac32e10917bb56af35856804f3cdbd88a0d7", "f46b5134322400e0b80e65cd2d308c982adfdc43", "5c2cc975714beb1e4a0e2333391e09eba9934cad", "b71660c257c625360a4919757857587c016374c8", "bda0f6c8996720c843a86451858176b4b7a4217c", "7362da1097866a805a1469cbc138983d3a7fb7b6"]},{"id": "f46b5134322400e0b80e65cd2d308c982adfdc43", "title": "Maximum likelihood analysis of spike trains of interacting nerve cells", "authors": ["David R. Brillinger"], "date": "1988", "abstract": "Suppose that a neuron is firing spontaneously or that it is firing under the influence of other neurons. Suppose that the data available are the firing times of the neurons present. An “integrate several inputs and fire” model is developed and studied empirically. For the model a neuron's firing occurs when an internal state variable crosses a random threshold. This conceptual model leads to maximum likelihood estimates of internal quantities, such as the postsynaptic potentials of the measured… ", "references": ["1d71c19cd5e287405e778d9652d7cd25006e7a36", "bc68bb5d62f536e15e4a18d41bb9ad9d0fba7d15", "bc68bb5d62f536e15e4a18d41bb9ad9d0fba7d15", "1d71c19cd5e287405e778d9652d7cd25006e7a36", "3e1d115a4a91a3d2d5219aa2caa79739c4f043ca", "7b1107899e6379d620ff8efa7056cd10acf09d8f", "ea1b834fd2f1b6c343e6eb4fee18ee9b99b92e7a", "1d71c19cd5e287405e778d9652d7cd25006e7a36", "1d71c19cd5e287405e778d9652d7cd25006e7a36", "ea1b834fd2f1b6c343e6eb4fee18ee9b99b92e7a"]},{"id": "64b3fed6224627f2f080f821391f3ff215850529", "title": "Dynamic Analysis of Neural Encoding by Point Process Adaptive Filtering", "authors": ["Uri T. Eden", "Loren M. Frank", "Emery N. Brown"], "date": "2004", "abstract": "Neural receptive fields are dynamic in that with experience, neurons change their spiking responses to relevant stimuli. To understand how neural systems adapt the irrepresentations of biological information, analyses of receptive field plasticity from experimental measurements are crucial. Adaptive signal processing, the well-established engineering discipline for characterizing the temporal evolution of system parameters, suggests a framework for studying the plasticity of receptive fields… ", "references": ["e718b343f48806f66abf9a627402614ffbc6ffc4", "94f236d6c46b4fac426a3095a917c47ab2640cde", "0dc7a6f16b952f441fbe6a2fb07f30a5a31ccfee", "006a34dcdfc63c5b7a2cf4d0b76cecedbb4b93d9", "e718b343f48806f66abf9a627402614ffbc6ffc4", "26b4ce3fb8808f771b025092ab258c482eac71e2", "1224875c2d5f98c39f887fa7acf5645848ec2526", "f168f04e6e003bf3bccd9a5e868cf29500a95e74", "94f236d6c46b4fac426a3095a917c47ab2640cde", "bda0f6c8996720c843a86451858176b4b7a4217c"]},{"id": "7a4e63b762e4046191cd4b818c3620228e3e700a", "title": "Autoclass — A Bayesian Approach to Classification", "authors": ["John Stutz", "Peter Cheeseman"], "date": "1996", "abstract": "We describe a Bayesian approach to the unsupervised discovery of classes in a set of cases, sometimes called finite mixture separation or clustering. The main difference between clustering and our approach is that we search for the “best” set of class descriptions rather than grouping the cases themselves. We describe our classes in terms of probability distribution or density functions, and the locally maximal posterior probability parameters. We rate our classifications with an approximate… ", "references": []},{"id": "5cb51d959c956c6cb326def3d8dbd261c4bb3a01", "title": "Choosing the Number of Component Clusters in the Mixture-Model Using a New Informational Complexity Criterion of the Inverse-Fisher Information Matrix", "authors": ["Hamparsum Bozdogan"], "date": "1993", "abstract": "This paper considers the problem of choosing the number of component clusters of individuals within the context of the standard mixture of multivariate normal distributions. Often the number of mixture clusters K is unknown, but varying and needs to be estimated. A two-stage iterative maximum-likelihood procedure is used as a clustering criterion to estimate the parameters of the mixture-model under several different covariance structures. An approximate component-wise inverse-Fisher… ", "references": []},{"id": "46b50c1cef5c1bb3e95dd0a9d08715d5e40edda8", "title": "Efficient Approximations for the Marginal Likelihood of Bayesian Networks with Hidden Variables", "authors": ["David Maxwell Chickering", "David Heckerman"], "date": "1997", "abstract": "We discuss Bayesian methods for model averaging and model selection among Bayesian-network models with hidden variables. In particular, we examine large-sample approximations for the marginal likelihood of naive-Bayes models in which the root node is hidden. Such models are useful for clustering or unsupervised learning. We consider a Laplace approximation and the less accurate but more computationally efficient approximation known as the Bayesian Information Criterion (BIC), which is… ", "references": ["8e68c54f39e87daf3a8bdc0ee005aece3c652d11", "41e77b3fc31f1c4898ad8670f47c61a2ee3f4700", "c760e0246bdfd660f681eecc7dd06ae82e6761ba", "b959164d1efca4b73986ba5d21e664aadbbc0457", "f09b90206271d48a1ed0a3638e9d69bddc2117ee", "b959164d1efca4b73986ba5d21e664aadbbc0457", "41e77b3fc31f1c4898ad8670f47c61a2ee3f4700", "b959164d1efca4b73986ba5d21e664aadbbc0457", "8e68c54f39e87daf3a8bdc0ee005aece3c652d11", "41e77b3fc31f1c4898ad8670f47c61a2ee3f4700"]},{"id": "305388319008f207aa121756da157f727f52f783", "title": "Principal Curve Clustering With Noise", "authors": ["Derek C. Stanford", "Adrian E. Raftery"], "date": "1997", "abstract": "Clustering on principal curves combines parametric modeling of noise with nonparametric modeling of feature shape This is useful for detecting curvilinear features in spatial point patterns with or without background noise Applications of this include the detection of curvilinear mine elds from reconnaissance images some of the points in which represent false detections and the detection of seismic faults from earthquake catalogs Our algorithm for principal curve clustering is in two steps the… ", "references": ["123261892642e377cf133a607b3199e60c7d4a41", "7139ed64878f965e9d5118c1d5d8e4323da3cdd0", "84653d94fcb33a198dbb8b7be32d28928a0b19f6", "19930aa9817929fc482a7e4533903007d255ecf6", "403f4ff746a87939889b84e126d5c92816dfa69d", "84653d94fcb33a198dbb8b7be32d28928a0b19f6", "123261892642e377cf133a607b3199e60c7d4a41", "123261892642e377cf133a607b3199e60c7d4a41", "123261892642e377cf133a607b3199e60c7d4a41", "d0a70c6f26b022f7373e4b6f2532bb9289308a37"]},{"id": "0ad9f3c2baf70d1fdfdfa4688b46a0c660888dbe", "title": "Model-based Gaussian and non-Gaussian clustering", "authors": ["Jeffrey D. Banfield", "Adrian E. Raftery"], "date": "1993", "abstract": "Abstract : The classification maximum likelihood approach is sufficiently general to encompass many current clustering algorithms, including those based on the sum of squares criterion and on the criterion of Friedman and Rubin (1967). However, as currently implemented, it does not allow the specification of which features (orientation, size and shape) are to be common to all clusters and which may differ between clusters. Also, it is restricted to Gaussian distributions and it does not allow… ", "references": ["43208db63f0ef8fe8724d104ed70a796b8ce0013", "f4a84dc7a42b528460c80b448dd5c7459192bf48", "561a265708761989b456101f0d9188221d14252b", "97d06716ddd66bd0b2b939e09a28f0c6fb9e75eb", "00b7d7d6ef41b2ef0f2d1fd07e190c9132b562c6", "97d06716ddd66bd0b2b939e09a28f0c6fb9e75eb", "561a265708761989b456101f0d9188221d14252b", "8ac29367b46930876b02817455ad705d6ff03f2c", "00b7d7d6ef41b2ef0f2d1fd07e190c9132b562c6", "8ac29367b46930876b02817455ad705d6ff03f2c"]},{"id": "96a5867d0b9b997108633ff3da314edf69b0122c", "title": "Finding MAPs for Belief Networks is NP-Hard", "authors": ["Solomon Eyal Shimony"], "date": "1994", "abstract": "Given a probabilistic world model, an important problem is to find the maximum a-posteriori probability (MAP) instantiation of all the random variables given the evidence. Numerous researchers using such models employ some graph representation for the distributions, such as a Bayesian belief network. This representation simplifies the complexity of specifying the distributions from exponential in n, the number of variables in the model, to linear in n, in many interesting cases. We show… ", "references": ["0a3767909649cf31d32e087693d93171af28ebe0", "b1e99c01e2f1f5783d476b78c8d0503623f3891c", "f8d16924d37ac2ad2fe23e641673f9f2b5434733", "7ffa848789d790527448fb3acdfce57e7f4b4641", "62988c173ce2acb7208389d8fe137b56041e089a", "637b0e1b0fc45c7d17d426ada7dfb4c5a551ddc8", "b1e99c01e2f1f5783d476b78c8d0503623f3891c", "7ffa848789d790527448fb3acdfce57e7f4b4641", "b1e99c01e2f1f5783d476b78c8d0503623f3891c", "9e0daca0acc6ee3baf7573fe2e2b3cc94276e7f4"]},{"id": "325ea1f2022ee3886a5810df76dcfbe4010ad439", "title": "Structured Learning with Approximate Inference", "authors": ["Alex Kulesza", "Fernando C Pereira"], "date": "NIPS", "abstract": "In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be sufficient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expressivity of an… ", "references": ["f3d831a9447ae7a8f9aa5c5beef8a28dbfacf352", "b28024225b22741035cf87203a3639c917959404", "f3d831a9447ae7a8f9aa5c5beef8a28dbfacf352", "64141d2e7a3cab0be864bbe2d5c7e0212fb17d27", "5aa70188f70d349580aed96c10a68f57dace2d33", "702c2fde33ccb4328be06405c11e208a4b3ee347", "5aa70188f70d349580aed96c10a68f57dace2d33", "ed5324bb3a19f0dcc2e90e482c06373b934fc28c", "a5c48c673b0d3152010e3374cac189314a13df10", "5aa70188f70d349580aed96c10a68f57dace2d33"]},{"id": "702c2fde33ccb4328be06405c11e208a4b3ee347", "title": "Learning associative Markov networks", "authors": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller"], "date": "ICML '04", "abstract": "Markov networks are extensively used to model complex sequential, spatial, and relational interactions in fields as diverse as image processing, natural language analysis, and bioinformatics. However, inference and learning in general Markov networks is intractable. In this paper, we focus on learning a large subclass of such models (called associative Markov networks) that are tractable or closely approximable. This subclass contains networks of discrete variables with K labels each and clique… ", "references": ["47865b56fee61d9c9ff477f7c79f090cc6663d3a", "b47e59a9e6f182a97fc6d93708bbbc624289de93", "47865b56fee61d9c9ff477f7c79f090cc6663d3a", "b47e59a9e6f182a97fc6d93708bbbc624289de93", "b47e59a9e6f182a97fc6d93708bbbc624289de93", "47865b56fee61d9c9ff477f7c79f090cc6663d3a", "47865b56fee61d9c9ff477f7c79f090cc6663d3a", "b47e59a9e6f182a97fc6d93708bbbc624289de93", "47865b56fee61d9c9ff477f7c79f090cc6663d3a", "47865b56fee61d9c9ff477f7c79f090cc6663d3a"]},{"id": "6aaab21d79b05130d8d0e7c4b05647d89f16b1d1", "title": "Algorithms for Model-Based Gaussian Hierarchical Clustering", "authors": ["Chris Fraley"], "date": "1998", "abstract": "Agglomerative hierarchical clustering methods based on Gaussian probability models have recently shown promise in a variety of applications. In this approach, a maximum-likelihood pair of clusters is chosen for merging at each stage. Unlike classical methods, model-based methods reduce to a recurrence relation only in the simplest case, which corresponds to the classical sum of squares method. We show how the structure of the Gaussian model can be exploited to yield efficient algorithms for… ", "references": ["9b7cc6b3f0f4318b2d71f7fd6f965702ca2c48fe", "c43d352c1463f58f9271e0bc8724eaee2e1ad7f0", "a6c3a45d234144025202795e0fe6bbade2457f58", "c43d352c1463f58f9271e0bc8724eaee2e1ad7f0", "9b7cc6b3f0f4318b2d71f7fd6f965702ca2c48fe", "1e9ba16c73865c79d7b211688ee6f466921cb88c", "b9efffc63f81bf3f6cb6357ddc15e9cd9da75d16", "0430b241bdd0b67d37e1143370f8d24fc46d83e9", "0ad9f3c2baf70d1fdfdfa4688b46a0c660888dbe", "b9efffc63f81bf3f6cb6357ddc15e9cd9da75d16"]},{"id": "c87f5836627a4ea7d928ff1aecf1b7cdebaf1302", "title": "Learning Determinantal Point Processes", "authors": ["Alex Kulesza", "Ben Taskar"], "date": "UAI", "abstract": "The increasing availability of both interesting data and processing capacity has led to widespread interest in machine learning techniques that deal with complex, structured output spaces in fields like image processing, computational biology, and natural language processing.", "references": ["83965fb947855d27d9936ed60941c3386cc5728a", "72029bdfc11f0b09010eb7687230b4fe16fc65fc", "9cc8c1e575c59f2c89212895054f7e2060837bee", "ce72362402738b024c0ba7919c3a89c07b8d66d2", "ce72362402738b024c0ba7919c3a89c07b8d66d2", "f9a00e41500c5e23fcc1ab3502a2c20acf4b8f6a", "f9a00e41500c5e23fcc1ab3502a2c20acf4b8f6a", "b91afd46236a9c9eda9056bf4e70fe9235867571", "9cc8c1e575c59f2c89212895054f7e2060837bee", "ed8a65d9f23b70f8ff37da5e119819f53b448751"]},{"id": "20d8e96db2de8a815e961d23c51205251b611733", "title": "The Power Ratio and the Interval Map: Spiking Models and Extracellular Recordings", "authors": ["Daniel S. Reich", "Jonathan D. Victor", "Bruce W. Knight"], "date": "1998", "abstract": "We describe a new, computationally simple method for analyzing the dynamics of neuronal spike trains driven by external stimuli. The goal of our method is to test the predictions of simple spike-generating models against extracellularly recorded neuronal responses. Through a new statistic called the power ratio, we distinguish between two broad classes of responses: (1) responses that can be completely characterized by a variable firing rate, (for example, modulated Poisson and gamma spike… ", "references": ["ee7b14beac17161aa0131b540f939e23dcd53b70", "ee7b14beac17161aa0131b540f939e23dcd53b70", "5384ac32e10917bb56af35856804f3cdbd88a0d7", "31a97e91c047053fe0cda6fc92a476144f8a6885", "ee7b14beac17161aa0131b540f939e23dcd53b70", "8c70c5d63b54cab1a9739f5018a12207a5ebc10f", "5384ac32e10917bb56af35856804f3cdbd88a0d7", "218271d4952bfcdd9272b18202134ec07ab54db3", "44da257334ed29c558767d3604ec3762a3022e0f", "a146d6514cb7db6eb511047abbc6983b04a077bb"]},{"id": "508c7605cc6b4110dd97a288c38f99ce48a52db0", "title": "Identification of non-linear time series via kernels", "authors": ["Tony J. Dodd", "Christopher J. Harris"], "date": "2002", "abstract": "Given a time series arising as the observations of some dynamical system, it is possible to reconstruct the qualitative behaviour of the dynamical system. Given such a reconstruction, this provides a general approach for making accurate short-term predictions. The time series reconstruction problem can be viewed as a function approximation problem which can be addressed by reproducing kernel Hilbert spaces (RKHS). Such an approach is described and related to Bayesian estimation with Gaussian… ", "references": ["cb698585328f87e4f2c6133befbe84af413a7a60", "71a8443c491c5c6c8ca6ab5c330a82287f81b15f", "55b8a70d0697f9ccd4a9f9f33c0b609fa79e11db", "b383e94763d8e0379554e197969787b8d25baff0", "cb698585328f87e4f2c6133befbe84af413a7a60", "b383e94763d8e0379554e197969787b8d25baff0", "4839029fff6b788aa2dd1f19f5ffa389d535050b", "b383e94763d8e0379554e197969787b8d25baff0", "4839029fff6b788aa2dd1f19f5ffa389d535050b", "497f3b885f749745543b3a9cb87a10c17a91c36b"]},{"id": "7e7fc800af3f2f9d013925935bb05f5ab84b331b", "title": "Neural Coding: Higher-Order Temporal Patterns in the Neurostatistics of Cell Assemblies", "authors": ["Laura Martignon", "Gustavo Deco", "Eilon Vaadia"], "date": "2000", "abstract": "Recent advances in the technology of multiunit recordings make it possible to test Hebb's hypothesis that neurons do not function in isolation but are organized in assemblies. This has created the need for statistical approaches to detecting the presence of spatiotemporal patterns of more than two neurons in neuron spike train data. We mention three possible measures for the presence of higher-order patterns of neural activationcoefficients of log-linear models, connected cumulants, and… ", "references": ["eacf7e793337fd3a1e42c1489677d388abcf91b9", "06731c3d8ba7fef34da2a7565d0e05a27eae0719", "bdc9edf5cd1e43104e5ec7b5175c35e1928f47af", "cd0331e67a79fca45389166e64bb70c5c44d06b0", "bdc9edf5cd1e43104e5ec7b5175c35e1928f47af", "eacf7e793337fd3a1e42c1489677d388abcf91b9", "cd0331e67a79fca45389166e64bb70c5c44d06b0", "dc85c68891c561e28034bfbb7ddaf4d577c86ed9", "ff0adecc1a8bc5c16c0584390f42be9e1cb5ee9c", "eacf7e793337fd3a1e42c1489677d388abcf91b9"]},{"id": "96fa3f8ef4d611b0ae81df42bfc82c266e12947c", "title": "The Spike Response Model: A Framework to Predict Neuronal Spike Trains", "authors": ["Renaud Jolivet", "Timothy J. Lewis", "Wulfram Gerstner"], "date": "ICANN", "abstract": "We propose a simple method to map a generic threshold model, namely the Spike Response Model, to artificial data of neuronal activity using a minimal amount of a priori information.", "references": ["a53db73382eb4c92503ecb698a208d94502b0c25", "c4201c4ea53372e9bf1b487585b8ea9261b2ee86", "5ef505dd538e64a5a13c66190cc979c9723319e1", "c347e8643150972f1e60cda5474c0681d3d48ab9", "c4201c4ea53372e9bf1b487585b8ea9261b2ee86", "09995f5f4c5eeb45af3f6d36f3f0256ebff82666", "fd7f2c8b23185a91674c2b2aa5446b88168c7f0d", "c4201c4ea53372e9bf1b487585b8ea9261b2ee86", "5ef505dd538e64a5a13c66190cc979c9723319e1", "c347e8643150972f1e60cda5474c0681d3d48ab9"]},{"id": "04e3b181e93428e6561e08fdc5e22e5625d67b52", "title": "Dynamic Analyses of Information Encoding in Neural Ensembles", "authors": ["Riccardo Barbieri", "Loren M. Frank", "Emery N. Brown"], "date": "2004", "abstract": "Neural spike train decoding algorithms and techniques to compute Shan-non mutual information are important methods for analyzing how neural systems represent biological signals. Decoding algorithms are also one of several strategies being used to design controls for brain-machine inter-faces. Developing optimal strategies to desig n decoding algorithms and compute mutual information are therefore important problems in com-putational neuroscience. We present a general recursive filter decoding… ", "references": ["0dc7a6f16b952f441fbe6a2fb07f30a5a31ccfee", "b0b85df88899ec6e1b4451a7dc4453603ab89ca2", "f168f04e6e003bf3bccd9a5e868cf29500a95e74", "bf5a481c60a76041b132cda8f339366b5d306f83", "bda0f6c8996720c843a86451858176b4b7a4217c", "c84c360e64b8bcaa4f4545d220fb46e1f4f80c45", "b0b85df88899ec6e1b4451a7dc4453603ab89ca2", "bda0f6c8996720c843a86451858176b4b7a4217c", "b5453a79263cbaf106e20aa9298abd985a4bc131", "b5453a79263cbaf106e20aa9298abd985a4bc131"]},{"id": "eb59dffc2f366518ea13a1399d76ce22c422ddf6", "title": "The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions", "authors": ["David Pearce", "Hans-Günter Hirsch"], "date": "INTERSPEECH", "abstract": "This paper describes a database designed to evaluate the performance of speech recognition algorithms in noisy conditions. The database may either be used for the evaluation of front-end feature extraction algorithms using a defined HMM recognition back-end or complete recognition systems. The source speech for this database is the TIdigits, consisting of connected digits task spoken by American English talkers (downsampled to 8kHz) . A selection of 8 different real-world noises have been added… ", "references": ["4db594b948e95b3239f733b1625b54b0fb476380", "a33cb603f95ac7c65a445c9022d1d60f7821a5a1", "f984ab6e0cde98397c9ccc74fb8b98bb3ff61818", "f984ab6e0cde98397c9ccc74fb8b98bb3ff61818", "4db594b948e95b3239f733b1625b54b0fb476380", "a33cb603f95ac7c65a445c9022d1d60f7821a5a1", "f984ab6e0cde98397c9ccc74fb8b98bb3ff61818", "4db594b948e95b3239f733b1625b54b0fb476380", "a33cb603f95ac7c65a445c9022d1d60f7821a5a1", "4db594b948e95b3239f733b1625b54b0fb476380"]},{"id": "3d82e058a5c40954b8f5db170a298a889a254c37", "title": "Connectionist Speech Recognition: A Hybrid Approach", "authors": ["Hervé Bourlard", "Nelson Morgan"], "date": "1993", "abstract": "From the Publisher: \nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and… ", "references": []},{"id": "e33cbb25a8c7390aec6a398e36381f4f7770c283", "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "authors": ["Geoffrey E. Hinton", "Li Deng", "Brian Kingsbury"], "date": "2012", "abstract": "Most current speech recognition systems use hidden Markov models ( HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternati ve way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients a s input and produces posterior probabilities over HMM states as output. Deep neural… ", "references": ["ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "be53d4def5e0601f2416e9345babc7ef1b30a664", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "5e9082caea65c76bfd23b8763872804473ee7872", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "2a6ae3d667a5c2601c1852a0753c8b1c749fec1e", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963"]},{"id": "f6bcea7b25e7dc7e156ebe611414e99e5e54ea67", "title": "Do septal neurons pace the hippocampal theta rhythm?", "authors": ["Mark Stewart", "Steven E Fox"], "date": "1990", "abstract": "The hippocampal theta rhythm (rhythmical slow activity, RSA) is one of the most thoroughly studied EEG phenomena. Much of this experimental interest has been stimulated by suggestions that the mnemonic functions of the hippocampus may depend upon theta-related neuronal activity. Inputs from the medial septal nuclei to the hippocampus were shown to be essential for the theta rhythm in the 1950s, but the role of these basal forebrain projections has not been clearly defined. Four models of the… ", "references": []},{"id": "8f613e5481d8d567573b0ffa7b8e1c5b07d33a04", "title": "Continuous speech recognition", "authors": ["Nelson Morgan", "Hervé Bourlard"], "date": "1995", "abstract": "The authors focus on a tutorial description of the hybrid HMM/ANN method. The approach has been applied to large vocabulary continuous speech recognition, and variants are in use by many researchers, The method provides a mechanism for incorporating a range of sources of evidence without strong assumptions about their joint statistics, and may have applicability to much more complex systems that can incorporate deep acoustic and linguistic context. The method is inherently discriminant and… ", "references": []},{"id": "614a41f5f7a0a7d70ba802d53b8c8c16cb52aa83", "title": "Hippocampus-independent phase precession in entorhinal grid cells", "authors": ["Torkel Hafting", "Marianne Fyhn", "Edvard I. Moser"], "date": "2008", "abstract": "Theta-phase precession in hippocampal place cells is one of the best-studied experimental models of temporal coding in the brain. Theta-phase precession is a change in spike timing in which the place cell fires at progressively earlier phases of the extracellular theta rhythm as the animal crosses the spatially restricted firing field of the neuron. Within individual theta cycles, this phase advance results in a compressed replication of the firing sequence of consecutively activated place… ", "references": ["de76cd6c61199445e2688402cabbb3119dadc877", "656a20925a9e5cde326115d476e4dc9862e04b4f", "656a20925a9e5cde326115d476e4dc9862e04b4f", "05c062736eb65357de6c40aecbdf9d3adfe87a47", "de76cd6c61199445e2688402cabbb3119dadc877", "95dcac65492641d27c82f8d92d818c14536a15fd", "656a20925a9e5cde326115d476e4dc9862e04b4f", "be6643c8afe8ba4722fa058e204475def4668c0b", "656a20925a9e5cde326115d476e4dc9862e04b4f", "71f4affc2d0f0f3fa8c5d003933dad8c1f9cc028"]},{"id": "6ac3b4840115b317dc3056b260ab4072132ce0bd", "title": "Neuronal diversity and temporal dynamics: the unity of hippocampal circuit operations.", "authors": ["Thomas Klausberger", "Peter Somogyi"], "date": "2008", "abstract": "In the cerebral cortex, diverse types of neurons form intricate circuits and cooperate in time for the processing and storage of information. Recent advances reveal a spatiotemporal division of labor in cortical circuits, as exemplified in the CA1 hippocampal area. In particular, distinct GABAergic (gamma-aminobutyric acid-releasing) cell types subdivide the surface of pyramidal cells and act in discrete time windows, either on the same or on different subcellular compartments. They also… ", "references": []},{"id": "efdfb01dd36feead24b9ed29a01aace59a6e51a0", "title": "Intrinsic and synaptic mechanisms determining the timing of neuron population activity during hippocampal theta oscillation.", "authors": ["Gergo Orbán", "Tamás Kiss", "Péter Érdi"], "date": "2006", "abstract": "Hippocampal theta (3-8 Hz) is a major electrophysiological activity in rodents, which can be found in primates and humans as well. During theta activity, pyramidal cells and different classes of interneurons were shown to discharge at different phases of the extracellular theta. A recent in vitro study has shown that theta-frequency oscillation can be elicited in a hippocampal CA1 slice by the activation of metabotropic glutamate receptors with similar pharmacological and physiological profile… ", "references": ["9473a2bf5eb8d4abd99b77d56096a6eb23b7afa4", "498ec4c969485dd77447270f185657cf229ca64c", "7f52d5c06f67c5d420b4ba38519fc7ede0cae6ab", "dcc91931479912d25e672cf44c44930e37857b37", "7696109227b08bbfea7131e633cbc21192a7b394", "498ec4c969485dd77447270f185657cf229ca64c", "498ec4c969485dd77447270f185657cf229ca64c", "7f52d5c06f67c5d420b4ba38519fc7ede0cae6ab", "dcc91931479912d25e672cf44c44930e37857b37", "498ec4c969485dd77447270f185657cf229ca64c"]},{"id": "6cd38fa04c85bd68686d9137ac885c01b59a96d2", "title": "Entrainment of Neocortical Neurons and Gamma Oscillations by the Hippocampal Theta Rhythm", "authors": ["Anton M. Sirota", "Sean Montgomery", "György Buzsáki"], "date": "2008", "abstract": "Although it has been tacitly assumed that the hippocampus exerts an influence on neocortical networks, the mechanisms of this process are not well understood. We examined whether and how hippocampal theta oscillations affect neocortical assembly patterns by recording populations of single cells and transient gamma oscillations in multiple cortical regions, including the somatosensory area and prefrontal cortex in behaving rats and mice. Laminar analysis of neocortical gamma bursts revealed… ", "references": ["8ae20541c5f95fe18c0783302eb2e68a34c2cde5", "535f72add821212ac38f7f2205365aa2c91b0668", "f4dc0682bb040de21f46e35c57f8b2cfbdeb4841", "621fce5ca4d2480276f27567a9377c56766de048", "eb69d66e10c548a54c5a21bcaafacdb689744803", "535f72add821212ac38f7f2205365aa2c91b0668", "8ae20541c5f95fe18c0783302eb2e68a34c2cde5", "c6fb92fc301264c055ed074cbe125cfd60ca55fe", "252c5cda2e2ecd795775cc96af7f74e278457cc0", "f4dc0682bb040de21f46e35c57f8b2cfbdeb4841"]},{"id": "0a9d3e04e21c17335f1f3646a621c481657f0a00", "title": "Population dynamics and theta rhythm phase precession of hippocampal place cell firing: a spiking neuron model.", "authors": ["Misha Tsodyks", "William E. Skaggs", "Bruce L. McNaughton"], "date": "1996", "abstract": "O'Keefe and Recce ([1993] Hippocampus 68:317-330) have observed that the spatially selective firing of pyramidal cells in the CA1 field of the rat hippocampus tends to advance to earlier phases of the electroencephalogram theta rhythm as a rat passes through the place field of a cell. We present here a neural network model based on integrate- and-fire neurons that accounts for this effect. In this model, place selectivity in the hippocampus is a consequence of synaptic interactions between… ", "references": ["95dcac65492641d27c82f8d92d818c14536a15fd", "f3fff4b9d119969d5b15129efad6481b009ef526", "b0cbd48999bfd6efb80c69015a882dcca63af872", "f3fff4b9d119969d5b15129efad6481b009ef526", "b0cbd48999bfd6efb80c69015a882dcca63af872", "7983cc562a8ea2df4198d8b9525ff120eba34fd4", "7dfd1cebf78f8ee62e190d52515bf76465e6a2ff", "9d04a3b4a71cf4d8b46b6c68264cbf6d5399050c", "95dcac65492641d27c82f8d92d818c14536a15fd", "8b1260972fc3cccd7e2aaac2c457c0f5bc67846b"]},{"id": "df40850beb0b7f573669b8e904c10afe80e98fba", "title": "Integration and Segregation of Activity in Entorhinal-Hippocampal Subregions by Neocortical Slow Oscillations", "authors": ["Yoshikazu Isomura", "Anton M. Sirota", "György Buzsáki"], "date": "2006", "abstract": "Brain systems communicate by means of neuronal oscillations at multiple temporal and spatial scales. In anesthetized rats, we find that neocortical \"slow\" oscillation engages neurons in prefrontal, somatosensory, entorhinal, and subicular cortices into synchronous transitions between UP and DOWN states, with a corresponding bimodal distribution of their membrane potential. The membrane potential of hippocampal granule cells and CA3 and CA1 pyramidal cells lacked bimodality, yet it was… ", "references": ["eb69d66e10c548a54c5a21bcaafacdb689744803", "6dae306c61b49a9b7416046c355d9e074c0fd7dd", "8ae20541c5f95fe18c0783302eb2e68a34c2cde5", "8ae20541c5f95fe18c0783302eb2e68a34c2cde5", "bd0419d481c85a7e9e5d20c344824d4f0e951c76", "1423ed1a45c287b91a3b0e236cd0ccbb3963cad2", "8ae20541c5f95fe18c0783302eb2e68a34c2cde5", "bd0419d481c85a7e9e5d20c344824d4f0e951c76", "6dae306c61b49a9b7416046c355d9e074c0fd7dd", "8ae20541c5f95fe18c0783302eb2e68a34c2cde5"]},{"id": "bac8f8283ac2be30e202b78295ce7f4d00957ffb", "title": "Dynamics of the hippocampal ensemble code for space.", "authors": ["Michael Geoffrey Andrew Wilson", "Bruce L. McNaughton"], "date": "1993", "abstract": "Ensemble recordings of 73 to 148 rat hippocampal neurons were used to predict accurately the animals' movement through their environment, which confirms that the hippocampus transmits an ensemble code for location. In a novel space, the ensemble code was initially less robust but improved rapidly with exploration. During this period, the activity of many inhibitory cells was suppressed, which suggests that new spatial information creates conditions in the hippocampal circuitry that are… ", "references": []},{"id": "aa88e6a057f1b7019ee4d61f16d98b274337e2e6", "title": "Behavior-Dependent Coordination of Multiple Theta Dipoles in the Hippocampus", "authors": ["Sean M. Montgomery", "Martha Inés Gómez Betancur", "György Buzsáki"], "date": "2009", "abstract": "Theta (4–10 Hz) oscillations in the hippocampus are thought to be important for plasticity, temporal coding, learning, and memory. The hippocampal system has been postulated to have two (or more) rhythmic sources of theta oscillations, but little is known about the behavior-dependent interplay of theta oscillations in different subregions and layers of the hippocampus. We tested rats in a hippocampus-dependent delayed spatial alternation task on a modified T-maze while simultaneously recording… ", "references": ["df40850beb0b7f573669b8e904c10afe80e98fba", "95dcac65492641d27c82f8d92d818c14536a15fd", "c6fb92fc301264c055ed074cbe125cfd60ca55fe", "7e99e2421440c0569092ed3dbcfe4bfa83bc5754", "71f4affc2d0f0f3fa8c5d003933dad8c1f9cc028", "7983cc562a8ea2df4198d8b9525ff120eba34fd4", "621fce5ca4d2480276f27567a9377c56766de048", "621fce5ca4d2480276f27567a9377c56766de048", "498ec4c969485dd77447270f185657cf229ca64c", "24fe02b71f0fc7377b4245afc29e76c0d4b57db7"]},{"id": "2f21bea4510afce38895905c8d4ba5d5e1c0e2c4", "title": "Place cell discharge is extremely variable during individual passes of the rat through the firing field.", "authors": ["André Antonio Fenton", "Robert U. Muller"], "date": "1998", "abstract": "The idea that the rat hippocampus stores a map of space is based on the existence of \"place cells\" that show \"location-specific\" firing. The discharge of place cells is confined with remarkable precision to a cell-specific part of the environment called the cell's \"firing field.\" We demonstrate here that firing is not nearly as reliable in the time domain as in the positional domain. Discharge during passes through the firing field was compared with a model with Poisson variance of the location… ", "references": ["ba1c520d8f4ac9aeaf33bd9e032a229ac81ca14b", "e818e9242afe3f61849c86e124b80c5f58409376", "d5a2e600642660923f5a0adc6a0181abb2ee30e8", "ba1c520d8f4ac9aeaf33bd9e032a229ac81ca14b", "e818e9242afe3f61849c86e124b80c5f58409376", "d5a2e600642660923f5a0adc6a0181abb2ee30e8", "ba1c520d8f4ac9aeaf33bd9e032a229ac81ca14b", "d5a2e600642660923f5a0adc6a0181abb2ee30e8", "ba1c520d8f4ac9aeaf33bd9e032a229ac81ca14b", "ba1c520d8f4ac9aeaf33bd9e032a229ac81ca14b"]},{"id": "01eaf11a1b26eb44ef999e3a7057665d2b16aaa0", "title": "Mechanisms of Gamma Oscillations in the Hippocampus of the Behaving Rat", "authors": ["Jozsef Csicsvari", "Brian D. Jamieson", "György Buzsáki"], "date": "2003", "abstract": "Gamma frequency oscillations (30-100 Hz) have been suggested to underlie various cognitive and motor functions. Here, we examine the generation of gamma oscillation currents in the hippocampus, using two-dimensional, 96-site silicon probes. Two gamma generators were identified, one in the dentate gyrus and another in the CA3-CA1 regions. The coupling strength between the two oscillators varied during both theta and nontheta states. Both pyramidal cells and interneurons were phase-locked to… ", "references": ["5f4686219c666dbb88b4d6ddacdf4dad37690a84", "e1419b026d92722564d877845ab24ca1429720c8", "48eb374f52d1d0fd1c4c8d0246dbf98eac3f8688", "1430a42d99a751facf96eb1989e1cca38fb39fea", "38f09f80c0206150fbb8a822cf7ed407488ccb78", "38f09f80c0206150fbb8a822cf7ed407488ccb78", "bdd04f65563f3cb930e0724de175fb08f780a0f4", "59a850815fce6d575694a6ecbe31148c50600627", "116ad2811e02eadb1460b715f0144dfd43efafa2", "59a850815fce6d575694a6ecbe31148c50600627"]},{"id": "95dcac65492641d27c82f8d92d818c14536a15fd", "title": "Phase relationship between hippocampal place units and the EEG theta rhythm.", "authors": ["John O'Keefe", "Michael L. Recce"], "date": "1993", "abstract": "Many complex spike cells in the hippocampus of the freely moving rat have as their primary correlate the animal's location in an environment (place cells). In contrast, the hippocampal electroencephalograph theta pattern of rhythmical waves (7-12 Hz) is better correlated with a class of movements that change the rat's location in an environment. During movement through the place field, the complex spike cells often fire in a bursting pattern with an interburst frequency in the same range as the… ", "references": ["6aa68970ec381a78a75fc30555884d4e8f1cb672", "f3fff4b9d119969d5b15129efad6481b009ef526", "4a17c8b1bffa3d08561bb715f872603f4f58d475", "3b913e71121d4406de7451a9321f48eefa90e6aa", "4a17c8b1bffa3d08561bb715f872603f4f58d475", "895082ba0b3658b68d6571cc83a21551457e1093", "895082ba0b3658b68d6571cc83a21551457e1093", "a1403cdd077f5e791c96c73dff06d401e4f44f3e", "895082ba0b3658b68d6571cc83a21551457e1093", "895082ba0b3658b68d6571cc83a21551457e1093"]},{"id": "0169c679f05fb4c8efc0db267dc1878ed35af9db", "title": "Spike train dynamics predicts theta-related phase precession in hippocampal pyramidal cells", "authors": ["Kenneth D. Harris", "Darrell A. Henze", "György Buzsáki"], "date": "2002", "abstract": "According to the temporal coding hypothesis, neurons encode information by the exact timing of spikes. An example of temporal coding is the hippocampal phase precession phenomenon, in which the timing of pyramidal cell spikes relative to the theta rhythm shows a unidirectional forward precession during spatial behaviour. Here we show that phase precession occurs in both spatial and non-spatial behaviours. We found that spike phase correlated with instantaneous discharge rate, and precessed… ", "references": ["ceba41d1f0dbc7ebb1aeab2c1d5d173da43145f5", "dc047ca89e4f07cde44fb47d9c09981c00042f63", "95dcac65492641d27c82f8d92d818c14536a15fd", "c7cfd53a384dc7e98df69006afd79cbdb3bebd6c", "c7cfd53a384dc7e98df69006afd79cbdb3bebd6c", "b341610b5877a79cce8f178a069742929d74733c", "c7cfd53a384dc7e98df69006afd79cbdb3bebd6c", "2f3dfa357aea57a60312e8240631aad9358dbb43", "7983cc562a8ea2df4198d8b9525ff120eba34fd4", "b341610b5877a79cce8f178a069742929d74733c"]},{"id": "bf5a481c60a76041b132cda8f339366b5d306f83", "title": "Role of experience and oscillations in transforming a rate code into a temporal code", "authors": ["Mayank R. Mehta", "Albert K Lee", "Margaret Ann Wilson"], "date": "2002", "abstract": "In the vast majority of brain areas, the firing rates of neurons, averaged over several hundred milliseconds to several seconds, can be strongly modulated by, and provide accurate information about, properties of their inputs. This is referred to as the rate code. However, the biophysical laws of synaptic plasticity require precise timing of spikes over short timescales (<10 ms). Hence it is critical to understand the physiological mechanisms that can generate precise spike timing in vivo, and… ", "references": ["d72dcb82bcc6c65f577793ddaf4fdfd031887669", "bac8f8283ac2be30e202b78295ce7f4d00957ffb", "0a9d3e04e21c17335f1f3646a621c481657f0a00", "9609a293f074352b967f85bd130438786fd0211c", "140a0dbb503e00bc08623d73471f14cbe2bb7471", "bac8f8283ac2be30e202b78295ce7f4d00957ffb", "0a9d3e04e21c17335f1f3646a621c481657f0a00", "0a9d3e04e21c17335f1f3646a621c481657f0a00", "d72dcb82bcc6c65f577793ddaf4fdfd031887669", "0a9d3e04e21c17335f1f3646a621c481657f0a00"]},{"id": "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "title": "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition", "authors": ["Dong Yu", "Li Deng", "George E. Dahl"], "date": "2010", "abstract": "Recently, deep learning techniques have been successfully applied to automatic speech recognition tasks -first to phonetic recognition with context-independent deep belief network (DBN) hidden Markov models (HMMs) and later to large vocabulary continuous speech recognition using context-dependent (CD) DBN-HMMs.", "references": ["622a40854f79fb385b61f1a3de1cdce4999e9f4b", "e3c1bf806c325f306e5084c3bd332b83d2077e2a", "622a40854f79fb385b61f1a3de1cdce4999e9f4b", "930bde26f600dade443e88af0e81c9695a96294e", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "e3c1bf806c325f306e5084c3bd332b83d2077e2a", "e3c1bf806c325f306e5084c3bd332b83d2077e2a", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "930bde26f600dade443e88af0e81c9695a96294e"]},{"id": "621fce5ca4d2480276f27567a9377c56766de048", "title": "Oscillatory Coupling of Hippocampal Pyramidal Cells and Interneurons in the Behaving Rat", "authors": ["Jozsef Csicsvari", "Hajime Hirase", "Gyorgy Buzsáki"], "date": "1999", "abstract": "We examined whether excitation and inhibition are balanced in hippocampal cortical networks. Extracellular field and single-unit activity were recorded by multiple tetrodes and multisite silicon probes to reveal the timing of the activity of hippocampal CA1 pyramidal cells and classes of interneurons during theta waves and sharp wave burst (SPW)-associated field ripples. The somatic and dendritic inhibition of pyramidal cells was deduced from the activity of interneurons in the pyramidal layer… ", "references": ["7f52d5c06f67c5d420b4ba38519fc7ede0cae6ab", "7f52d5c06f67c5d420b4ba38519fc7ede0cae6ab", "7f52d5c06f67c5d420b4ba38519fc7ede0cae6ab", "7f52d5c06f67c5d420b4ba38519fc7ede0cae6ab", "97d59f1cb59376391bd7e5a8bbc525f04d71790e", "7983cc562a8ea2df4198d8b9525ff120eba34fd4", "908ac5ada42360fb11c9e4cf579bfe337d8eb56c", "7983cc562a8ea2df4198d8b9525ff120eba34fd4", "7765aea62b86c6beddedff9124b0c9224f91acfc", "908ac5ada42360fb11c9e4cf579bfe337d8eb56c"]},{"id": "008e9e2d3908c964d5b1c408c478215709dbea10", "title": "Improved Bottleneck Features Using Pretrained Deep Neural Networks", "authors": ["Dong Yu", "Michael L. Seltzer"], "date": "INTERSPEECH", "abstract": "Bottleneck features have been shown to be effective in improving the accuracy of automatic speech recognition (ASR) systems.", "references": ["d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "9b2e2c520ecb48be9dfe1594bcd0d93fd5a6c339", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "5fc7204b78c4d85e9d59496fcc067d62f6f4a7e1", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "bfb336c099e941467606f5053b140f4431cc33d8", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "9360e5ce9c98166bb179ad479a9d2919ff13d022"]},{"id": "473f0739666af2791ad6592822118240ed968b70", "title": "Conversational Speech Transcription Using Context-Dependent Deep Neural Networks", "authors": ["Frank Seide", "Dong Yu"], "date": "ICML", "abstract": "Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, combine the classic artificial-neural-network HMMs with traditional context-dependent acoustic modeling and deep-belief-network pre-training. CD-DNN-HMMs greatly outperform conventional CD-GMM (Gaussian mixture model) HMMs: The word error rate is reduced by up to one third on the difficult benchmarking task of speaker-independent single-pass transcription of telephone conversations. ", "references": ["7f58b2140534a391c2decb4ab09ab4cecdb548a4", "4e3ba28fb3493afd2c3db4bd8be6d8d41cf3647a", "084fe54d45c8014d76a1862cd0e2fa2a0187210c", "a08c99425ad94eed67d059813511fe9ca55e73eb", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "7f58b2140534a391c2decb4ab09ab4cecdb548a4", "e95d3934e51107da7610acd0b1bcb6551671f9f1", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "084fe54d45c8014d76a1862cd0e2fa2a0187210c"]},{"id": "23752f55103a1a0e94992c81075f31c9b6d170f5", "title": "Error back propagation for sequence training of Context-Dependent Deep NetworkS for conversational speech transcription", "authors": ["Hang Su", "Gang Li", "Frank Seide"], "date": "2013", "abstract": "We investigate back-propagation based sequence training of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, for conversational speech transcription. Theoretically, sequence training integrates with backpropagation in a straight-forward manner. However, we find that to get reasonable results, heuristics are needed that point to a problem with lattice sparseness: The model must be adjusted to the updated numerator lattices by additional iterations of frame-based cross-entropy (CE… ", "references": ["ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "c25f3a963f62165a8fc46bc63865e6bec1477e59", "6658bbf68995731b2083195054ff45b4eca38b3a", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "473f0739666af2791ad6592822118240ed968b70", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1"]},{"id": "d7174b0cf599408fb723e6702504e27dc9d6c203", "title": "Making Deep Belief Networks effective for large vocabulary continuous speech recognition", "authors": ["Tara N. Sainath", "Brian Kingsbury", "Abdel-rahman Mohamed"], "date": "2011", "abstract": "To date, there has been limited work in applying Deep Belief Networks (DBNs) for acoustic modeling in LVCSR tasks, with past work using standard speech features. However, a typical LVCSR system makes use of both feature and model-space speaker adaptation and discriminative training. This paper explores the performance of DBNs in a state-of-the-art LVCSR system, showing improvements over Multi-Layer Perceptrons (MLPs) and GMM/HMMs across a variety of features on an English Broadcast News task… ", "references": ["473f0739666af2791ad6592822118240ed968b70", "473f0739666af2791ad6592822118240ed968b70", "3d82e058a5c40954b8f5db170a298a889a254c37", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "473f0739666af2791ad6592822118240ed968b70", "473f0739666af2791ad6592822118240ed968b70", "137b572ca406ba8fc86e985c185233b8cd6517d2", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "3d82e058a5c40954b8f5db170a298a889a254c37", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada"]},{"id": "f0fa267389e4dc801b93880ec46eb3e409980bdf", "title": "Correlated neuronal discharge rate and its implications for psychophysical performance", "authors": ["Ehud Zohary", "Michael N. Shadlen", "William T. Newsome"], "date": "1994", "abstract": "SINGLE neurons can signal subtle changes in the sensory environment with surprising fidelity, often matching the perceptual sensitivity of trained psychophysical observers1–10. This similarity poses an intriguing puzzle: why is psychophysical sensitivity not greater than that of single neurons? Pooling responses across neurons should average out noise in the activity of single cells, leading to substantially improved psychophysical performance. If, however, noise is correlated among these… ", "references": []},{"id": "df1ec1bfb66ffa141bca936e8dbf9226378c77d1", "title": "Synaptic modification by correlated activity: Hebb's postulate revisited.", "authors": ["Guo-Qiang Bi", "Mu-Ming Poo"], "date": "2001", "abstract": "Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type-specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing-dependent modifications, together with selective spread of synaptic changes, provide a set of… ", "references": ["766ce751a9268e654d28a1a0b294964bd7c74e06", "6cfe1add81918f6143cdf6a7f2efb6b7833061fc", "99fa74130727064680a2f27f78f1bc72131a73fb", "e15f6c8d966d92ca2c90b991c0318a5f93bd011b", "9f5eba3b6a93813876b599d668fa09554a95f518", "ce252b1429738f68c124a57eb5e54798f00f652d", "7db812e40142b2be590c97c3d78503f17cb22475", "7db812e40142b2be590c97c3d78503f17cb22475", "6d96ee1097a1eb65c9a70ebd668d017d5e3c3689", "6d96ee1097a1eb65c9a70ebd668d017d5e3c3689"]},{"id": "c25f3a963f62165a8fc46bc63865e6bec1477e59", "title": "Scalable Minimum Bayes Risk Training of Deep Neural Network Acoustic Models Using Distributed Hessian-free Optimization", "authors": ["Brian Kingsbury", "Tara N. Sainath", "Hagen Soltau"], "date": "INTERSPEECH", "abstract": "Training neural network acoustic models with sequencediscriminative criteria, such as state-level minimum Bayes risk (sMBR), been shown to produce large improvements in performance over cross-entropy.", "references": ["d531dc3d20d35b69b9962c2bedeb60fda89e8a72", "d7174b0cf599408fb723e6702504e27dc9d6c203", "1c94c8d5a50790bfaf13fc965bdd5414593a06b8", "a98483785378bde7e2384a3035b2b501ee03654b", "1c94c8d5a50790bfaf13fc965bdd5414593a06b8", "01eb07582d7c8368f9fb79b49d22c0e9832c7d74", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "f151b800104bc5945b33520845089b727c58a7d8", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f"]},{"id": "d72dcb82bcc6c65f577793ddaf4fdfd031887669", "title": "Dynamics of neuronal interactions in monkey cortex in relation to behavioural events", "authors": ["Eilon Vaadia", "Iris Haalman", "Ad Aertsen"], "date": "1995", "abstract": "IT is possible that brain cortical function is mediated by dynamic modulation of coherent firing in groups of neurons. Indeed, a correlation of firing between cortical neurons, seen following sensory stimuli or during motor behaviour, has been described1–5. However, the time course of modifications of correlation in relation to behaviour was not evaluated systematically. Here we show that correlated firing between single neurons, recorded simultaneously in the frontal cortex of monkeys… ", "references": []},{"id": "14a5dd6c835641ad0337898874b65a8a2c443c5a", "title": "Nonlinear Population Codes", "authors": ["Maoz Shamir", "Haim Sompolinsky"], "date": "2004", "abstract": "Theoretical and experimental studies of distributed neuronal representations of sensory and behavioral variables usually assume that the tuning of the mean firing rates is the main source of information. However, recent theoretical studies have investigated the effect of cross-correlations in the trial-to-trial fluctuations of the neuronal responses on the accuracy of the representation. Assuming that only the first-order statistics of the neuronal responses are tuned to the stimulus, these… ", "references": ["837d110fccfc34560fa7126387e03ef947ebc66d", "fdd77cdd4520fdfef927382ee50e7c472d62b390", "8f9e315303309ceafe64b9217c0703002cc0314c", "ea48b9b746161a88ccf33ab4d24ee20701b5f4d4", "aaef71f4685682387f4e5d2e8110646a26838d20", "bac8f8283ac2be30e202b78295ce7f4d00957ffb", "fdd77cdd4520fdfef927382ee50e7c472d62b390", "8f9e315303309ceafe64b9217c0703002cc0314c", "ebe0eb918efbd3f317f8aa8e02aad192b21c9c3d", "837d110fccfc34560fa7126387e03ef947ebc66d"]},{"id": "d3986480be64dce5ae55c1f64df660d7d698f8fb", "title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "authors": ["Benjamin Recht", "Christopher Ré"], "date": "2013", "abstract": "This paper develops Jellyfish, an algorithm for solving data-processing problems with matrix-valued decision variables regularized to have low rank. Particular examples of problems solvable by Jellyfish include matrix completion problems and least-squares problems regularized by the nuclear norm or $$\\gamma _2$$-norm. Jellyfish implements a projected incremental gradient method with a biased, random ordering of the increments. This biased ordering allows for a parallel implementation that… ", "references": ["04e4134c41e95ec7654f3adc3651b373f038a682", "6d389485b399ae7b60c1f426f1168f4eacaba64f", "04e4134c41e95ec7654f3adc3651b373f038a682", "ec26b01efa0639be672d07651d32a06bae4c8c63", "c369d9192c9754fb7a04529c68f2fb286f646df7", "5e097e8b7b1cf267da5c2dd638c511456839ea2b", "5e097e8b7b1cf267da5c2dd638c511456839ea2b", "04e4134c41e95ec7654f3adc3651b373f038a682", "1e826f01d02a8d514b8a687932d228781243496e", "36632b6ea5ea553d7e48d5f342182650d58cc79d"]},{"id": "5a5989208e171ca1b94b973a544aedcfbfb3bde9", "title": "Redundancy in the Population Code of the Retina", "authors": ["Jason L. Puchalla", "Elad Schneidman", "Michael J. Berry"], "date": "2005", "abstract": "We have explored the manner in which the population of retinal ganglion cells collectively represent the visual world. Ganglion cells in the salamander were recorded simultaneously with a multielectrode array during stimulation with both artificial and natural visual stimuli, and the mutual information that single cells and pairs of cells conveyed about the stimulus was estimated. We found significant redundancy between cells spaced as far as 500 mum apart. When we used standard methods for… ", "references": ["d80c00fce35017f5b81ac0703aa803dae5ad1245", "48cba1159bb9ff98cc1d86af3232204afb484a93", "dd1c8a114d7970484c29e3be180f37c9a6e3a7e6", "177069378970feb9cff407c6985a7123d8ffdc8e", "9ff65958b363ff5465f7af6a2e8800dcd3948a28", "d80c00fce35017f5b81ac0703aa803dae5ad1245", "20b90548a512b48b5f84c4294b4dce4cb95b458e", "20b90548a512b48b5f84c4294b4dce4cb95b458e", "9ff65958b363ff5465f7af6a2e8800dcd3948a28", "20b90548a512b48b5f84c4294b4dce4cb95b458e"]},{"id": "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "authors": ["Brian Kingsbury"], "date": "2009", "abstract": "Acoustic models used in hidden Markov model/neural-network (HMM/NN) speech recognition systems are usually trained with a frame-based cross-entropy error criterion. In contrast, Gaussian mixture HMM systems are discriminatively trained using sequence-based criteria, such as minimum phone error or maximum mutual information, that are more directly related to speech recognition accuracy. This paper demonstrates that neural-network acoustic models can be trained with sequence classification… ", "references": ["ce30f4e9c5be7be945f22e1d0ed7cd625331fe0f", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "de8ceb72bf54293959813c101c4f7ce54fbd3a20", "5e9082caea65c76bfd23b8763872804473ee7872", "acf4e90062ca28e12f9e3a8c8b117030469d3e4b", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "ce30f4e9c5be7be945f22e1d0ed7cd625331fe0f", "5e9082caea65c76bfd23b8763872804473ee7872", "acf4e90062ca28e12f9e3a8c8b117030469d3e4b", "5e9082caea65c76bfd23b8763872804473ee7872"]},{"id": "771dc579bf86d269c9565b824f48f0666e84e58c", "title": "Backpropagation without Multiplication", "authors": ["Patrice Y. Simard", "Hans Peter Graf"], "date": "NIPS", "abstract": "The back propagation algorithm has been modified to work without any multiplications and to tolerate computations with a low resolution, which makes it. more attractive for a hardware implementation. Numbers are represented in floating point format with 1 bit mantissa and 3 bits in the exponent for the states, and 1 bit mantissa and 5 bit exponent. for the gradients, while the weights are 16 bit fixed-point numbers. In this way, all the computations can be executed with shift and add operations… ", "references": ["c91d07e3a0da4ee494c741b387bb65f8ca66c5cc", "2bb63bd162d7bea204454381db9e98c7fa069553", "a129781d52d8a1cd24582b940dabb32e8f8c35d2", "a487d8b6f4ad01ca20705a2a4ef3edd42430a5ec"]},{"id": "cca4c343ea0b0d0dd76912d39d44d8aa42c3c8bf", "title": "Coding of visual information by precisely correlated spikes in the lateral geniculate nucleus", "authors": ["Yang Dan", "Jose-Manuel Alonso", "R. Clay Reid"], "date": "1998", "abstract": "Correlated firing among neurons is widespread in the nervous system. Precisely correlated spiking, occurring on a millisecond time scale, has recently been observed among neurons in the lateral geniculate nucleus with overlapping receptive fields. We have used an information-theoretic analysis to examine the role of these correlations in visual coding. Considerably more information can be extracted from two cells if temporal correlations between them are considered. The percentage increase in… ", "references": ["65a234b4696b3596956da2ecc80519d788c27cfb", "65a234b4696b3596956da2ecc80519d788c27cfb", "1f589a46f8d6b9481786e1c3dd06f73d44d12ba9", "e9e1be89dcb7352689a86f17db451c34e4aa72cc", "c84c360e64b8bcaa4f4545d220fb46e1f4f80c45", "1f589a46f8d6b9481786e1c3dd06f73d44d12ba9", "c84c360e64b8bcaa4f4545d220fb46e1f4f80c45", "c84c360e64b8bcaa4f4545d220fb46e1f4f80c45", "82e3abb831e44e9438b687a8054450ddc2c26aa9", "c84c360e64b8bcaa4f4545d220fb46e1f4f80c45"]},{"id": "d23c567f7f521bc49a67ebfd70a4ad2e795a5d23", "title": "Selective Adaptation in Networks of Cortical Neurons", "authors": ["Danny Eytan", "Naama Brenner", "Shimon Marom"], "date": "2003", "abstract": "A key property of neural systems is their ability to adapt selectively to stimuli with different features. Using multisite electrical recordings from networks of cortical neurons developing ex vivo, we show that neurons adapt selectively to different stimuli invading the network. We focus on selective adaptation to frequent and rare stimuli; networks were stimulated at two sites with two different stimulus frequencies. When both stimuli were presented within the same period, neurons in the… ", "references": ["4dcdbfa0c3995d6d1c4d9021db38f9be816ede3b", "7e97691e31da79f85a51e22dcd128c9f188b0b34", "7e97691e31da79f85a51e22dcd128c9f188b0b34", "4dcdbfa0c3995d6d1c4d9021db38f9be816ede3b", "57974bb5db4b5db003ae3ee1b2f53ed130ead4b2", "57974bb5db4b5db003ae3ee1b2f53ed130ead4b2", "8c8b5d95faf38c82ab71532c98b9962e59790b28", "25a75ede015468c36b2daed01da94c97867b6103", "7101579fb37dc965867bbd710473a926cec88401", "7101579fb37dc965867bbd710473a926cec88401"]},{"id": "0e4d042b668805e19f097b7eb0f223babec68f67", "title": "Performance Prediction for Exponential Language Models", "authors": ["Stanley F. Chen"], "date": "HLT-NAACL", "abstract": "We investigate the task of performance prediction for language models belonging to the exponential family. First, we attempt to empirically discover a formula for predicting test set cross-entropy for n-gram language models. We build models over varying domains, data set sizes, and n-gram orders, and perform linear regression to see whether we can model test set performance as a simple function of training set performance and various model statistics. Remarkably, we find a simple relationship… ", "references": ["c8c8912b8afc4485f4f563a97d26a22ebb387e51", "ae7f2177b485737883235b9cc4233e7fd98e1365", "e0e833b6c3f429f606de7fe32b1386da9c86cd14", "47caf6698d6b2439a8980b6d65ba10b5cff92f34", "adefbc730e3862a821598efd152aae0cc1b7262b", "c8c8912b8afc4485f4f563a97d26a22ebb387e51", "7860d20f1900089c09bfb0305807408f7ef9dc76", "c8c8912b8afc4485f4f563a97d26a22ebb387e51", "e0e833b6c3f429f606de7fe32b1386da9c86cd14", "47caf6698d6b2439a8980b6d65ba10b5cff92f34"]},{"id": "bbc18f70c3a85586ce90ef71bd9f2ada23f2df7f", "title": "Learning Sparse Overcomplete Codes for Images", "authors": ["Joseph F. Murray", "Kenneth Kreutz-Delgado"], "date": "2007", "abstract": "Images can be coded accurately using a sparse set of vectors from a learned overcomplete dictionary, with potential applications in image compression and feature selection for pattern recognition. We present a survey of algorithms that perform dictionary learning and sparse coding and make three contributions. First, we compare our overcomplete dictionary learning algorithm (FOCUSS-CNDL) with overcomplete independent component analysis (ICA). Second, noting that once a dictionary has been… ", "references": ["9067db319ecb338312070a92b081341e6f03a6c6", "2805537bec87a6177037b18f9a3a9d3f1038867b", "41f48bb4c3523d8927c613425be5f5a1b971b506", "17e7cca7e795d8ba1fa9d2c88bf2675c2d6ddfe8", "0ca26f9a98dda0abb737692f72ffa682df14cb2f", "306de9c553695822ae9e6de044b6856baf0cce7d", "2805537bec87a6177037b18f9a3a9d3f1038867b", "9af121fbed84c3484ab86df8f17f1f198ed790a0", "9af121fbed84c3484ab86df8f17f1f198ed790a0", "41f48bb4c3523d8927c613425be5f5a1b971b506"]},{"id": "306ddd8b7ea3ead125491efc3e8a9f738ce65b89", "title": "A Unified Energy-Based Framework for Unsupervised Learning", "authors": ["Marc'Aurelio Ranzato", "Y-Lan Boureau", "Yann LeCun"], "date": "AISTATS", "abstract": "We introduce a view of unsupervised learning that integrates probabilistic and nonprobabilistic methods for clustering, dimensionality reduction, and feature extraction in a unified framework.", "references": ["2805537bec87a6177037b18f9a3a9d3f1038867b", "b95799a25def71b100bd12e7ebb32cbcee6590bf", "7fc604e1a3e45cd2d2742f96d62741930a363efa", "c1d428e32ede50c4d4854fd2f712eee44c96d676", "29bae9472203546847ec1352a604566d0f602728", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "c1d428e32ede50c4d4854fd2f712eee44c96d676", "c1d428e32ede50c4d4854fd2f712eee44c96d676", "85791491919e1f740f0e882366046acbe56fb14c", "7fc604e1a3e45cd2d2742f96d62741930a363efa"]},{"id": "4157ed3db4c656854e69931cb6089b64b08784b9", "title": "DaDianNao: A Machine-Learning Supercomputer", "authors": ["Yunji Chen", "Tao Luo", "Olivier Temam"], "date": "2014", "abstract": "Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area… ", "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "fdb813d8b927bdd21ae1858cafa6c34b66a36268", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "fdb813d8b927bdd21ae1858cafa6c34b66a36268", "b568a32f663511430a2bc94ad02d833ed590a4ce", "fdb813d8b927bdd21ae1858cafa6c34b66a36268", "b568a32f663511430a2bc94ad02d833ed590a4ce", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b568a32f663511430a2bc94ad02d833ed590a4ce"]},{"id": "386cbc45ceb59a7abb844b5078e5c944f17723b4", "title": "On the approximate realization of continuous mappings by neural networks", "authors": ["Ken-ichi Funahashi"], "date": "1989", "abstract": "Abstract In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the… ", "references": ["918aeead4adb3052bd0c437ac40939c116ba65db", "1339348aeef592802288d9d929a085cb3ae61c4b", "f258b49b4a65d5b30a9bd539067ded2a3b2c5531", "d1382a29539b3de419d567f679b5f28cee459a49", "834b3738673dacc767563c2714239852a8a6d4b4", "f1fb17dc0a4656aae5b0bb3f2c21cd5e5190f4f1", "1339348aeef592802288d9d929a085cb3ae61c4b", "de996c32045df6f7b404dda2a753b6a9becf3c08", "b8778bb692cf105254fe767ef11a3a8afac4a068", "b8778bb692cf105254fe767ef11a3a8afac4a068"]},{"id": "1e05247708515d45166ef96a153f4e22811aa2c6", "title": "Hamming Distance Metric Learning", "authors": ["Mohammad Norouzi", "David J. Fleet", "Ruslan Salakhutdinov"], "date": "NIPS", "abstract": "Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper… ", "references": ["d191544940caac5f57363968539856343ad9a02d", "8a4fb7c39fde7ae172f35cd68a802de168cd4466", "8dbf5b2addb3cfbc8c72f15d65fe234268a628e6", "97163b4aea5f5e001bd5f85c5cc2372455e57f72", "9bfd3f4f006e056e635caf5ffa0f68f6ad20252c", "8a4fb7c39fde7ae172f35cd68a802de168cd4466", "8a4fb7c39fde7ae172f35cd68a802de168cd4466", "97163b4aea5f5e001bd5f85c5cc2372455e57f72", "8c054e1f378e033f39a7b95c7f68da4b3b5544e4", "d191544940caac5f57363968539856343ad9a02d"]},{"id": "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "title": "Scaling learning algorithms towards AI", "authors": ["Yoshua Bengio", "Yann LeCun"], "date": "2007", "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and… ", "references": ["b7d471970467a99bec4bce34c7dba5ef6745ad06", "125842668eab7decac136db8a59d392dc5e4e395", "162d958ff885f1462aeda91cd72582323fd6a1f4", "f354310098e09c1e1dc88758fca36767fd9d084d", "852c6f55f3df7c7118fd4165f25ce8dc1ceab0ab", "f354310098e09c1e1dc88758fca36767fd9d084d", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "f354310098e09c1e1dc88758fca36767fd9d084d", "07b54bac0159028aed116dbdbc2b747f723e585e", "25ca8792a1c183f4dd88dcebcf8a54202b483bb0"]},{"id": "932c2a02d462abd75af018125413b1ceaa1ee3f4", "title": "Efficient Learning of Sparse Representations with an Energy-Based Model", "authors": ["Marc'Aurelio Ranzato", "Christopher S. Poultney", "Yann LeCun"], "date": "NIPS", "abstract": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code… ", "references": ["b95799a25def71b100bd12e7ebb32cbcee6590bf", "b95799a25def71b100bd12e7ebb32cbcee6590bf", "5562a56da3a96dae82add7de705e2bd841eb00fc", "29bae9472203546847ec1352a604566d0f602728", "b95799a25def71b100bd12e7ebb32cbcee6590bf", "c1d428e32ede50c4d4854fd2f712eee44c96d676", "3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f", "b95799a25def71b100bd12e7ebb32cbcee6590bf", "7c032d555a9d7096f7bb88441f10e33d3302d5be", "2805537bec87a6177037b18f9a3a9d3f1038867b"]},{"id": "2964d30862d0402b0d0ad4a427067f69e4a52130", "title": "Higher Order Contractive Auto-Encoder", "authors": ["Salah Rifai", "Grégoire Mesnil", "Xavier Glorot"], "date": "ECML/PKDD", "abstract": "We propose a novel regularizer when training an autoencoder for unsupervised feature extraction. We explicitly encourage the latent representation to contract the input space by regularizing the norm of the Jacobian (analytically) and the Hessian (stochastically) of the encoder's output with respect to its input, at the training points. While the penalty on the Jacobian's norm ensures robustness to tiny corruption of samples in the input space, constraining the norm of the Hessian extends this… ", "references": ["2d851f681f82c71a934aebd16e8112adf1239f85", "ff32cebbdb8a436ccd8ae797647428615ae32d74", "195d0a8233a7a46329c742eaff56c276f847fadc", "843959ffdccf31c6694d135fad07425924f785b1", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "0bb754879154b0a10ee9966636348d831081e151", "be9a17321537d9289875fe475b71f4821457b435", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "0bb754879154b0a10ee9966636348d831081e151"]},{"id": "355d44f53428b1ac4fb2ab468d593c720640e5bd", "title": "Greedy Layer-Wise Training of Deep Networks", "authors": ["Yoshua Bengio", "Pascal Lamblin", "Hugo Larochelle"], "date": "NIPS", "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions.", "references": ["19bb461ebc18b43d44b3589659a2e450fff74c32", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "995a3b11cc8a4751d8e167abc4aa937abc934df0", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "758b1d823ac975720e6e81e375cd4432009e5bca", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "19bb461ebc18b43d44b3589659a2e450fff74c32", "b7d471970467a99bec4bce34c7dba5ef6745ad06", "b7d471970467a99bec4bce34c7dba5ef6745ad06"]},{"id": "f5821548720901c89b3b7481f7500d7cd64e99bd", "title": "Auto-association by multilayer perceptrons and singular value decomposition", "authors": ["Hervé Bourlard", "Yves Kamp"], "date": "2004", "abstract": "The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix… ", "references": ["e01c3ccf5a59ca224966bd0f1f41f38a0167883a", "9b901efe44aa6685048077000c2e2838a21e31bd", "6db399b4afd41d29c06bbb88c1de370a4b93f994", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "c386d04198321657907153c2ce97ffed56c47ca1", "01a362de9d0a8714f2726f53dba1777513f79c2f"]},{"id": "29bae9472203546847ec1352a604566d0f602728", "title": "Learning the parts of objects by non-negative matrix factorization", "authors": ["Daniel D. Lee", "H. Sebastian Seung"], "date": "1999", "abstract": "Is perception of the whole based on perception of its parts.", "references": ["e0314ee5840bbe4a722539c2ee060c9b820f1f1a", "ff1152582155acaa0e9d0ccbc900a4641504256d", "e2eef466573de571ad5621e4780dc0a465b9a253", "e2eef466573de571ad5621e4780dc0a465b9a253", "cd5a48558380c81f29ba9de6b64b561aed0657b5", "e0314ee5840bbe4a722539c2ee060c9b820f1f1a", "552e26d425c6cd6d79e428716adcd2897ee62e0e", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "e0314ee5840bbe4a722539c2ee060c9b820f1f1a", "e2eef466573de571ad5621e4780dc0a465b9a253"]},{"id": "54d2b5c64a67f65c5dd812b89e07973f97699552", "title": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition", "authors": ["Antonio Torralba", "Rob Fergus", "William T. Freeman"], "date": "2008", "abstract": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is… ", "references": ["fedf7729b620ec2cf4e79705d2898f82e9a2ba66", "67c928ff329e540b580fbf2b08e6235f2240f56b", "fedf7729b620ec2cf4e79705d2898f82e9a2ba66", "a11d6a90126d97d61d85098c8731c6f6d781e5ca", "fedf7729b620ec2cf4e79705d2898f82e9a2ba66", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "67c928ff329e540b580fbf2b08e6235f2240f56b", "67c928ff329e540b580fbf2b08e6235f2240f56b", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "67c928ff329e540b580fbf2b08e6235f2240f56b"]},{"id": "5a47ba057a858f8c024d2518cc3731fc7eb40de1", "title": "Flexible, High Performance Convolutional Neural Networks for Image Classification", "authors": ["Dan C. Ciresan", "Ueli Meier", "Jürgen Schmidhuber"], "date": "IJCAI", "abstract": "We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants.", "references": ["5a2668bf420d8509a4dfa28e1cdcdac14c649975", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "2cc157afda51873c30b195fff56e917b9c06b853", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "3fa5450f1c0795527939cfef5fbe3912c4dab3ab", "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd", "3fa5450f1c0795527939cfef5fbe3912c4dab3ab", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "581528b2215e017eba96ef4ee16d33a74645755f", "162d958ff885f1462aeda91cd72582323fd6a1f4"]},{"id": "053912e76e50c9f923a1fc1c173f1365776060cc", "title": "On optimization methods for deep learning", "authors": ["Quoc V. Le", "Jiquan Ngiam", "Andrew Y. Ng"], "date": "ICML", "abstract": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs.", "references": ["e60ff004dde5c13ec53087872cfcdd12e85beb57", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "4c46347fbc272b21468efe3d9af34b4b2bad6684"]},{"id": "6348bb3b140c47ea29621d1dc5218db52433840b", "title": "MVAPICH2-GPU: optimized GPU to GPU communication for InfiniBand clusters", "authors": ["Hao Wang", "Sreeram Potluri", "Dhabaleswar K. Panda"], "date": "2011", "abstract": "Data parallel architectures, such as General Purpose Graphics Units (GPGPUs) have seen a tremendous rise in their application for High End Computing. However, data movement in and out of GPGPUs remain the biggest hurdle to overall performance and programmer productivity. Applications executing on a cluster with GPUs have to manage data movement using CUDA in addition to MPI, the de-facto parallel programming standard. Currently, data movement with CUDA and MPI libraries is not integrated and it… ", "references": ["6a402389c2a4594e8fb1499e40b2616b110cff17", "7c3f833e3b4062cb367da0ae5eb7e4e0aa535f27"]},{"id": "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "title": "Convolutional Deep Belief Networks on CIFAR-10", "authors": ["Alex Krizhevsky"], "date": "2010", "abstract": "We describe how to train a two-layer convolutional Deep Belief Network (DBN) on the 1.6 million tiny images dataset. When training a convolutional DBN, one must decide what to do with the edge pixels of teh images. As the pixels near the edge of an image contribute to the fewest convolutional lter outputs, the model may see it t to tailor its few convolutional lters to better model the edge pixels. This is undesirable becaue it usually comes at the expense of a good model for the interior parts… ", "references": []},{"id": "05cc38e249a6f642363b5a5cbd71cda67cea5893", "title": "Tiled convolutional neural networks", "authors": ["Quoc V. Le", "Jiquan Ngiam", "Andrew Y. Ng"], "date": "NIPS", "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition.", "references": ["3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "265069b3670930fd884b02062d7e7b79ff2a49d5", "1e80f755bcbf10479afd2338cec05211fdbd325c", "f354310098e09c1e1dc88758fca36767fd9d084d", "1e80f755bcbf10479afd2338cec05211fdbd325c", "f354310098e09c1e1dc88758fca36767fd9d084d", "265069b3670930fd884b02062d7e7b79ff2a49d5", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "5a2668bf420d8509a4dfa28e1cdcdac14c649975"]},{"id": "9db00dc5532e4c1b11404a6f957d806ca42c4e73", "title": "Reference and comprehension: A topic-comment analysis of sentence-picture verification", "authors": ["Steven Lloyd Greenspan", "Erwin M. Segal"], "date": "1984", "abstract": "Abstract A sentence often contains two components: a topic, which refers to a nonlinguistic object whose existence is presupposed; and a comment, which asserts some thought about this object. In addition, a sentence will often repeat (or refer back to) information that was expressed earlier in a text or conversation. The purpose of this article is to propose a single theoretical framework to describe the mechanisms that relate a sentence to its nonlinguistic environment as well as those that… ", "references": []},{"id": "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "title": "Measuring Invariances in Deep Networks", "authors": ["Ian J. Goodfellow", "Quoc V. Le", "Andrew Y. Ng"], "date": "NIPS", "abstract": "For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that… ", "references": ["ccd52aff02b0f902f4ce7247c4fee7273014c41c", "f354310098e09c1e1dc88758fca36767fd9d084d", "1e80f755bcbf10479afd2338cec05211fdbd325c", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "1e80f755bcbf10479afd2338cec05211fdbd325c", "5127759530ce213f488af2859190697770f557f3", "f354310098e09c1e1dc88758fca36767fd9d084d", "f354310098e09c1e1dc88758fca36767fd9d084d", "202cbbf671743aefd380d2f23987bd46b9caaf97"]},{"id": "65974be9cb2d147e44f93ca0ca0ab5f4c9e22cd7", "title": "Analogical Processes in Learning", "authors": ["David E. Rumelhart", "Donald A. Norman"], "date": "1980", "abstract": "Abstract : This paper examines the role of analogy and procedural representation in learning. Examples of analogical manipulation of knowledge schemata are presented from several domains, including turtle geometry, kinship terms, and the learning of a computer text editor. The view presented in this paper has a number of implications for instruction and for performance. In particular, the learner or user of a system should be presented with a conceptual model that has the following properties… ", "references": []},{"id": "a211a693915981acea30e9360b11e055baed8299", "title": "On extensions of an inequality among means", "authors": ["F. Chan", "Dan. Goldberg", "Steven M. Gonek"], "date": "1974", "abstract": "An inequality of Fan relates the arithmetic and geometric means of x and 1 -x. An extension to generalized means is conjectured. This conjecture is proven for several special cases. In addition, some counterexamples are given. ", "references": []},{"id": "0120eefaf05bfad5293e87f56d2e787c05f78cf7", "title": "Pattern-recognizing stochastic learning automata", "authors": ["Andrew G. Barto", "P. Anandan"], "date": "1985", "abstract": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks. These tasks are called associative reinforcement learning tasks. An algorithm is presented, called the associative reward-penalty, or AR-P algorithm for which a form of optimal performance is proved. This algorithm simultaneously generalizes a class of stochastic learning automata and a class of supervised learning pattern-classification methods related… ", "references": ["7a08f1b9d48c4a3ba5b3672dccafd0e9434e3d1b", "7a08f1b9d48c4a3ba5b3672dccafd0e9434e3d1b", "f5884b40f776c85d7689ea2c440f7982cb1bec46", "e1140bddcd08706767375a3af6076a632b38c964", "434bff7f69273967c46cdda4209b77b4958d0ce4", "c3d5b5a03e2eb1f8c654a92f7c547ca5b9aa37e4", "b467c4aaf6a74e3015707f367de315ad446c53c9", "434bff7f69273967c46cdda4209b77b4958d0ce4", "434bff7f69273967c46cdda4209b77b4958d0ce4", "f5884b40f776c85d7689ea2c440f7982cb1bec46"]},{"id": "69e68bfaadf2dccff800158749f5a50fe82d173b", "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "authors": ["Kunihiko Fukushima"], "date": "1980", "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper.", "references": ["03b4a233b19cf202ba9117d501a82a48ef3ed6e9", "e9c63ea314bd66b56e1751e8b9f1cc8598df4486", "e9c63ea314bd66b56e1751e8b9f1cc8598df4486"]},{"id": "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "authors": ["Marc'Aurelio Ranzato", "Fu Jie Huang", "Yann LeCun"], "date": "2007", "abstract": "We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level… ", "references": ["e596a25fe95ab91df1db20f66e37eca9aee39616", "e596a25fe95ab91df1db20f66e37eca9aee39616", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6", "e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6", "ceb0e1a86dc35e21ce5f0524c8476f15e1b08988", "f9e65fcb0e04174577f211d702d3f837e3624c5b"]},{"id": "a6cb14ffce1dda07281c4ae47813c43f33a85a27", "title": "Antibodies to glycyl-transfer RNA synthetase in patients with myositis and interstitial lung disease.", "authors": ["Ira N. Targoff", "Edward P Trieu", "Frederick W Miller"], "date": "1992", "abstract": "OBJECTIVE\nWe have previously described anti-EJ antibodies, and provided evidence that these antibodies react with glycyl-transfer RNA (gly-tRNA) synthetase. The aim of the present study was to identify patients with anti-EJ antibodies and describe the clinical associations of the antibody, in particular, whether it is associated with the syndrome of myositis and interstitial lung disease (ILD) that has been previously associated with autoantibodies to the aminoacyl-tRNA synthetases for… ", "references": []},{"id": "65d994fb778a8d9e0f632659fb33a082949a50d3", "title": "Visualizing Higher-Layer Features of a Deep Network", "authors": ["Dumitru Erhan", "Yoshua Bengio", "Pascal Vincent"], "date": "2009", "abstract": "Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders… ", "references": ["05fd1da7b2e34f86ec7f010bef068717ae964332", "9dd2138155a0fe39834695057b2f4987181b9dbd", "843959ffdccf31c6694d135fad07425924f785b1", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "202cbbf671743aefd380d2f23987bd46b9caaf97", "b8012351bc5ebce4a4b3039bbbba3ce393bc3315", "843959ffdccf31c6694d135fad07425924f785b1", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "43c8a545f7166659e9e21c88fe234e0323855216", "b8012351bc5ebce4a4b3039bbbba3ce393bc3315"]},{"id": "4bc6de42b9fb2143ab096025ce32400445483ee3", "title": "Myositis-specific autoantibodies: their clinical and pathogenic significance in disease expression.", "authors": ["Harsha Gunawardena", "Zoe E. Betteridge", "Neil J McHugh"], "date": "2009", "abstract": "The idiopathic inflammatory myopathies (IIMs)--DM and PM--have been historically defined by broad clinical and pathological criteria. These conditions affect both adults and children with clinical features including muscle weakness, skin disease, internal organ involvement and an association with cancer in adults. Using a clinico-serological approach, DM and PM can be defined into more homogeneous subsets. Over the last few years, myositis-specific autoantibodies (MSAs) have been better… ", "references": ["af746d8ef0d72c0d704c0fa8e5f12bce5bd592d5", "84e686817881bf4954843c77e6de4fde4767943b", "fd770fee84db14cc02560f240c35c0b6c74b5c76", "e5a5387d139ee079b669e00134a19a77e0e43976", "84e686817881bf4954843c77e6de4fde4767943b", "32dc3258ae1b3005f77708cedae3a0451a2b3af5", "a5b76d63eee815d60efe8684a7f18cfb3d8a9850", "af746d8ef0d72c0d704c0fa8e5f12bce5bd592d5", "a76695a1c716b884c3c69b11b34f913c5880e29c", "44c5a385b511b252c9139b6e478b1a73ec26dfda"]},{"id": "bb4bad84a2fd896edfa4f5c22061b2913fec500d", "title": "Feature discovery by competitive learning", "authors": ["David E. Rumelhart", "David Zipser"], "date": "1985", "abstract": "This paper reporis the results of our studies with an unsupervised learning paradigm which we have called “Competitive Learning.” We have examined competitive learning using both computer simulation and formal analysis and hove found that when it is applied to parallel networks of neuron-like elements, many potentially useful learning tasks can be accomplished. We were attracted to competitive learning because it seems to provide o way to discover the salient, general features which can be used… ", "references": ["6a43ce2ce984862b1c9ef8eabe8ea676fa0ca76b", "09517a9d68a9cd8c0a10bd3411c8f59385a5894e"]},{"id": "f8eb135fd75cb72409e2bdeec5ccc81c379f43d4", "title": "[Inflammatory muscle diseases: dermatomyositis, polymyositis, and inclusion body myositis].", "authors": ["Ekkehard Genth"], "date": "2005", "abstract": "Dermatomyositis, polymyositis, inclusion body myositis and myositis overlap syndromes are systemic immune disorders of unknown origin with muscle weakness and elevated values of creatinkinase in the serum. Muscle biopsy is pivotal for a proper clinical diagnosis. Extramuscular findings at the skin, the joints or internal organs (lung, heart) are characteristic for the different clinical presentations of dermato- or polymyositis and are usually absent in inclusion body myositis. With the… ", "references": []},{"id": "ac102aecffdecc00f95ed7871f525dc802a3b0c0", "title": "Growing Significance of Myeloperoxidase in Non-infectious Diseases", "authors": ["Aline Hoy", "Brigitte Leininger-Muller", "Sophie Visvikis"], "date": "Clinical chemistry and…", "abstract": "Abstract Myeloperoxidase (MPO) is a glycoprotein released by activated polymorphonuclear neutrophils, which takes part in the defense of the organism through production of hypochlorous acid (HOCl), a potent oxidant. Since the discovery of MPO deficiency, initially regarded as rare and restricted to patients suffering from severe infections, MPO has attracted clinical attention. The development of new technologies allowing screening for this defect has permitted new advances in the comprehension… ", "references": ["ef3fff25720dc8af9e15668797b2a667e1b134a9", "ef3fff25720dc8af9e15668797b2a667e1b134a9", "f690b02cd0d1fb760bde965dbdc7655bb6b621a8", "16124ac9c03735dd9aabc8cb83ba851d766e8a15", "700e779c175ad60ca5a45b41926acb4b4decda5d", "5f37029df407aa746dac1aa4d5791ec5883223e0", "16124ac9c03735dd9aabc8cb83ba851d766e8a15", "13113dc9690a3338969acff016ed928e4e9929c8", "16124ac9c03735dd9aabc8cb83ba851d766e8a15", "700e779c175ad60ca5a45b41926acb4b4decda5d"]},{"id": "f2ea9a76819a952e6963e3df93b88b9f3b636c84", "title": "Clinical characteristics of Japanese patients with anti-OJ (anti-isoleucyl-tRNA synthetase) autoantibodies.", "authors": ["Sayuri Sato", "Masataka Kuwana", "Michito Hirakata"], "date": "2007", "abstract": "OBJECTIVES\nThe clinical and laboratory characteristics of seven patients with anti-aminoacyl-tRNA synthetase (ARS) autoantibodies, specifically anti-OJ (anti-isoleucyl-tRNA synthetase), were examined and compared with previously published findings.\n\n\nMETHODS\nSerum samples from 1135 Japanese patients with various autoimmune diseases and 48 normal individuals were screened for anti-OJ antibodies using RNA and protein immunoprecipitation assays. The patients whose sera contained anti-OJ antibodies… ", "references": []},{"id": "cc278353721406a248bf733e40cdecbda8ff3a48", "title": "Bayesian Mixture Modeling", "authors": ["Radford M. Neal"], "date": "1992", "abstract": "It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation. This method exhibits the true Bayesian predictive distribution, implicitly integrating over the entire underlying parameter space. An infinite number of mixture components can be accommodated without difficulty, using a prior distribution for mixing proportions that selects a reasonable subset of components to explain any finite training set. The need to decide… ", "references": []},{"id": "906e33843520fa2395c72d71f8d20a1a5d9cd989", "title": "Robust Parameter Estimation and Model Selection for Neural Network Regression", "authors": ["Yong Liu"], "date": "NIPS", "abstract": "In this paper, it is shown that the conventional back-propagation (BPP) algorithm for neural network regression is robust to leverages (data with x corrupted), but not to outliers (data with y corrupted). A robust model is to model the error as a mixture of normal distribution. The influence function for this mixture model is calculated and the condition for the model to be robust to outliers is given. EM algorithm [5] is used to estimate the parameter. The usefulness of model selection… ", "references": ["bdfb57141b2141095ed942b28be24808aeba8d54", "8b2ac86933d7d35e2ff475e63f86ce7fd48db228", "561404258ea37f86e5dc36bb15902c37d0611957"]},{"id": "46e0169f7d98db958ad45fde734e6e33939d1a96", "title": "Anti-aminoacyl-tRNA synthetase antibodies in clinical course prediction of interstitial lung disease complicated with idiopathic inflammatory myopathies", "authors": ["Hajime Yoshifuji", "Takao Fujii", "Tsuneyo Mimori"], "date": "2006", "abstract": "In the treatment of polymyositis and dermatomyositis (PM/DM), the complication of interstitial lung disease (ILD) is an important prognostic factor. It has been reported that autoantibodies against aminoacyl-tRNA synthetases (ARS) are strongly associated with ILD. The aim of this study is to examine the correlation between anti-ARS and the clinical course of ILD. We investigated 41 cases of PM/DM with ILD. The response of ILD to corticosteroids (CS) was determined according to the change in… ", "references": ["70fbe23bfdf90c65239191b4c950b25c142f1e44", "dff725bb87d228932bb8293be2d9e603ea7ebcab", "c2eea37b0c978bd3977294a915fe2115ed850b88", "9c455280c12dd915938d745eb5efc26f9eb8de01", "70fbe23bfdf90c65239191b4c950b25c142f1e44", "84e686817881bf4954843c77e6de4fde4767943b", "dff725bb87d228932bb8293be2d9e603ea7ebcab", "9c455280c12dd915938d745eb5efc26f9eb8de01", "c2eea37b0c978bd3977294a915fe2115ed850b88", "84e686817881bf4954843c77e6de4fde4767943b"]},{"id": "be53d4def5e0601f2416e9345babc7ef1b30a664", "title": "Deep Belief Networks using discriminative features for phone recognition", "authors": ["Abdel-rahman Mohamed", "Tara N. Sainath", "Michael Picheny"], "date": "2011", "abstract": "Deep Belief Networks (DBNs) are multi-layer generative models. They can be trained to model windows of coefficients extracted from speech and they discover multiple layers of features that capture the higher-order statistical structure of the data. These features can be used to initialize the hidden units of a feed-forward neural network that is then trained to predict the HMM state for the central frame of the window. Initializing with features that are good at generating speech makes the… ", "references": ["956b80fe8a56f90782fcb4fe1c536b24496094ba", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "90b63e917d5737b06357d50aa729619e933d9614", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "3034afcd45fc190ed71982828b77f6e4154bdc5c"]},{"id": "d5ae04ca51e76d69f5ad15ba40a3eea520d3860d", "title": "Exploring Posterior Distributions Using Markov Chains", "authors": ["Luke Tierney"], "date": "1992", "abstract": "Abstract : Several Markov chain-based methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the strategies that are available, and discusses some theoretical and practical issues in the use of these strategies. In addition, some preliminary efforts to use Markov chains to control dynamic graphics for… ", "references": []},{"id": "050fd794879e685b1cfe243e7f107389ec37493e", "title": "Antibodies against PM/Scl-75 and PM/Scl-100 are independent markers for different subsets of systemic sclerosis patients", "authors": ["Katharina Hanke", "Claudia S Brueckner", "Gabriela Riemekasten"], "date": "2009", "abstract": "IntroductionAnti-PM/Scl antibodies are present in sera from patients with polymyositis (PM), systemic sclerosis (SSc), and PM/SSc overlap syndromes. The prevalence of antibodies against the 75- and 100-kDa PM/Scl proteins and their clinical associations have not been studied in SSc patients in detail so far but could provide a valuable tool for risk assessment in these patients. Furthermore, it remains speculative whether commercially available test systems detecting only anti-PM/Scl-100… ", "references": ["9bc83168bf8c6b8eff8f8e6fea86ca464a367ae8", "6d4b8cb504e9a703f2792b87f1629c9c714011d6", "348ee4de33b1c82a91ce920e07874d9e478ed89e", "17b37ce8333ec611defe3dc79c14a1cd675fe632", "17b37ce8333ec611defe3dc79c14a1cd675fe632", "6d4b8cb504e9a703f2792b87f1629c9c714011d6", "17b37ce8333ec611defe3dc79c14a1cd675fe632", "e1bf4551a5c2890e0a10de1e25e8c0a44605b321", "d0be3c23c2d30e5c9de34f01774bb4a2f7971e14", "9bc83168bf8c6b8eff8f8e6fea86ca464a367ae8"]},{"id": "5104689e412832ea5c3af39e86321e93f298d849", "title": "A Bayesian approach to prediction using polynomials", "authors": ["Alan S. Young"], "date": "1977", "abstract": "SUMMARY We have an unknown function h(x) which we want to estimate within a finite interval. The observed values of h(x) are independent observations of a random variable y whose mean is to be approximated by a polynomial of unknown degree. The problem of estimating h(x) then translates into that of predicting y. We assume that the mean of y is a polynomial of an arbitrarily large degree and derive a prior distribution for its coefficients which expresses the belief that these coefficients will… ", "references": []},{"id": "25c9f33aceac6dcff357727cbe2faf145b01d13c", "title": "Keeping the neural networks simple by minimizing the description length of the weights", "authors": ["Geoffrey E. Hinton", "Drew van Camp"], "date": "COLT '93", "abstract": "Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases.", "references": ["7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b"]},{"id": "4c51e3c0b4ed0f073d9bfd935b3a6824126336ab", "title": "Learning and Memory", "authors": ["Donald A. Norman"], "date": "1982", "abstract": "How the brain codes, stores, and retrieves memories is among the most important and baffling questions in science. The uniqueness of each human being is due largely to the memory store—the biological residue of memory from a lifetime of experience. The cellular basis of this ability to learn can be traced to simpler organisms. In the past generation, understanding of the biological basis of learning and memory has undergone a revolution. It is clear that various forms and aspects of learning… ", "references": []},{"id": "58821c2fde1ec9f42feda075d5e034379870a7a7", "title": "Disjunctive models of Boolean category learning", "authors": ["Steven E. Hampson", "Dennis J. Volper"], "date": "1987", "abstract": "Four connectionistic/neural models which are capable of learning arbitrary Boolean functions are presented. Three are provably convergent, but of differing generalization power. The fourth is not necessarily convergent, but its empirical behavior is quite good. The time and space characteristics of the four models are compared over a diverse range of functions and testing conditions. These include the ability to learn specific instances, to effectively generalize, and to deal with irrelevant or… ", "references": ["9accb739795089128e3f999b7cb62b3545202aa0", "84cdfa79e6eb9bf9e625e3af38d9f968df18a880", "43369b75cc3ef7df87eb049872428cf5d11d1706", "84cdfa79e6eb9bf9e625e3af38d9f968df18a880", "60944c5243db70a687a320a2622d3bd1610802a8", "63d33c0ea6269b6fffd3b7aa3a60682be08500cb", "eff582311c97ced7a0a3282d3eaf2ae2e6caf19d", "43369b75cc3ef7df87eb049872428cf5d11d1706", "84cdfa79e6eb9bf9e625e3af38d9f968df18a880", "1ec0e85eaf619fdb21ef3d41e234828b7d1a1230"]},{"id": "dc298a55900b5149b69bbd7708342cc91ecc940d", "title": "THE SPACING EFFECT ON NETTALK, A MASSIVELY-PARALLEL NETWORK", "authors": ["Charles R. Rosenberg"], "date": "1986", "abstract": "NETtalk is a massively-parallel network that learns to convert English text to phonemes. In NETtalk, the memory representations are shared among many processing units, and these representations are learned by practice. In humans, distributed practice is more effective for longterm retention than massed practice, and we wondered whether learning in NETtalk had similar properties. NETtalk was tested on cued paired-associate recall using nonwords as stimuli. Retention of these target items was… ", "references": ["8859e1028b99b7ae0e740521c03b451930910716", "4519ea5687f5c2b6db3a186ba4e2f8f62b0b29f6", "b90a4ad33cc57778b999b0cfc9ccb44e0f74d477", "18658c214078f3dd4e4977899f55f6a03762ff14", "18658c214078f3dd4e4977899f55f6a03762ff14", "178de5b7394d6b43474edbab7d9269c96b699521", "c581fb4a3050c30bced787dbc2995c2187e0cd1b", "8859e1028b99b7ae0e740521c03b451930910716", "4519ea5687f5c2b6db3a186ba4e2f8f62b0b29f6", "4519ea5687f5c2b6db3a186ba4e2f8f62b0b29f6"]},{"id": "ab4aec5e0714b352e6c90d063fe830cbc70912bc", "title": "Connectionist models and their properties", "authors": ["Jerome A. Feldman", "Dana H. Ballard"], "date": "1988", "abstract": "Much of the progress in the fields constituting cognitive science has been based upon the use of explicit information processing models, almost exclusively patterned after conventional serial computers. An extension of these ideas to massively parallel, connectianist models appears to offer a number of advantages. After a preliminary discussion, this paper introduces a general connectionist model and considers how it might be used in cognitive science. Among the issues addressed are: stability… ", "references": []},{"id": "84cdfa79e6eb9bf9e625e3af38d9f968df18a880", "title": "Learning by statistical cooperation of self-interested neuron-like computing elements.", "authors": ["Andrew G. Barto"], "date": "1985", "abstract": "Since the usual approaches to cooperative computation in networks of neuron-like computating elements do not assume that network components have any \"preferences\", they do not make substantive contact with game theoretic concepts, despite their use of some of the same terminology. In the approach presented here, however, each network component, or adaptive element, is a self-interested agent that prefers some inputs over others and \"works\" toward obtaining the most highly preferred inputs. Here… ", "references": []},{"id": "6f70bb581325440cddbbcf4ba0e7120357d5c7d9", "title": "Perceptron Trees: A Case Study In Hybrid Concept Representations", "authors": ["Paul E. Utgoff"], "date": "AAAI", "abstract": "The paper presents a case study in examining the bias of two particular formalisms: decision trees and linear threshold units. The immediate result is a new hybrid representation, called a perceptron tree, and an associated learning algorithm called the perceptron tree error correction procedure. The longer term result is a model for exploring issues related to understanding representational bias and constructing other useful hybrid representations. ", "references": []},{"id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex.", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "date": "1962", "abstract": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and inter-connexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we… ", "references": ["401185a7ba67af1e01e5278932747e5fd777d3a6", "401185a7ba67af1e01e5278932747e5fd777d3a6", "401185a7ba67af1e01e5278932747e5fd777d3a6", "34898b2e1cb9f7872d2d66fdfcf3a9644948463b", "2923e0dca809bf904a653915f1fe1f16db69fb84", "34898b2e1cb9f7872d2d66fdfcf3a9644948463b", "6e56f97881931233c5cf5b83c293cfcf0581f4a9", "2923e0dca809bf904a653915f1fe1f16db69fb84", "34898b2e1cb9f7872d2d66fdfcf3a9644948463b", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3"]},{"id": "82fa37d5be8e747131a5857992cc33bb95469ce3", "title": "Developments in Maximum Entropy Data Analysis", "authors": ["S. F. Gull"], "date": "1989", "abstract": "The Bayesian derivation of “Classic” MaxEnt image processing (Skilling 1989a) shows that exp(αS(f,m)), where S(f,m) is the entropy of image f relative to model m, is the only consistent prior probability distribution for positive, additive images. In this paper the derivation of “Classic” MaxEnt is completed, showing that it leads to a natural choice for the regularising parameter α, that supersedes the traditional practice of setting x2=N. The new condition is that the dimensionless measure of… ", "references": []},{"id": "0cabc69aaf5dd7ff104bcec693a9ebe7bfb238c4", "title": "A theory for the acquisition and loss of neuron specificity in visual cortex", "authors": ["Leon N. Cooper", "Fishel Liberman", "Erkki Oja"], "date": "1979", "abstract": "We assume that between lateral geniculate and visual cortical cells there exist labile synapses that modify themselves in a new fashion called threshold passive modification and in addition, non-labile synapses that contain permanent information. In the theory which results there is an increase in the specificity of response of a cortical cell when it is exposed to stimuli due to normal patterned visual experience. Non-patterned input, such as might be expected when an animal is dark-reared or… ", "references": ["eb09c3d1f74629a4ad7c34a1b452448ea0986af6", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "d6c2d502141b6639e97bb903ec94369eae4b4df2", "ebef75644a346fb5ce0a49dac9273069a463848b", "ebef75644a346fb5ce0a49dac9273069a463848b", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "3518f5d8c33b8c41e4831da7ffb4df95e796dcc3"]},{"id": "2cee043045b529fceda7964a70e626d45657245a", "title": "Predicting the Future: a Connectionist Approach", "authors": ["Andreas S. Weigend", "Bernardo A. Huberman", "David E. Rumelhart"], "date": "1990", "abstract": "We investigate the effectiveness of connectionist architectures for predicting the future behavior of nonlinear dynamical systems. We focus on real-world time series of limited record length. Two examples are analyzed: the benchmark sunspot series and chaotic data from a computational ecosystem. The problem of overfitting, particularly serious for short records of noisy data, is addressed both by using the statistical method of validation and by adding a complexity term to the cost function… ", "references": []},{"id": "6f3175b3930d0c71495a52a7bccb3889e5f33520", "title": "Generalization and Parameter Estimation in Feedforward Netws: Some Experiments", "authors": ["Nelson Morgan", "Hervé Bourlard"], "date": "NIPS", "abstract": "We have done an empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance. Two experiments are reported. In one, we use simulated data sets with well-controlled parameters, such as the signal-to-noise ratio of continuous-valued data. In the second, we train the network on vector-quantized mel cepstra from real speech samples. In each case, we use back-propagation to train the feedforward net to discriminate in a multiple class… ", "references": []},{"id": "7fc8100e73591ce8af1d553d4296ec38f939c248", "title": "Knowledge Acquisition Via Incremental Conceptual Clustering", "authors": ["Douglas H. Fisher"], "date": "1987", "abstract": "Conceptual clustering is an important way of summarizing and explaining data. However, the recent formulation of this paradigm has allowed little exploration of conceptual clustering as a means of improving performance. Furthermore, previous work in conceptual clustering has not explicitly dealt with constraints imposed by real world environments. This article presents COBWEB, a conceptual clustering system that organizes data so as to maximize inference ability. Additionally, COBWEB is… ", "references": ["6c71c75aaa43fe6a49151aa13c46c763f5ac7857", "c628c388f17753728ebae459d4841932e080e595", "19f1bf3db5bee640f4d71440c646b43344010f70", "6c71c75aaa43fe6a49151aa13c46c763f5ac7857", "f4b990a2ab370476399264c94b7ed24dd732c336", "c628c388f17753728ebae459d4841932e080e595", "dfbc2c4beedade06d8c4e497e3a6adb7472a136e", "f4b990a2ab370476399264c94b7ed24dd732c336", "f18dac29aa506d2129a4ce3a91c35fc0eb83797d", "f1c0599f84e189098b9707b39c41a736cb3b62f1"]},{"id": "59fa47fc237a0781b4bf1c84fedb728d20db26a1", "title": "Soft competitive adaptation: neural network learning algorithms based on fitting statistical mixtures", "authors": ["Steven J. Nowlan"], "date": "1991", "abstract": "In this thesis, we consider learning algorithms for neural networks which are based on fitting a mixture probability density to a set of data. \nWe begin with an unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms. Rather than updating only the parameters of the \"winner\" on each case, the parameters of all competitors are updated in proportion to their relative responsibility for the case. Use of such a \"soft\" competitive algorithm is shown to… ", "references": []},{"id": "39756c8a5ac11462e7df98ef7f7baf5b130ec5c9", "title": "A Stochastic Approximation Method for Optimization Problems", "authors": ["Tamio Shimizu"], "date": "1969", "abstract": null, "references": []},{"id": "4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "title": "Experiments on Learning by Back Propagation.", "authors": ["David C. Plaut", "Steven J. Nowlan", "Geoffrey E. Hinton"], "date": "1986", "abstract": "Abstract : Rumelhart, Hinton and Williams (Rumelhart 86) describe a learning procedure for layered networks of deterministic, neuron-like units.", "references": ["a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "cbd1ade5b869b13d1853aa0753b82fb35c26bcba", "44f2679f8169e7f6449c52e058ebe6a45838b3c0", "44f2679f8169e7f6449c52e058ebe6a45838b3c0", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "a8059c598e4f9643eb040685af0d6e7055391048", "e4a2416f8eea0b608827a593d399bdd22fa81c60", "e4a2416f8eea0b608827a593d399bdd22fa81c60", "e4a2416f8eea0b608827a593d399bdd22fa81c60", "cbd1ade5b869b13d1853aa0753b82fb35c26bcba"]},{"id": "0b718a3f9dae8abc741411aed5fe5d423079200f", "title": "Implicit Mixtures of Restricted Boltzmann Machines", "authors": ["Vinod Nair", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units… ", "references": ["d4b2dd99bd1bcebc2bca1415b8e9429e95654f7b", "d4b2dd99bd1bcebc2bca1415b8e9429e95654f7b", "08d0ea90b53aba0008d25811268fe46562cfb38c", "2184fb6d32bc46f252b940035029273563c4fc82", "2077d0f30507d51a0d3bbec4957d55e817d66a59", "f354310098e09c1e1dc88758fca36767fd9d084d", "f354310098e09c1e1dc88758fca36767fd9d084d", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "363b56f85e12389017ba8894056a1b309e46a5f7", "09ef86868035bbfd4803a9e1c98640804bf8f4a4"]},{"id": "f8830ea439ca695e7dd848275e534f1024c2fe8a", "title": "Using Relevance to Reduce Network Size Automatically", "authors": ["Michael C. Mozer", "Paul Smolensky"], "date": "1989", "abstract": "Abstract This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically remove the least relevant units. This skeletonization… ", "references": []},{"id": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage", "authors": ["Yann LeCun", "John S. Denker", "Sara A. Solla"], "date": "NIPS", "abstract": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness… ", "references": ["33fdc91c520b54e097f5e09fae1cfc94793fbfcf", "28b0563fcd3364077dfc39f42c9684ec00dcd249", "28b0563fcd3364077dfc39f42c9684ec00dcd249", "33fdc91c520b54e097f5e09fae1cfc94793fbfcf", "d4a7e54446d52f066ee692fa38d9aa972519c2f5", "a36b028d024bf358c4af1a5e1dc3ca0aed23b553", "a87953825b0bea2a5d52bfccf09d2518295c5053", "a87953825b0bea2a5d52bfccf09d2518295c5053", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "33fdc91c520b54e097f5e09fae1cfc94793fbfcf"]},{"id": "9360e5ce9c98166bb179ad479a9d2919ff13d022", "title": "Training Products of Experts by Minimizing Contrastive Divergence", "authors": ["Geoffrey E. Hinton"], "date": "2002", "abstract": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is… ", "references": ["b348e98f869a5b656f98688cb9d77208b8475379", "b348e98f869a5b656f98688cb9d77208b8475379", "7c58d06f906db4ff7dcc605ce1e6b7ecb6c3d8ea", "56efc84e0858f1e0a7cf052e5c4275d4c46c21c2", "73e93d0346e8eee6c2ab45e46c26eaafb66e12a8", "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "459b30a9a960080f3b313e41886b1aa0e51e882c", "db228acde490e9fd35b318ad7dea5910b71b5b96", "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "b348e98f869a5b656f98688cb9d77208b8475379"]},{"id": "a0125b014ff5171c74bd6d8365f4cffe3714c0b0", "title": "Combining phonetic attributes using conditional random fields", "authors": ["Jeremy Morris", "Eric Fosler-Lussier"], "date": "INTERSPEECH", "abstract": "A Conditional Random Field is a mathematical model for sequences that is similar in many ways to a Hidden Markov Model, but is discriminative rather than generative in nature. In this paper, we explore the application of the CRF model to ASR processing of discriminative phonetic features by building a system that performs first-pass phonetic recognition using discriminatively trained phonetic features. With this system, we show that this CRF model achieves an accuracy level in a phone… ", "references": ["7dad01eb07e1044fedcca20f7ef6fec66e77692d", "a574e320d899e7e82e341eb64baef7dfe8a24642", "5e9082caea65c76bfd23b8763872804473ee7872", "7dad01eb07e1044fedcca20f7ef6fec66e77692d", "a574e320d899e7e82e341eb64baef7dfe8a24642", "5e9082caea65c76bfd23b8763872804473ee7872", "878783964ab23c97052ea82685368099d85c500d", "7dad01eb07e1044fedcca20f7ef6fec66e77692d", "5e9082caea65c76bfd23b8763872804473ee7872", "a574e320d899e7e82e341eb64baef7dfe8a24642"]},{"id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "title": "Connectionist Learning of Belief Networks", "authors": ["Radford M. Neal"], "date": "1992", "abstract": "Abstract Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks.", "references": ["a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "9f48079c278b02decf14f71b9f94c6ce1756940b", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "9f48079c278b02decf14f71b9f94c6ce1756940b", "9f48079c278b02decf14f71b9f94c6ce1756940b", "9f48079c278b02decf14f71b9f94c6ce1756940b", "9f48079c278b02decf14f71b9f94c6ce1756940b", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657"]},{"id": "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "title": "The acoustic-modeling problem in automatic speech recognition", "authors": ["P. F. Brown"], "date": "1987", "abstract": "Abstract : This thesis examines the acoustic-modeling problem in automatic speech recognition from an information-theoretic point of view. This problem is to design a speech-recognition system which can extract from the speech waveform as much information as possible about the corresponding word sequence. The information extraction process is broken down into two steps: a signal processing step which converts a speech waveform into a sequence of information bearing acoustic feature vectors, and… ", "references": ["b0130277677e5b915d5cd86b3afafd77fd08eb2e", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "bdb3f20fe41bb95f6bc9d162e827de8db3f952d7", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "be53f2c226f277549ea1c993dbb6c3b74ca76fd6", "ed321dac166ea2240a1f67848f231477a3077bb0"]},{"id": "9f87a11a523e4680e61966e36ea2eac516096f23", "title": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants", "authors": ["Radford M. Neal", "Geoffrey E. Hinton"], "date": "Learning in Graphical Models", "abstract": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved.", "references": ["e0dc01248f2e19124afaac8f62e3ed2d935d652d"]},{"id": "7257eacd80458e70c74494eb1b6759b52ff21399", "title": "Using fast weights to deblur old memories", "authors": ["Geoffrey E. Hinton"], "date": "1987", "abstract": "Connectionist models usually have a single weight on each connection. Some interesting new properties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associations are \"blurred\" by subsequent learning, all the original associations can be \"deblurred\" by rehearsing on just… ", "references": ["b0cb3be87b7f4f50d62d8dbba5a2e8d78c7d90a9", "9928cac725ebe6db7b974bbd65738d33dc95332d", "ebfc4748436f1eed9947f7c33d2dba496bdf2d74", "ebfc4748436f1eed9947f7c33d2dba496bdf2d74", "8592e46a5435d18bba70557846f47290b34c1aa5", "ebfc4748436f1eed9947f7c33d2dba496bdf2d74", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "ae9a8c850c8e7ac8397d1a60b52d052d6c73be5d", "1d453386011ef21285fa81fb4f87fdf811c6ad7a", "8592e46a5435d18bba70557846f47290b34c1aa5"]},{"id": "97f7d20e1e82347d78cef335218692207b29d23f", "title": "Learning Representations by Recirculation", "authors": ["Geoffrey E. Hinton", "James L. McClelland"], "date": "NIPS", "abstract": "We describe a new learning procedure for networks that contain groups of nonlinear units arranged in a closed loop. The aim of the learning is to discover codes that allow the activity vectors in a \"visible\" group to be represented by activity vectors in a \"hidden\" group. One way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector. The difference between the original and the reconstructed visible vectors is called the… ", "references": []},{"id": "3034afcd45fc190ed71982828b77f6e4154bdc5c", "title": "Speaker-independent phone recognition using hidden Markov models", "authors": ["Kai-Fu Lee", "Hsiao-Wuen Hon"], "date": "1989", "abstract": "Hidden Markov modeling is extended to speaker-independent phone recognition.", "references": ["f64038de5e388bce2f0575cfb4a291a41e3bab57", "df53e0dc66eb13bb51c6e4803ceae56d3ebe6f23", "f64038de5e388bce2f0575cfb4a291a41e3bab57", "e90c15e0de8b5452c6291359e98ddc099e3b93f6", "fcc924f8e4cbd36ef2b244dce20fdf3893e256a1", "5815d252012f952bd5b654441ee84289ce676bb6", "5815d252012f952bd5b654441ee84289ce676bb6", "e90c15e0de8b5452c6291359e98ddc099e3b93f6", "86f98021d21a05ae380920296a723dccdec7c171", "3a72d995c3870b52607bf790c42ae171d7cc5340"]},{"id": "709b4bfc5198336ba5d70da987889a157f695c1e", "title": "Optimal unsupervised learning in a single-layer linear feedforward neural network", "authors": ["Terence D. Sanger"], "date": "1989", "abstract": "Abstraet--A new approach to unsupervised learning in a single-layer linear feedforward neural network is discussed.", "references": ["dbed69808aeccb697d125c049a6554bfac76493d", "06f5a18780d7332ed68a9c786e1c597b27a8e0f6", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "3975117d907c0e582a35c34137231c87956aa93b", "9f22cf81654dd50b95e65b86b1125cfe6803a67b", "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "de7cf7c01258719cc3be4321f780db378831f2f4", "de7cf7c01258719cc3be4321f780db378831f2f4"]},{"id": "3e6bea2649298c68d17b9421fc7dd19eeacc935e", "title": "Learning Translation Invariant Recognition in Massively Parallel Networks", "authors": ["Geoffrey E. Hinton"], "date": "PARLE", "abstract": "One major goal of research on massively parallel networks of neuron-like processing elements is to discover efficient methods for recognizing patterns. Another goal is to discover general learning procedures that allow networks to construct the internal representations that are required for complex tasks. This paper describes a recently developed procedure that can learn to perform a recognition task. The network is trained on examples in which the input vector represents an instance of a… ", "references": []},{"id": "2184fb6d32bc46f252b940035029273563c4fc82", "title": "Exponential Family Harmoniums with an Application to Information Retrieval", "authors": ["Max Welling", "Michal Rosen-Zvi", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected… ", "references": ["355b86dafd852e4df905f6ad9402c7d03831d618", "939d584316be99e2db3fec3fbf7d71f22a477f67", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "6436dce0e39f15a1ca9269e6ca813dfebb0af3a2", "939d584316be99e2db3fec3fbf7d71f22a477f67", "500a6f7fa91921a461ec41424ea2fd0ea8800b7b", "cbb985cba78d7c42986381fa4a3fa92ba8092063", "500a6f7fa91921a461ec41424ea2fd0ea8800b7b", "9360e5ce9c98166bb179ad479a9d2919ff13d022"]},{"id": "865787016949fefd4f0a31862a76db18077f2cf3", "title": "Neural Representation of Conceptual Knowledge.", "authors": ["Jerome A. Feldman"], "date": "1986", "abstract": "Abstract : The neural encoding of memory is a problem of great interest and importance. Trlalditional proposals have taken one of two extreme views: The one-concept, one-neuron, punctate view and the full distributed, holographic alternative. Major advances in the behavioral, biological and computational sciences have greatly increased our understanding of the question and its potential answers. There is now good reason to reject both extreme views, but a compact encoding that derives from the… ", "references": []},{"id": "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "title": "Linear spatial pyramid matching using sparse coding for image classification", "authors": ["Jianchao Yang", "Kai Ping Yu", "Thomas S. Huang"], "date": "CVPR", "abstract": "Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial… ", "references": []},{"id": "8a7acaf6469c06ae5876d92f013184db5897bb13", "title": "Neuronlike adaptive elements that can solve difficult learning control problems", "authors": ["Andrew G. Barto", "Richard S. Sutton", "Charles W. Anderson"], "date": "1983", "abstract": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem.", "references": ["0f2d0e9c57d268fc1d05ce657eaf64eaaeb323c7", "b419910ab2914fcb2cf75bc66708146125ae686c", "cacfb77e3dc8faa3cae25b3128f3b3c4c44fc266", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "0f2d0e9c57d268fc1d05ce657eaf64eaaeb323c7", "0c1accd2ef7218534a1726a8de7d6e7c14271a75", "b419910ab2914fcb2cf75bc66708146125ae686c", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "0c1accd2ef7218534a1726a8de7d6e7c14271a75", "0c1accd2ef7218534a1726a8de7d6e7c14271a75"]},{"id": "3a1525d582936db10ee9a710581e5d47a0a78d19", "title": "Ovalbumin gene is split in chicken DNA", "authors": ["Richard Breathnach", "J. L. Mandel", "Pierre Chambon"], "date": "1977", "abstract": "The ovalbumin gene is split in chicken DNA. Two interruptions in the sequences coding for ovalbumin mRNA have been detected, at least one of them lying in the protein coding sequence. The unexpected gene organisation is present both in oviduct cells highly specialised in ovalbumin synthesis and in erythrocytes. ", "references": []},{"id": "2ed814434625724b08b4dd4ec664e3d533f37efd", "title": "Prediction of splice junctions in mRNA sequences.", "authors": ["Katuhisa Nakata", "Minoru Kanehisa", "Charles DeLisi"], "date": "1985", "abstract": "A general method based on the statistical technique of discriminant analysis is developed to distinguish boundaries of coding and non-coding regions in nucleic acid sequences. In particular, the method is applied to the prediction of splicing sites in messenger RNA precursors. Information used for discrimination includes consensus sequence patterns around splice junctions, free energy of snRNA and mRNA base pairing, and statistical differences between coding and non-coding regions such as… ", "references": []},{"id": "0926057bc5e996a7f071c7ef5057639d877a4b46", "title": "遺伝子の分子生物学 = Molecular biology of the gene", "authors": ["James D. Watson", "Nancy Hopkins", "田口 マミ子"], "date": "1970", "abstract": "The long-awaited Fifth Edition of James D. Watson's classic text, Molecular Biology of the Gene, has been thoroughly revised and is published to coincide with the 50th anniversary of Watson and Crick's paper on the structure of the DNA double-helix. \nThough completely updated, the new edition retains the distinctive character of earlier editions that made it the most widely used book in molecular biology. Twenty-one concise chapters, co-authored by five highly respected molecular biologists… ", "references": []},{"id": "995a3b11cc8a4751d8e167abc4aa937abc934df0", "title": "The Cascade-Correlation Learning Architecture", "authors": ["Scott E. Fahlman", "Christian Lebiere"], "date": "NIPS", "abstract": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for… ", "references": ["d4a7e54446d52f066ee692fa38d9aa972519c2f5", "cf895330739ec25aa4077ca375daa2cf3d265215", "7b3e6b20a66a83a5cd9a7fc2503c2d8feb02cc3c", "29d48d81272e498f2059190d471634087487f72d", "d4a7e54446d52f066ee692fa38d9aa972519c2f5", "29d48d81272e498f2059190d471634087487f72d", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "de8f292d456f8541b53587f46f30ea22a9b8ad52", "d4a7e54446d52f066ee692fa38d9aa972519c2f5", "7b3e6b20a66a83a5cd9a7fc2503c2d8feb02cc3c"]},{"id": "42b35bff4a707aa8f0e2f1db351b47696a3ccd1f", "title": "An Approach to Combining Explanation-based and Neural Learning Algorithms", "authors": ["Jude W. Shavlik", "Geoffrey G. Towell"], "date": "1989", "abstract": "Abstract Machine learning is an area where both symbolic and neural approaches to artificial intelligence have been heavily investigated. However, there has been little research into the synergies achievable by combining these two learning paradigms. A hybrid system that combines the symbolically-oriented explanation-based learning paradigm with the neural backpropagation algorithm is described. In the presented EBL-ANN algorithm, the initial neural network configuration is determined by the… ", "references": []},{"id": "9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "title": "Traffic sign recognition with multi-scale Convolutional Networks", "authors": ["Pierre Sermanet", "Yann LeCun"], "date": "2011", "abstract": "We apply Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the GTSRB competition.", "references": ["22fe619996b59c09cb73be40103a123d2e328111", "a0d1f3078208fb101e66d54765f86aeb8d606678", "559302b64d41868ba7cca91eaedfdb3bfada8592", "559302b64d41868ba7cca91eaedfdb3bfada8592", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "a0d1f3078208fb101e66d54765f86aeb8d606678", "a0d1f3078208fb101e66d54765f86aeb8d606678", "76b739826a1e3ad3300f58f806f0a78966e74fcc", "a0d1f3078208fb101e66d54765f86aeb8d606678"]},{"id": "ec8f2fcae2de3a904f58f6ab622a98c970dd7834", "title": "On the statistical significance of primary structural features found in DNA-protein interaction sites.", "authors": ["Gene Dykes", "Robert A Bambara", "Ray Yen-Hui Wu"], "date": "1975", "abstract": "Probabilities of occurrence for a number of the symmetries and other sequence regularities found in DNA-protein interaction site sequences have been calculated for segments of random DNA sequence. Results show that many of the symmetrical and repetitive features seen in these interaction sites are likely to have occured by chance. Other features are so unlikely to have occurred by chance that they are probably involved in the DNA-protein interaction processes. ", "references": []},{"id": "5fdadc3f070634f6cfb1be8e6bf984068fc8d676", "title": "Recognition of protein coding regions in DNA sequences.", "authors": ["James W. Fickett"], "date": "1982", "abstract": "We give a test for protein coding regions which is based on simple and universal differences between protein-coding and noncoding DNA.", "references": []},{"id": "40bb155edc56515638f001fe6c35cc44c382fb86", "title": "Learning Scalable Discriminative Dictionary with Sample Relatedness", "authors": ["Jiashi Feng", "Stefanie Jegelka", "Trevor Darrell"], "date": "2014", "abstract": "Attributes are widely used as mid-level descriptors of object properties in object recognition and retrieval. Mostly, such attributes are manually pre-defined based on domain knowledge, and their number is fixed. However, pre-defined attributes may fail to adapt to the properties of the data at hand, may not necessarily be discriminative, and/or may not generalize well. In this work, we propose a dictionary learning framework that flexibly adapts to the complexity of the given data set and… ", "references": ["f6908334853988faf987be40024ba88480170441", "f6908334853988faf987be40024ba88480170441", "39f3b1804b8df5be645a1dcb4a876e128385d9be", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "f6908334853988faf987be40024ba88480170441", "f8b5dee83543369488aff195564f6b90c6f9db49", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "be86d88ecb4192eaf512f29c461e684eb6c35257", "a1dd806b8f4f418d01960e22fb950fe7a56c18f1", "be86d88ecb4192eaf512f29c461e684eb6c35257"]},{"id": "5fc662287842e5cb2d23b5fa917354e957c573bf", "title": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids", "authors": ["Forrest N. Iandola", "Matthew W. Moskewicz", "Kurt Keutzer"], "date": "2014", "abstract": "Convolutional Neural Networks (CNNs) can provide accurate object classification. They can be extended to perform object detection by iterating over dense or selected proposed object regions. However, the runtime of such detectors scales as the total number and/or area of regions to examine per image, and training such detectors may be prohibitively slow. However, for some CNN classifier topologies, it is possible to share significant work among overlapping regions to be classified. This paper… ", "references": ["38b6540ddd5beebffd05047c78183f7575559fb2", "cec734d7097ab6b1e60d95228ffd64248eb89d66", "1109b663453e78a59e4f66446d71720ac58cec25", "82635fb63640ae95f90ee9bdc07832eb461ca881", "38b6540ddd5beebffd05047c78183f7575559fb2", "afcb15a67c8a68d01b3c9965d182e48c88502c38", "b8de958fead0d8a9619b55c7299df3257c624a96", "1109b663453e78a59e4f66446d71720ac58cec25", "68299ec9b72e3ac378a1fdc9d86039ebba203deb", "56b3ff898cadde865d20ddb4e7a33434de186794"]},{"id": "005b668ef278941f584df96f2aca1ca88f056470", "title": "Online Incremental Feature Learning with Denoising Autoencoders", "authors": ["Guanyu Zhou", "Kihyuk Sohn", "Honglak Lee"], "date": "AISTATS", "abstract": "While determining model complexity is an important problem in machine learning, many feature learning algorithms rely on cross-validation to choose an optimal number of features, which is usually challenging for online learning from a massive stream of data.", "references": ["41fef1a197fab9684a4608b725d3ae72e1ab4b39", "4fff7cd12c1731ade3ba95dad48cf799d3acc0a5", "4fff7cd12c1731ade3ba95dad48cf799d3acc0a5", "fe6b6e69766f6a062463fbd96a220f9a46a7fb69", "6d3428553abb9586f573c536ff8c1c9bae91fcfc", "60660032357a0f9d5965a76d9a70bb079c977d12", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "053912e76e50c9f923a1fc1c173f1365776060cc", "4fff7cd12c1731ade3ba95dad48cf799d3acc0a5", "dbb3342599c9b431a3152a0d5c813d3e56967a27"]},{"id": "d743430cb2329caa5d446c17fc9ec07f5e916ab0", "title": "Adaptive deconvolutional networks for mid and high level feature learning", "authors": ["Matthew D. Zeiler", "Graham W. Taylor", "Rob Fergus"], "date": "2011", "abstract": "We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling.", "references": ["8d6227e26a4bfc5482c12b8f072496ac6e97ed21", "498efaa51f5eda731dc6199c3547b9465717fa68", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "498efaa51f5eda731dc6199c3547b9465717fa68", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "1e80f755bcbf10479afd2338cec05211fdbd325c", "8d6227e26a4bfc5482c12b8f072496ac6e97ed21", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "83dfe3980b875c4e5fe6f2cb1df131cc46d175c8"]},{"id": "3ad998a9b2c071c4a1971048f8a2d754530f08e8", "title": "Convolutional feature masking for joint object and stuff segmentation", "authors": ["Jifeng Dai", "Kaiming He"], "date": "2015", "abstract": "The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs) [13]. The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a… ", "references": ["cbb19236820a96038d000dc629225d36e0b6294a", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "b8de958fead0d8a9619b55c7299df3257c624a96", "b8de958fead0d8a9619b55c7299df3257c624a96", "317aee7fc081f2b137a85c4f20129007fd8e717e", "fb444dc25bab36a8e273ed654d49e3841905e5af", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "cbb19236820a96038d000dc629225d36e0b6294a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "4328ec9d98eff5d7eb70997f76d81b27849f3220", "title": "Scalable, High-Quality Object Detection", "authors": ["Christian Szegedy", "Scott E. Reed", "Dragomir Anguelov"], "date": "2014", "abstract": "Current high-quality object detection approaches use the scheme of salience-based object proposal methods followed by post-classification using deep convolutional features.", "references": ["713f73ce5c3013d9fb796c21b981dc6629af0bd5", "f8e79ac0ea341056ef20f2616628b3e964764cfd", "67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6", "7b1e6ed85dae91843f3d986a001fb59439adbc39", "51a1a02688d00ead31b6f5177fc7a7cd2e08477f", "51a1a02688d00ead31b6f5177fc7a7cd2e08477f", "636cea485f77373ae18e32ac8e1c4e37555db5bd", "7b1e6ed85dae91843f3d986a001fb59439adbc39", "51a1a02688d00ead31b6f5177fc7a7cd2e08477f", "1109b663453e78a59e4f66446d71720ac58cec25"]},{"id": "e6f2f3a5cc7c7213835b9aede15715b5830520e1", "title": "Tensorizing Neural Networks", "authors": ["Alexander Novikov", "Dmitry Podoprikhin", "Dmitry P. Vetrov"], "date": "2015", "abstract": "Deep neural networks currently demonstrate state-of-the-art performance in several domains.", "references": ["eb42cf88027de515750f230b23b1a057dc782108", "62e348e26976c3ef77909b9af9788ebc2509009a", "e8650503ab80ad7299f0845b1843abf3a97f313a", "62e348e26976c3ef77909b9af9788ebc2509009a", "5cea23330c76994cb626df20bed31cc2588033df", "e8650503ab80ad7299f0845b1843abf3a97f313a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5cea23330c76994cb626df20bed31cc2588033df", "eb42cf88027de515750f230b23b1a057dc782108", "e7bf9803705f2eb608db1e59e5c7636a3f171916"]},{"id": "9716e4f69040f3f182714d7fb16ab9a65fb34ba6", "title": "Fully convolutional networks for semantic segmentation", "authors": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "date": "2015", "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to… ", "references": []},{"id": "71ab580b72ec5a5b6d0884a54485cd69b9af21bb", "title": "Stochastic dynamics of learning with momentum in neural networks", "authors": ["Wim Wiegerinck", "Andrzej Komoda", "Tom Heskes"], "date": "1994", "abstract": "We study on-line learning with a momentum term for nonlinear learning rules. Through introduction of auxiliary variables, we show that the learning process can be described by a Markov process. For small learning parameters eta and momentum parameters alpha close to 1, such that gamma = eta /(1- alpha )2 is finite, the time-scales for the evolution of the weights and the auxiliary variables are the same. In this case Van Kampen's expansion can be applied in a straightforward manner. We obtain… ", "references": []},{"id": "f2a0fbba89f0d18ea0abd29639d4e43babe59cf3", "title": "Training Deep and Recurrent Networks with Hessian-Free Optimization", "authors": ["James Martens", "Ilya Sutskever"], "date": "Neural Networks: Tricks of…", "abstract": "In this chapter we will first describe the basic HF approach, and then examine well-known performance-improving techniques such as preconditioning which we have found to be beneficial for neural network training, as well as others of a more heuristic nature which are harder to justify, but which we have found to work well in practice. We will also provide practical tips for creating efficient and bug-free implementations and discuss various pitfalls which may arise when designing and using an… ", "references": ["b305c18d17fd6a17e8e52a21bcd680220d322cc3", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "a98483785378bde7e2384a3035b2b501ee03654b", "b305c18d17fd6a17e8e52a21bcd680220d322cc3", "d0be39ee052d246ae99c082a565aba25b811be2d", "43c8a545f7166659e9e21c88fe234e0323855216", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "d0be39ee052d246ae99c082a565aba25b811be2d", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "a98483785378bde7e2384a3035b2b501ee03654b"]},{"id": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "title": "Understanding the difficulty of training deep feedforward neural networks", "authors": ["Xavier Glorot", "Yoshua Bengio"], "date": "AISTATS", "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures.", "references": ["ccf415df5a83b343dae261286d29a40e8b80e6c6", "05fd1da7b2e34f86ec7f010bef068717ae964332", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "843959ffdccf31c6694d135fad07425924f785b1", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "d0be39ee052d246ae99c082a565aba25b811be2d", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "ccf415df5a83b343dae261286d29a40e8b80e6c6", "d0be39ee052d246ae99c082a565aba25b811be2d"]},{"id": "202cbbf671743aefd380d2f23987bd46b9caaf97", "title": "Sparse deep belief net model for visual area V2", "authors": ["Honglak Lee", "Chaitanya Ekanadham", "Andrew Y. Ng"], "date": "NIPS", "abstract": "Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or \"deep,\" structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This… ", "references": ["43c8a545f7166659e9e21c88fe234e0323855216", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "d8d01934cb26064b253dbd0f1627519133c3df3e", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "d8d01934cb26064b253dbd0f1627519133c3df3e", "ca1d23be869380ac9e900578c601c2d1febcc0c9", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "43c8a545f7166659e9e21c88fe234e0323855216"]},{"id": "3fa5450f1c0795527939cfef5fbe3912c4dab3ab", "title": "Large-scale object recognition with CUDA-accelerated hierarchical neural networks", "authors": ["Rafael Uetz", "Sven Behnke"], "date": "2009", "abstract": "Robust recognition of arbitrary object classes in natural visual scenes is an aspiring goal with numerous practical applications, for instance, in the area of autonomous robotics and autonomous vehicles. One obstacle on the way towards human-like recognition performance is the limitation of computational power, restricting the size of the training and testing dataset as well as the complexity of the object recognition system. In this work, we present a hierarchical, locally-connected neural… ", "references": ["092c275005ae49dc1303214f6d02d134457c7053", "092c275005ae49dc1303214f6d02d134457c7053", "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2", "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2", "162d958ff885f1462aeda91cd72582323fd6a1f4", "cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb", "cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb", "f354310098e09c1e1dc88758fca36767fd9d084d", "162d958ff885f1462aeda91cd72582323fd6a1f4", "cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb"]},{"id": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd", "title": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition", "authors": ["Dan C. Ciresan", "Ueli Meier", "Jürgen Schmidhuber"], "date": "2010", "abstract": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning. ", "references": ["218fa09cfcda71e722920e9f6cbd94f8f04ffda2", "162d958ff885f1462aeda91cd72582323fd6a1f4", "70bae88192de8a63dc14be588cbd6b471297968f", "51ff037291582df4c205d4a9cbe6e7dcec8f5973", "bd14aa8bef5afc7d248a04de681242f2e64c6a1e", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "218fa09cfcda71e722920e9f6cbd94f8f04ffda2", "70bae88192de8a63dc14be588cbd6b471297968f", "70bae88192de8a63dc14be588cbd6b471297968f", "932c2a02d462abd75af018125413b1ceaa1ee3f4"]},{"id": "3efe5e292d5356fefd5b239c431ebdd1cb4fe354", "title": "Building text features for object image classification", "authors": ["Gang Wang", "Derek Hoiem", "David A. Forsyth"], "date": "CVPR", "abstract": "We introduce a text-based image feature and demonstrate that it consistently improves performance on hard object classification problems. The feature is built using an auxiliary dataset of images annotated with tags, downloaded from the Internet. We do not inspect or correct the tags and expect that they are noisy. We obtain the text feature of an unannotated image from the tags of its k-nearest neighbors in this auxiliary collection. A visual classifier presented with an object viewed under… ", "references": []},{"id": "d46fd54609e09bcd135fd28750003185a5ee4125", "title": "A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation", "authors": ["Nicolas Pinto", "David Doukhan", "David D. Cox"], "date": "2009", "abstract": "While many models of biological object recognition share a common set of “broad-stroke” properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model—e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space… ", "references": ["f5a79fd404d3417f6e10ed683460d51d391f02d4", "f5a79fd404d3417f6e10ed683460d51d391f02d4", "f5a79fd404d3417f6e10ed683460d51d391f02d4", "55fb23d8e3280298df8e275c9f1ea57869047739", "55fb23d8e3280298df8e275c9f1ea57869047739", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "85abadb689897997f1e37baa7b5fc6f7d497518b", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "8d94f68fb8f15eaca70003d26d88c326ca0407af", "85abadb689897997f1e37baa7b5fc6f7d497518b"]},{"id": "581528b2215e017eba96ef4ee16d33a74645755f", "title": "Neocognitron for handwritten digit recognition", "authors": ["Kunihiko Fukushima"], "date": "2003", "abstract": "The author previously proposeda neural network mod el neocognitron for robust visual pattern recognition.", "references": ["3cf875bd7b37e5cd0fd9aa34c318af94472907c5", "a7cb91814cee7922d821a0419b85368467eadcdf", "612b4216adb0c85761ef1c6ee7140d754fac2b71", "8009f3794e34e10e73c4629f62418e84d3234353", "612b4216adb0c85761ef1c6ee7140d754fac2b71", "33c3e56439b11e2d77d99da667ae86afbf6e1ec3", "a322bfcebc60e5d9240845d9f05c452109019484", "921380b74984255bc72385b929aa1ef9013ba97f", "921380b74984255bc72385b929aa1ef9013ba97f", "8009f3794e34e10e73c4629f62418e84d3234353"]},{"id": "5a5effa909cdeafaddbbb7855037e02f8e25d632", "title": "Caltech-256 Object Category Dataset", "authors": ["G. S. Griffin", "Alex Holub", "Pietro Perona"], "date": "2007", "abstract": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts… ", "references": []},{"id": "23694b6d61668e62bb11f17c1d75dde3b4951948", "title": "Fisher Kernels on Visual Vocabularies for Image Categorization", "authors": ["Florent Perronnin", "Christopher R. Dance"], "date": "2007", "abstract": "Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to characterize a signal with a gradient vector derived from a generative probability model and to subsequently feed this representation to a discriminative classifier. We propose to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary… ", "references": ["642e328cae81c5adb30069b680cf60ba6b475153", "3e30ea4e0ce9fce49e7bfd6196e48a94ec284dba", "ff412399f71d907cea8b15e3ecde14e249e358cb", "642e328cae81c5adb30069b680cf60ba6b475153", "c3b45689daa33519cb6bbedafa0596a2df8f7629", "a1c9e18ac7a069764a7b0f6c754cb83386eb9681", "a1c9e18ac7a069764a7b0f6c754cb83386eb9681", "a1c9e18ac7a069764a7b0f6c754cb83386eb9681", "a1c9e18ac7a069764a7b0f6c754cb83386eb9681", "a1c9e18ac7a069764a7b0f6c754cb83386eb9681"]},{"id": "aa1fa18231b8c6b35a21796af446899fc681a107", "title": "Efficient Match Kernel between Sets of Features for Visual Recognition", "authors": ["Liefeng Bo", "Cristian Sminchisescu"], "date": "NIPS", "abstract": "In visual recognition, the images are frequently modeled as unordered collections of local features (bags). We show that bag-of-words representations commonly used in conjunction with linear classifiers can be viewed as special match kernels, which count 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse, motivating research into the design of match kernels that more accurately measure the… ", "references": ["8d32093cd04d6beffb6d757f58b5ac950543ff7d", "034d0ffe364ccb0b986d515725d7b65f9817def3", "9f73f60fb07a979e53beac26c92eaaafb644a648", "d4632690af6cb5522f6895586c16cb741e0757c5", "ceb0e1a86dc35e21ce5f0524c8476f15e1b08988", "d4632690af6cb5522f6895586c16cb741e0757c5", "b6371f8c70c2684faefd99fffcc556c3a75dd7f4", "625bce34ec80d29242340400d916e799d2975430", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d"]},{"id": "a90998e0023db48b207cee3b39b0441b3935aaa7", "title": "Learning long-range vision for autonomous off-road driving", "authors": ["Raia Hadsell", "Pierre Sermanet", "Yann LeCun"], "date": "2009", "abstract": "Most vision-based approaches to mobile robotics suffer from the limitations imposed by stereo obstacle detection, which is short range and prone to failure. We present a self-supervised learning process for long-range vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing superior strategic planning. The success of the learning process is due to the self-supervised training data that are generated on every frame: robust, visually consistent… ", "references": []},{"id": "1762498d7ef09cc706b551c54ce6894a7b2ee14d", "title": "Perception of multiple objects - a connectionist approach", "authors": ["Michael C. Mozer"], "date": "Neural network modeling and…", "abstract": "\"The Perception of Multiple Objects \"describes a neurally inspired computational model of two-dimensional object recognition and spatial attention that can explain many characteristics of human visual perception. The model, called MORSEL (named for its ability to perform Multiple Object Recognition and attentional Selection), is unique in providing a broad and unified explanation for a wide range of experimental psychological data on visual perception and attention. Although it draws on… ", "references": []},{"id": "e952c51379567889753b2df005107520207ab337", "title": "Large Scale Visual Recognition", "authors": ["Jia Deng"], "date": "2012", "abstract": "Abstract : Visual recognition remains one of the grand goals of artificial intelligence research. One major challenge is endowing machines with human ability to recognize tens of thousands of categories. Moving beyond previous work that is mostly focused on hundreds of categories we make progress toward human scale visual recognition. Specifically, our contributions are as follows First, we have constructed ImageNet, a large scale image ontology. The Fall 2011 version consists of 22 thousand… ", "references": ["99cbf8f5b0be78b1cb360217350ab18448fe8961", "54d48a42a34f368240b79e7c98c7d9283f79b350", "54d2b5c64a67f65c5dd812b89e07973f97699552", "cc15498e5b3b527e59d4ec69233f84bc851c377d", "54d48a42a34f368240b79e7c98c7d9283f79b350", "54d2b5c64a67f65c5dd812b89e07973f97699552", "9800e3c3394c569be83379ee2ebe3424e09c2919", "604e97df40e7a1be1bb85ffff0d6a0b1bcd9db41", "d6a3ee89a1af7c02ca55a2b75a98aeecd8a7a0c2", "604e97df40e7a1be1bb85ffff0d6a0b1bcd9db41"]},{"id": "fcbe764317d7ab97be0713038f772afe2e4ad7f9", "title": "Visual category recognition using Spectral Regression and Kernel Discriminant Analysis", "authors": ["Muhammad Atif Tahir", "Josef Kittler", "Theo Gevers"], "date": "2009", "abstract": "Visual category recognition (VCR) is one of the most important tasks in image and video indexing. Spectral methods have recently emerged as a powerful tool for dimensionality reduction and manifold learning. Recently, Spectral Regression combined with Kernel Discriminant Analysis (SR-KDA) has been successful in many classification problems. In this paper, we adopt this solution to VCR and demonstrate its advantages over existing methods both in terms of speed and accuracy. The distinctiveness… ", "references": ["dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "1aa5a8ad5b7031ba39e1dc0537484694364a1312", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "4e782b64c750440c98e01a708c4fca1a5213a947", "6bb4de9c81b221f97d541b81134aab9029c42b91", "d9c1bbe9f84432bf827f8f62c26e15b88001a842", "c197f935f4e94ec46f6295b06d28ab9423df3b33", "dc98fcae6a44735d38600500b789bd47bc986d8c", "d9c1bbe9f84432bf827f8f62c26e15b88001a842"]},{"id": "860a9d55d87663ca88e74b3ca357396cd51733d0", "title": "A discriminatively trained, multiscale, deformable part model", "authors": ["Pedro F. Felzenszwalb", "David A. McAllester", "Deva Ramanan"], "date": "2008", "abstract": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection.", "references": ["03cf107beae8394c685cd3997c8c4740e2468a7c", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "6853fa964ce06d21e1728946b3315d538890708e", "03cf107beae8394c685cd3997c8c4740e2468a7c", "dbf98990383ee38413f55c831f89095a1b009420", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "cd9ab441df8b24f473a3635370c69620b00c1e60", "dbf98990383ee38413f55c831f89095a1b009420", "03cf107beae8394c685cd3997c8c4740e2468a7c"]},{"id": "f566b1f24e63151ddae652826638af054973a27f", "title": "Supervised Learning of Image Restoration with Convolutional Networks", "authors": ["Viren Jain", "Joseph F. Murray", "H. Sebastian Seung"], "date": "2007", "abstract": "Convolutional networks have achieved a great deal of success in high-level vision problems such as object recognition.", "references": ["a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "ed91ce023b6500c586802de7d23d8f8f01e5aa1b", "3120324069ec20eed853d3f9bbbceb32e4173b93", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "6c62fdf1e6a520d9fee8ca9981fb588d07f2c6fa", "496c3d75b81b336411e53da1ac632a8139655604", "6c62fdf1e6a520d9fee8ca9981fb588d07f2c6fa", "b9701ad65e256bd8841c4f80ced09b4ca1d5e331", "b9701ad65e256bd8841c4f80ced09b4ca1d5e331", "b9701ad65e256bd8841c4f80ced09b4ca1d5e331"]},{"id": "b2af2a2f2d1be22ebf473f7e0f501f1f5c02f222", "title": "Natural Image Denoising with Convolutional Networks", "authors": ["Viren Jain", "H. Sebastian Seung"], "date": "NIPS", "abstract": "We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models.", "references": ["bee150f94d61d09e08d858605af36d1b96d2bc52", "18099792b2c19b40e6c83e70d5b910ee0967643f", "363b56f85e12389017ba8894056a1b309e46a5f7", "10d9faa6632efbee081ed3bb1c6e64ecb885b8b5", "18099792b2c19b40e6c83e70d5b910ee0967643f", "f566b1f24e63151ddae652826638af054973a27f", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "10d9faa6632efbee081ed3bb1c6e64ecb885b8b5", "18099792b2c19b40e6c83e70d5b910ee0967643f", "18099792b2c19b40e6c83e70d5b910ee0967643f"]},{"id": "688b6fbc3c5c06e254961f70de9d855d3d008d09", "title": "Why is Real-World Visual Object Recognition Hard?", "authors": ["Nicolas Pinto", "David D. Cox", "James J. DiCarlo"], "date": "2008", "abstract": "Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show… ", "references": ["1f8e801eef2e9bb305cda7dc583280fe5573e36b", "c011d1f86ccf5641baf50394db07c560b78dfea1", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "7c7dfc71122c99959e561b62a7fc2764f13a401a", "1f8e801eef2e9bb305cda7dc583280fe5573e36b", "71e3d9fc53ba14c2feeb7390f0dc99076553b05a", "1f8e801eef2e9bb305cda7dc583280fe5573e36b", "a7886ec9d38ff28020e5e7e280ac930759a64483", "1f8e801eef2e9bb305cda7dc583280fe5573e36b", "a7886ec9d38ff28020e5e7e280ac930759a64483"]},{"id": "f9e65fcb0e04174577f211d702d3f837e3624c5b", "title": "Multiclass Object Recognition with Sparse, Localized Features", "authors": ["Jim Mutch", "David G. Lowe"], "date": "2006", "abstract": "We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition… ", "references": ["62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "7d7721e2c556e02f35654428953ed83cfa8adff8", "3452d42804cbea13b8b252d08ec249b008e8df93", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "fc3098cff5469c55c3e81dc127563afe6dbadf22", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5"]},{"id": "3bb6d5834bfb355553588e382ac5f9fa8a8d831d", "title": "Distributed GraphLab: A Framework for Machine Learning in the Cloud", "authors": ["Yucheng Low", "Joseph E. Gonzalez", "Joseph M. Hellerstein"], "date": "2012", "abstract": "While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems.", "references": ["2d867297dfe0d3ce2ed5b1d0f2dff88cac46ee94", "a0858dd960c635c3e6294908b794fae52f66e42f", "400c8f6d1bc0284b887f3f6412e07f9be70650f8", "7526048dc4c5d8a06ef0679b6ab0ede3439add56", "400c8f6d1bc0284b887f3f6412e07f9be70650f8", "be7b69e044d224f6d5c62bdc5034ff62e20b9b2b", "d92ae1f1e30328f7735d449f589ac801b9e58f8b", "32a3af340d2f095697cde2eecb0edab0821dbbdb", "337bb008eb531c4e8152d6c99423e715ce0d9b05", "7526048dc4c5d8a06ef0679b6ab0ede3439add56"]},{"id": "140d2acd4cdbc30b102dac34f4c68f279ace6a26", "title": "Object Recognition by Sequential Figure-Ground Ranking", "authors": ["João Carreira", "Fuxin Li", "Cristian Sminchisescu"], "date": "2011", "abstract": "We present an approach to visual object-class segmentation and recognition based on a pipeline that combines multiple figure-ground hypotheses with large object spatial support, generated by bottom-up computational processes that do not exploit knowledge of specific categories, and sequential categorization based on continuous estimates of the spatial overlap between the image segment hypotheses and each putative class. We differ from existing approaches not only in our seemingly unreasonable… ", "references": ["88482475e5dffab106149c7b358732e6c973e611", "0f78804622e975deff527e9401701f738c6f51a8", "7ca434a6eaf5cf4d6e86aca524e753fadb607979", "35da186e1d3e6a95cc605331a1fc1c3859685cbc", "88482475e5dffab106149c7b358732e6c973e611", "88482475e5dffab106149c7b358732e6c973e611", "8ec1f8cbe8c9da709d519f99fc670604c268742f", "4131a2862d9f926c6727da6dc75c8fda25f4a9e5", "cb2115a6765e8484830865b8ad5e6cc5dd29b48d", "35da186e1d3e6a95cc605331a1fc1c3859685cbc"]},{"id": "6e987ea8d60aace318f58c282ed30b50cfac958c", "title": "Mitigating the Paucity-of-Data Problem: Exploring the Effect of Training Corpus Size on Classifier Performance for Natural Language Processing", "authors": ["Michele Banko", "Eric Brill"], "date": "HLT", "abstract": "In this paper, we discuss experiments applying machine learning techniques to the task of confusion set disambiguation, using three orders of magnitude more training data than has previously been used for any disambiguation-in-string-context problem. In an attempt to determine when current learning methods will cease to benefit from additional training data, we analyze residual errors made by learners when issues of sparse data have been significantly mitigated. Finally, in the context of our… ", "references": ["c5207f9816ae3f9a7f902b9391009c8c42868005", "e334b3fed7561c7799674ba49efd9244a27b8fb5", "232e66748382ded9d217de554574fbf70df0f6b6", "19f37330057a76d32f32f92b11f42c53fb6c2a87", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "e334b3fed7561c7799674ba49efd9244a27b8fb5", "e6140a793a4554806eb39d15c018d8f782d2ac1e", "e0ad46338401cf4259ed33f675833446455ac764", "232e66748382ded9d217de554574fbf70df0f6b6", "e0ad46338401cf4259ed33f675833446455ac764"]},{"id": "371bd20afe88e73eafea2298b52beca7b9b5660a", "title": "Distributed Training Strategies for the Structured Perceptron", "authors": ["Ryan T. McDonald", "Keith B. Hall", "Gideon Mann"], "date": "HLT-NAACL", "abstract": "Perceptron training is widely applied in the natural language processing community for learning complex structured models. Like all structured prediction learning frameworks, the structured perceptron can be costly to train as training complexity is proportional to inference, which is frequently non-linear in example sequence length. In this paper we investigate distributed training strategies for the structured perceptron as a means to reduce training times when computing clusters are… ", "references": ["0c450531e1121cfb657be5195e310217a4675397", "0c450531e1121cfb657be5195e310217a4675397", "e46ebd37d34379415025dec3fe3438ff6f7dee8f", "e46ebd37d34379415025dec3fe3438ff6f7dee8f", "554fabcedec7ee5361661614c6b45dc5661a5f79", "93aa298b40bb3ec23c25239089284fdf61ded917", "5a7958b418bceb48a315384568091ab1898b1640", "0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff", "0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff", "5a7958b418bceb48a315384568091ab1898b1640"]},{"id": "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "title": "Convolutional face finder: a neural architecture for fast and robust face detection", "authors": ["Christophe Garcia", "Manolis Delakis"], "date": "2004", "abstract": "In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to /spl plusmn/20 degrees in image plane and turned up to /spl plusmn/60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning… ", "references": ["5ab9d1a4ac312267c4ef2f552471e138bacf68df", "ffda2eefb2458bb50f1c71d6fab6f3d6470fac9a", "ddbb6e0913ac127004be73e2d4097513a8f02d37", "e744c3eef4fbc4ac52b2458eb2d545a4432bcb86", "ebb34b75982f628f9ce5995821fff81fd967dc2d", "887567782cb859ecd339693589056903b0071353", "887567782cb859ecd339693589056903b0071353", "ebfab3c7d0fdffc3d2f570c105e7a4e43997d04c", "ebb34b75982f628f9ce5995821fff81fd967dc2d", "ebfab3c7d0fdffc3d2f570c105e7a4e43997d04c"]},{"id": "38b6540ddd5beebffd05047c78183f7575559fb2", "title": "Selective Search for Object Recognition", "authors": ["Jasper R. R. Uijlings", "Koen E. A. van de Sande", "Arnold W. M. Smeulders"], "date": "2013", "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition.", "references": ["8ec1f8cbe8c9da709d519f99fc670604c268742f", "8ec1f8cbe8c9da709d519f99fc670604c268742f", "2eb6caace8296fd4dfd4947efa4fe911c8e133b2", "0e19e69403501be0c4e9cb19bd5a11632721ba58", "5fae850e7b85e91b11a2874252ec617c3cb064c6", "4131a2862d9f926c6727da6dc75c8fda25f4a9e5", "2dd55b3bcaf50c1228569d0efe5620a910c1cd07", "5fae850e7b85e91b11a2874252ec617c3cb064c6", "4131a2862d9f926c6727da6dc75c8fda25f4a9e5", "120081166fc0d780c84e198622d638152a7cdf3e"]},{"id": "79ef1a3843a2dc01bde67c3a9a17c6deb352e285", "title": "Transforming Autoencoders", "authors": ["Geoffrey E. Hinton", "Alex Krizhevsky", "Sida Wang"], "date": "2011", "abstract": "One way to design an object recognition system is to define objects recursively in terms of their parts and the required spatial relationships between the parts and the whole. A natural way for a neural network to implement this knowledge is by using a matrix of weights to represent each part-whole relationship and a vector of neural activities to represent the pose of each part or whole relative to the viewer [10]. This leads to neural networks that can recognize objects over a wide range of… ", "references": ["0eb2e4a205a628ab059cab41d3b772f614ad29f2", "85abadb689897997f1e37baa7b5fc6f7d497518b", "272cb3db246145e13f8db4acbe1d2eae088c7677", "272cb3db246145e13f8db4acbe1d2eae088c7677", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "0eb2e4a205a628ab059cab41d3b772f614ad29f2", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "06adae42f8c6bf2cd733db8515b1711cfed67ad1", "85abadb689897997f1e37baa7b5fc6f7d497518b", "162d958ff885f1462aeda91cd72582323fd6a1f4"]},{"id": "437fce6c281031a9dc69db9c54027b531dcbeecc", "title": "Effective Training of a Neural Network Character Classifier for Word Recognition", "authors": ["Larry S. Yaeger", "Richard F. Lyon", "Brandyn J. Webb"], "date": "NIPS", "abstract": "We have combined an artificial neural network (ANN) character classifier with context-driven search over character segmentation, word segmentation, and word recognition hypotheses to provide robust recognition of hand-printed English text in new models of Apple Computer's Newton Message Pad. We present some innovations in the training and use of ANNs as character classifiers for word recognition, including normalized output error, frequency balancing, error emphasis, negative training, and… ", "references": ["7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "ff32cebbdb8a436ccd8ae797647428615ae32d74", "c8d90974c3f3b40fa05e322df2905fc16204aa56", "8314dda1ec43ce57ff877f8f02ed89acb68ca035", "7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "8f613e5481d8d567573b0ffa7b8e1c5b07d33a04", "7c6be95e99e6d5538dfa362d18ac9b7e3ecce92a", "8f613e5481d8d567573b0ffa7b8e1c5b07d33a04", "a5b3ce16666f0d9a7ac1636370a58838a7843b0f"]},{"id": "e92cc6ecabaf1fbe904b35dd6183e24da01f66ec", "title": "An improved recognition module for the identification of handwritten digits", "authors": ["Anshu Sinha"], "date": "1999", "abstract": "Optical Character Recognition (OCR), more specifically handwriting recognition, is a complex subject that has received much attention over the past 35 years. However, in spite of many years of research, optical readers that can recognize handwritten materials at a satisfactory rate are rare. Reasons for difficulty in reading hand-printed alphanumeric data include the variety of handwritten characters and the ambiguity in the distinction of characters. This paper proposes a new and improved… ", "references": ["b9eb182fcaf3cb2b18540930509ff1ed3ad3c62a", "f88d8eaaaa6ee02d59c834370b1c8621f1bc6034", "aac6e412e5e79970dd331a572c29960f10165a8b", "9067a6a5af70d1b8507d85ca888d75f4caec3fc5", "aac6e412e5e79970dd331a572c29960f10165a8b", "b9eb182fcaf3cb2b18540930509ff1ed3ad3c62a", "3cb00eaa2b49771ee2b86771f64a7bdea4bd4bed", "c45dda640007a181aa88ba3ceffe124209dc4673", "6d6c07e21a649983c84f2643447b9d99769bb98b", "b9eb182fcaf3cb2b18540930509ff1ed3ad3c62a"]},{"id": "37807e97c624fb846df7e559553b32539ba2ea5d", "title": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks", "authors": ["Kurt Hornik", "Maxwell B. Stinchcombe", "Halbert White"], "date": "1990", "abstract": "A shoulder strap retainer having a base to be positioned on the exterior shoulder portion of a garment with securing means attached to the undersurface of the base for removably securing the base to the exterior shoulder portion of the garment.", "references": ["386cbc45ceb59a7abb844b5078e5c944f17723b4", "47ecef2302b6d2d64fcd559399cef1b923748cf9", "47ecef2302b6d2d64fcd559399cef1b923748cf9", "47ecef2302b6d2d64fcd559399cef1b923748cf9", "47ecef2302b6d2d64fcd559399cef1b923748cf9"]},{"id": "7d1e282f1613c161585dfc9dd077282cb37b5b89", "title": "Multiframe deep neural networks for acoustic modeling", "authors": ["Vincent Vanhoucke", "Matthieu Devin", "Georg Heigold"], "date": "2013", "abstract": "Deep neural networks have been shown to perform very well as acoustic models for automatic speech recognition. Compared to Gaussian mixtures however, they tend to be very expensive computationally, making them challenging to use in real-time applications. One key advantage of such neural networks is their ability to learn from very long observation windows going up to 400 ms. Given this very long temporal context, it is tempting to wonder whether one can run neural networks at a lower frame… ", "references": ["2dce4204c5d247f3ceef8d85423b7b0600a4a3ae", "3127190433230b3dc1abd0680bb58dced4bcd90e", "57eb04cf0dc27a47cbd373bc8e1d44624769ad2f", "3127190433230b3dc1abd0680bb58dced4bcd90e", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "473f0739666af2791ad6592822118240ed968b70", "473f0739666af2791ad6592822118240ed968b70", "fbeaa499e10e98515f7e1c4ad89165e8c0677427", "fbeaa499e10e98515f7e1c4ad89165e8c0677427"]},{"id": "31f04f8f83365fabf7ba9c9be1179c0da6815128", "title": "Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields", "authors": ["Karol Gregor", "Yann LeCun"], "date": "2010", "abstract": "We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images.", "references": ["2af18c7b3495955d1ffa4f1b4c4ae5ea786cdedb", "583cd7d794ea4d86d4125eade3939d6bfc220998", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "583cd7d794ea4d86d4125eade3939d6bfc220998", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "e5f2b2634cdfd43f820b10edde2cc425f48cb788", "7c032d555a9d7096f7bb88441f10e33d3302d5be", "2af18c7b3495955d1ffa4f1b4c4ae5ea786cdedb", "5127759530ce213f488af2859190697770f557f3", "498efaa51f5eda731dc6199c3547b9465717fa68"]},{"id": "661e5d7bd7454dc03a96f61fcbc7329ac13ed56c", "title": "An offline cursive handwritten word recognition system", "authors": ["Yong Haur Tay", "P.-M. Lallican", "Silvia Kneer"], "date": "2001", "abstract": "This paper describes an offline cursive handwritten word recognition system that combines hidden Markov models (HMM) and neural networks (NN.", "references": ["67319d0d92885e0dc24b363a54d512004659ec66"]},{"id": "265069b3670930fd884b02062d7e7b79ff2a49d5", "title": "On Random Weights and Unsupervised Feature Learning", "authors": ["Andrew M. Saxe", "Pang Wei Koh", "Andrew Y. Ng"], "date": "ICML", "abstract": "Recently two anomalous results in the literature have shown that certain feature learning architectures can yield useful features for object recognition tasks even with untrained, random weights. In this paper we pose the question: why do random weights sometimes do so well? Our answer is that certain convolutional pooling architectures can be inherently frequency selective and translation invariant, even with random weights. Based on this we demonstrate the viability of extremely fast… ", "references": ["1f88427d7aa8225e47f946ac41a0667d7b69ac52", "5d90f06bb70a0a3dced62413346235c02b1aa086", "79d1b330f0ef51f63ecb9b291dd5a05de5a858c0", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "00cd1dab559a9671b692f39f14c1573ab2d1416b", "05cc38e249a6f642363b5a5cbd71cda67cea5893", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "05cc38e249a6f642363b5a5cbd71cda67cea5893", "5a2668bf420d8509a4dfa28e1cdcdac14c649975"]},{"id": "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "title": "The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization", "authors": ["Adam Coates", "Andrew Y. Ng"], "date": "ICML", "abstract": "While vector quantization (VQ) has been applied widely to generate features for visual recognition problems, much recent work has focused on more powerful methods. In particular, sparse coding has emerged as a strong alternative to traditional VQ approaches and has been shown to achieve consistently higher performance on benchmark datasets. Both approaches can be split into a training phase, where the system learns a dictionary of basis functions, and an encoding phase, where the dictionary is… ", "references": ["bea5780d621e669e8069f05d0f2fc0db9df4b50f", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec", "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec", "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec", "e8f811399746c059bf4d4c3d43334045e0222209", "e8f811399746c059bf4d4c3d43334045e0222209", "2f7713dcc35e7c05becf3be5522f36c9546b0364"]},{"id": "5c0fe8ba39bda83d6ca3b9705a780809d52a67b4", "title": "Unsupervised and Transfer Learning Challenge: a Deep Learning Approach", "authors": ["Grégoire Mesnil", "Yann Dauphin", "James Bergstra"], "date": "ICML Unsupervised and…", "abstract": "Learning good representations from a large set of unlabeled data is a particularly challenging task. Recent work (see Bengio (2009) for a review) shows that training deep architectures is a good way to extract such representations, by extracting and disentangling gradually higher-level factors of variation characterizing the input distribution. In this paper, we describe different kinds of layers we trained for learning representations in the setting of the Unsupervised and Transfer Learning… ", "references": ["3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "e2c04849a3802715d5a9d89179c9f161014d6c2a", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "e2c04849a3802715d5a9d89179c9f161014d6c2a", "843959ffdccf31c6694d135fad07425924f785b1", "162d958ff885f1462aeda91cd72582323fd6a1f4", "e2c04849a3802715d5a9d89179c9f161014d6c2a", "162d958ff885f1462aeda91cd72582323fd6a1f4", "355d44f53428b1ac4fb2ab468d593c720640e5bd"]},{"id": "cd0568b4faa03910ae3c07d00c627666f404305d", "title": "Continuous speech recognition using multilayer perceptrons with hidden Markov models", "authors": ["Nelson Morgan", "Hervé Bourlard"], "date": "1990", "abstract": "A phoneme based, speaker-dependent continuous-speech recognition system embedding a multilayer perceptron (MLP) (i.e. a feedforward artificial neural network) into a hidden Markov model (HMM) approach is described. Contextual information from a sliding window on the input frames is used to improve frame or phoneme classification performance over the corresponding performance for simple maximum-likelihood probabilities, or even maximum a posteriori (MAP) probabilities which are estimated without… ", "references": ["cbd05f1b1f325e4514a3ef20563d9e00a438ec58", "6f3175b3930d0c71495a52a7bccb3889e5f33520", "72820bb2e725b73af7e2cd848ef0b103d15cdc39", "280d0643b26a10ecc302d986514a154374a8fda4", "72820bb2e725b73af7e2cd848ef0b103d15cdc39", "ef5d8931810e3f1e943fd3064aada40c3bcd87ef", "9bc28ae97fa99fc2463b6e8a107c01ff84db9fdd", "6bfdc1a312c8177970b85bceb63427a802cec6b6", "a55d31784aca11871985096644a025f036633569", "8f93ee2e335d83ee32787693861f4d48e78e6786"]},{"id": "f8c8619ea7d68e604e40b814b40c72888a755e95", "title": "Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives", "authors": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent"], "date": "2012", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although domain knowledge can be used to help design representations, learning can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms. This paper reviews recent work in the area of… ", "references": ["41fef1a197fab9684a4608b725d3ae72e1ab4b39", "dc1d132c79a72dba386dc47750a49b0c29b54568", "f93844c68d96f2f01da973b2ed3c236c8a369e57", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "843959ffdccf31c6694d135fad07425924f785b1", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "dc1d132c79a72dba386dc47750a49b0c29b54568", "182015c5edff1956cbafbcb3e7bbe294aa54f9fc"]},{"id": "522e90b9fccfd3c1c0603359eb04757d770c1ab5", "title": "Practical Recommendations for Gradient-Based Training of Deep Architectures", "authors": ["Yoshua Bengio"], "date": "Neural Networks: Tricks of…", "abstract": "Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when… ", "references": ["05fd1da7b2e34f86ec7f010bef068717ae964332", "43c8a545f7166659e9e21c88fe234e0323855216", "01373261bfbba42a806d21d5b759d5a27f509892", "03911c85305d42aa2eeb02be82ef6fb7da644dd0", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "1a700d6cce09f1711cdaf8f5f21b6ab2a8ce7bae", "053912e76e50c9f923a1fc1c173f1365776060cc", "43c8a545f7166659e9e21c88fe234e0323855216", "43c8a545f7166659e9e21c88fe234e0323855216", "01373261bfbba42a806d21d5b759d5a27f509892"]},{"id": "d0965d8f9842f2db960b36b528107ca362c00d1a", "title": "Better Mixing via Deep Representations", "authors": ["Yoshua Bengio", "Grégoire Mesnil", "Salah Rifai"], "date": "ICML", "abstract": "It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we… ", "references": ["8a9a10170ee907acb3e582742bec5fa09116f302", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "8a9a10170ee907acb3e582742bec5fa09116f302", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "210da45e57f86a50c04bdd7b37d498c8ecc288da", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f", "aaaea06da21f22221d5fbfd61bb3a02439f0fe02", "3137bc367c61c0e507a5e3c1f8caeb26f292d79f"]},{"id": "26cf16673269bdb0979bc601a340083448e5ad44", "title": "Speech recognitionwith segmental conditional random fields: A summary of the JHU CLSP 2010 Summer Workshop", "authors": ["Geoffrey Zweig", "Patrick Nguyen", "Justine T. Kao"], "date": "2011", "abstract": "This paper summarizes the 2010 CLSP Summer Workshop on speech recognition at Johns Hopkins University. The key theme of the workshop was to improve on state-of-the-art speech recognition systems by using Segmental Conditional Random Fields (SCRFs) to integrate multiple types of information. This approach uses a state-of-the-art baseline as a springboard from which to add a suite of novel features including ones derived from acoustic templates, deep neural net phoneme detections, duration models… ", "references": ["e3383014825fee865bd6e4950397db5b024e0328", "c13f4eedbe98bca38c22e0d67f514a987b2b83f4", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "348a4b11c2543072471747533553cbb579181d2b", "e3383014825fee865bd6e4950397db5b024e0328", "348a4b11c2543072471747533553cbb579181d2b", "47e4346e697c2c4054d2439d173dba2e97ac4327", "47e4346e697c2c4054d2439d173dba2e97ac4327", "e3383014825fee865bd6e4950397db5b024e0328"]},{"id": "9287071f38b6700794a71cd7ba210a1a8cd21a6d", "title": "Is Early Vision Optimized for Extracting Higher-order Dependencies?", "authors": ["Yan Karklin", "Michael S. Lewicki"], "date": "NIPS", "abstract": "Linear implementations of the efficient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1,2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive fields in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6… ", "references": ["ca1d23be869380ac9e900578c601c2d1febcc0c9", "a87e0d75a8c17e464cf8e95a0466533e14b97c5e", "a87e0d75a8c17e464cf8e95a0466533e14b97c5e", "ca1d23be869380ac9e900578c601c2d1febcc0c9", "a87e0d75a8c17e464cf8e95a0466533e14b97c5e", "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1", "18cb1401de291dc0f8be199f8d21ce1e17ddd091", "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1", "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1", "18cb1401de291dc0f8be199f8d21ce1e17ddd091"]},{"id": "c1ba0d38d855af0f4a2c2cf0f4fa48e4477fc4ec", "title": "Efficient Highly Over-Complete Sparse Coding Using a Mixture Model", "authors": ["Jianchao Yang", "Kai Yu", "Thomas S. Huang"], "date": "ECCV", "abstract": "Sparse coding of sensory data has recently attracted notable attention in research of learning useful features from the unlabeled data. Empirical studies show that mapping the data into a significantly higher-dimensional space with sparse coding can lead to superior classification performance. However, computationally it is challenging to learn a set of highly over-complete dictionary bases and to encode the test data with the learned bases. In this paper, we describe a mixture sparse coding… ", "references": ["c65be1f97642510843667d36e399de58837d3419", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "d5eec41043d91964879c4c745c7165f823967f29", "2f7713dcc35e7c05becf3be5522f36c9546b0364", "c65be1f97642510843667d36e399de58837d3419", "2f7713dcc35e7c05becf3be5522f36c9546b0364", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "88ccd21990de84d84e6090e4091d920366075b55"]},{"id": "f4a25e36050132f2e695d936e15e88387e742ee1", "title": "OpenVIDIA: parallel GPU computer vision", "authors": ["James Fung", "Steve Mann"], "date": "MULTIMEDIA '05", "abstract": "Graphics and vision are approximate inverses of each other: ordinarily Graphics Processing Units (GPUs) are used to convert \"numbers into pictures\" (i.e. computer graphics). In this paper, we propose using GPUs in approximately the reverse way: to assist in \"converting pictures into numbers\" (i.e. computer vision). The OpenVIDIA project uses single or multiple graphics cards to accelerate image analysis and computer vision. It is a library and API aimed at providing a graphics hardware… ", "references": []},{"id": "d28328674da2c85ad9d2fcb9fdd262721f143393", "title": "Fast Image Segmentation and Smoothing Using Commodity Graphics Hardware", "authors": ["Ruigang Yang", "Greg Welch"], "date": "2002", "abstract": "Abstract We present a novel use of commodity graphics hardware to perform real-time image segmentation and image morphology operations. Our preliminary results show a performance increase of over 30% using an nVidia GeForce4 when compared to an implementation using Intel MMX optimized code on a 2.2 Ghz Intel P4 CPU. ", "references": []},{"id": "b3275d20929462b051cc99a47383af9c7ca0ac0e", "title": "Towards deeper understanding: Deep convex networks for semantic utterance classification", "authors": ["Gökhan Tür", "Li Deng", "Xiaodong He"], "date": "2012", "abstract": "Following the recent advances in deep learning techniques, in this paper, we present the application of special type of deep architecture - deep convex networks (DCNs) - for semantic utterance classification (SUC.", "references": ["1fbb80e8033cf3f2fa3fad50d4ec8a330157da29", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "e3c1bf806c325f306e5084c3bd332b83d2077e2a", "4ca91c58eb35395e4a5fb5ffaee925e7c4f1ae81", "e3c1bf806c325f306e5084c3bd332b83d2077e2a", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "93ba5b6a6d983c618d324e05086698ba43bf127a", "5352b7ca90cbe4938f8e71a25d49517e7f94670a", "5352b7ca90cbe4938f8e71a25d49517e7f94670a", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656"]},{"id": "e0acc24337501da9a68f03e8a9a5b42d52ffa927", "title": "Boosting attribute and phone estimation accuracies with deep neural networks for detection-based speech recognition", "authors": ["Dong Yu", "Sabato Marco Siniscalchi", "Chin-Hui Lee"], "date": "2012", "abstract": "Generation of high-precision sub-phonetic attribute (also known as phonological features) and phone lattices is a key frontend component for detection-based bottom-up speech recognition. In this paper we employ deep neural networks (DNNs) to improve detection accuracy over conventional shallow MLPs (multi-layer perceptrons) with one hidden layer. A range of DNN architectures with five to seven hidden layers and up to 2048 hidden units per layer have been explored. Training on the SI84 and… ", "references": ["7ed02cd4781c7106d44f26845e55a39b43f5706d", "3229759c9a8e46f6958d66f1dc374cdacbf15690", "f4abbaee0ae75ebbc3c0519f0a311a097f4e2faf", "e91d52b091c747306a9d5d26e05810c6671e0214", "6658bbf68995731b2083195054ff45b4eca38b3a", "73ab03af23f606d8bdfeccc3e9c9c0883b5f40b3", "008e9e2d3908c964d5b1c408c478215709dbea10", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "7ed02cd4781c7106d44f26845e55a39b43f5706d", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1"]},{"id": "561a39360b59879f73a7503675e304f856ca9a14", "title": "Fast and Efficient Dense Variational Stereo on GPU", "authors": ["Julien Mairal", "Renaud Keriven", "Alexandre Chariot"], "date": "2006", "abstract": "Thanks to their high performance and programmability, the latest graphics cards can now be used for scientific purpose. They are indeed very efficient parallel single instruction multiple data (SIMD) machines. This new trend is called general purpose computation on graphics processing unit (GPGPU). Regarding the stereo problem, variational methods based on deformable models provide dense, smooth and accurate results. Nevertheless, they prove to be slower than usual disparity-based approaches… ", "references": ["af2acbd4443760a592025d777226076c63f63a77", "425adb845f5e45c01b925b93f49d131cdac4d4ba", "3d19343a69ffef3ec62a1a23c57380f5273c9cca", "825e86de55569cc96e76d1566c0a085ce00ff547", "825e86de55569cc96e76d1566c0a085ce00ff547", "85e9f361e06e5cbcc620ef78ceed3b49b92afc7e", "825e86de55569cc96e76d1566c0a085ce00ff547", "7b282b6d0d72d47adcb4d2b8b813debf17fda45a", "85e9f361e06e5cbcc620ef78ceed3b49b92afc7e", "825e86de55569cc96e76d1566c0a085ce00ff547"]},{"id": "a205801dbd56f93f3b98fd6d9a535ed1961806fa", "title": "TeraFLOP computing on a desktop PC with GPUs for 3D CFD", "authors": ["Jonas Tölke", "Manfred Krafczyk"], "date": "2008", "abstract": "A very efficient implementation of a lattice Boltzmann (LB) kernel in 3D on a graphical processing unit using the compute unified device architecture interface developed by nVIDIA is presented. By exploiting the explicit parallelism offered by the graphics hardware, we obtain an efficiency gain of up to two orders of magnitude with respect to the computational performance of a PC. A non-trivial example shows the performance of the LB implementation, which is based on a D3Q13 model that is… ", "references": ["2d8c7a0b87eb5023f826268d896abcb7704bde84", "4cbc419a4d0a5b55d3d302d648c0827b5d135f4b", "0510f734fb1f84c61922d518988d0ccb2409fe04", "b5aa2c16c495e44a3a69e246e17eb155a0e9acc4", "0a5c3cfa46f51d2da153daa89db9e01a2de7ccfc", "18ab79e8dae0327dcdd06fbfc23c32382a2428b7", "2d8c7a0b87eb5023f826268d896abcb7704bde84", "b5aa2c16c495e44a3a69e246e17eb155a0e9acc4", "f51d2cdb2cc5d7d9683f712be94cb1d7f4c18200", "18ab79e8dae0327dcdd06fbfc23c32382a2428b7"]},{"id": "8770b4a5ca7734c88e5755f9558f79e93229c023", "title": "Boosted MMI for model and feature-space discriminative training", "authors": ["Daniel Povey", "Dimitri Kanevsky", "Karthik Visweswariah"], "date": "2008", "abstract": "We present a modified form of the maximum mutual information (MMI) objective function which gives improved results for discriminative training. The modification consists of boosting the likelihoods of paths in the denominator lattice that have a higher phone error relative to the correct transcript, by using the same phone accuracy function that is used in Minimum Phone Error (MPE) training. We combine this with another improvement to our implementation of the Extended Baum-Welch update… ", "references": ["e35a50c251edc9e1ebcd919c09413661420c7ccf", "0687573a482d84385ddd55e708e240f3e303fab9", "f151b800104bc5945b33520845089b727c58a7d8", "e084bbb9cbbce7c0d282df263cf70cba4042f067", "0687573a482d84385ddd55e708e240f3e303fab9", "f151b800104bc5945b33520845089b727c58a7d8", "f30da12f78987cd18e007a1a5312605081c5ac62", "e084bbb9cbbce7c0d282df263cf70cba4042f067", "547fbce9f33d6944970a2e523f713782f2f7332c", "f151b800104bc5945b33520845089b727c58a7d8"]},{"id": "b5410a46dd09267b5d90eab26db897e9ab7a9e70", "title": "Recognition using regions", "authors": ["Chunhui Gu", "Joseph J. Lim", "Jitendra Malik"], "date": "CVPR", "abstract": "This paper presents a unified framework for object detection, segmentation, and classification using regions. Region features are appealing in this context because: (1) they encode shape and scale information of objects naturally; (2) they are only mildly affected by background clutter. Regions have not been popular as features due to their sensitivity to segmentation errors. In this paper, we start by producing a robust bag of overlaid regions for each image using Arbeldez et al., CVPR 2009… ", "references": []},{"id": "d02f832e2848a40d3ae9b62b7a245eb918a5b667", "title": "Automated Empirical Optimization", "authors": ["Rajesh K. Karmani", "Gul A. Agha", "Richard W. Vuduc"], "date": "Encyclopedia of Parallel…", "abstract": null, "references": []},{"id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "title": "Regression Shrinkage and Selection via the Lasso", "authors": ["Robert Tibshirani"], "date": "1996", "abstract": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable… ", "references": ["4aad883fa1f7c8a55912534804a96c6cc0f39b00", "2b20b4095236f461ab067d0c23bc9653d3d1f9c0", "4aad883fa1f7c8a55912534804a96c6cc0f39b00", "12e5939a80ec59f81214d7e729c577c350af9501", "040cf85e53e48aab1512ed8dd02d994eea0e4bcc", "cae7f8b4ee917793ad310ac571f1ed181f41c7c0", "2b20b4095236f461ab067d0c23bc9653d3d1f9c0", "b7134309fc923d0395edb6c6c7d6695c82f053fc", "2b20b4095236f461ab067d0c23bc9653d3d1f9c0", "cae7f8b4ee917793ad310ac571f1ed181f41c7c0"]},{"id": "2e2089ae76fe914706e6fa90081a79c8fe01611e", "title": "Practical Bayesian Optimization of Machine Learning Algorithms", "authors": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "date": "2012", "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters.", "references": ["cd5a26b89f0799db1cbc1dff5607cb6815739fe7", "182015c5edff1956cbafbcb3e7bbe294aa54f9fc", "c67ff56d0f5d252e80b48de917e3fc71e47d13a0", "e37bb83b713e3b83706dfc0ee807bb497ed3873f", "c67ff56d0f5d252e80b48de917e3fc71e47d13a0", "81f9e2051a47ab85e90f5f19256d2b113aea4f9b", "81f9e2051a47ab85e90f5f19256d2b113aea4f9b", "5d90f06bb70a0a3dced62413346235c02b1aa086", "cd5a26b89f0799db1cbc1dff5607cb6815739fe7", "cd5a26b89f0799db1cbc1dff5607cb6815739fe7"]},{"id": "37e41557932cc0035eab23fd767bde68f6475c3a", "title": "Segmentation as selective search for object recognition", "authors": ["Koen E. A. van de Sande", "Jasper R. R. Uijlings", "Arnold W. M. Smeulders"], "date": "2011", "abstract": "For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised… ", "references": ["562b04ee16f27c47625b4989ab011510518d0b0a", "2dd55b3bcaf50c1228569d0efe5620a910c1cd07", "82635fb63640ae95f90ee9bdc07832eb461ca881", "b5410a46dd09267b5d90eab26db897e9ab7a9e70", "4131a2862d9f926c6727da6dc75c8fda25f4a9e5", "5fae850e7b85e91b11a2874252ec617c3cb064c6", "2dd55b3bcaf50c1228569d0efe5620a910c1cd07", "2dd55b3bcaf50c1228569d0efe5620a910c1cd07", "d4b7c413586aab14d86ee0e4f71b1f791a3ebc10", "6ad32b70ee21b6fc16ff4caf7b4ada2aaf13cabc"]},{"id": "e79272fe3d65197100eae8be9fec6469107969ae", "title": "Object Detection with Discriminatively Trained Part Based Models", "authors": ["Pedro F. Felzenszwalb", "Ross B. Girshick", "Deva Ramanan"], "date": "2009", "abstract": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin… ", "references": ["62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "f465d81e26e17298c67cfe3ad6155bc769c32f01", "00a9fd0d4333fe55e2bc230947fad895a18c6d2b", "162065fb9de1928f7abd593ee9a1b7d41b5a4310", "162065fb9de1928f7abd593ee9a1b7d41b5a4310", "860a9d55d87663ca88e74b3ca357396cd51733d0", "162065fb9de1928f7abd593ee9a1b7d41b5a4310", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "162065fb9de1928f7abd593ee9a1b7d41b5a4310", "2c3cac0f568ae9261ff9c80eeda55a13e83ae7fb"]},{"id": "54b224478a63e33441c651175c522f3702062fc4", "title": "Beyond sliding windows: Object localization by efficient subwindow search", "authors": ["Christoph H. Lampert", "Matthew B. Blaschko", "Thomas Hofmann"], "date": "2008", "abstract": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization… ", "references": ["d052c54b633a02fd54835d657ce8914b81948e2b", "02bc39849a15c84c6ebea35c7923d1824981cb7c", "b82d251ed367593366680acebc81fdb070b04a18", "b98c68f01d84ac07dc7fc51af782018070da748f", "d052c54b633a02fd54835d657ce8914b81948e2b", "0b43f48c933c615e305ebd25521635cff8df4707", "b98c68f01d84ac07dc7fc51af782018070da748f", "1c41c1f86b92a8c011e0324d90624d539a849b8b", "1c41c1f86b92a8c011e0324d90624d539a849b8b", "f9e65fcb0e04174577f211d702d3f837e3624c5b"]},{"id": "648a65e31a56f54dd906d40872b1a8ac78309b0b", "title": "Object-Class Segmentation using Deep Convolutional Neural Networks", "authors": ["Hannes Schulz", "Sven Behnke"], "date": "2011", "abstract": "After successes at image classification, segmentation is the next step towards image understanding for neural networks. We propose a convolutional network architecture that outperforms current methods on the challenging INRIA-Graz02 dataset with regards to accuracy and speed. ", "references": ["ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "49957368eceaa751c0b9c49251512ca6a8800cff", "5d21006fa32ff69f6b0a646f26ce0db84f2f4d33", "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "f354310098e09c1e1dc88758fca36767fd9d084d", "49957368eceaa751c0b9c49251512ca6a8800cff", "a8f6abd8794269194119f6e129012b8d4c94371d", "82b9099ddf092463f497bd48bb112c46ca52c4d1"]},{"id": "82635fb63640ae95f90ee9bdc07832eb461ca881", "title": "The Pascal Visual Object Classes (VOC) Challenge", "authors": ["Mark Everingham", "Luc Van Gool", "Andrew Zisserman"], "date": "2009", "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures.", "references": ["66fc74fa5dc8c4fc9192b97019a0e68b217de968", "23694b6d61668e62bb11f17c1d75dde3b4951948", "6a2ed19ac684022aa3186887cd4893484ab8f80c", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "c3d937494053ace1fd3fb46f4fe530947502159a", "7c6f0c1917bb0f7e23c4c35b553045fa39663211", "0b43f48c933c615e305ebd25521635cff8df4707", "530b99c3819bd4c4f1884fa89c9a0d6e024156bd", "eb0ab2ee44ebf9c08bd2bf478c7444adfdcb2bd7", "6a2ed19ac684022aa3186887cd4893484ab8f80c"]},{"id": "120081166fc0d780c84e198622d638152a7cdf3e", "title": "Latent hierarchical structural learning for object detection", "authors": ["Long Zhu", "Yuanhao Chen", "William T. Freeman"], "date": "2010", "abstract": "We present a latent hierarchical structural learning method for object detection.", "references": ["af797220d537f9601cea3ee6a55cfe4637a23344", "af797220d537f9601cea3ee6a55cfe4637a23344", "af797220d537f9601cea3ee6a55cfe4637a23344", "8483da1bc3302c34b437baf8e329390da8f2c9bf", "5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398", "1c43cb40404272d3b3577ffcab7473af1645f011", "78662a293888d7e982061d16f6a71d0223420fad", "af797220d537f9601cea3ee6a55cfe4637a23344", "47dfddafed43bc5afef93ac90ea3376a02046151", "93aa298b40bb3ec23c25239089284fdf61ded917"]},{"id": "0f602c33b1762d57223c9f9656579f9d1dc2e30a", "title": "Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts", "authors": ["Sanja Fidler", "Aleš Leonardis"], "date": "2007", "abstract": "This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of object categories. Inspired by the principles of efficient indexing (bottom-up,), robust matching (top-down,), and ideas of compositionality, our approach learns a hierarchy of spatially flexible compositions, i.e. parts, in an unsupervised, statistics-driven manner. Starting with simple, frequent features, we learn the… ", "references": ["a5331349557fababfac48d47e49b44583e3bd5f6", "cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb", "a5331349557fababfac48d47e49b44583e3bd5f6", "a5331349557fababfac48d47e49b44583e3bd5f6", "b4bf169531bb0ecc0018e6c32822030550211c2e", "bb35ae8a50de54c9ca29fbdf1ea2fbbb4e8c4662", "568c2db94c3c4da7321f83e7713bd51daf9db7b2", "61b933b8ef5b10ae4f6491a89f89972322534cf0", "b4bf169531bb0ecc0018e6c32822030550211c2e", "a5331349557fababfac48d47e49b44583e3bd5f6"]},{"id": "5d777a71d36ec929f70c2f7b5eca47456c34a4e0", "title": "Inference and Learning with Hierarchical Shape Models", "authors": ["Iasonas Kokkinos", "Alan L. Yuille"], "date": "2010", "abstract": "In this work we introduce a hierarchical representation for object detection. We represent an object in terms of parts composed of contours corresponding to object boundaries and symmetry axes; these are in turn related to edge and ridge features that are extracted from the image.We propose a coarse-to-fine algorithm for efficient detection which exploits the hierarchical nature of the model. This provides a tractable framework to combine bottom-up and top-down computation. We learn our models… ", "references": ["36f762acb212e2e583a2224bfd99b3a89120ce09", "162065fb9de1928f7abd593ee9a1b7d41b5a4310", "162065fb9de1928f7abd593ee9a1b7d41b5a4310", "d7362fa8358e4ffef5b8656314eb8c5d5ef66c2a", "3d081b80b1850df9b1e382f97a7a244890d6485e", "a5331349557fababfac48d47e49b44583e3bd5f6", "3d081b80b1850df9b1e382f97a7a244890d6485e", "d47e172c013fe7ea1515a58f3debc26f9d5816e0", "d7362fa8358e4ffef5b8656314eb8c5d5ef66c2a", "8c22cfce73d9f42aeda4019bdeeb555392072141"]},{"id": "e0f49caabbf79ffda35432219bb0ec9b41753dff", "title": "Multimodal semi-supervised learning for image classification", "authors": ["Matthieu Guillaumin", "Jakob J. Verbeek", "Cordelia Schmid"], "date": "2010", "abstract": "In image categorization the goal is to decide if an image belongs to a certain category or not.", "references": ["cd32111096e78055e935344142a9ac66daa9a55f", "e2de29049d62de925cf709024b92774cd82b0a5a", "23c6cb079b9ec45ae462cae743e01a1185fc4c2c", "3efe5e292d5356fefd5b239c431ebdd1cb4fe354", "3efe5e292d5356fefd5b239c431ebdd1cb4fe354", "e2de29049d62de925cf709024b92774cd82b0a5a", "3efe5e292d5356fefd5b239c431ebdd1cb4fe354", "e2de29049d62de925cf709024b92774cd82b0a5a", "9465208bf0524d3a90b99ab88a0086af09121233", "40903135e329a09c4530bb068130ca9bba02b7a7"]},{"id": "bff05119cd30c2c61323861a5e2a28094388427f", "title": "Hierarchical Regularization Cascade for Joint Learning", "authors": ["Alon Zweig", "Daphna Weinshall"], "date": "ICML", "abstract": "As the sheer volume of available benchmark datasets increases, the problem of joint learning of classifiers and knowledge-transfer between classifiers, becomes more and more relevant. We present a hierarchical approach which exploits information sharing among different classification tasks, in multitask and multi-class settings. It engages a top-down iterative method, which begins by posing an optimization problem with an incentive for large scale sharing among all classes. This incentive to… ", "references": ["5233fcac30a29b49a92ccd10788102ecff5dce8a", "dde10a037835380cf23d0156550bb4ec830b0eb2", "047eb665389b5fbf49a83b3f7e39ff28f536a284", "e5a97aad11c105d7c2defc90b4ea9e5fd6f736b5", "dde10a037835380cf23d0156550bb4ec830b0eb2", "5233fcac30a29b49a92ccd10788102ecff5dce8a", "5233fcac30a29b49a92ccd10788102ecff5dce8a", "d2d35fc47bfcd9bbcdf1905b23be6e5dcee05e9c", "a3ea706f6604a1e6e87c33d7a3b4b97b1bb338ef", "f5ec0bebd6ce4b77897ad2c6e07a15eaa9146c90"]},{"id": "89c808af926ecb20870b2521fbaa7dcbb85be106", "title": "Multi-Task Learning for Classification with Dirichlet Process Priors", "authors": ["Ya Xue", "Xuejun Liao", "Balaji Krishnapuram"], "date": "2007", "abstract": "Consider the problem of learning logistic-regression models for multiple classification tasks, where the training data set for each task is not drawn from the same statistical distribution. In such a multi-task learning (MTL) scenario, it is necessary to identify groups of similar tasks that should be learned jointly. Relying on a Dirichlet process (DP) based statistical model to learn the extent of similarity between classification tasks, we develop computationally efficient algorithms for two… ", "references": ["5fd2b7cb05b30c59bbc13a5f2b7ec68555fba261", "6f17768a9fe231a2fd38708be90f98db3890c986", "45db16309fbf7e9fc8907047e7e1a9933c4e1b85", "5fd2b7cb05b30c59bbc13a5f2b7ec68555fba261", "6f17768a9fe231a2fd38708be90f98db3890c986", "45db16309fbf7e9fc8907047e7e1a9933c4e1b85", "e1ba33f56f75c40aed15538073e377c23f4013da", "727e1e16ede6eaad241bad11c525da07b154c688", "6f17768a9fe231a2fd38708be90f98db3890c986", "6f17768a9fe231a2fd38708be90f98db3890c986"]},{"id": "e24a5e843d2ea999393b9f278f4b5c80f8a651d1", "title": "Learning to Learn with Compound HD Models", "authors": ["Ruslan Salakhutdinov", "Joshua B. Tenenbaum", "Antonio Torralba"], "date": "NIPS", "abstract": "We introduce HD (or \"Hierarchical-Deep\") models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian models. Specifically we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltzmann Machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that… ", "references": ["26cd528d687b3f50043b3762b504abde0a45f587", "100a038fdf29b4b20801887f0ec40e3f10d9a4f9", "100a038fdf29b4b20801887f0ec40e3f10d9a4f9", "26cd528d687b3f50043b3762b504abde0a45f587", "73e93d0346e8eee6c2ab45e46c26eaafb66e12a8", "73e93d0346e8eee6c2ab45e46c26eaafb66e12a8", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "1e80f755bcbf10479afd2338cec05211fdbd325c"]},{"id": "184ac0766262312ba76bbdece4e7ffad0aa8180b", "title": "Representation Learning: A Review and New Perspectives", "authors": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent"], "date": "2013", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors… ", "references": ["aaaea06da21f22221d5fbfd61bb3a02439f0fe02", "f93844c68d96f2f01da973b2ed3c236c8a369e57", "843959ffdccf31c6694d135fad07425924f785b1", "01373261bfbba42a806d21d5b759d5a27f509892", "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14", "843959ffdccf31c6694d135fad07425924f785b1", "5c0fe8ba39bda83d6ca3b9705a780809d52a67b4", "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "dc1d132c79a72dba386dc47750a49b0c29b54568", "8a9a10170ee907acb3e582742bec5fa09116f302"]},{"id": "0d2336389dff3031910bd21dd1c44d1b4cd51725", "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "authors": ["Dumitru Erhan", "Aaron C. Courville", "Pascal Vincent"], "date": "2010", "abstract": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets.", "references": ["6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "ccf415df5a83b343dae261286d29a40e8b80e6c6", "ccf415df5a83b343dae261286d29a40e8b80e6c6", "05fd1da7b2e34f86ec7f010bef068717ae964332", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "43c8a545f7166659e9e21c88fe234e0323855216", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "e60ff004dde5c13ec53087872cfcdd12e85beb57"]},{"id": "8de174ab5419b9d3127695405efd079808e956e8", "title": "Curriculum learning", "authors": ["Yoshua Bengio", "Jérôme Louradour", "Jason Weston"], "date": "ICML '09", "abstract": "Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore… ", "references": ["d73b02fb80bec1b65bd6187148fd6371e9c669df", "47133d54d4a5f1fb3c46bdbf3a7a5e270d930e2f", "d73b02fb80bec1b65bd6187148fd6371e9c669df", "d5ddb30bf421bdfdf728b636993dc48b1e879176", "ad33d1fa8628cb55c32fb52feb537f65184c3b29", "d5ddb30bf421bdfdf728b636993dc48b1e879176", "d73b02fb80bec1b65bd6187148fd6371e9c669df", "ad33d1fa8628cb55c32fb52feb537f65184c3b29", "d73b02fb80bec1b65bd6187148fd6371e9c669df", "47133d54d4a5f1fb3c46bdbf3a7a5e270d930e2f"]},{"id": "05fd1da7b2e34f86ec7f010bef068717ae964332", "title": "Exploring Strategies for Training Deep Neural Networks", "authors": ["Hugo Larochelle", "Yoshua Bengio", "Pascal Lamblin"], "date": "2009", "abstract": "Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM… ", "references": ["8978cf7574ceb35f4c3096be768c7547b28a35d0", "43c8a545f7166659e9e21c88fe234e0323855216", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04"]},{"id": "ec88941835c88a2956fc84c9e7cfd6dedf1b36f6", "title": "Scan primitives for GPU computing", "authors": ["Shubhabrata Sengupta", "Mark J. Harris", "John Douglas Owens"], "date": "GH '07", "abstract": "The scan primitives are powerful, general-purpose data-parallel primitives that are building blocks for a broad range of applications. We describe GPU implementations of these primitives, specifically an efficient formulation and implementation of segmented scan, on NVIDIA GPUs using the CUDA API. Using the scan primitives, we show novel GPU implementations of quicksort and sparse matrix-vector multiply, and analyze the performance of the scan primitives, several sort algorithms that use the… ", "references": []},{"id": "54a9c2553138932426faebcaa67a63a84a56b55d", "title": "Learning invariant features through topographic filter maps", "authors": ["Koray Kavukcuoglu", "Marc'Aurelio Ranzato", "Yann LeCun"], "date": "2009", "abstract": "Several recently-proposed architectures for high-performance object recognition are composed of two main stages: a feature extraction stage that extracts locally-invariant feature vectors from regularly spaced image patches, and a somewhat generic supervised classifier. The first stage is often composed of three main modules: (1) a bank of filters (often oriented edge detectors); (2) a non-linear transform, such as a point-wise squashing functions, quantization, or normalization; (3) a spatial… ", "references": ["ccd52aff02b0f902f4ce7247c4fee7273014c41c", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "f354310098e09c1e1dc88758fca36767fd9d084d", "cec734d7097ab6b1e60d95228ffd64248eb89d66", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "f02f3eccc1cf74e435721d09e4834aff6c1d12ed", "f354310098e09c1e1dc88758fca36767fd9d084d", "0a072cbdee54b83c8df43a431065f009d2cd2e70"]},{"id": "0dadb25842ef596a0f676c04cbd3dad4e1876964", "title": "Training Hierarchical Feed-Forward Visual Recognition Models Using Transfer Learning from Pseudo-Tasks", "authors": ["Amr Ahmed", "Kai Yu", "Eric P. Xing"], "date": "ECCV", "abstract": "Building visual recognition models that adapt across different domains is a challenging task for computer vision. While feature-learning machines in the form of hierarchial feed-forward models (e.g., convolutional neural networks) showed promise in this direction, they are still difficult to train especially when few training examples are available. In this paper, we present a framework for training hierarchical feed-forward models for visual recognition, using transfer learning from pseudo… ", "references": ["161ffb54a3fdf0715b198bb57bd22f910242eb49", "35a198cc4d38bd2db60cda96ea4cb7b12369fd3c", "162d958ff885f1462aeda91cd72582323fd6a1f4", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "161ffb54a3fdf0715b198bb57bd22f910242eb49", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "162d958ff885f1462aeda91cd72582323fd6a1f4"]},{"id": "f707a81a278d1598cd0a4493ba73f22dcdf90639", "title": "Generalization by Weight-Elimination with Application to Forecasting", "authors": ["Andreas S. Weigend", "David E. Rumelhart", "Bernardo A. Huberman"], "date": "NIPS", "abstract": "Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and… ", "references": ["bb3ace0f1eaf83cbf0d294fd28bc5362e14ed639", "63dc59554bb2cdb1c945c470239a5a523b575de4", "41bbbe4368e717f25abe61812807d0b1c1e682f6", "e1fbc6e0905fe020e95bda87efb3aeee7bbe2867", "bb3ace0f1eaf83cbf0d294fd28bc5362e14ed639", "e1fbc6e0905fe020e95bda87efb3aeee7bbe2867", "41bbbe4368e717f25abe61812807d0b1c1e682f6", "bb3ace0f1eaf83cbf0d294fd28bc5362e14ed639", "905926a7bfd8172c4648025cb730fe9d264ccb1e", "41bbbe4368e717f25abe61812807d0b1c1e682f6"]},{"id": "f354310098e09c1e1dc88758fca36767fd9d084d", "title": "Learning methods for generic object recognition with invariance to pose and lighting", "authors": ["Yann LeCun", "Fu Jie Huang", "Léon Bottou"], "date": "2004", "abstract": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five… ", "references": ["48d28acc0f3ac2b25bc62ae71525fa099b2d1052", "ed7935cc73cc16cf6123eff0d17949c1aff5a847", "397f22d68805551c500077f4a9b4dbea868d1fb3", "397f22d68805551c500077f4a9b4dbea868d1fb3", "1fda96d554f4e5a21e35bf33b9720141da47664b", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "3473ce22ada00842b355883a5ddde5a8c7c76ba6", "472ff211d18aa2ca2b65b69d86499430ad287499", "ed7935cc73cc16cf6123eff0d17949c1aff5a847", "472ff211d18aa2ca2b65b69d86499430ad287499"]},{"id": "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "title": "Object recognition with features inspired by visual cortex", "authors": ["Thomas Serre", "Lior Wolf", "Tomaso A. Poggio"], "date": "2005", "abstract": "We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object… ", "references": ["62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "d44f85fcc2eaa17e3c08313ad3ce70f8accf46fb", "830eb22c97caae687fbd22bae103c554a1d496a5", "1cf1527807ebb16020b04d4166e7ba8d27652302", "33b98b5dc150680fc02a14c0cf629168dd0af08b", "7aa7c0df2c2e154d7bddb873168ebc8446472425", "1cf1527807ebb16020b04d4166e7ba8d27652302", "f9f836d28f52ad260213d32224a6d227f8e8849a", "85abadb689897997f1e37baa7b5fc6f7d497518b", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5"]},{"id": "ceb0e1a86dc35e21ce5f0524c8476f15e1b08988", "title": "SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition", "authors": ["Hao Zhang", "Alexander C. Berg", "Jitendra Malik"], "date": "2006", "abstract": "We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories.", "references": ["fc3098cff5469c55c3e81dc127563afe6dbadf22", "fc3098cff5469c55c3e81dc127563afe6dbadf22", "4f948352d3dd8e53a83315359cf5797084a480d2", "249297cebeede0304f2d88e7dee04fe1979bbada", "249297cebeede0304f2d88e7dee04fe1979bbada", "09370d132a1e238a778f5e39a7a096994dc25ec1", "12c7fc38debaf3589e712973642246bd54fe63b3", "fc04643fec8b23b418611a92fb7dadc38e4ffc7e", "8314dda1ec43ce57ff877f8f02ed89acb68ca035", "fc3098cff5469c55c3e81dc127563afe6dbadf22"]},{"id": "5d9a3036181676e187c9c0ff995d8bed1db3557d", "title": "Adapting Visual Category Models to New Domains", "authors": ["Kate Saenko", "Brian Kulis", "Trevor Darrell"], "date": "ECCV", "abstract": "Domain adaptation is an important emerging topic in computer vision.", "references": ["1949636081c32463b3fea0afc6ba8198cfbd325d", "b98c68f01d84ac07dc7fc51af782018070da748f", "ee1ee30ee54dc23d813cf85d926d28e3af6a84f7", "5c781a1c150080b3fde5f0fc1f2ebf55b59b1169", "5b4ecd2d39b4ee76fa231bf1362dcc9cb67fa036", "a3bcaed530c914d1b18fa1dc2211b11ed0960e39", "ee1ee30ee54dc23d813cf85d926d28e3af6a84f7", "a3bcaed530c914d1b18fa1dc2211b11ed0960e39", "ee1ee30ee54dc23d813cf85d926d28e3af6a84f7", "ee1ee30ee54dc23d813cf85d926d28e3af6a84f7"]},{"id": "42269d0438c0ae4ca892334946ed779999691074", "title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "authors": ["Quoc V. Le", "Will Y. Zou", "Andrew Y. Ng"], "date": "2011", "abstract": "Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs… ", "references": ["0f86767732f76f478d5845f2e59f99ba106e9265", "117d576d72515e900e6fc5a4a0e7f1d0142a8924", "a39e6968580762ac5ae3cd064e86e1849f3efb7f", "b480f6a3750b4cebaf1db205692c8321d45926a2", "0f86767732f76f478d5845f2e59f99ba106e9265", "0f86767732f76f478d5845f2e59f99ba106e9265", "80bfcf1be2bf1b95cc6f36d229665cdf22d76190", "a39e6968580762ac5ae3cd064e86e1849f3efb7f", "263442bab9faba9ad84c8a1dfb6ca9a0947bd4d4", "aca29d7bbbf54078f842c8ca1d75d8d8c68191d2"]},{"id": "7de1d1612debcbde32cd588fa607a408df79c717", "title": "Geodesic flow kernel for unsupervised domain adaptation", "authors": ["Boqing Gong", "Yuan Shi", "Kristen Grauman"], "date": "2012", "abstract": "In real-world applications of visual recognition, many factors - such as pose, illumination, or image quality - can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied.", "references": ["3b282b22975e7220059616d6b08eb87482926db3", "1657ad3a046b40886eaf9161d85c98a64ba5d8ef", "64441c8396211b5e799b9ad5138dade15ff5cd0a", "96c6bc559b79d8fd518f431c707e8b44ce3bc4de", "0302bb2d5476540cfb21467473f5eca843caf90b", "d3edbfee56884d2b6d9aa51a6c525f9a05248802", "1657ad3a046b40886eaf9161d85c98a64ba5d8ef", "d3edbfee56884d2b6d9aa51a6c525f9a05248802", "64441c8396211b5e799b9ad5138dade15ff5cd0a", "0302bb2d5476540cfb21467473f5eca843caf90b"]},{"id": "953e2cfa58679ff6ea8c0bb432afd641f15d3657", "title": "Transfer learning for image classification with sparse prototype representations", "authors": ["Ariadna Quattoni", "Michael Collins", "Trevor Darrell"], "date": "2008", "abstract": "To learn a new visual category from few examples, prior knowledge from unlabeled data as well as previous related categories may be useful. We develop a new method for transfer learning which exploits available unlabeled data and an arbitrary kernel function; we form a representation based on kernel distances to a large set of unlabeled data points. To transfer knowledge from previous related problems we observe that a category might be learnable using only a small subset of reference… ", "references": ["8bbd29b4b1e3364e551c638ee46e0cde3cd3b31d", "23c6cb079b9ec45ae462cae743e01a1185fc4c2c", "6fe31db0f89a81705f6aad16066cb9a5d1885134", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "b3852f0113fcf8a3913c55ae92393ae6ccde347e", "328f88380422573a4ff9ada1fc5aa9f198a32bc5", "b3852f0113fcf8a3913c55ae92393ae6ccde347e", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "b3852f0113fcf8a3913c55ae92393ae6ccde347e", "6fe31db0f89a81705f6aad16066cb9a5d1885134"]},{"id": "d2810567138cb8a17b73de8913013487300d4b89", "title": "Geometric ℓp-norm feature pooling for image classification", "authors": ["Jiashi Feng", "Bingbing Ni", "Shuicheng Yan"], "date": "2011", "abstract": "Modern visual classification models generally include a feature pooling step, which aggregates local features over the region of interest into a statistic through a certain spatial pooling operation. Two commonly used operations are the average and max poolings. However, recent theoretical analysis has indicated that neither of these two pooling techniques may be qualified to be optimal. Besides, we further reveal in this work that more severe limitations of these two pooling methods are from… ", "references": ["1f88427d7aa8225e47f946ac41a0667d7b69ac52", "8ec1f8cbe8c9da709d519f99fc670604c268742f", "8ec1f8cbe8c9da709d519f99fc670604c268742f", "2f7713dcc35e7c05becf3be5522f36c9546b0364", "f2e186f2ebdd4ea5d4d324711f14c20b33b647ab", "2f7713dcc35e7c05becf3be5522f36c9546b0364", "ceb0e1a86dc35e21ce5f0524c8476f15e1b08988", "ceb0e1a86dc35e21ce5f0524c8476f15e1b08988", "f2e186f2ebdd4ea5d4d324711f14c20b33b647ab", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8"]},{"id": "e2c04849a3802715d5a9d89179c9f161014d6c2a", "title": "Modeling pixel means and covariances using factorized third-order boltzmann machines", "authors": ["Marc'Aurelio Ranzato", "Geoffrey E. Hinton"], "date": "2010", "abstract": "Learning a generative model of natural images is a useful way of extracting features that capture interesting regularities. Previous work on learning such models has focused on methods in which the latent features are used to determine the mean and variance of each pixel independently, or on methods in which the hidden units determine the covariance matrix of a zero-mean Gaussian distribution. In this work, we propose a probabilistic model that combines these two approaches into a single… ", "references": ["e7c64258997838087c9ba4e87225627b015122b2", "d8d01934cb26064b253dbd0f1627519133c3df3e", "d8d01934cb26064b253dbd0f1627519133c3df3e", "e7c64258997838087c9ba4e87225627b015122b2", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "092d70604a76fa691503cdc41449ec1ddc87630f", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "d8d01934cb26064b253dbd0f1627519133c3df3e", "5d90f06bb70a0a3dced62413346235c02b1aa086", "ccd52aff02b0f902f4ce7247c4fee7273014c41c"]},{"id": "7abda1941534d3bb558dd959025d67f1df526303", "title": "The Evidence Framework Applied to Classification Networks", "authors": ["David J. C. MacKay"], "date": "1992", "abstract": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in… ", "references": ["1f462943c8d0af69c12a09058251848324135e5a", "2046412fecff64e095cc5190b69172055afd2094", "fc053fd3feade79df85fd0612d7f817f5ae3cd44", "b0f2433c088591d265891231f1c22424047f1bc1", "8e68c54f39e87daf3a8bdc0ee005aece3c652d11", "2046412fecff64e095cc5190b69172055afd2094", "dcce2a3564685657c23d1afa00155c03560e76ac", "b0f2433c088591d265891231f1c22424047f1bc1", "b0f2433c088591d265891231f1c22424047f1bc1", "1f462943c8d0af69c12a09058251848324135e5a"]},{"id": "c6db1c92fc28fff14434b645861c0f4df5065e9e", "title": "Transformation Invariance in Pattern Recognition - Tangent Distance and Tangent Propagation", "authors": ["Patrice Y. Simard", "Yann LeCun", "Bernard Victorri"], "date": "Neural Networks: Tricks of…", "abstract": "In pattern recognition, statistical modeling, or regression, the amount of data is a critical factor affecting the performance. If the amount of data and computational resources are unlimited, even trivial algorithms will converge to the optimal solution. However, in the practical case, given limited data and other resources, satisfactory performance requires sophisticated methods to regularize the problem by introducing a priori knowledge. Invariance of the output with respect to certain… ", "references": []},{"id": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "authors": ["Vinod Nair", "Geoffrey E. Hinton"], "date": "ICML", "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the… ", "references": ["f354310098e09c1e1dc88758fca36767fd9d084d", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "8ba5455aaeb6ace02ea3ed47801d9ccb38513347", "1603a40b7bb56d563d9401f0d24c67d428e509f2", "f354310098e09c1e1dc88758fca36767fd9d084d", "73e93d0346e8eee6c2ab45e46c26eaafb66e12a8", "1603a40b7bb56d563d9401f0d24c67d428e509f2", "73e93d0346e8eee6c2ab45e46c26eaafb66e12a8", "f354310098e09c1e1dc88758fca36767fd9d084d", "8ba5455aaeb6ace02ea3ed47801d9ccb38513347"]},{"id": "68447483e80991ca718cad40e73ac14c08da7413", "title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "authors": ["Yangqing Jia", "Chang Huang", "Trevor Darrell"], "date": "2012", "abstract": "In this paper we examine the effect of receptive field designs on classification accuracy in the commonly adopted pipeline of image classification.", "references": ["ccd52aff02b0f902f4ce7247c4fee7273014c41c", "9791f1e47a48fa05387cb8dd93da53bf8f43c1f4", "405aed4b8ecdd869b2e83095dde51c396334115f", "b1c23451334b0f9294a6a8de5be59d361547a946", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "d2810567138cb8a17b73de8913013487300d4b89", "498efaa51f5eda731dc6199c3547b9465717fa68", "405aed4b8ecdd869b2e83095dde51c396334115f", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "498efaa51f5eda731dc6199c3547b9465717fa68"]},{"id": "05a8d8f1d2dadf01c35a363b1c37eed1dd27120f", "title": "Noise injection into inputs in back-propagation learning", "authors": ["Kiyotoshi Matsuoka"], "date": "1992", "abstract": "Back-propagation can be considered a nonlinear regression technique, allowing a nonlinear neural network to acquire an input/output (I/O) association using a limited number of samples chosen from a population of input and output patterns. A crucial problem on back-propagation is its generalization capability. A network successfully trained for given samples is not guaranteed to provide desired associations for untrained inputs as well. Concerning this problem some authors showed experimentally… ", "references": []},{"id": "dc0975ae518a5b30e60fde23a41c74bafd7c6f8c", "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "authors": ["Sida I. Wang", "Christopher D. Manning"], "date": "ACL", "abstract": "Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/dataset.", "references": ["6af58c061f2e4f130c3b795c21ff0c7e3903278f", "2e20ed644e7d6e04dd7ab70084f1bf28f93f75e9", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "2e12a485325776d3c23eae2b488d4812d86b4052", "04ce064505b1635583fa0d9cc07cac7e9ea993cc", "cfa2646776405d50533055ceb1b7f050e9014dcb", "cfa2646776405d50533055ceb1b7f050e9014dcb", "04ce064505b1635583fa0d9cc07cac7e9ea993cc", "da5cd00115f7ec108de8eebf071c5f3f19807df4", "cfa2646776405d50533055ceb1b7f050e9014dcb"]},{"id": "c5ee421735abee2669a687dd8cad95376a4b7fee", "title": "Learning to classify with missing and corrupted features", "authors": ["Ofer Dekel", "Ohad Shamir"], "date": "ICML", "abstract": "After a classifier is trained using a machine learning algorithm and put to use in a real world system, it often faces noise which did not appear in the training data. Particularly, some subset of features may be missing or may become corrupted. We present two novel machine learning techniques that are robust to this type of classification-time noise. First, we solve an approximation to the learning problem using linear programming. We analyze the tightness of our approximation and prove… ", "references": []},{"id": "c3ecd8e19e016d15670c8953b4b9afaa5186b0f3", "title": "Training with Noise is Equivalent to Tikhonov Regularization", "authors": ["Charles M. Bishop"], "date": "1995", "abstract": "It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used… ", "references": ["3d565fb42892f20c52b9fc615cc537835f30d094", "c6867b6b564462d6b902f68e0bfa58f4717ca1cc", "b66a67879edd9651618af00c04db4f0f2eb3021c", "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "3d565fb42892f20c52b9fc615cc537835f30d094", "b0f2433c088591d265891231f1c22424047f1bc1", "d435726856778fca3a2eae3eafa5e6fa9010b0a7", "3d565fb42892f20c52b9fc615cc537835f30d094", "f707a81a278d1598cd0a4493ba73f22dcdf90639", "0a41ca65a80b5644d23649043f2f625b4002a225"]},{"id": "62c0ac04b5b4e9e67efb0027c983eb0f211671d0", "title": "Contrastive Unsupervised Word Alignment with Non-Local Features", "authors": ["Yang Liu", "Maosong Sun"], "date": "2015", "abstract": "Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We… ", "references": ["64a007a07cbeab1b6949f196e58fdbe93ef1a297", "36d7098def8e8ade4834f9e5d79b7e232ccb4af4", "a7fb2cf3d5a0b30458e74d316a1ce3d6ab6f40b1", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "5245d7d59557134872b2f59270020ab4030da2b3", "36d7098def8e8ade4834f9e5d79b7e232ccb4af4", "5245d7d59557134872b2f59270020ab4030da2b3", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "9e022fa8effeaaff77d86be0a3d1bf50d899b5b8", "64a007a07cbeab1b6949f196e58fdbe93ef1a297"]},{"id": "8a756d4d25511d92a45d0f4545fa819de993851d", "title": "Recurrent Models of Visual Attention", "authors": ["Volodymyr Mnih", "Nicolas Manfred Otto Heess", "Koray Kavukcuoglu"], "date": "NIPS", "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels.", "references": ["e2b5dca2c46232a232356ca9034e35bc5b559dad", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "e2b5dca2c46232a232356ca9034e35bc5b559dad", "e2b5dca2c46232a232356ca9034e35bc5b559dad", "4816f0b6f0d05da3901441bfa5cc7be044b4da8b", "4816f0b6f0d05da3901441bfa5cc7be044b4da8b", "e2b5dca2c46232a232356ca9034e35bc5b559dad", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "fcba51774867c77f491581d3625d375a0a8f473b", "title": "Estimating a Kernel Fisher Discriminant in the Presence of Label Noise", "authors": ["Neil D. Lawrence", "Bernhard Schölkopf"], "date": "ICML", "abstract": "Data noise is present in many machine learning problems domains, some of these are well studied but others have received less attention.", "references": ["fd01781254105eda92bb41cbd4ce062618559ece", "3e43d731d638f769f12f8ab413d14a77a761856c", "fa11394b216de02514eb9a4854b28791ba842c70", "fa11394b216de02514eb9a4854b28791ba842c70", "847d6ece37d22430a0d9e061b5dc1d1b8c679055", "3e43d731d638f769f12f8ab413d14a77a761856c", "847d6ece37d22430a0d9e061b5dc1d1b8c679055", "3e43d731d638f769f12f8ab413d14a77a761856c", "20700ddb035ffc2d75d6fe3d7307bd5da9125b39", "c4a422669ec9b6a60b05d2d2595314008a5fb419"]},{"id": "5ac5fbae8a7faf2e9bd49ad01106cec4a2d8f20a", "title": "Feature selection, L1 vs. L2 regularization, and rotational invariance", "authors": ["Andrew Y. Ng"], "date": "ICML '04", "abstract": "We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn \"well,\") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1… ", "references": ["eec69c19da2b1404b8578b9d758c85baae8b21f1", "23afab3f249477d086819e890ac4aa417998568c", "385197d4c02593e2823c71e4f90a0993b703620e", "a1dace286582d91916fe470d08f30381cf453f20", "23afab3f249477d086819e890ac4aa417998568c", "23afab3f249477d086819e890ac4aa417998568c", "6d2e0be460e0e3b2b74ec8260d886b2397f8f320", "23afab3f249477d086819e890ac4aa417998568c", "385197d4c02593e2823c71e4f90a0993b703620e", "23afab3f249477d086819e890ac4aa417998568c"]},{"id": "100a038fdf29b4b20801887f0ec40e3f10d9a4f9", "title": "One shot learning of simple visual concepts", "authors": ["Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum"], "date": "2011", "abstract": "One shot learning of simple visual concepts Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum Department of Brain and Cognitive Sciences Massachusetts Institute of Technology Abstract People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is… ", "references": ["56efc84e0858f1e0a7cf052e5c4275d4c46c21c2", "39dd0024bddbef0f7e29d25169867c37574a4231", "812355cec91fa30bb50e9e992a3549af39e4f6eb", "473f114220bf0004ffbf9d9cac70b36abdb1748f", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "e3d4f463823b5a50963073f71d3c8ea29d6005fb", "e3d4f463823b5a50963073f71d3c8ea29d6005fb", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "dc55b93ee9e25b11f912cb76284fa94804ec5d71"]},{"id": "3554953715a4d7af3d4c9201d4080899b84fbad7", "title": "Max-margin Classification of Data with Absent Features", "authors": ["Gal Chechik", "Geremy Heitz", "Daphne Koller"], "date": "2008", "abstract": "We consider the problem of learning classifiers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to first complete their unknown values, and then… ", "references": ["5db7dc2239f820eae498b07a955f31b3d113179f", "addc878150c33c34147ae367c75be736e2b7f1ff", "da4a5ce1523e3bd72b1d6556e6b0bd98a51caf50", "f49d8df59dff02fadd26152b5585af6e753907c5", "f49d8df59dff02fadd26152b5585af6e753907c5", "12c7fc38debaf3589e712973642246bd54fe63b3", "12c7fc38debaf3589e712973642246bd54fe63b3", "b48e8b68b7ccdc10bc8d568c37e26e488629968f", "da4a5ce1523e3bd72b1d6556e6b0bd98a51caf50", "1f0540f184914bcdf498806ba9ef092ed440f560"]},{"id": "60cce28d1f56786930e86e5798d55e4a7948b0da", "title": "Better Alignments = Better Translations?", "authors": ["Kuzman Ganchev", "João Graça", "Ben Taskar"], "date": "ACL", "abstract": "Automatic word alignment is a key step in training statistical machine translation systems. Despite much recent work on word alignment methods, alignment accuracy increases often produce little or no improvements in machine translation quality. In this work we analyze a recently proposed agreement-constrained EM algorithm for unsupervised alignment models. We attempt to tease apart the effects that this simple but effective modification has on alignment precision and recall trade-offs, and how… ", "references": []},{"id": "9bd6cdae71506eb307507e44df7abe0c285b3ca7", "title": "Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015", "authors": ["Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "date": "WAT", "abstract": "This year, the Nara Institute of Science and Technology (NAIST)'s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry… ", "references": ["944a1cfd79dbfb6fef460360a0765ba790f4027a", "1956c239b3552e030db1b78951f64781101125ed", "601408d6617bf72894c9f41ae54cf9c17905903a", "f282db3b17282483821066cc9db0848624febd1f", "761a01ae66102502eadbe1e2220a1501e4d7b4a0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "601408d6617bf72894c9f41ae54cf9c17905903a", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "944a1cfd79dbfb6fef460360a0765ba790f4027a"]},{"id": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "title": "Improving Neural Machine Translation Models with Monolingual Data", "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "date": "2016", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to… ", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "897690c464c6abd8c4c15ad81153f39d07b7ee2e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1af68821518f03568f913ab03fc02080247a27ff", "15ef3e35fdea806b5e255b63bb908c40d5c7df45", "40cbaf4106ab60586e8ffa7624fc779172cfd490", "2826f9dccdcceb113b33ccf2841d488f1419bb30", "15ef3e35fdea806b5e255b63bb908c40d5c7df45", "40cbaf4106ab60586e8ffa7624fc779172cfd490"]},{"id": "b40e631a1988ee4f70400d3830ecaa462414d058", "title": "Generative Model for Layers of Appearance and Deformation", "authors": ["Anitha Kannan", "Nebojsa Jojic", "Brendan J. Frey"], "date": "AISTATS", "abstract": "We are interested in learning generative models of objects that can be used in wide range of tasks such as video summarization, image segmentation and frame interpolation. Learning object-based appearance/shape models and estimating motion fields (deformation field) are highly interdependent problems. At the extreme, all motions can be represented as an excessively large set of appearance exemplars. However, a more efficient representation of a video sequence would save on frame description if… ", "references": ["1365bc5de434771f05f186c6be75bb9400b15495", "1365bc5de434771f05f186c6be75bb9400b15495", "d237b135d9cb6c5ea76faa421fa461d3128b61e8", "a73f23484f890fafff6dd1e79ae25b33de1e666b", "ac38583145732c9c11165a49494d4ad0e2c0cbf1", "d237b135d9cb6c5ea76faa421fa461d3128b61e8", "7d280352ce01f0ff87dba3aed7aad66093a7ea20", "a73f23484f890fafff6dd1e79ae25b33de1e666b", "581db5af31e5c557015aefc993c6bbfca4e9150f", "e26a23b57bff5e1246e49dae394eb636a3c099d1"]},{"id": "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "title": "On Using Monolingual Corpora in Neural Machine Translation", "authors": ["Çaglar Gülçehre", "Orhan Firat", "Yoshua Bengio"], "date": "2015", "abstract": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation.", "references": ["944a1cfd79dbfb6fef460360a0765ba790f4027a", "5f08df805f14baa826dbddcb002277b15d3f1556", "8cecce5f192268bc0f1316b6afa085a159312cec", "5f08df805f14baa826dbddcb002277b15d3f1556", "8cecce5f192268bc0f1316b6afa085a159312cec", "c64d27b122d5b6ef0be135e63df05c3b24bd80c5", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "0b544dfe355a5070b60986319a3f51fb45d1348e"]},{"id": "f91b17e852774d80f4d11b9c7f5b99b1dd8aacf7", "title": "Minimum Risk Annealing for Training Log-Linear Models", "authors": ["David A. Smith", "Jason Eisner"], "date": "ACL", "abstract": "When training the parameters for a natural language system, one would prefer to minimize 1-best loss (error) on an evaluation set. Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex. We propose training instead to minimize the expected loss, or risk. We define this expectation using a probability distribution over hypotheses that we gradually… ", "references": ["1e19a94d547ee023837c14c361139185e2353fc0", "fd0060b30c2741375ed45e8a29f22931c7f42d75", "cd5a169879504ea91660a443b9151753cc29c42f", "1e19a94d547ee023837c14c361139185e2353fc0", "0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a", "cd5a169879504ea91660a443b9151753cc29c42f", "1f12451245667a85d0ee225a80880fc93c71cc8b", "0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a", "0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a", "0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a"]},{"id": "ae0ba9000628263f0f883fce3411fa435c8a9e53", "title": "Amodal Completion and Size Constancy in Natural Scenes", "authors": ["Abhishek Kar", "Shubham Tulsiani", "Jitendra Malik"], "date": "2015", "abstract": "We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale… ", "references": ["a9ce496186120df8f9ed3367e76a4947419e992e", "baddac96864c86538d3bd8bf495f00f818475a9e", "e37993b6612f433057f737ad37785743f3c4436b", "60c7a5d919806e18e31f139b7df9f1172b776f17", "83f80bfc168604ede6489b484d0de18981f99842", "a6902db7972a7631d186bbf59c5ef116c205b1e8", "baddac96864c86538d3bd8bf495f00f818475a9e", "e37993b6612f433057f737ad37785743f3c4436b", "83f80bfc168604ede6489b484d0de18981f99842", "a6902db7972a7631d186bbf59c5ef116c205b1e8"]},{"id": "489d6e4cc55c6eb945f12d2813e06cb482294d06", "title": "Layered Object Models for Image Segmentation", "authors": ["Yi Yang", "Sam Hallman", "Charless C. Fowlkes"], "date": "2012", "abstract": "We formulate a layered model for object detection and image segmentation. We describe a generative probabilistic model that composites the output of a bank of object detectors in order to define shape masks and explain the appearance, depth ordering, and labels of all pixels in an image. Notably, our system estimates both class labels and object instance labels. Building on previous benchmark criteria for object detection and image segmentation, we define a novel score that evaluates both class… ", "references": ["0385a8b21b50ba67e1b0e72d94cfc35675299afa", "38101fac622a70b78f13625fc6502000b8756d3a", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "0385a8b21b50ba67e1b0e72d94cfc35675299afa", "2e0e519dbf9aa58e153fb7c887935a371e24d7e3", "0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28", "72a2c172cf49edb4a33708e05f53938f4d475432", "38101fac622a70b78f13625fc6502000b8756d3a", "0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28", "d2efe575c931cf923e47ec5c7f444d53aae549cd"]},{"id": "687e80eb70c7bbad6001006d9269b202650a3354", "title": "Deep Convolutional Inverse Graphics Network", "authors": ["Tejas D. Kulkarni", "William F. Whitney", "Joshua B. Tenenbaum"], "date": "NIPS", "abstract": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to… ", "references": ["20f0357688876fa4662f806f985779dce6e24f3c", "1e80f755bcbf10479afd2338cec05211fdbd325c", "0132afe0be22b2cbbb5cbfee68307bedfef0d0c5", "0132afe0be22b2cbbb5cbfee68307bedfef0d0c5", "1e80f755bcbf10479afd2338cec05211fdbd325c", "8b1d79fd4db235be3920be043215664e1f36754b", "0132afe0be22b2cbbb5cbfee68307bedfef0d0c5", "43d36a22629114e14a0952675e15c9c76f1f024c", "b437b5a0445f17b06b12791bc48aeb8110e95dc5", "0132afe0be22b2cbbb5cbfee68307bedfef0d0c5"]},{"id": "9eb50914621312edf12fc96ee6d2fbe501388f67", "title": "Learning a Generative Model of Images by Factoring Appearance and Shape", "authors": ["Nicolas Le Roux", "Nicolas Manfred Otto Heess", "John M. Winn"], "date": "2011", "abstract": "Computer vision has grown tremendously in the past two decades.", "references": ["80c330eee12decb84aaebcc85dc7ce414134ad61", "cca9200d9da958b7f90eab901b2f30c04f1e0e9c", "53574f8c24537ae9d06438310f0222656aa2005e", "80c330eee12decb84aaebcc85dc7ce414134ad61", "cca9200d9da958b7f90eab901b2f30c04f1e0e9c", "3452d42804cbea13b8b252d08ec249b008e8df93", "53574f8c24537ae9d06438310f0222656aa2005e", "1e80f755bcbf10479afd2338cec05211fdbd325c", "80c330eee12decb84aaebcc85dc7ce414134ad61", "2077d0f30507d51a0d3bbec4957d55e817d66a59"]},{"id": "47900aca2f0b50da3010ad59b394c870f0e6c02e", "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks", "authors": ["Emily L. Denton", "Soumith Chintala", "Rob Fergus"], "date": "2015", "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images.", "references": ["2077d0f30507d51a0d3bbec4957d55e817d66a59", "80c330eee12decb84aaebcc85dc7ce414134ad61", "5d90f06bb70a0a3dced62413346235c02b1aa086", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "5d90f06bb70a0a3dced62413346235c02b1aa086", "32ef19e90e7834ec09ef19fcef7cd2aa6eff85a9", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "80c330eee12decb84aaebcc85dc7ce414134ad61", "32ef19e90e7834ec09ef19fcef7cd2aa6eff85a9"]},{"id": "018300f5f0e679cee5241d9c69c8d88e00e8bf31", "title": "Neural Variational Inference and Learning in Belief Networks", "authors": ["Andriy Mnih", "Karol Gregor"], "date": "2014", "abstract": "Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a… ", "references": ["6120cc252bc74239012f11b8b075cb7cb16bee26", "6120cc252bc74239012f11b8b075cb7cb16bee26", "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3", "00cd1dab559a9671b692f39f14c1573ab2d1416b", "32f078a7478d1ec2169599500a4507aceaccdda7", "6120cc252bc74239012f11b8b075cb7cb16bee26", "00cd1dab559a9671b692f39f14c1573ab2d1416b", "32f078a7478d1ec2169599500a4507aceaccdda7", "f87247fb37f6b48da0757d7a1acf38da44510cdb", "08d0ea90b53aba0008d25811268fe46562cfb38c"]},{"id": "0ae2e4e974f7ee57f590a691aada75c27c4c5394", "title": "Unsupervised Variational Bayesian Learning of Nonlinear Models", "authors": ["Antti Honkela", "Harri Valpola"], "date": "NIPS", "abstract": "In this paper we present a framework for using multi-layer perceptron (MLP) networks in nonlinear generative models trained by variational Bayesian learning. The nonlinearity is handled by linearizing it using a Gauss-Hermite quadrature at the hidden neurons. This yields an accurate approximation for cases of large posterior variance. The method can be used to derive nonlinear counterparts for linear algorithms such as factor analysis, independent component/factor analysis and state-space… ", "references": ["48c7e6c96df0c7865250be76fcdd45eddba9893a", "84e1e5cba162d06a6f267fd045c4e19699f0c599", "25c9f33aceac6dcff357727cbe2faf145b01d13c", "48c7e6c96df0c7865250be76fcdd45eddba9893a", "84e1e5cba162d06a6f267fd045c4e19699f0c599", "278600ebe6f56c6e1363788f464b9138bbdef35f", "8c8ee0bc40d8158bba48df860622649a12cabd49", "8c8ee0bc40d8158bba48df860622649a12cabd49", "4b83b4577f72ca90d995ba8572209d1c503abb72", "e5f22d558526017f130c75ca35fe0a737c01aaee"]},{"id": "c8b509be29721ee6b12c880b4d97ed6b60bad217", "title": "Generative Moment Matching Networks", "authors": ["Yujia Li", "Kevin Swersky", "Richard S. Zemel"], "date": "ICML", "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD… ", "references": ["d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "aaaea06da21f22221d5fbfd61bb3a02439f0fe02", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0", "540f9b12885911f80f5ca3a4931e6416ac0719e1", "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d", "540f9b12885911f80f5ca3a4931e6416ac0719e1", "43c8a545f7166659e9e21c88fe234e0323855216", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "540f9b12885911f80f5ca3a4931e6416ac0719e1"]},{"id": "7a24ec97e7f2881e245d20c46a56cbbfc734a4ff", "title": "Reweighted Wake-Sleep", "authors": ["Jörg Bornschein", "Yoshua Bengio"], "date": "2014", "abstract": "Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We… ", "references": ["32f078a7478d1ec2169599500a4507aceaccdda7", "705fd4febe2fff810d2f72f48dcda20826eca77a", "32f078a7478d1ec2169599500a4507aceaccdda7", "08d0ea90b53aba0008d25811268fe46562cfb38c", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "705fd4febe2fff810d2f72f48dcda20826eca77a", "08d0ea90b53aba0008d25811268fe46562cfb38c", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864"]},{"id": "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "title": "Deep AutoRegressive Networks", "authors": ["Karol Gregor", "Ivo Danihelka", "Daan Wierstra"], "date": "2014", "abstract": "We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a… ", "references": ["aaaea06da21f22221d5fbfd61bb3a02439f0fe02", "705fd4febe2fff810d2f72f48dcda20826eca77a", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "705fd4febe2fff810d2f72f48dcda20826eca77a", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "705fd4febe2fff810d2f72f48dcda20826eca77a", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "843959ffdccf31c6694d135fad07425924f785b1", "5a9ef216bf11f222438fff130c778267d39a9564", "705fd4febe2fff810d2f72f48dcda20826eca77a"]},{"id": "4e2f6b4bc889eed1afe5833d5190f6f02e501061", "title": "Training generative neural networks via Maximum Mean Discrepancy optimization", "authors": ["Gintare Karolina Dziugaite", "Daniel M. Roy", "Zoubin Ghahramani"], "date": "2015", "abstract": "We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic—informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by… ", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "8fe8a4ae3cabd0a33f670f6e8df23c5287ff0d38", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "162d958ff885f1462aeda91cd72582323fd6a1f4", "8fe8a4ae3cabd0a33f670f6e8df23c5287ff0d38", "225f78ae8a44723c136646044fd5c5d7f1d3d15a", "162d958ff885f1462aeda91cd72582323fd6a1f4", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "705fd4febe2fff810d2f72f48dcda20826eca77a", "54e325aee6b2d476bbbb88615ac15e251c6e8214"]},{"id": "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3", "title": "Variational Bayesian Inference with Stochastic Search", "authors": ["John W. Paisley", "David M. Blei", "Michael I. Jordan"], "date": "ICML", "abstract": "Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on… ", "references": ["e996344199af190bdd47ee31a652e76811a5487b", "93792105974f4d42c83172c4fc9f24be77fe781b", "c980686b001f55b1e514efaccb6cfbe1a8726db8", "81f2ede5c1e11c0e92a33430f25a7e3a8c39c918", "b7819760c0642eb7e4e6a46479f7a3800e661c59", "93792105974f4d42c83172c4fc9f24be77fe781b", "81f2ede5c1e11c0e92a33430f25a7e3a8c39c918", "ade741995dd4c20edb0225a721ddf179c58fde05", "81f2ede5c1e11c0e92a33430f25a7e3a8c39c918", "81f2ede5c1e11c0e92a33430f25a7e3a8c39c918"]},{"id": "00cf63a7926a826f7cf73c1d5edb117f98d70c2c", "title": "Variational algorithms for approximate Bayesian inference", "authors": ["Matthew J. Beal"], "date": "2003", "abstract": "The Bayesian framework for machine learning allows for the incorporation of prior knowledge in a coherent way, avoids overfitting problems, and provides a principled basis for selecting between alternative models.", "references": []},{"id": "5a9ef216bf11f222438fff130c778267d39a9564", "title": "Practical Variational Inference for Neural Networks", "authors": ["Alex Graves"], "date": "NIPS", "abstract": "Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also… ", "references": ["25c9f33aceac6dcff357727cbe2faf145b01d13c", "78d19e190dc5ece1e4e42a76662c5e98c2bb0842", "8d3d9ea15ef7f17d5ae2dcbff5a567f2bf2b9329", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "25c9f33aceac6dcff357727cbe2faf145b01d13c", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39", "e0a94da4e899fd4f1c31107cc2042a805d41927c", "3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39", "78d19e190dc5ece1e4e42a76662c5e98c2bb0842"]},{"id": "32f078a7478d1ec2169599500a4507aceaccdda7", "title": "The Neural Autoregressive Distribution Estimator", "authors": ["Hugo Larochelle", "Iain Murray"], "date": "AISTATS", "abstract": "We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even… ", "references": ["190e4800c67ef445e4bd0944a55debaccebcf43f", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "b2e5a7515d76675961462db97c70f4d2b592acd6", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "b2e5a7515d76675961462db97c70f4d2b592acd6", "00cd1dab559a9671b692f39f14c1573ab2d1416b", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "495ffba7d66c365403cb8bb84c61b4b5425b251c"]},{"id": "006c42929dcd480490fdb367fd7478b2956dbc99", "title": "A learning algorithm for analog, fully recurrent neural networks", "authors": ["Michael Gherrity"], "date": "1989", "abstract": "A learning algorithm for recurrent neural networks is derived. This algorithm allows a network to learn specified trajectories in state space in response to various input sequences. The network dynamics are described by a system of coupled differential equations that specify the continuous change of the unit activities and weights over time. The algorithm is nonlocal, in that a change in the connection weight between two units may depend on the values for some of the weights between different… ", "references": []},{"id": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "title": "DRAW: A Recurrent Neural Network For Image Generation", "authors": ["Karol Gregor", "Ivo Danihelka", "Daan Wierstra"], "date": "2015", "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates… ", "references": ["5d90f06bb70a0a3dced62413346235c02b1aa086", "5d90f06bb70a0a3dced62413346235c02b1aa086", "6ef259c2f6d50373abfec14fcb8fa924f7b7af0b", "72829d537f0ec8b1cc0ced2f278bb56ce89f1b0c", "5d90f06bb70a0a3dced62413346235c02b1aa086", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6", "0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6", "72829d537f0ec8b1cc0ced2f278bb56ce89f1b0c"]},{"id": "fbd616a659e8412ba37f1bd54cfe8ed543a35eb6", "title": "Picture: A probabilistic programming language for scene perception", "authors": ["Tejas D. Kulkarni", "Pushmeet Kohli", "Vikash K. Mansinghka"], "date": "2015", "abstract": "Recent progress on probabilistic modeling and statistical learning, coupled with the availability of large training datasets, has led to remarkable progress in computer vision. Generative probabilistic models, or “analysis-by-synthesis” approaches, can capture rich scene structure but have been less widely applied than their discriminative counterparts, as they often require considerable problem-specific engineering in modeling and inference, and inference is typically seen as requiring slow… ", "references": ["8978cf7574ceb35f4c3096be768c7547b28a35d0", "5c306ce578a8da634a4a64fce282a48d0eacfda1", "07be11c9f2629c87a4ea22ba687237d66dad5f26", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "07be11c9f2629c87a4ea22ba687237d66dad5f26", "e37993b6612f433057f737ad37785743f3c4436b", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "a8e25aeeed33551ac0e7eeababb768cb90402b8a", "e37993b6612f433057f737ad37785743f3c4436b", "8978cf7574ceb35f4c3096be768c7547b28a35d0"]},{"id": "5c306ce578a8da634a4a64fce282a48d0eacfda1", "title": "Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs", "authors": ["Vikash K. Mansinghka", "Tejas D. Kulkarni", "Joshua B. Tenenbaum"], "date": "NIPS", "abstract": "The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs… ", "references": ["d0e5040b24f51f400726e7deba7277174e8739df", "d0e5040b24f51f400726e7deba7277174e8739df", "3f079ada86116e62dd316dcc50259d96279c62f7", "d0e5040b24f51f400726e7deba7277174e8739df", "90d5a98943d4c9f22a79843898568e3a2e0b29e7", "b8f57509a228f1c84bf67094ec1fa8a99407368b", "46f9cfd3800b821c6cb09cedff5ecb7157457c19", "ac2f5e6d2a7c1b3b6aebe11f91933118c4f7529e", "d0e5040b24f51f400726e7deba7277174e8739df", "c045011905caf139326bcbd944398053dbb33e21"]},{"id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "title": "Learning State Space Trajectories in Recurrent Neural Networks", "authors": ["Barak A. Pearlmutter"], "date": "1989", "abstract": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit… ", "references": ["a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "cd62c9976534a6a2096a38244f6cbb03635a127e", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "0882e54463c5d20269318185cfcdd6b73748d157", "0882e54463c5d20269318185cfcdd6b73748d157", "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "d8ba8ef00d46170cf608e0e76f468cd3a348c6af", "f05debe71ab89dc5bd612cc4e9f498b09419d532", "7a23da2c14f9355dd63a434b62cf5b28aeebc305"]},{"id": "3a0de0ad4bf796e2506080d508f83205cf8d76fc", "title": "Faster Learning for Dynamic Recurrent Backpropagation", "authors": ["Yan Fang", "Terrence J. Sejnowski"], "date": "1990", "abstract": "The backpropagation learning algorithm for feedforward networks (Rumelhart et al. 1986) has recently been generalized to recurrent networks (Pineda 1989). The algorithm has been further generalized by Pearlmutter (1989) to recurrent networks that produce time-dependent trajectories. The latter method requires much more training time than the feedforward or static recurrent algorithms. Furthermore, the learning can be unstable and the asymptotic accuracy unacceptable for some problems. In this… ", "references": ["f8c5359e841a71480eb7436b897f51610486fde5", "e273e23196d0e235a34257b40015b19af6dbb7fa", "6602985bd326d9996c68627b56ed389e2c90fd08", "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "f8c5359e841a71480eb7436b897f51610486fde5", "a9ef2995e8e1bd57a74343073219364811c2ace0", "e273e23196d0e235a34257b40015b19af6dbb7fa", "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "a9ef2995e8e1bd57a74343073219364811c2ace0", "e273e23196d0e235a34257b40015b19af6dbb7fa"]},{"id": "54e34d0053b71d78cec26e8c29f57a3b9e85de49", "title": "Training recurrent networks using the extended Kalman filter", "authors": ["Ronald J. Williams"], "date": "1992", "abstract": "The author describes some relationships between the extended Kalman filter (EKF) as applied to recurrent net learning and some simpler techniques that are more widely used. In particular, making certain simplifications to the EKF gives rise to an algorithm essentially identical to the real-time recurrent learning (RTRL) algorithm. Since the EKF involves adjusting unit activity in the network, it also provides a principled generalization of the teacher forcing technique. Preliminary simulation… ", "references": ["bda89a6b28f234e9159b4fc884980bdd6163819a", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "3c5dc6523b01b20c563670342522ebfd1211a672", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "3c5dc6523b01b20c563670342522ebfd1211a672", "424710825d726e10b016204ed2bc979e2a342d10", "3c5dc6523b01b20c563670342522ebfd1211a672", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "bd46c1b5948abe04e565a8bae6454da63a1b021e"]},{"id": "976e3bb77b343d08f68063be5db2c1352458dbb1", "title": "Data rectification using recurrent (Elman) neural networks", "authors": ["Thomas W. Karjala", "David M. Himmelblau", "Risto Miikkulainen"], "date": "1992", "abstract": "Nonlinear programming techniques are used to train Elman-type simple recurrent neural networks to reconcile simulated measurements for a simple dynamic system (a draining tank). The networks are trained in a batch mode using the BFGS quasi-Newton nonlinear optimization algorithm. This makes it possible to avoid the trial and error associated with tuning the learning rate and momentum terms required in the various backpropagation algorithms. The random measurement errors used are uncorrelated… ", "references": []},{"id": "cccd3fd7a45e7643f26391bd539ffbede0690f36", "title": "Gradient-based learning algorithms for recurrent connectionist networks", "authors": ["Ronald J. Williams", "David Zipser"], "date": "1990", "abstract": "Recurrent connectionist networks are important because they can perform temporally extended tasks, giving them considerable power beyond the static mappings performed by the now-familiar multilayer feedforward networks. This ability to perform highly nonlinear dynamic mappings makes these networks particularly interesting to study and potentially quite useful in tasks which have an important temporal component not easily handled through the use of simple tapped delay lines. Some examples are… ", "references": ["9c5943e53c334a5dff1bb08a62cd2f64ab1fac41", "424710825d726e10b016204ed2bc979e2a342d10", "9c5943e53c334a5dff1bb08a62cd2f64ab1fac41", "fd0da2f1d2b95e5b62221a00ff132219d0c853b7", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "424710825d726e10b016204ed2bc979e2a342d10", "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "fd0da2f1d2b95e5b62221a00ff132219d0c853b7"]},{"id": "f87247fb37f6b48da0757d7a1acf38da44510cdb", "title": "Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "date": "2014", "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic backpropagation – rules for back-propagation through stochastic variables – and use this to develop an algorithm that… ", "references": ["5a9ef216bf11f222438fff130c778267d39a9564", "32f078a7478d1ec2169599500a4507aceaccdda7", "32f078a7478d1ec2169599500a4507aceaccdda7", "aeed631d6a84100b5e9a021ec1914095c66de415", "65bb01b985035307f7b1102e17b8a5c0f2dafff8", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "00cf63a7926a826f7cf73c1d5edb117f98d70c2c", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "cc63e22c962c1e3fe66a8f604191b2d71603e070", "5a9ef216bf11f222438fff130c778267d39a9564"]},{"id": "d6deb1ddc764259fbdc7733ef80473081bff31d5", "title": "Learning by Choice of Internal Representations", "authors": ["Tal Grossman", "Ron Meir", "Eytan Domany"], "date": "Complex Systems", "abstract": "We introduce a learning algorithm for multilayer neural networks composed of binary linear threshold elements. Whereas existing algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. Once a correct set of internal representations is arrived at, the weights are found by the local and biologically plausible Perceptron Learning Rule (PLR). We tested our learning algorithm on… ", "references": ["ff2c2e3e83d1e8828695484728393c76ee07a101"]},{"id": "85597bc9c4724ad11f6d6d04627afab0ada2ecd7", "title": "The utility driven dynamic error propagation network", "authors": ["Anthony J. Robinson", "Frank Fallside"], "date": "1987", "abstract": null, "references": []},{"id": "bccb2f99a9d1c105699f5d88c479569085e2c7ba", "title": "Stochastic variational inference", "authors": ["Matthew D. Hoffman", "David M. Blei", "John W. Paisley"], "date": "2013", "abstract": "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from… ", "references": ["8f3d22c0caf53655c6542aed0a6101629db7d5f3", "1dda1a4414675729f46594a5e609938ef3a48382", "a97a19ee8eb086df03961634cca804b551cd4a4c", "a97a19ee8eb086df03961634cca804b551cd4a4c", "8f3d22c0caf53655c6542aed0a6101629db7d5f3", "1dda1a4414675729f46594a5e609938ef3a48382", "1dda1a4414675729f46594a5e609938ef3a48382", "8f3d22c0caf53655c6542aed0a6101629db7d5f3", "8f3d22c0caf53655c6542aed0a6101629db7d5f3", "a97a19ee8eb086df03961634cca804b551cd4a4c"]},{"id": "26bc0449360d7016f684eafae5b5d2feded32041", "title": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories", "authors": ["Ronald J. Williams", "Jing Peng"], "date": "1990", "abstract": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the… ", "references": ["006c42929dcd480490fdb367fd7478b2956dbc99", "424710825d726e10b016204ed2bc979e2a342d10", "424710825d726e10b016204ed2bc979e2a342d10", "424710825d726e10b016204ed2bc979e2a342d10", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "8be3f21ab796bd9811382b560507c1c679fae37f", "d18b17417128322f86528f60d652a5b998a51409", "8be3f21ab796bd9811382b560507c1c679fae37f", "006c42929dcd480490fdb367fd7478b2956dbc99"]},{"id": "6a667700100e228cb30a5d884258a0db921603fe", "title": "Black Box Variational Inference", "authors": ["Rajesh Ranganath", "Sean Gerrish", "David M. Blei"], "date": "2014", "abstract": "Variational inference has become a widely used method to approximate posteriors in complex latent variables models.", "references": ["e407ea7fda6d152d2186f4b5e27aa04ec2d32dcd", "15bbea3d58ac3f7c21392eaba72b8b166888f81f", "e5f22d558526017f130c75ca35fe0a737c01aaee", "e407ea7fda6d152d2186f4b5e27aa04ec2d32dcd", "e407ea7fda6d152d2186f4b5e27aa04ec2d32dcd", "de58806bca096e0be83d74c353233898e37b69f3", "15bbea3d58ac3f7c21392eaba72b8b166888f81f", "de58806bca096e0be83d74c353233898e37b69f3", "de58806bca096e0be83d74c353233898e37b69f3", "de58806bca096e0be83d74c353233898e37b69f3"]},{"id": "f8620fb17d7e0e41c44c1e87fe3693daad0d30bd", "title": "Learning to Control Fast-weight Memories: an Alternative to Dynamic Recurrent Networks", "authors": ["J Urgen Schmidhuber"], "date": "1991", "abstract": "Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes alternative gradient-based systems consisting of two feed-forward nets which learn to deal with temporal sequences by using fast weights: The rst net learns to produce context dependent weight changes for the second net whose weights may vary very quickly. The method oers a potential for STM storage e-ciency: A simple weight (instead of a full-BLOCKINedged unit) may be sucient for… ", "references": ["ce03ad84b59a616368035f1f73ed666dfaadd16c", "ce03ad84b59a616368035f1f73ed666dfaadd16c", "56623a496727d5c71491850e04512ddf4152b487", "56623a496727d5c71491850e04512ddf4152b487"]},{"id": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "authors": ["Pascal Vincent", "Hugo Larochelle", "Pierre-Antoine Manzagol"], "date": "2010", "abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs.", "references": ["202cbbf671743aefd380d2f23987bd46b9caaf97", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "202cbbf671743aefd380d2f23987bd46b9caaf97", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "202cbbf671743aefd380d2f23987bd46b9caaf97", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "e5013a8b793bc3f9f08411a1b37e571d72491a2a", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "202cbbf671743aefd380d2f23987bd46b9caaf97"]},{"id": "fee5cc60c185e5d2942fd925bbde612f368ea7ef", "title": "Using random weights to train multilayer networks of hard-limiting units", "authors": ["Peter L. Bartlett", "Tom Downs"], "date": "1992", "abstract": "A gradient descent algorithm suitable for training multilayer feedforward networks of processing units with hard-limiting output functions is presented. The conventional backpropagation algorithm cannot be applied in this case because the required derivatives are not available. However, if the network weights are random variables with smooth distribution functions, the probability of a hard-limiting unit taking one of its two possible values is a continuously differentiable function. In the… ", "references": ["247698d0a716f0d99c0645050d049525e0b08ec2", "32eed28d16ca4c8302b8502275fa1eb6ef121f11", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "247698d0a716f0d99c0645050d049525e0b08ec2", "8977f8a86ac464b7396b27cfd8db21c56261a055", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "8977f8a86ac464b7396b27cfd8db21c56261a055", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657"]},{"id": "9a607334c1d963b4af29676578e1ef6aa11ba6e7", "title": "Local Feedback Multilayered Networks", "authors": ["Paolo Frasconi", "Marco Gori", "Giovanni Soda"], "date": "1992", "abstract": "In this paper, we investigate the capabilities of local feedback multilayered networks, a particular class of recurrent networks, in which feedback connections are only allowed from neurons to themselves. In this class, learning can be accomplished by an algorithm that is local in both space and time. We describe the limits and properties of these networks and give some insights on their use for solving practical problems. ", "references": ["34468c0aa95a7aea212d8738ab899a69b2fc14c6", "7ab73467f48f9c453be07c389c5dd1d1bfa70f25"]},{"id": "424710825d726e10b016204ed2bc979e2a342d10", "title": "Experimental Analysis of the Real-time Recurrent Learning Algorithm", "authors": ["Ronald J. Williams", "David Zipser"], "date": "1989", "abstract": "Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any… ", "references": []},{"id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks", "authors": ["Ronald J. Williams", "David Zipser"], "date": "1989", "abstract": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These… ", "references": ["8be3f21ab796bd9811382b560507c1c679fae37f", "7d51742bdb98221b3f42208baaa3df91ba06a617", "8be3f21ab796bd9811382b560507c1c679fae37f", "268e772554222e1dc738f87077b64f1b67258225", "5146d7902132bcb0b2e6fe5f607358768fc47323", "9c5943e53c334a5dff1bb08a62cd2f64ab1fac41", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "9c5943e53c334a5dff1bb08a62cd2f64ab1fac41", "7d51742bdb98221b3f42208baaa3df91ba06a617", "9c5943e53c334a5dff1bb08a62cd2f64ab1fac41"]},{"id": "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "title": "Global optimization of a neural network-hidden Markov model hybrid", "authors": ["Yoshua Bengio", "Renato De Mori", "Ralf Kompe"], "date": "1992", "abstract": "The integration of multilayered and recurrent artificial neural networks (ANNs) with hidden Markov models (HMMs) is addressed.", "references": ["039900eaeeddd13752aa8d6c61759f0b0e54f0de", "49a09dbad1eef44b360d2a87621a544b64e1c5b3", "1c9781457be3b0eb58c47576d4c8bc58daf82114", "d33c7c733a5960827fe6abe841ef1faa68cef6f4", "039900eaeeddd13752aa8d6c61759f0b0e54f0de", "f866ac085771f5676800db2d9b102975b2a1b2d7", "49a09dbad1eef44b360d2a87621a544b64e1c5b3", "c1e32f7d9fb6d849ceadd63cb7e446f39b911af5", "d33c7c733a5960827fe6abe841ef1faa68cef6f4", "1c9781457be3b0eb58c47576d4c8bc58daf82114"]},{"id": "2245d7893aeebb06e9bbd990a1d21d4608951f5e", "title": "Complexity of exact gradient computation algorithms for recurrent neural networks", "authors": ["Ronald J. Williams"], "date": "1989", "abstract": null, "references": []},{"id": "06778bd87125a28f0d045e0221ca1b8ad1d469b6", "title": "Recurrent Networks and NARMA Modeling", "authors": ["Jerome T. Connor", "Les E. Atlas", "R. Douglas Martin"], "date": "NIPS", "abstract": "There exist large classes of time series, such as those with nonlinear moving average components, that are not well modeled by feedforward networks or linear models, but can be modeled by recurrent networks. We show that recurrent neural networks are a type of nonlinear autoregressive-moving average (NARMA) model. Practical ability will be shown in the results of a competition sponsored by the Puget Sound Power and Light Company, where the recurrent networks gave the best performance on… ", "references": ["0b46d387fa47297de935572381ce55d371262efe", "f113278cd716cc4ab53a22b4b7b371e09d69ed98", "f113278cd716cc4ab53a22b4b7b371e09d69ed98", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "bdc3d618db015b2f17cd76224a942bfdfc36dc73", "8dcaf96f66340c453e775ab217a1b1bd9ba63449", "f113278cd716cc4ab53a22b4b7b371e09d69ed98", "f113278cd716cc4ab53a22b4b7b371e09d69ed98", "0b46d387fa47297de935572381ce55d371262efe"]},{"id": "59f884480d293672213ca315beec332943b64434", "title": "Comparison of four neural net learning methods for dynamic system identification", "authors": ["S. Qin", "Hong-Te Su", "Thomas McAvoy"], "date": "1992", "abstract": "Four types of neural net learning rules are discussed for dynamic system identification. It is shown that the feedforward network (FFN) pattern learning rule is a first-order approximation of the FFN-batch learning rule. As a result, pattern learning is valid for nonlinear activation networks provided the learning rate is small. For recurrent types of networks (RecNs), RecN-pattern learning is different from RecN-batch learning. However, the difference can be controlled by using small learning… ", "references": ["26bc0449360d7016f684eafae5b5d2feded32041", "6b235711b35468bed92c4ea5c089221f81abd962", "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "b5005aeb3c6d68f8edafbdbb733ec41aeeb1ff49", "dae7ab44d1a2c72c0c65910837f0bfeda41f6904", "11165f19d1784eaeaf3550b8531c5b19475fbe72", "11165f19d1784eaeaf3550b8531c5b19475fbe72", "11165f19d1784eaeaf3550b8531c5b19475fbe72", "dae7ab44d1a2c72c0c65910837f0bfeda41f6904", "3c243f77e85185706abcb6f9a3b25348ad324759"]},{"id": "9c790c8e2d3bc565d59a91600dac0d8b4d1eedc4", "title": "An Empirical Test of New Forecasting Methods Derived from a Theory of Intelligence: The Prediction of Conflict in Latin America", "authors": ["Paul J. Werbos", "Jim Titus"], "date": "1978", "abstract": "The \"compromise\" method is a new computer-based forecasting tool, available within the conversational CS package on the MIT Multics. Like regression (least squares) or new forms of Box-Jenkins methods, it estimates the parameters of a multivariate dynamic model and may be used for causal analysis or policy impact analysis. Unlike those maximum-likelihood methods, it does not assume that errors are \"white noise,\" random and normal. It follows the newer robust philosophy of trying to minimize… ", "references": []},{"id": "f038237446e79e02d2960b41d61853f23737a7ac", "title": "Nonlinear system identification using neural networks", "authors": ["Johan A. K. Suykens", "Joos Vandewalle", "Bart De Moor"], "date": "1996", "abstract": "In this Chapter we treat the problem of nonlinear system identification using neural networks. Model structures and their parametrization by multilayer perceptrons are discussed, together with learning algorithms, practical aspects and examples. The Chapter is organized as follows. In Section 3.1 we review model structures such as NARX, NARMAX and nonlinear state space models. In Section 3.2 parametrizations of these models by multilayer neural nets are made. Neural state space models are… ", "references": []},{"id": "668087f0ae7ce1de6e0bd0965dbb480c08103260", "title": "Finding Structure in Time", "authors": ["Jeffrey L. Elman"], "date": "1990", "abstract": "Time underlies many interesting human behaviors.", "references": ["b35b9c21e558899245dddfdec4c750bcd7279c56", "de996c32045df6f7b404dda2a753b6a9becf3c08", "bcdc334454d8513ff71357681b481c3db6c7e0ed", "3106e66537a0c8f53278e553bcb38f0b0992ec0e", "ebf13a6eb44fe3cc498404d0137d21fd13555690", "ebf13a6eb44fe3cc498404d0137d21fd13555690", "bcdc334454d8513ff71357681b481c3db6c7e0ed", "268e772554222e1dc738f87077b64f1b67258225", "bcdc334454d8513ff71357681b481c3db6c7e0ed", "bcdc334454d8513ff71357681b481c3db6c7e0ed"]},{"id": "99c3d399b14b2d3c8375706b048aedba46515a32", "title": "High-dimensional sequence transduction", "authors": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "date": "2013", "abstract": "We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise… ", "references": ["0228810a988f6b8f06337e14f564e2fd3f6e1056", "1279163ee31bb8c1b4a7f4422b6c80164261e8c2", "0228810a988f6b8f06337e14f564e2fd3f6e1056", "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "843959ffdccf31c6694d135fad07425924f785b1", "0228810a988f6b8f06337e14f564e2fd3f6e1056", "1279163ee31bb8c1b4a7f4422b6c80164261e8c2", "4de9647f1102dbe3718a9534fd51121d97389e78", "0228810a988f6b8f06337e14f564e2fd3f6e1056", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f"]},{"id": "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "title": "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription", "authors": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "date": "2012", "abstract": "We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to… ", "references": ["831b3a28c070a0a6047c13848f49159eac069b1c", "c74e230a5a6fd5e2db6ace765ce38afe65f96214", "0113c0d62be40a3c64b1151df1055546146b56ea", "598d394d156aab29eab94a9b38461e211f8ba0b3", "a4ffd2f5dd98ee744c013060c5bc06503336d931", "4de9647f1102dbe3718a9534fd51121d97389e78", "d99530af7203ae6b40f1d42aaa26e3ffb836a3fc", "6fbd89f915ae5fd06e8e105d30132372b5433c0e", "0113c0d62be40a3c64b1151df1055546146b56ea", "4de9647f1102dbe3718a9534fd51121d97389e78"]},{"id": "86e951e190586b84c530f9f03504f9ad70cc650a", "title": "Learning Features from Music Audio with Deep Belief Networks", "authors": ["Philippe Hamel", "Douglas Eck"], "date": "ISMIR", "abstract": "Feature extraction is a crucial part of many MIR tasks.", "references": ["1d1936a3488d1fde6ef03c124a798c30c6cb7d9c", "16109ffe9d0a10e40703a3888271b3af770eaf16", "61732c81caf65c07a4fdb1a12a7eaf6ad60fe03c", "00c250035c7316ef9a86ad1f5df94190cad26af3", "00c250035c7316ef9a86ad1f5df94190cad26af3", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "937fa97ea59f85c30391e10a3aa2be7189b0e912", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "61732c81caf65c07a4fdb1a12a7eaf6ad60fe03c"]},{"id": "365b3ff789a3433b1b66e7a257d9721e018734fb", "title": "Polyphonic piano note transcription with recurrent neural networks", "authors": ["Sebastian Böck", "Markus Schedl"], "date": "2012", "abstract": "In this paper a new approach for polyphonic piano note onset transcription is presented. It is based on a recurrent neural network to simultaneously detect the onsets and the pitches of the notes from spectral features. Long Short-Term Memory units are used in a bidirectional neural network to model the context of the notes. The use of a single regression output layer instead of the often used one-versus-all classification approach enables the system to significantly lower the number of… ", "references": ["598d394d156aab29eab94a9b38461e211f8ba0b3", "6934f2d974b4fc1351a401c2b4ee551508d07d0b", "2d5e8044c0de765c37cd5f58be12582ae107e90f", "c9d26a0d690b354ff4e2853fb41a36a3fe81ae47", "ab121ce91c3e4fa9a2ea003fc5d8a7b0c6e4a99e", "2d5e8044c0de765c37cd5f58be12582ae107e90f", "c9d26a0d690b354ff4e2853fb41a36a3fe81ae47", "ca536a967e7e3c5703f970701e52f56b2f508f48", "af3a725ea36756d39c4958ecafd234bc05637ff8", "2d5e8044c0de765c37cd5f58be12582ae107e90f"]},{"id": "4de9647f1102dbe3718a9534fd51121d97389e78", "title": "A Classification-Based Polyphonic Piano Transcription Approach Using Learned Feature Representations", "authors": ["Juhan Nam", "Jiquan Ngiam", "Malcolm Slaney"], "date": "ISMIR", "abstract": "Recentlyunsupervisedfeaturelearningmethodshaveshown great promise as a way of extracting features from high dimensional data, such as image or audio. In this paper, we apply deep belief networks to musical data and evaluate the learned feature representations on classification-based polyphonic piano transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three public piano… ", "references": ["86e951e190586b84c530f9f03504f9ad70cc650a", "86e951e190586b84c530f9f03504f9ad70cc650a", "86e951e190586b84c530f9f03504f9ad70cc650a", "c9d26a0d690b354ff4e2853fb41a36a3fe81ae47", "af3a725ea36756d39c4958ecafd234bc05637ff8", "e62b7149268a2c6ed37206d116bf2568b5eeb5b9", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "c51a3c44978d046d6d8fe2da0847e62979ae02e8", "c9d26a0d690b354ff4e2853fb41a36a3fe81ae47", "c51a3c44978d046d6d8fe2da0847e62979ae02e8"]},{"id": "acc4a0dc5f3a6c30e46f98b53fd688b3c9797aaa", "title": "Regression and Ranking based Optimisation for Sentence Level MT Evaluation", "authors": ["Xingyi Song", "Trevor Cohn"], "date": "WMT@EMNLP", "abstract": "Automatic evaluation metrics are fundamentally important for Machine Translation, allowing comparison of systems performance and efficient training. Current evaluation metrics fall into two classes: heuristic approaches, like BLEU, and those using supervised learning trained on human judgement data. While many trained metrics provide a better match against human judgements, this comes at the cost of including lots of features, leading to unwieldy, non-portable and slow metrics. In this paper… ", "references": ["d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "5dbba694b86cc3f18d2ce6045ada71e648f66d71", "cfd4259d305a00f13d5f08841230389f61322422", "0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7", "d7da009f457917aa381619facfa5ffae9329a6e9", "a5899f1ec92af7d01f35225161430116a6eabbea", "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5", "443516aeb2819d4d362ffe7d5418a54e5427a016", "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5"]},{"id": "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5", "title": "Online Large-Margin Training of Syntactic and Structural Translation Features", "authors": ["David Chiang", "Yuval Marton", "Philip Resnik"], "date": "EMNLP", "abstract": "Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and… ", "references": ["067b67a955302dab7e450bd51c246fd6bfbec545", "1f12451245667a85d0ee225a80880fc93c71cc8b", "1e19a94d547ee023837c14c361139185e2353fc0", "efa287ee23f4fee1d850fcca0ac0ef1d1d25f823", "efa287ee23f4fee1d850fcca0ac0ef1d1d25f823", "7e0a9e64b4c838ff1ffb1075e3c2deceda0fa5a0", "b7f9873024c1955e20fa60254d8d927d953ebf3c", "8d125e6de1e77e9168ad0ed355551cf72062e1c2", "efa287ee23f4fee1d850fcca0ac0ef1d1d25f823", "7e0a9e64b4c838ff1ffb1075e3c2deceda0fa5a0"]},{"id": "805aee7f3b50856df85e13e67b3034ac7ed23824", "title": "Automatic chord transcription from audio using computational models of musical context", "authors": ["Matthias Mauch"], "date": "2010", "abstract": "This thesis is concerned with the automatic transcription of chords from audio, with an emphasis on modern popular music.", "references": ["b91f04154723a22a40c01d54d557c37163769695", "dee26fd31f479b1f092ca019cd1eca49c73841e1", "42236daed80fed3d32003918391bf25abaf97641", "dee26fd31f479b1f092ca019cd1eca49c73841e1", "dee26fd31f479b1f092ca019cd1eca49c73841e1", "2b19b3cbd5852e7a8e89d52aea84cc53fd79cbcf", "4330d5fa0f2d9fd5c7c395ce4977717054b0b4ee", "1666a6635f9ab3b0c0a8d924367d28281c7fb84a", "e854e01a7b72ee79d6b1ad86a912c5043ed63e17", "250ae3be1e51ef4f62557cbdce6065f3d7257fab"]},{"id": "5dbba694b86cc3f18d2ce6045ada71e648f66d71", "title": "Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms", "authors": ["David Chiang", "Steve DeNeefe", "Hwee Tou Ng"], "date": "EMNLP", "abstract": "Bleu is the de facto standard for evaluation and development of statistical machine translation systems. We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in Bleu scores that are questionable or even absurd. These situations arise because Bleu lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification… ", "references": ["cb826a3899752b796f14df1c50378c64954a6b0a", "20c11546a035d2fa2fa1121a7b31e890d20d6b6b", "cb826a3899752b796f14df1c50378c64954a6b0a", "20c11546a035d2fa2fa1121a7b31e890d20d6b6b", "f5b1146b7ca79322aab124fd63825b9c175c02cf", "cb826a3899752b796f14df1c50378c64954a6b0a", "4229d684574bec9e73af9d3b39098317b0447012", "d7da009f457917aa381619facfa5ffae9329a6e9", "cb826a3899752b796f14df1c50378c64954a6b0a", "d7da009f457917aa381619facfa5ffae9329a6e9"]},{"id": "22e9f9e34055c66917a321cf8c9ed222cce82770", "title": "Reading in the brain", "authors": ["Masae Sato"], "date": "2012", "abstract": null, "references": []},{"id": "be9bca1e9b0192fc49b316f2701242b50d98d456", "title": "Further Meta-Evaluation of Machine Translation", "authors": ["Chris Callison-Burch", "Cameron S. Fordyce", "Josh Schroeder"], "date": "WMT@ACL", "abstract": "This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments… ", "references": ["9b53e2a30f070a3bef3ca17a1872a70acfe478f9", "99cb3f1fa41ee8f148bf3a77054698f299da7ecc", "99cb3f1fa41ee8f148bf3a77054698f299da7ecc", "710f79804a50d93521a15a1ce9663d1f221ea7f4", "710f79804a50d93521a15a1ce9663d1f221ea7f4", "0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7", "009f60203ac86cda58a783a7003c575f06c475fd", "52cd0c0a6d83c3eb9cce0674a8b05f8b020a18c5", "99cb3f1fa41ee8f148bf3a77054698f299da7ecc", "52cd0c0a6d83c3eb9cce0674a8b05f8b020a18c5"]},{"id": "da6918ed87095d1313bd20606a934f899d4084b0", "title": "Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation", "authors": ["Chris Callison-Burch", "Philipp Koehn", "Omar Zaidan"], "date": "WMT@ACL", "abstract": "This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments… ", "references": ["57b32d134fe9f9ea223adf63a6f5e5b4aff36531", "db117b2e2052b7754eafd2d8970b2d565a8c3db9", "8a9fc507c3c7fb15253c608d3c760dd87cc347ee", "702ba28f1d42cf8ab1856069670b56c44f4ee04c", "be9bca1e9b0192fc49b316f2701242b50d98d456", "8a9fc507c3c7fb15253c608d3c760dd87cc347ee", "a2b5db095197b6bcd39c4f6c8a5a0a7322b6ef90", "6f30a4d23e4c50919a87e5cad4f2586ad4d46297", "6f30a4d23e4c50919a87e5cad4f2586ad4d46297", "0682d6cad6791564fb69616ca6bb3c8671c4d936"]},{"id": "b8bc656a1935f07e894833b608cc4671b9fa828f", "title": "Application of the ANNA neural network chip to high-speed character recognition", "authors": ["Eduard Säckinger", "Bernhard E. Boser", "Lawrence D. Jackel"], "date": "1992", "abstract": "A neural network with 136000 connections for recognition of handwritten digits has been implemented using a mixed analog/digital neural network chip. The neural network chip is capable of processing 1000 characters/s. The recognition system has essentially the same rate (5%) as a simulation of the network with 32-b floating-point precision. ", "references": ["2c25cb7cb7c0be41510efeaef5cb96dea339823f", "3d79f9bea2be35f4708f2313242d01a62c7f5dab", "3d79f9bea2be35f4708f2313242d01a62c7f5dab", "adf724f637afdb300426df8d2ff4c4342f1e7528", "adf724f637afdb300426df8d2ff4c4342f1e7528", "c2c5940ff6f12d7fc907c9af6ce7854f9f60082d"]},{"id": "0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7", "title": "Re-evaluation the Role of Bleu in Machine Translation Research", "authors": ["Chris Callison-Burch", "Miles Osborne", "Philipp Koehn"], "date": "EACL", "abstract": "We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. ", "references": ["e0cf7771a02921f9d4725f973a01c240d1a20634", "f493b8a1f05fd788b1bcaecf18eb87a3f5965b35", "30158bd75197cd47ad21c6b49c9ef05cc2b609b4", "3ad60c4b460e54fe94894d9e80f695fd3d41b294", "c63bb976dc0d3a897f3b0920170a4c573ef904c6", "e0cf7771a02921f9d4725f973a01c240d1a20634", "4fb5c3f5dcb865134e99402a39a3fb2eff0ab628", "30158bd75197cd47ad21c6b49c9ef05cc2b609b4", "f493b8a1f05fd788b1bcaecf18eb87a3f5965b35", "3ad60c4b460e54fe94894d9e80f695fd3d41b294"]},{"id": "3aa4c691289f56f9af6cf543633cfb3917274281", "title": "Handwritten digit recognition: applications of neural network chips and automatic learning", "authors": ["Y. Le Cun", "L.D. Jackel", "W. Hubbard"], "date": "1989", "abstract": "Two novel methods for achieving handwritten digit recognition are described. The first method is based on a neural network chip that performs line thinning and feature extraction using local template matching. The second method is implemented on a digital signal processor and makes extensive use of constrained automatic learning. Experimental results obtained using isolated handwritten digits taken from postal zip codes, a rather difficult data set, are reported and discussed.<<ETX>> ", "references": ["5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69", "5b5461d8ba70ecd7358b6edd8f39bda711f73a69"]},{"id": "847d6ece37d22430a0d9e061b5dc1d1b8c679055", "title": "Integrated Segmentation and Recognition of Hand-Printed Numerals", "authors": ["James D. Keeler", "David E. Rumelhart", "Wee Kheng Leow"], "date": "NIPS", "abstract": "Neural network algorithms have proven useful for recognition of individual, segmented characters. However, their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm. Conventional, rule-based segmentation algorithms encounter difficulty if the characters are touching, broken, or noisy. The problem in these situations is that often one cannot properly segment a character until it is recognized yet one cannot properly recognize a character until it is… ", "references": ["69e68bfaadf2dccff800158749f5a50fe82d173b"]},{"id": "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "title": "Sequence Transduction with Recurrent Neural Networks", "authors": ["Alex Graves"], "date": "2012", "abstract": "Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few.", "references": ["a0aca3246845016bd8dc996944476f3dd5a5ba56", "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "90b63e917d5737b06357d50aa729619e933d9614", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "96494e722f58705fa20302fe6179d483f52705b4", "a0aca3246845016bd8dc996944476f3dd5a5ba56", "a0aca3246845016bd8dc996944476f3dd5a5ba56", "030a977bf32e81fb694117d78ac84a3fbe2a1d81", "96494e722f58705fa20302fe6179d483f52705b4"]},{"id": "92a9311686e48d5d20fbfcdc21362251b121096c", "title": "Connectionist architectural learning for high performance character and speech recognition", "authors": ["Ulrich Bodenhausen", "Stefan Manke"], "date": "1993", "abstract": "The authors applied an automatic structure optimization (ASO) algorithm to the optimization of multistate time-delay neural networks (MSTDNNs), an extension of the TDNN. These networks allow the recognition of sequences of ordered events that have to be observed jointly. For example, in many speech recognition systems the recognition of words is decomposed into the recognition of sequences of phonemes or phonemelike units. In handwritten character recognition the recognition of characters can… ", "references": ["25406e6733a698bfc4ac836f8e74f458e75dad4f", "c7bfd3dff5f6f0c541bf334b53f63577056c951d", "cd62c9976534a6a2096a38244f6cbb03635a127e", "c7bfd3dff5f6f0c541bf334b53f63577056c951d", "cd62c9976534a6a2096a38244f6cbb03635a127e", "2cee043045b529fceda7964a70e626d45657245a", "cd62c9976534a6a2096a38244f6cbb03635a127e", "25406e6733a698bfc4ac836f8e74f458e75dad4f", "c7bfd3dff5f6f0c541bf334b53f63577056c951d", "c7bfd3dff5f6f0c541bf334b53f63577056c951d"]},{"id": "32315b101afe9bacb725cf80944884e7ac053245", "title": "Shortest path segmentation: a method for training a neural network to recognize character strings", "authors": ["Cjc Burges", "Ofer Matan", "J.I. Ben"], "date": "1992", "abstract": "The authors describe a method which combines dynamic programming and a neural network recognizer for segmenting and recognizing character strings. The method selects the optimal consistent combination of cuts from a set of candidate cuts generated using heuristics. The optimal segmentation is found by representing the image, the candidate segments, and their scores as a graph in which the shortest path corresponds to the optimal interpretation. The scores are given by neural net outputs for… ", "references": []},{"id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories", "authors": ["Svetlana Lazebnik", "Cordelia Schmid", "Jean Ponce"], "date": "2006", "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence.", "references": ["12c7fc38debaf3589e712973642246bd54fe63b3", "a737c107623bcffefa0bac20f1b64677f6a1255a", "a737c107623bcffefa0bac20f1b64677f6a1255a", "fc3098cff5469c55c3e81dc127563afe6dbadf22", "a737c107623bcffefa0bac20f1b64677f6a1255a", "12c7fc38debaf3589e712973642246bd54fe63b3", "0a32f6f23c05827e466580647467a322b7db9f7d", "ee3391a1c0288ccf150b7d4c883918cfedb655bc", "ba6417baed41a8f0fd4cab342aa214704389dcf9", "12c7fc38debaf3589e712973642246bd54fe63b3"]},{"id": "0b8f0e60a648880ddeaed371c339714f66f24624", "title": "Decoding Complexity in Word-Replacement Translation Models", "authors": ["Kevin Knight"], "date": "1999", "abstract": "Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer. Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts. The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel). In order to translate (or \"decode\") a French string, we look for the… ", "references": ["231f6de83cfa4d641da1681e97a11b689a48e3aa", "b95ce39dfeafbdf86dc11c377339897e52571f75", "231f6de83cfa4d641da1681e97a11b689a48e3aa", "c447c0cb2673037633f71faf8ccf4f89806ba1b0", "b95ce39dfeafbdf86dc11c377339897e52571f75", "b95ce39dfeafbdf86dc11c377339897e52571f75", "231f6de83cfa4d641da1681e97a11b689a48e3aa", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "231f6de83cfa4d641da1681e97a11b689a48e3aa", "c447c0cb2673037633f71faf8ccf4f89806ba1b0"]},{"id": "86890c82b589e24007c56e1f40c5f928a0e04183", "title": "Face recognition: a convolutional neural-network approach", "authors": ["Steve Lawrence", "C. Lee Giles", "Andrew D. Back"], "date": "1997", "abstract": "We present a hybrid neural-network for human face recognition which compares favourably with other methods.", "references": ["aa1755e87301af36485ca01e3454bf8888dde8d1", "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "aa1755e87301af36485ca01e3454bf8888dde8d1", "4fb52984078d75ec5655962dc94dc7848182286b", "6bcdca49ed64ec6b15d975adaea49508e9e941d2", "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "6bcdca49ed64ec6b15d975adaea49508e9e941d2", "3b043915a9601fc42ca49808d1d84e24720b0e16", "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "4fb52984078d75ec5655962dc94dc7848182286b"]},{"id": "0bdbc5a8a5ccb718007f0c1999ea7546deb0b473", "title": "A connectionist recognizer for on-line cursive handwriting recognition", "authors": ["Stefan Manke", "Ulrich Bodenhausen"], "date": "1994", "abstract": "Shows how the multi-state time delay neural network (MS-TDNN), which is already used successfully in continuous speech recognition tasks, can be applied both to online single character and cursive (continuous) handwriting recognition. The MS-TDNN integrates the high accuracy single character recognition capabilities of a TDNN with a non-linear time alignment procedure (dynamic time warping algorithm) for finding stroke and character boundaries in isolated, handwritten characters and words. In… ", "references": ["ba98d0bca8df2b6c3699dc2795a96a24478703c1", "cd62c9976534a6a2096a38244f6cbb03635a127e", "24ca5231df7cbd31e11a154c0c63ad48295f0398", "ba98d0bca8df2b6c3699dc2795a96a24478703c1", "85566fa42ab12104679621b1dd003d09c287133d", "85566fa42ab12104679621b1dd003d09c287133d", "cd62c9976534a6a2096a38244f6cbb03635a127e", "c4d2207b9b9ee71da7b24c7172e5107492dd421d", "c4d2207b9b9ee71da7b24c7172e5107492dd421d", "cd62c9976534a6a2096a38244f6cbb03635a127e"]},{"id": "ad3d2f463916784d0c14a19936c1544309a0a440", "title": "A Hierarchical Phrase-Based Model for Statistical Machine Translation", "authors": ["David Chiang"], "date": "ACL", "abstract": "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over… ", "references": ["c9214ebe91454e6369720136ab7dd990d52a07d4", "3e4681ece0316438b9984097894fab3ef56d3b7a", "c6a83c4fcc99ba6753109301949c5b7cfa978079", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "b5b0ab2d445a393341d292b26c8a9a6d01c62051", "b5b0ab2d445a393341d292b26c8a9a6d01c62051"]},{"id": "c6a83c4fcc99ba6753109301949c5b7cfa978079", "title": "The Alignment Template Approach to Statistical Machine Translation", "authors": ["Franz Josef Och", "Hermann Ney"], "date": "2004", "abstract": "A phrase-based statistical machine translation approach the alignment template approach is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the… ", "references": ["f630bbd08dc16e8e61deef8183eaf80d03590d28", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "b2e03d014ac0bf8df47821a2a3e10015c87ceda5", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "0b8f0e60a648880ddeaed371c339714f66f24624", "7ad3903a0c2645585946d00d075676dfe5f220f1", "ab7b5917515c460b90451e67852171a531671ab8"]},{"id": "2a9d6137c95cc7c106bf84f18fb13449eded7826", "title": "Syntax-Directed Transduction", "authors": ["Philip M. Lewis", "Richard Edwin Stearns"], "date": "1968", "abstract": "A transduction is a mapping from one set of sequences to another. A syntax-directed transduction is a particular type of transduction which is defined on the grammar of a context-free language and which is meant to be a model of part of the translation process used in many compilers. The transduction is considered from an automata theory viewpoint as specifying the input-output relation of a machine. Special consideration is given to machines called translators which both transduce and… ", "references": ["30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "e55b40da789c2db358bd0fbc61375b24255df995", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2", "e55b40da789c2db358bd0fbc61375b24255df995", "e55b40da789c2db358bd0fbc61375b24255df995", "e55b40da789c2db358bd0fbc61375b24255df995", "e55b40da789c2db358bd0fbc61375b24255df995"]},{"id": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories", "authors": ["Fei-Fei Li", "Rob Fergus", "Pietro Perona"], "date": "CVPR Workshops", "abstract": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it… ", "references": []},{"id": "53b4aaf51c6d1c164b19e8f9df5cfde560eeb6a7", "title": "Statistical Syntax-Directed Translation with Extended Domain of Locality", "authors": ["Liang Huang", "Kevin Knight", "Aravind K. Joshi"], "date": "2006", "abstract": "In syntax-directed translation, the sourcelanguage input is first parsed into a parsetree, which is then recursively converted into a string in the target-language. We model this conversion by an extended treeto-string transducer that has multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to… ", "references": ["11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "c6a83c4fcc99ba6753109301949c5b7cfa978079", "c4138748eb5dc1bbd1df2951f299d701304147a2", "f5b1146b7ca79322aab124fd63825b9c175c02cf", "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d"]},{"id": "d01737b617acc555153f4660417908bf3971b1a5", "title": "Scalable Inference and Training of Context-Rich Syntactic Translation Models", "authors": ["Michel Galley", "Jonathan Graehl", "Ignacio Thayer"], "date": "ACL", "abstract": "Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we… ", "references": ["1c7f01ee56433232b3d6ef57ce6e9fcec9ae14c3", "1c7f01ee56433232b3d6ef57ce6e9fcec9ae14c3", "1c7f01ee56433232b3d6ef57ce6e9fcec9ae14c3", "13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "b5cd35709797ac02a69e930d97f7917f377107de", "13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "ad3d2f463916784d0c14a19936c1544309a0a440", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "ad3d2f463916784d0c14a19936c1544309a0a440", "b5cd35709797ac02a69e930d97f7917f377107de"]},{"id": "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "title": "A Path-based Transfer Model for Machine Translation", "authors": ["Dekang Lin"], "date": "COLING", "abstract": "We propose a path-based transfer model for machine translation. The model is trained with a word-aligned parallel corpus where the source language sentences are parsed. The training algorithm extracts a set of transfer rules and their probabilities from the training corpus. A rule translates a path in the source language dependency tree into a fragment in the target dependency tree. The problem of finding the most probable translation becomes a graph-theoretic problem of finding the minimum… ", "references": ["b5cd35709797ac02a69e930d97f7917f377107de", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "4e009d39e838aa37654bb4ad5c9be75a8e58fa39", "c24507c2bd75cfc2abf7f83add5c0273ac0ec55a", "4e009d39e838aa37654bb4ad5c9be75a8e58fa39", "4e009d39e838aa37654bb4ad5c9be75a8e58fa39", "4e009d39e838aa37654bb4ad5c9be75a8e58fa39"]},{"id": "7e982f360b44094552264010781a476d85ac78a7", "title": "Tree-to-String Alignment Template for Statistical Machine Translation", "authors": ["Yang Liu", "Qun Liu", "Shouxun Lin"], "date": "ACL", "abstract": "We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source… ", "references": ["ab7b5917515c460b90451e67852171a531671ab8", "c232bb1d6f85acffd936e32f578a4417367a17da", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d", "c232bb1d6f85acffd936e32f578a4417367a17da", "ab7b5917515c460b90451e67852171a531671ab8", "ad3d2f463916784d0c14a19936c1544309a0a440", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "c232bb1d6f85acffd936e32f578a4417367a17da"]},{"id": "83b3eabf8780cc3ca4058d60eb11ca407fb12810", "title": "11 , 001 New Features for Statistical Machine Translation", "authors": ["Marina del Rey", "L. D. Weaver"], "date": "2009", "abstract": "We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new features and the performance of the learning algorithm. ", "references": ["d01737b617acc555153f4660417908bf3971b1a5", "0db6eb46ca9941660acc775e3ca39bf4434c18be", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5", "bbe013543e9c1d8f00499036121c363ad3ab3d7d", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "1f12451245667a85d0ee225a80880fc93c71cc8b", "8d125e6de1e77e9168ad0ed355551cf72062e1c2"]},{"id": "248d32911670e551db4835a5a5279d2d9673ee37", "title": "A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model", "authors": ["Libin Shen", "Jinxi Xu", "Ralph M. Weischedel"], "date": "ACL", "abstract": "In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04… ", "references": ["33c8447e7b7ac1338ae151bba154bd2a78f8981e", "d07db07e9f98e3dd97544bd835619357683fc936", "8dfdedcbeb3b67ecaa4d85ca4b9d1aff368b8e2a", "8dfdedcbeb3b67ecaa4d85ca4b9d1aff368b8e2a", "5a03eea43e128f49218ed95b909da1136c757e57", "ad3d2f463916784d0c14a19936c1544309a0a440", "ad3d2f463916784d0c14a19936c1544309a0a440", "8dfdedcbeb3b67ecaa4d85ca4b9d1aff368b8e2a", "5a03eea43e128f49218ed95b909da1136c757e57", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d"]},{"id": "bbb31e16733c0e71b01baa11001738ab616c2393", "title": "A Dependency Treelet String Correspondence Model for Statistical Machine Translation", "authors": ["Deyi Xiong", "Qun Liu", "Shouxun Lin"], "date": "WMT@ACL", "abstract": "This paper describes a novel model using dependency structures on the source side for syntax-based statistical machine translation: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. In this model translation pairs of source treelets and target strings with their word alignments are learned automatically from the parsed and aligned corpus. The DTSC model allows source treelets and target strings with variables so that the… ", "references": ["33c8447e7b7ac1338ae151bba154bd2a78f8981e", "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "f14847a4565e44661df5afad11d0ae6296c43296", "33c8447e7b7ac1338ae151bba154bd2a78f8981e", "7e982f360b44094552264010781a476d85ac78a7", "ad3d2f463916784d0c14a19936c1544309a0a440", "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d", "7e982f360b44094552264010781a476d85ac78a7"]},{"id": "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "title": "3D Object Recognition with Deep Belief Nets", "authors": ["Vinod Nair", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task.", "references": ["ccd52aff02b0f902f4ce7247c4fee7273014c41c", "f354310098e09c1e1dc88758fca36767fd9d084d", "f354310098e09c1e1dc88758fca36767fd9d084d", "162d958ff885f1462aeda91cd72582323fd6a1f4", "162d958ff885f1462aeda91cd72582323fd6a1f4", "a53da9916b87fa295837617c16ef2ca6462cafb8", "162d958ff885f1462aeda91cd72582323fd6a1f4", "a53da9916b87fa295837617c16ef2ca6462cafb8", "162d958ff885f1462aeda91cd72582323fd6a1f4", "162d958ff885f1462aeda91cd72582323fd6a1f4"]},{"id": "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "title": "A Systematic Comparison of Various Statistical Alignment Models", "authors": ["Franz Josef Och", "Hermann Ney"], "date": "2003", "abstract": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical… ", "references": ["8a1fdc0d36ef7fc3b8ce18d496776e948a047dc3", "8a1fdc0d36ef7fc3b8ce18d496776e948a047dc3", "34d7f231fe2b9f243a60a4a64c06028ad7ba776b", "d18af6780f9242ec988c89ed0b67dc7d05a7785a", "23e1993ad65ef984bd11d58463d59f1e3cec1341", "d18af6780f9242ec988c89ed0b67dc7d05a7785a", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "ab7b5917515c460b90451e67852171a531671ab8", "d4d5bca8dfa851630326a61a3b6211eb8c62416e", "d18af6780f9242ec988c89ed0b67dc7d05a7785a"]},{"id": "d9f2186e2e3aab0e697532aeec0d1b88a61af02d", "title": "Dependency Treelet Translation: Syntactically Informed Phrasal SMT", "authors": ["Chris Quirk", "Arul Menezes", "Colin Cherry"], "date": "ACL", "abstract": "We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an… ", "references": ["b2e03d014ac0bf8df47821a2a3e10015c87ceda5", "4e82deb20354fc7fdf07214d9bf1b4c269a165da", "baa4de120a631c719928c2466681e322c9da7848", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "b2e03d014ac0bf8df47821a2a3e10015c87ceda5", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "b2e03d014ac0bf8df47821a2a3e10015c87ceda5", "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "377113d85df1d3244ef8675ba0d01c286bb3ef40", "b2e03d014ac0bf8df47821a2a3e10015c87ceda5"]},{"id": "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "authors": ["Richard Socher", "Andrej Karpathy", "Andrew Y. Ng"], "date": "2014", "abstract": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use… ", "references": ["ae5e6c6f5513613a161b2c85563f9708bf2e9178", "553fb89d5858826c02f26e94262e8958debc777e", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "acc4e56c44771ebf69302a06af51498aeb0a6ac8", "57458bc1cffe5caa45a885af986d70f723f406b4", "553fb89d5858826c02f26e94262e8958debc777e", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75"]},{"id": "a73ae2ce1cfafae61238b3fdb1bbb61093962b4d", "title": "Factored Neural Language Models", "authors": ["Andrei Alexandrescu", "Katrin Kirchhoff"], "date": "HLT-NAACL", "abstract": "We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models. ", "references": ["ed22171ce376d213bf64d998d594f876f6912cb7", "8b7e3e2af584a5994eb459a20aa32b7f5ef35fd1", "ed22171ce376d213bf64d998d594f876f6912cb7", "3bb45466dfb9770e706d1e63205e266e7761f915", "3bb45466dfb9770e706d1e63205e266e7761f915", "8b7e3e2af584a5994eb459a20aa32b7f5ef35fd1", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "c20b4068be640ebaffbc56382c3e4e0bcf62664e"]},{"id": "182c9ba291d97dc8d7482533044416869cb15f23", "title": "Heterogeneous measurements and multiple classifiers for speech recognition", "authors": ["Andrew K. Halberstadt", "James R. Glass"], "date": "ICSLP", "abstract": "This paper addresses the problem of acoustic phonetic modeling. First, heterogeneous acoustic measurements are chosen in order to maximize the acoustic-phonetic information extracted from the speech signal in preprocessing. Second, classifier systems are presented for successfully utilizing high-dimensional acoustic measurement spaces. The techniques used for achieving these two goals can be broadly categorized as hierarchical, committeebased, or a hybrid of these two. This paper presents… ", "references": ["3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8", "387e7349b8e31e316c2a7380603d1b6c0b1417c8", "33f3f567911bb4c7958a6783b8cffc87957d5213", "6293480445a3cd8cb7e6625d1aca10755364272d", "3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8", "f4fb4a0c71480cda4b8b4fa51d7a58c90e89945e", "3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8", "3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8", "6293480445a3cd8cb7e6625d1aca10755364272d", "f4fb4a0c71480cda4b8b4fa51d7a58c90e89945e"]},{"id": "62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417", "title": "Structured speech modeling", "authors": ["Li Deng", "Dong Yu", "Alex Acero"], "date": "2006", "abstract": "Modeling dynamic structure of speech is a novel paradigm in speech recognition research within the generative modeling framework, and it offers a potential to overcome limitations of the current hidden Markov modeling approach. Analogous to structured language models where syntactic structure is exploited to represent long-distance relationships among words , the structured speech model described in this paper makes use of the dynamic structure in the hidden vocal tract resonance space to… ", "references": ["e7d7e8671c1f7857a46f4d88768b7e7c34588d19", "c171ce5d2aec5c570fafc20c5844cca0862a4774", "e7d7e8671c1f7857a46f4d88768b7e7c34588d19", "4917c57f8e070553c24209d40a4b39a05edef799", "2ffd476d481fa644b8721b43427138f858e2d9ae", "d45b8dcc1f929a43f4dae4dbd69a12d163aa8ed8", "e7d7e8671c1f7857a46f4d88768b7e7c34588d19", "3702e71dd9a051808f2cb30a3564fb96da6d56b4", "8658d645b716d59c5edc80e33e1936e33574bc26", "4917c57f8e070553c24209d40a4b39a05edef799"]},{"id": "71874b310bd6fff855086956bd5a1e7eb56d1739", "title": "Buried Markov models for speech recognition", "authors": ["Jeff A. Bilmes"], "date": "1999", "abstract": "Good HMM-based speech recognition performance requires at most minimal inaccuracies to be introduced by HMM conditional independence assumptions.", "references": ["3d82e058a5c40954b8f5db170a298a889a254c37", "3d82e058a5c40954b8f5db170a298a889a254c37", "ff5a3d6464f842c8bcd4f2c60a9796de0aac25b9", "1e6f96bee0a7b78402866e1461d00a72612dcc69", "aca0186b4167f58ca506ee0f2d4b1e577f33c738", "8c77c78b76355679562fa341fc6ea8eec3f1a2f3", "8c77c78b76355679562fa341fc6ea8eec3f1a2f3", "ff5a3d6464f842c8bcd4f2c60a9796de0aac25b9", "f56fd40cb9c30e440f56eace24924d1538c744da"]},{"id": "c427ef90813a3f324212d66ff0d02e6a49706ece", "title": "Structural design of hidden Markov model speech recognizer using multivalued phonetic features: comparison with segmental speech units.", "authors": ["L Deng", "Kevin Erler"], "date": "1992", "abstract": "A novel approach to speech recognition, on the basis of a multidimensional multivalued phonetic-feature description of speech signals, is presented and evaluated. The hidden Markov model (HMM) framework is used to provide the recognition algorithm, which assumes that the underlying Markov chain tracks the temporal evolution of the features. It is shown that this approach can naturally accommodate such coarticulatory effects as feature spreading and formant transition in the functionality of the… ", "references": []},{"id": "d45b8dcc1f929a43f4dae4dbd69a12d163aa8ed8", "title": "A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal", "authors": ["Li Deng"], "date": "1992", "abstract": "Abstract The standard hidden Markov model (HMM) and the hidden filter model assume local or state-conditioned stationarity for the modeled signal. In this work we generalize these models and develop the ‘trended HMM’ to allow the local, as well as the global (via a Markov chain), non-stationarity to be represented in the model. The mathematical structure of the trended HMM can be described by a discrete-time Markov process with its states associated with distinct regression functions on time… ", "references": []},{"id": "a493a23b86192aa74e6f394061288082e1e7cdb7", "title": "Random clusterings for language modeling", "authors": ["Ahmad Emami", "Frederick Jelinek"], "date": "2005", "abstract": "We present an application of randomization techniques to class-based n-gram language models used in speech recognizers. The idea is to derive a language model from the combination of a set of random class-based models. Each of the constituent random class-based models is built using a separate clustering obtained via a different run of a randomized clustering algorithm. The random class-based model can compensate for some of the shortcomings of conventional class-based models by combining the… ", "references": ["3de5d40b60742e3dfa86b19e7f660962298492af", "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986", "3696da952ee2f16793094455fdff394023b80e5c", "dc4f12712491bf450c978b9af3e6db055b37e5da", "09c76da2361d46689825c4efc37ad862347ca577", "3de5d40b60742e3dfa86b19e7f660962298492af", "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986", "dc4f12712491bf450c978b9af3e6db055b37e5da", "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986", "de5e95325e139fd0a46df1dd28aabecd0273b772"]},{"id": "e41498c05d4c68e4750fb84a380317a112d97b01", "title": "Connectionist language modeling for large vocabulary continuous speech recognition", "authors": ["Holger Schwenk", "Jean-Luc Gauvain"], "date": "2002", "abstract": "This paper describes ongoing work on a new approach for language modeling for large vocabulary continuous speech recognition.", "references": ["94f70b874913d65bdad1be4061e794b485457e77", "5d9180dfc867d2b6642bb2e0467484157e9c0586"]},{"id": "8592a744942cabc7596c90dd6a4e13bdd233b677", "title": "Building continuous space language models for transcribing european languages", "authors": ["Holger Schwenk", "Jean-Luc Gauvain"], "date": "INTERSPEECH", "abstract": "Large vocabulary continuous speech recognizers for English Broadcast News achieve today word error rates below 10%. An important factor for this succes is the availability of large amounts of acoustic and language modeling training data. In this paper the recognition of French Broadcast News and English and Spanish parliament speeches is addressed, tasks for which less resources are available. A neural network language model is applied that takes better advantage of the limited amount of… ", "references": ["d6fb7546a29320eadad868af66835059db93d99f", "e41498c05d4c68e4750fb84a380317a112d97b01", "d6fb7546a29320eadad868af66835059db93d99f", "399da68d3b97218b6c80262df7963baa89dcc71b", "399da68d3b97218b6c80262df7963baa89dcc71b", "d6fb7546a29320eadad868af66835059db93d99f", "d6fb7546a29320eadad868af66835059db93d99f", "e41498c05d4c68e4750fb84a380317a112d97b01", "d6fb7546a29320eadad868af66835059db93d99f", "d6fb7546a29320eadad868af66835059db93d99f"]},{"id": "c256a54a5f3f07a6dcf2dea3a220d0024cf3bfe5", "title": "Connectionist speaker normalization and adaptation", "authors": ["Victor Abrash", "Horacio Franco", "Michael Cohen"], "date": "EUROSPEECH", "abstract": "In a speaker-independent, large-vocabulary continuous speech recognition systems, recognition accuracy varies considerably from speaker to speaker, and performance may be significantly degraded for outlier speakers such as nonnative talkers. In this paper, we explore supervised speaker adaptation and normalization in the MLP component of a hybrid hidden Markov model/ multilayer perceptron version of SRI's DECIPHERTM speech recognition system. Normalization is implemented through an additional… ", "references": ["b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0", "b6f463e1b9b1c8da24f1f3ff7477128a810cd4c0"]},{"id": "4e3ba28fb3493afd2c3db4bd8be6d8d41cf3647a", "title": "Recent innovations in speech-to-text transcription at SRI-ICSI-UW", "authors": ["Andreas Stolcke", "Barry Y. Chen", "Qifeng Zhu"], "date": "2006", "abstract": "We summarize recent progress in automatic speech-to-text transcription at SRI, ICSI, and the University of Washington. The work encompasses all components of speech modeling found in a state-of-the-art recognition system, from acoustic features, to acoustic modeling and adaptation, to language modeling. In the front end, we experimented with nonstandard features, including various measures of voicing, discriminative phone posterior features estimated by multilayer perceptrons, and a novel phone… ", "references": ["1f5b833d743aeaa492bdb6ab890224412e99bb7c", "5b4973d35327badc03f4c4b78991ad41eeeefe67", "57e8957ea13ff5a213c56645051ecc5a53fd07a7", "976f8982cbe7836e1f4c966a42e88f211059f082", "361ca6a00b8e10dd075bc807ad0ab77351002106", "57e8957ea13ff5a213c56645051ecc5a53fd07a7", "2a6ae3d667a5c2601c1852a0753c8b1c749fec1e", "1f5b833d743aeaa492bdb6ab890224412e99bb7c", "1f5b833d743aeaa492bdb6ab890224412e99bb7c", "a82dca0f629819d963a01f3c34237589632b03be"]},{"id": "399da68d3b97218b6c80262df7963baa89dcc71b", "title": "SRILM - an extensible language modeling toolkit", "authors": ["Andreas Stolcke"], "date": "INTERSPEECH", "abstract": "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This… ", "references": ["fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87", "3492842d6ec502cd9d314ccf1080b0defdb7955f", "75d7a13f82da430171db6fe5b3765e485970ec7c", "b54bcfca3fddc26b8889739a247a25e445818149", "3de5d40b60742e3dfa86b19e7f660962298492af", "766018bf388646e23bd53eb3692afb0f67915a14", "b54bcfca3fddc26b8889739a247a25e445818149", "3de5d40b60742e3dfa86b19e7f660962298492af", "c6586e7c73cc1c9e9a251947425c54c5051be626", "be1fed9544830df1137e72b1d2396c40d3e18365"]},{"id": "7f58b2140534a391c2decb4ab09ab4cecdb548a4", "title": "ACID/HNN: clustering hierarchies of neural networks for context-dependent connectionist acoustic modeling", "authors": ["Jürgen Fritsch", "Michael Finke"], "date": "1998", "abstract": "We present the ACID/HNN framework, a principled approach to hierarchical connectionist acoustic modeling in large vocabulary conversational speech recognition (LVCSR). Our approach consists of an agglomerative clustering algorithm based on information divergence (ACID) to automatically design and robustly estimate hierarchies of neural networks (HNN) for arbitrarily large sets of context-dependent decision tree clustered HMM states. We argue that a hierarchical approach is crucial in applying… ", "references": ["692af6e8ca5b28d6e21cabddaba43b9b490bd3f6", "0d8c323cf5af3ed2ce85cd78692fcf53b6d0635b", "8a0a14d8f84fcd8003cc0417cb97b2455dac3eee", "583f48d72127c00e0499513c1b2f4e8ac71e4406", "0d8c323cf5af3ed2ce85cd78692fcf53b6d0635b", "692af6e8ca5b28d6e21cabddaba43b9b490bd3f6", "692af6e8ca5b28d6e21cabddaba43b9b490bd3f6", "0d8c323cf5af3ed2ce85cd78692fcf53b6d0635b", "81f87408690b155ee08896acf33cf1b74be60460", "0b45d669ac412a98569fcb04ac766a8440927e8e"]},{"id": "4ccfd99d94047c590899e19d4886d716fd1172c0", "title": "Using random forest language models in the IBM RT-04 CTS system", "authors": ["Peng Xu", "Lidia Mangu"], "date": "INTERSPEECH", "abstract": "One of the challenges in large vocabulary speech recognition is the availability of large amounts of data for training language models. In most state-of-the-art speech recognition systems, -gram models with Kneser-Ney smoothing still prevail due to their simplicity and effectiveness. In this paper, we study the performance of a new language model, the random forest language model, in the IBM conversational telephony speech recognition system. We show that although the random forest language… ", "references": ["231f6de83cfa4d641da1681e97a11b689a48e3aa", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "8dba93649dac30f00970cfbd706eac9ba794b836", "9576c0f4202ea4e6b30f9c13b397e3ccc5476250", "de5e95325e139fd0a46df1dd28aabecd0273b772", "231f6de83cfa4d641da1681e97a11b689a48e3aa", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "9576c0f4202ea4e6b30f9c13b397e3ccc5476250", "de5e95325e139fd0a46df1dd28aabecd0273b772", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab"]},{"id": "be1fed9544830df1137e72b1d2396c40d3e18365", "title": "A Cache-Based Natural Language Model for Speech Recognition", "authors": ["Roland Kuhn", "Renato De Mori"], "date": "1990", "abstract": "Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented… ", "references": []},{"id": "ed22171ce376d213bf64d998d594f876f6912cb7", "title": "Neural network language models for conversational speech recognition", "authors": ["Holger Schwenk", "Jean-Luc Gauvain"], "date": "2004", "abstract": "Recently there is growing interest in using neural networks for language modeling. In contrast to the well known backoff ngram language models (LM), the neural network approach tries to limit problems from the data sparseness by performing the estimation in a continuous space, allowing by these means smooth interpolations. Therefore this type of LM is interesting for tasks for which only a very limited amount of in-domain training data is available, such as the modeling of conversational speech… ", "references": ["bbc659924151c767f907de9b412d9b7619d26690", "d1ee87290fa827f1217b8fa2bccb3485da1a300e", "399da68d3b97218b6c80262df7963baa89dcc71b", "e41498c05d4c68e4750fb84a380317a112d97b01"]},{"id": "1ac8987534ad3be87d4b70195c1beb039b102409", "title": "The use of a linguistically motivated language model in conversational speech recognition", "authors": ["Weiqi Wang", "Andreas Stolcke", "Mary P. Harper"], "date": "2004", "abstract": "Structured language models have recently been shown to give significant improvements in large-vocabulary recognition relative to traditional word N-gram models, but typically imply a heavy computational burden and have not been applied to large training sets or complex recognition systems. Previously, we developed a linguistically motivated and computationally efficient almost-parsing language model, using a data structure derived from constraint dependency grammar parsing, that tightly… ", "references": ["e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3", "673992da19d9209434615b12d55bdd36be706e9e", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "bb8e5322dca1657e0cd2925fe1209a16a0c3aefb", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "3de5d40b60742e3dfa86b19e7f660962298492af", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "673992da19d9209434615b12d55bdd36be706e9e", "57e8957ea13ff5a213c56645051ecc5a53fd07a7"]},{"id": "bbc659924151c767f907de9b412d9b7619d26690", "title": "Conversational telephone speech recognition", "authors": ["Jean-Luc Gauvain", "Lori Lamel", "Fabrice Lefèvre"], "date": "2003", "abstract": "This paper describes the development of a speech recognition system for the processing of telephone conversations, starting with a state-of-the-art broadcast news transcription system. We identify major changes and improvements in acoustic and language modeling, as well as decoding, which are required to achieve state-of-the-art performance on conversational speech. Some major changes on the acoustic side include the use of speaker normalization (VTLN), the need to cope with channel variability… ", "references": ["d80000d84223e177d070a01a734dba56d5f5c069", "952b518411bef2331a53368b2f6471bdb7ec2bdf", "23318a16f8a049409be848be6e5fcdef22e78012", "952b518411bef2331a53368b2f6471bdb7ec2bdf", "94f70b874913d65bdad1be4061e794b485457e77", "23318a16f8a049409be848be6e5fcdef22e78012", "107d72c63ecba241a2e3a9b01931e2c373550b88", "aabf8af6403034b0fe5ad7ca7624b5584e5dcfef", "5d9180dfc867d2b6642bb2e0467484157e9c0586", "d80000d84223e177d070a01a734dba56d5f5c069"]},{"id": "6769e78616bc3a332a1829d1a3c2220e5b94555d", "title": "MODELING CHARACTERS VERSUS WORDS FOR MANDARIN SPEECH RECOGNITION", "authors": ["Jun Luo", "Lori Lamel", "Jean-Luc Gauvain"], "date": "2009", "abstract": "Word based models are widely used in speech recognition since they typically perform well. However, the question of whether it is better to use a word-based or a character-based model warrants being for the Mandarin Chinese language. Since Chinese is written without any spaces or word delimiters, a word segmentation algorithm is applied in a pre-processing step prior to training a wordbased language model. Chinese characters carry meaning and speakers are free to combine characters to construct… ", "references": []},{"id": "bfab4ffa229c8af0174a683ff1eda524c4f59d00", "title": "Can artificial neural networks learn language models?", "authors": ["Wei Xu", "Alex Rudnicky"], "date": "INTERSPEECH", "abstract": "Currently, N-gram models are the most common and widely used models for statistical language modeling. In this paper, we investigated an alternative way to build language models, i.e., using artificial neural networks to learn the language model. Our experiment result shows that the neural network can learn a language model that has performance even better than standard statistical methods. ", "references": []},{"id": "a1a4d2f3621ce7c56d7c396bb59f4eaf0e2b14fa", "title": "Shrinking Exponential Language Models", "authors": ["Stanley F. Chen"], "date": "HLT-NAACL", "abstract": "In (Chen, 2009), we show that for a variety of language models belonging to the exponential family, the test set cross-entropy of a model can be accurately predicted from its training set cross-entropy and its parameter values. In this work, we show how this relationship can be used to motivate two heuristics for \"shrinking\" the size of a language model to improve its performance. We use the first heuristic to develop a novel class-based language model that outperforms a baseline word trigram… ", "references": ["29053eab305c2b585bcfbb713243b05646e7d62d", "add8a2aad71e86622ebaa6a514692408defc707c", "1ac8987534ad3be87d4b70195c1beb039b102409", "29053eab305c2b585bcfbb713243b05646e7d62d", "1ac8987534ad3be87d4b70195c1beb039b102409", "55b9d797014276977c32cc7cc1e8825ed8dcd01c", "09c76da2361d46689825c4efc37ad862347ca577", "55b9d797014276977c32cc7cc1e8825ed8dcd01c", "29053eab305c2b585bcfbb713243b05646e7d62d", "55b9d797014276977c32cc7cc1e8825ed8dcd01c"]},{"id": "ed23c461535afc492e80c63ee8d1ed55b8a176e1", "title": "Scaling shrinkage-based language models", "authors": ["Stanley F. Chen", "Lidia Mangu", "Abhinav Sethy"], "date": "2009", "abstract": "In [1], we show that a novel class-based language model, Model M, and the method of regularized minimum discrimination information (rMDI) models outperform comparable methods on moderate amounts of Wall Street Journal data. Both of these methods are motivated by the observation that shrinking the sum of parameter magnitudes in an exponential language model tends to improve performance [2]. In this paper, we investigate whether these shrinkage-based techniques also perform well on larger… ", "references": ["dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715", "29053eab305c2b585bcfbb713243b05646e7d62d", "29053eab305c2b585bcfbb713243b05646e7d62d", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715", "0e4d042b668805e19f097b7eb0f223babec68f67", "a1a4d2f3621ce7c56d7c396bb59f4eaf0e2b14fa", "0687165a9f0360bde0469fd401d966540e0897c3", "a1a4d2f3621ce7c56d7c396bb59f4eaf0e2b14fa", "a1a4d2f3621ce7c56d7c396bb59f4eaf0e2b14fa"]},{"id": "84b533115bfe031aaef4722bae8f54e1a39db01e", "title": "Improving Mandarin Chinese STT system with Random Forests language models", "authors": ["Ilya Oparin", "Lori Lamel", "Jean-Luc Gauvain"], "date": "2010", "abstract": "The goal of this work is to assess the capacity of random forest language models estimated on a very large text corpus to improve the performance of an STT system. Previous experiments with random forests were mainly concerned with small or medium size data tasks. In this work the development version of the 2009 LIMSI Mandarin Chinese STT system was chosen as a challenging baseline to improve upon. This system is characterized by a language model trained on a very large text corpus (over 3.2… ", "references": ["b27bfa0b165b98067f633400dfc3c8757a285779", "b27bfa0b165b98067f633400dfc3c8757a285779", "94f70b874913d65bdad1be4061e794b485457e77", "cc65823588e0a6aab19c9989b128b41dc05fe2c4", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "05d4a2711e0540907b7f1830c4f71812ed5095ef", "6f2dae30e16730cf2ef134737cf642632b7235cd", "94f70b874913d65bdad1be4061e794b485457e77", "ff80a400198f0ce26887672407d8872825e663bf", "8b395470a57c48d174c4216ea21a7a58bc046917"]},{"id": "26180488dac9be0d26eba8ab5e3cd9a0ba5213be", "title": "Efficient estimation of maximum entropy language models with n-gram features: an SRILM extension", "authors": ["Tanel Alumäe", "Mikko Kurimo"], "date": "INTERSPEECH", "abstract": "We present an extension to the SRILM toolkit for training maximum entropy language models with N -gram features. The extension uses a hierarchical parameter estimation procedure [1] for making the training time and memory consumption feasible for moderately large training data (hundreds of millions of words). Experiments on two speech recognition tasks indicate that the models trained with our implementation perform equally to or better than N -gram models built with interpolated Kneser-Ney… ", "references": ["f652e8af69fc13a74c54ff332223827822a12339", "4af41f4d838daa7ca6995aeb4918b61989d1ed80", "371edb7eec6fb1edb12ffa5ece51de2a6273a90b", "f652e8af69fc13a74c54ff332223827822a12339", "371edb7eec6fb1edb12ffa5ece51de2a6273a90b", "0b26fa1b848ed808a0511db34bce2426888f0b68", "0b26fa1b848ed808a0511db34bce2426888f0b68", "399da68d3b97218b6c80262df7963baa89dcc71b", "0b26fa1b848ed808a0511db34bce2426888f0b68", "0b26fa1b848ed808a0511db34bce2426888f0b68"]},{"id": "77dfe038a9bdab27c4505444931eaa976e9ec667", "title": "Empirical Evaluation and Combination of Advanced Language Modeling Techniques", "authors": ["Anoop Deoras", "Jan Černocký"], "date": "INTERSPEECH", "abstract": "We present results obtained with several advanced language modeling techniques, including class based model, cache model, maximum entropy model, structured language model, random forest language model and several types of neural network based language models. We show results obtained after combining all these models by using linear interpolation. We conclude that for both small and moderately sized tasks, we obtain new state of the art results with combination of models, that is significantly… ", "references": ["a493a23b86192aa74e6f394061288082e1e7cdb7", "bb8e5322dca1657e0cd2925fe1209a16a0c3aefb", "7fefba4d85d8eb32efe43fd54a13c9b396ac19dc", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "9319ca5a532462f9f3515ac3d317668aa9650d5b", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "bd7d93193aad6c4b71cc8942e808753019e87706", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "26180488dac9be0d26eba8ab5e3cd9a0ba5213be", "9319ca5a532462f9f3515ac3d317668aa9650d5b"]},{"id": "8606671e9036a58c9e3fd96f8ed161edd4536e47", "title": "Improved Discriminative Bilingual Word Alignment", "authors": ["Robert C. Moore", "Wen-tau Yih", "Andreas Bode"], "date": "ACL", "abstract": "For many years, statistical machine translation relied on generative models to provide bilingual word alignments. In 2005, several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach. Building on this work, we demonstrate substantial improvement in word-alignment accuracy, partly though improved training methods, but predominantly through selection of more and better features. Our best model produces the lowest alignment… ", "references": ["ddddd9be714919db194a371070a1e13a6a0546b4", "942ab13277b7f9f3dc7fc00419537c7bfb039f3f", "cee30e5fe700b98bc408bc40ea9ec396520b473a", "64a007a07cbeab1b6949f196e58fdbe93ef1a297", "ddddd9be714919db194a371070a1e13a6a0546b4", "cee30e5fe700b98bc408bc40ea9ec396520b473a", "9ab1b0907011792ad510abb9fb505beb4ae65502", "bd1a8fafc2c5f807984331c60a758d31b58294b0", "9ab1b0907011792ad510abb9fb505beb4ae65502", "94d6f1910680b46b44dd03de78b892e158381fa5"]},{"id": "bf32a271a17c9c3376127d287f746e4876779d49", "title": "Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies", "authors": ["John G. McMahon", "Francis Jack Smith"], "date": "1996", "abstract": "An automatic word-classification system has been designed that uses word unigram and bigram frequency statistics to implement a binary top-down form of word clustering and employs an average class mutual information metric. Words are represented as structural tags---n-bit numbers the most significant bit-patterns of which incorporate class information. The classification system has revealed some of the lexical structure of English, as well as some phonemic and semantic structure. The system has… ", "references": ["ad283aa8c159d695a75d35d9f540ba4b3bd21f61", "8bf3db586759634c4ea334a6732c6472a692ef02", "01a9a9686d45a3dc8182f59a1a77f1ac4f233761", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "8bf3db586759634c4ea334a6732c6472a692ef02", "01a9a9686d45a3dc8182f59a1a77f1ac4f233761", "01a9a9686d45a3dc8182f59a1a77f1ac4f233761", "8bf3db586759634c4ea334a6732c6472a692ef02", "af386a4e0f2615ed929fdc64a86df8e383bd6121"]},{"id": "cee30e5fe700b98bc408bc40ea9ec396520b473a", "title": "A Discriminative Framework for Bilingual Word Alignment", "authors": ["Robert C. Moore"], "date": "HLT/EMNLP", "abstract": "Bilingual word alignment forms the foundation of most approaches to statistical machine translation. Current word alignment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small… ", "references": ["ab7b5917515c460b90451e67852171a531671ab8", "bd1a8fafc2c5f807984331c60a758d31b58294b0", "bd1a8fafc2c5f807984331c60a758d31b58294b0", "ab7b5917515c460b90451e67852171a531671ab8", "c232bb1d6f85acffd936e32f578a4417367a17da", "5a7958b418bceb48a315384568091ab1898b1640", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "3a7d2b69742436efd700e044360a0bbdc6022cc0", "ab7b5917515c460b90451e67852171a531671ab8", "c232bb1d6f85acffd936e32f578a4417367a17da"]},{"id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "title": "Distributional Clustering of English Words", "authors": ["Fernando C Pereira", "Naftali Tishby", "Lillian Lee"], "date": "ACL", "abstract": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts.", "references": ["f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "8c24136a3465880ec820b7a0a147fdaba8cf16a5", "3de5d40b60742e3dfa86b19e7f660962298492af", "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0", "78f749705501e89fd953e9683f7cee464caca687", "7dbdb4209626fd92d2436a058663206216036e68", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "8c24136a3465880ec820b7a0a147fdaba8cf16a5", "7dbdb4209626fd92d2436a058663206216036e68", "297e478f92cef1cd090706fc59fde5ea0836ce80"]},{"id": "38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1", "title": "Models of Translational Equivalence among Words", "authors": ["I. Dan Melamed"], "date": "2000", "abstract": "Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is typically only partialmany words in each text have no clear equivalent in the other text. This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are… ", "references": ["5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "f02cd2a3a8b7fe72d0be23cbf57bc6e5bc8b7797", "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "13a5c0e48580415e825ecba20d6db50b93949c1b", "230ffcb119b64b4d0c9e46d1f603a5dc60e93fa8", "42fd4d469c53e4eedd7eb76e7859e3270367f795", "42fd4d469c53e4eedd7eb76e7859e3270367f795", "15693f74eee3728eb576b466de51296298de1e82", "13a5c0e48580415e825ecba20d6db50b93949c1b", "42fd4d469c53e4eedd7eb76e7859e3270367f795"]},{"id": "6c8f9b2b61e49c43e4639b3c1ce68f993fc2aa91", "title": "Semi-Supervised Training for Statistical Word Alignment", "authors": ["Alexander M. Fraser", "Daniel Marcu"], "date": "ACL", "abstract": "We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. ", "references": ["59c442932e9fcfcac6df5566c2bcd1ec331548c9", "9ab1b0907011792ad510abb9fb505beb4ae65502", "94d6f1910680b46b44dd03de78b892e158381fa5", "d7da009f457917aa381619facfa5ffae9329a6e9", "c232bb1d6f85acffd936e32f578a4417367a17da", "c232bb1d6f85acffd936e32f578a4417367a17da", "efaffeae84ee93f87f70e42415b67feb639e0809", "c232bb1d6f85acffd936e32f578a4417367a17da", "52805ca2a7f5f6e73dc90ff20f1ca2f198dd031b", "64a007a07cbeab1b6949f196e58fdbe93ef1a297"]},{"id": "94d6f1910680b46b44dd03de78b892e158381fa5", "title": "A Probability Model to Improve Word Alignment", "authors": ["Colin Cherry", "Dekang Lin"], "date": "ACL", "abstract": "Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. ", "references": ["49373d6fac6196753efa8c976e6076d63c0c5f4b", "cdceaaa4067b96c27dcfd446bbbcfc1c5e6a07fc", "ab7b5917515c460b90451e67852171a531671ab8", "49373d6fac6196753efa8c976e6076d63c0c5f4b", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "cdceaaa4067b96c27dcfd446bbbcfc1c5e6a07fc", "ab7b5917515c460b90451e67852171a531671ab8", "49373d6fac6196753efa8c976e6076d63c0c5f4b", "b5cd35709797ac02a69e930d97f7917f377107de", "59c442932e9fcfcac6df5566c2bcd1ec331548c9"]},{"id": "f76b28565a0c677f14f802dca10d8d0db09a6cc3", "title": "Efficient Handling of N-gram Language Models for Statistical Machine Translation", "authors": ["Marcello Federico", "Mauro Cettolo"], "date": "WMT@ACL", "abstract": "Statistical machine translation, as well as other areas of human language processing, have recently pushed toward the use of large scale n-gram language models. This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder, an open source toolkit for statistical machine translation. Experiments are reported with a high performing baseline, trained on the Chinese-English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC… ", "references": ["5cf44cc2df288bee7933fe49875548742426b854"]},{"id": "0aee499c099b74d4bb2c860ab044a54becd08a4b", "title": "Machine translation in continuous space", "authors": ["Ruhi Sarikaya", "Yonggang Deng", "Yuqing Gao"], "date": "INTERSPEECH", "abstract": "We present a different perspective on the machine translation problem that relies upon continuous-space probabilistic models for words and phrases. Within this perspective we propose a method called Tied-Mixture Machine Translation (TMMT) that uses a trainable parametric model employing Gaussian mixture probability density functions to represent wordand phrase–pairs. In the new perspective, machine translation is treated in the same way as acoustic modeling in speech recognition. This new… ", "references": ["1a7feb0408eb146f8a16c14763e549362e9ff7fd", "c06b7af4128a2c7c1b9977585bc026c6916322e9", "f83615a105d602fefb4076293a3c95d3659f26b6", "ae4614f758dfa344a04b33377c96abc10d5eeda7", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "c06b7af4128a2c7c1b9977585bc026c6916322e9", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "d7da009f457917aa381619facfa5ffae9329a6e9"]},{"id": "113c32635f3b83fbbc1c7af720dfc68d37a37848", "title": "Improving statistical MT by coupling reordering and decoding", "authors": ["Josep Maria Crego", "José B. Mariño"], "date": "2006", "abstract": "In this paper we describe an elegant and efficient approach to coupling reordering and decoding in statistical machine translation, where the n-gram translation model is also employed as distortion model. The reordering search problem is tackled through a set of linguistically motivated rewrite rules, which are used to extend a monotonic search graph with reordering hypotheses. The extended graph is traversed in the global search when a fully informed decision can be taken. Further experiments… ", "references": ["ad53fda492f253d89e1f1649582706bdddca8acd", "f5b1146b7ca79322aab124fd63825b9c175c02cf", "750f03c4cb56190b66401809999d6fb47c7c44c7", "426baf47d1792113d4418c587a3a5317d7a73757", "f5b1146b7ca79322aab124fd63825b9c175c02cf", "67cba9e480a1759144af5b3697fb745730308050", "40c7b354c92e179d170ddfdf457424f92ff20d81", "223dcd0e44532fc02444709e61327432c74fe46d", "40c7b354c92e179d170ddfdf457424f92ff20d81", "750f03c4cb56190b66401809999d6fb47c7c44c7"]},{"id": "057cb927b59c7cc16141fca2c825da1e3e3ef81a", "title": "Smooth Bilingual N-Gram Translation", "authors": ["Holger Schwenk", "Marta R. Costa-jussà", "José A. R. Fonollosa"], "date": "EMNLP-CoNLL", "abstract": "We address the problem of smoothing translation probabilities in a bilingual N-grambased statistical machine translation system. It is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation. A neural network is used to perform the projection and the probability estimation. Smoothing probabilities is most important for tasks with a limited amount of training material. We consider here the BTEC task of the 2006 IWSLT… ", "references": ["79794914adfbe844151b01bd0216c275cc33de4f", "45309ab66457ed450ef8d487cefca27e51bf8eec", "68709f1c324bcc485b4895240fab59353570be9b", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "79794914adfbe844151b01bd0216c275cc33de4f", "028a35d6835b2717c02a7acd5be1c71e0749df26", "cb3dcb13abd096a33780e6268ee4aaa583b198e8"]},{"id": "231a173bf9e0474c8eef8184d014507021ab2434", "title": "Wider Context by Using Bilingual Language Models in Machine Translation", "authors": ["Jan Niehues", "Teresa Herrmann", "Alexander H. Waibel"], "date": "WMT@EMNLP", "abstract": "In past Evaluations for Machine Translation of European Languages, it could be shown that the translation performance of SMT systems can be increased by integrating a bilingual language model into a phrase-based SMT system.", "references": ["aa1dc69e299eaef7911a35afb56ca079864325c2", "06a335f349ff121613e65a9221b0074fe021a191", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "cfac14399ef841fe2d7315f70aa6c8482adc5259", "d5eb53055c57e42268c61533f7b2e911db2a5dad", "e421fc9b8d368d6a9542cc384abe0d43cdd57209", "06a335f349ff121613e65a9221b0074fe021a191", "aa1dc69e299eaef7911a35afb56ca079864325c2", "229f9ae6a3b4470f3c6f22ccaa3a8fbcee9d1d52", "c36d355e01e1ed90e57bffbbfc274d4d98952b96"]},{"id": "f73126f276c4cd1566d99bef9d996a2a6050a130", "title": "LIMSI's experiments in domain adaptation for IWSLT11", "authors": ["Thomas Lavergne", "Alexandre Allauzen", "François Yvon"], "date": "IWSLT", "abstract": "LIMSI took part in the IWSLT 2011 TED task in the MT track for English to French using the in-house n-code system, which implements the n-gram based approach to Machine Translation. This framework not only allows to achieve state-of-the-art results for this language pair, but is also appealing due to its conceptual simplicity and its use of well understood statistical language models. Using this approach, we compare several ways to adapt our existing systems and resources to the TED task with… ", "references": ["15ef3e35fdea806b5e255b63bb908c40d5c7df45", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "195df0de3c4c181d26391dd73746c7aefe709ab6", "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "4b7ec490154397c2691d3404eccd412665fa5e6a", "b2a83db053c69acf28c935356d1400c14ca0fda4", "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "4b7ec490154397c2691d3404eccd412665fa5e6a", "195df0de3c4c181d26391dd73746c7aefe709ab6"]},{"id": "b3e89f05876d47b9bd6ece225aaeee457a6824e8", "title": "Statistical Machine Translation", "authors": ["Miles Osborne"], "date": "Encyclopedia of Machine…", "abstract": "Statistical Machine Translation (SMT) deals with automati cally mapping sentences in one human language (for example French) into another huma n language (such as English.", "references": ["d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "ad3d2f463916784d0c14a19936c1544309a0a440"]},{"id": "24653b3b33d48c409deb672f8d8ee0eff31cd418", "title": "Category-Based Statistical Language Models", "authors": ["Thomas R Niesler"], "date": "1997", "abstract": "Language models are computational techniques and structures that describe word sequences produced by human subjects, and the work presented here considers primarily their application to automatic speech-recognition systems. Due to the very complex nature of natural languages as well as the need for robust recognition, statistically-based language models, which assign probabilities to word sequences, have proved most successful. This thesis focuses on the use of linguistically defined word… ", "references": ["b00383b0a28388a1ed6d3765131c540566eba745", "be1fed9544830df1137e72b1d2396c40d3e18365", "d61ab1c4c4046beb44823a059ae23fc7c4fc19fc", "0687165a9f0360bde0469fd401d966540e0897c3", "218395e3d1198b5ae2bf3bfd2a18b164fea2d79a", "0687165a9f0360bde0469fd401d966540e0897c3", "a8fcc058607129590fa6ab692a38807efc7d7f1f", "ab9cb5b954770509c0e956b7613eec13caf33503", "8bf3db586759634c4ea334a6732c6472a692ef02", "8bf3db586759634c4ea334a6732c6472a692ef02"]},{"id": "26080f0969a520ebd82f252dd060f5a4948bbd6e", "title": "Empirical study of neural network language models for Arabic speech recognition", "authors": ["Ahmad Emami", "Lidia Mangu"], "date": "2007", "abstract": "In this paper we investigate the use of neural network language models for Arabic speech recognition. By using a distributed representation of words, the neural network model allows for more robust generalization and is better able to fight the data sparseness problem. We investigate different configurations of the neural probabilistic model, experimenting with such parameters as N-gram order, output vocabulary, normalization method, and model size and parameters. Experiments were carried out… ", "references": ["eade33a08823bbc4dca40c445305897d715ab507", "eade33a08823bbc4dca40c445305897d715ab507", "eade33a08823bbc4dca40c445305897d715ab507", "a73ae2ce1cfafae61238b3fdb1bbb61093962b4d", "3d6036af971c1f11ab712cc41487376a94e63673", "a73ae2ce1cfafae61238b3fdb1bbb61093962b4d", "6ddcd96156d45dd3faa40e7815c7aa2fa02bcb70", "a73ae2ce1cfafae61238b3fdb1bbb61093962b4d", "8b395470a57c48d174c4216ea21a7a58bc046917", "6a92dd7a90b95b570ca46b9d880cea404030fea6"]},{"id": "82e3794a2f7de37d60602681a25eef7711ec8ab8", "title": "Statistical and Discriminative Methods for Speech Recognition", "authors": ["Biing-Hwang Juang", "Wu Chou", "Chin-Hui Lee"], "date": "1996", "abstract": "A critical component in the pattern matching approach to speech recognition is the training algorithm which aims at producing typical (reference) patterns or models for accurate pattern comparison. In this chapter, we discuss the issue of speech recognizer training from a broad perspective with root in the classical Bayes decision theory. We differentiate the method of classifier design by way of distribution estimation and the method of discriminative training based on the fact that in many… ", "references": []},{"id": "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "title": "Continuous-Space Language Models for Statistical Machine Translation", "authors": ["Holger Schwenk"], "date": "2010", "abstract": "Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. \n \nIn this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the… ", "references": ["37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "1a8c3a6e8b773fb475b376bee7c9622390a5ff88", "8b395470a57c48d174c4216ea21a7a58bc046917", "1a8c3a6e8b773fb475b376bee7c9622390a5ff88", "6a5f9307b8b8473b233432b0e8aa0b4bef311996", "8b395470a57c48d174c4216ea21a7a58bc046917", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "5ed0f2a01ccd10405910ca281574404ca207a72a", "399da68d3b97218b6c80262df7963baa89dcc71b", "399da68d3b97218b6c80262df7963baa89dcc71b"]},{"id": "bd7d93193aad6c4b71cc8942e808753019e87706", "title": "Three new graphical models for statistical language modelling", "authors": ["Andriy Mnih", "Geoffrey E. Hinton"], "date": "ICML '07", "abstract": "The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as… ", "references": ["2c27f79a8c5db9347a85b024e68bb21a26d7eefd", "2c27f79a8c5db9347a85b024e68bb21a26d7eefd", "3d6036af971c1f11ab712cc41487376a94e63673", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "4823ecc86603ae5b8238c3101c2089c91a5e1da1", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "3d6036af971c1f11ab712cc41487376a94e63673", "2c27f79a8c5db9347a85b024e68bb21a26d7eefd", "3d6036af971c1f11ab712cc41487376a94e63673", "3d6036af971c1f11ab712cc41487376a94e63673"]},{"id": "fd0101dfbdd6768efe1e99a5ccd3ec0415fe723f", "title": "Accelerated DP based search for statistical translation", "authors": ["Christoph Tillmann", "Stephan E. Vogel", "Hassan Sawaf"], "date": "EUROSPEECH", "abstract": "In this paper, we describe a fast search algorithm for statistical translation based on dynamic programming (DP) and present experimental results. The approach is based on the assumption that the word alignment is monotone with respect to the word order in both languages. To reduce the search e ort for this approach, we introduce two methods: an acceleration technique to e ciently compute the dynamic programming recursion equation and a beam search strategy as used in speech recognition. The… ", "references": ["3ef4b3a503b3355c1ec09bfb5fe68b04c74a0a33"]},{"id": "53ca064b9f1b92951c1997e90b776e95b0880e52", "title": "Learning word embeddings efficiently with noise-contrastive estimation", "authors": ["Andriy Mnih", "Koray Kavukcuoglu"], "date": "NIPS", "abstract": "Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks.", "references": ["c19fbefdeead6a4154a22a9c8551a18b1530033a", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "57458bc1cffe5caa45a885af986d70f723f406b4", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "fac2ca048fdd7e848f0b9ba2f7be25bb49186770", "dac72f2c509aee67524d3321f77e97e8eff51de6"]},{"id": "e653891a9a9ac5585abb3348732d7e5a8f4e686c", "title": "Model-based MCE bound to the true Bayes' error", "authors": ["R. A. Schluter", "H. Ney"], "date": "2001", "abstract": "We show that the minimum classification error (MCE) criterion gives an upper bound to the true Bayes' error rate independent of the corresponding model distribution. In addition, we show that model-free optimization of the MCE criterion leads to a closed form solution in the asymptotic case of infinite training data. While leading to the Bayes' error rate, the resulting model distribution differs from the true distribution. This suggests that the structure of model distributions trained with… ", "references": []},{"id": "bbf24db4c7e1112188f931f0751944e8ebef68a0", "title": "Minimum Classification Error Training in Exponential Language Models", "authors": ["Chris J. Paciorek", "Roni Rosenfeld"], "date": "2000", "abstract": "Minimum Classification Error (MCE) training is difficult to apply to language modeling due to inherent scarcity of training data (N-best lists). However, a whole-sentence exponential language model is particularly suitable for MCE training, because it can use a relatively small number of powerful features to capture global sentential phenomena. We review the model, discuss feature induction, find features in both the Broadcast News and Switchboard domains, and build an MCE-trained model for the… ", "references": ["b951b9f78b98a186ba259027996a48e4189d37e5", "4d8cb09f19c0afdc68d39cc55743104ec396d86e", "4d8cb09f19c0afdc68d39cc55743104ec396d86e", "1ed73fbe981e7ed608f34745505be3a7a1eea26d"]},{"id": "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "title": "Discriminative Training and Maximum Entropy Models for Statistical Machine Translation", "authors": ["Franz Josef Och", "Hermann Ney"], "date": "ACL", "abstract": "We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.", "references": ["fb486e03369a64de2d5b0df86ec0a7b55d3907db", "2a4f13491efd1d8e4c3ad941d1e8bbceefb8b4bf", "2479cc774fcf8486a98c7b18d131ba7807e58dd6", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "ab7b5917515c460b90451e67852171a531671ab8", "9a2a4d31e26703d5cdd346fcbab7331b44422a6c", "2a4f13491efd1d8e4c3ad941d1e8bbceefb8b4bf", "82e3794a2f7de37d60602681a25eef7711ec8ab8", "2a4f13491efd1d8e4c3ad941d1e8bbceefb8b4bf", "9a2a4d31e26703d5cdd346fcbab7331b44422a6c"]},{"id": "dd85cca2c133835ea29069a6a4438c70185bd427", "title": "Parsing Algorithms and Metrics", "authors": ["Joshua Goodman"], "date": "ACL", "abstract": "Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the \"Labelled Recall Algorithm,\" which… ", "references": ["584dfd7f167221bb68908b6228a208d6cf47a1b1", "1762744", "155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa", "1762744"]},{"id": "b281a9d0c728979f7ffccba1a61d0fc1d29530c1", "title": "Language and Translation Model Adaptation using Comparable Corpora", "authors": ["Matthew G. Snover", "Bonnie J. Dorr", "Richard M. Schwartz"], "date": "EMNLP", "abstract": "Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model. While bi-lingual parallel data are expensive to generate, monolingual data are relatively common. Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language. This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on… ", "references": ["bf6cefab34c62596d486a86a0187c427d38f0b29", "81164d7f2d676b6044888219426cbb248a020930", "bf6cefab34c62596d486a86a0187c427d38f0b29", "15c28b0edbd2296324c07a0f218de83033781831", "4fe43233fbab5f3ddbd719df69eac8295966267a", "62ea6a86962d6c639731017d11bbd005efff395f", "eddc57c88fcef195538035eb205355db656cba98", "4fe43233fbab5f3ddbd719df69eac8295966267a", "2da32ee5371a1b2b523f6fc77a56e1368fbbb248", "4fe43233fbab5f3ddbd719df69eac8295966267a"]},{"id": "e7b1e437701b58081102a6799e95791d6405ff53", "title": "Method of Selecting Training Data to Build a Compact and Efficient Translation Model", "authors": ["Keiji Yasuda", "Ruiqiang Zhang", "Eiichiro Sumita"], "date": "IJCNLP", "abstract": "Target task matched parallel corpora are required for statistical translation model training. However, training corpora sometimes include both target task matched and unmatched sentences. In such a case, training set selection can reduce the size of the translation model. In this paper, we propose a training set selection method for translation model training using linear translation model interpolation and a language model technique. According to the experimental results, the proposed method… ", "references": ["de2df29b0a0312de7270c3f5a0af6af5645cf91a", "beb2a1a9b8c06be839a05a63a85c34ff904549b9", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "b819c73d1ef2771305cdcf18af691f08dfb1ef97", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "1f12451245667a85d0ee225a80880fc93c71cc8b", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "d838bb8bf9bd7f9b8f3a222f660ef252abe05a78", "b819c73d1ef2771305cdcf18af691f08dfb1ef97", "d4e8bed3b50a035e1eabad614fe4218a34b3b178"]},{"id": "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "authors": ["George R. Doddington"], "date": "2002", "abstract": "Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT… ", "references": ["d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9"]},{"id": "0f2a387a69a424f1e10998831dedd81abc2460c2", "title": "Statistics-based segment pattern lexicon-a new direction for Chinese language modeling", "authors": ["Kae-Cherng Yang", "Tai-Hsuan Ho", "L. Lee"], "date": "1998", "abstract": "This paper presents a new direction for Chinese language modeling based on a different concept of the lexicon. Because every Chinese character has its own meaning and there are no \"blanks\" in Chinese sentences serving as word boundaries, also because the wording structure in the Chinese language is extremely flexible, the \"words\" in Chinese are actually not well defined, and there does not exist a commonly accepted lexicon. This makes language modeling very sophisticated in the Chinese language… ", "references": ["84f71569120087cc7323ffc5e620e7f39f9a040f", "490c836ddb9e1b18732a46eac9ad7a7d1ce0df0e", "ae7f2177b485737883235b9cc4233e7fd98e1365", "84f71569120087cc7323ffc5e620e7f39f9a040f", "ae7f2177b485737883235b9cc4233e7fd98e1365", "ae7f2177b485737883235b9cc4233e7fd98e1365", "ae7f2177b485737883235b9cc4233e7fd98e1365", "490c836ddb9e1b18732a46eac9ad7a7d1ce0df0e", "490c836ddb9e1b18732a46eac9ad7a7d1ce0df0e", "490c836ddb9e1b18732a46eac9ad7a7d1ce0df0e"]},{"id": "7e6d09530490561f1fc4dbfbd82fc4ff456f046c", "title": "The Web as a Parallel Corpus", "authors": ["Philip Resnik", "Noah A. Smith"], "date": "2003", "abstract": "Parallel corpora have become an essential resource for work in multilingual natural language processing. In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web, first reviewing the original algorithm and results and then presenting a set of significant enhancements. These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new content-based measure of… ", "references": ["2ae45f09d0c9ca18c9e1f6e24f92cc7a4bff8038", "a5a99b855c6fccae5fe40b78eae815d68169f826", "8457e5ac4b519fbf9420ddcc67e32e272bba427c", "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "444dc7fe40b5702e79e908834ca2fcfdbc422cd2", "33d7b7d73c5d3ec724a669d1204af3dfbfa724a0", "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "a5a99b855c6fccae5fe40b78eae815d68169f826", "0d4c9f554da47b6f4de7106782bc65d4fe9dd728", "0d4c9f554da47b6f4de7106782bc65d4fe9dd728"]},{"id": "974a522a4b09f65df68e1e0025d68f1c896e457c", "title": "\\self-organized Language Modeling for Speech Recognition\". In", "authors": ["J. Teahan", "John G. Cleary"], "date": "1997", "abstract": "\\A new data structure for cumulative probability tables\". Soft-\\The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression\". ", "references": ["c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58"]},{"id": "81164d7f2d676b6044888219426cbb248a020930", "title": "Improving Statistical Machine Translation Performance by Training Data Selection and Optimization", "authors": ["Yajuan Lü", "Jun Huang", "Qun Liu"], "date": "EMNLP-CoNLL", "abstract": "Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT.", "references": ["223dcd0e44532fc02444709e61327432c74fe46d", "76ad5e0fcb1ed7380ac990277e064715f229e6ce", "1937ecbd3ef4fbe29bd730d2796e43a2c1044c3f", "223dcd0e44532fc02444709e61327432c74fe46d", "7e6d09530490561f1fc4dbfbd82fc4ff456f046c", "223dcd0e44532fc02444709e61327432c74fe46d", "02e2b6bcf6a87fef41f029b9170ab84656774896", "c6432ad153f64ab5599eafb95d4b6ee19114d62a", "fbce987cca7f7cbf99519364af1ecc204e536ce7", "fbce987cca7f7cbf99519364af1ecc204e536ce7"]},{"id": "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f", "title": "Scalable backoff language models", "authors": ["Kristie Seymore", "Ronald Rosenfeld"], "date": "1996", "abstract": "When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff model's perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of… ", "references": ["da0f0015ea687fd7285796f8f806a3c49033163f", "da0f0015ea687fd7285796f8f806a3c49033163f", "974a522a4b09f65df68e1e0025d68f1c896e457c", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "b0130277677e5b915d5cd86b3afafd77fd08eb2e"]},{"id": "51951073580f6995e55be873db9a7f6a9736ca86", "title": "A Study of Translation Edit Rate with Targeted Human Annotation", "authors": ["Matthew Snover", "Bonnie J. Dorr", "Linnea Micciulla"], "date": "2006", "abstract": "We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant… ", "references": ["15315ed05451c88f83c50d56a66a0b85517c5f4f", "d61c0b9d86f60ce2220be9ee2baf5009c5ce8841", "a5c4afe44521893b5454aaee235be4363813acfd", "8f1b5c6bbd47cebff3f2c08e0773b0b878642e39", "0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "d61c0b9d86f60ce2220be9ee2baf5009c5ce8841", "a875de22ce33def6e0146be64e19491ef1a9861d", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9"]},{"id": "327c88dd06722a967be9c6b1176fbd79554967e7", "title": "Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection Across Aligned Corpora", "authors": ["David Yarowsky", "Grace Ngai"], "date": "NAACL", "abstract": "This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora. First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences. Performance is then substantially… ", "references": []},{"id": "404fffebcdb9b597489f62735d8ce59eff41f623", "title": "Modeling long distance dependence in language: topic mixtures vs. dynamic cache models", "authors": ["Rukmini Iyer", "Mari Ostendorf"], "date": "1996", "abstract": "We investigate a new statistical language model which captures topic-related dependencies of words within and across sentences. First, we develop a sentence-level mixture language model that takes advantage of the topic constraints in a sentence or article. Second, we introduce topic-dependent dynamic cache adaptation techniques in the framework of the mixture model. Experiments with the static (or unadapted) mixture model on the 1994 WSJ task indicated a 21% reduction in perplexity and a 3-4… ", "references": []},{"id": "0157dcd6122c20b5afc359a799b2043453471f7f", "title": "Exploiting Similarities among Languages for Machine Translation", "authors": ["Quoc V. Le", "Ilya Sutskever"], "date": "2013", "abstract": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages… ", "references": ["330da625c15427c6e42ccfa3b747fb29e5835bf0", "acc4e56c44771ebf69302a06af51498aeb0a6ac8", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "dac72f2c509aee67524d3321f77e97e8eff51de6", "120ac09b2734ba9d785f6f3def85fe1936aa4322", "db856853d93e104f6e7d2121450d6900dde068ed", "db856853d93e104f6e7d2121450d6900dde068ed", "db856853d93e104f6e7d2121450d6900dde068ed", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "bc1022b031dc6c7019696492e8116598097a8c12"]},{"id": "606df60d518db088986e74fad1f357ea6e5312f2", "title": "On the dynamic adaptation of stochastic language models", "authors": ["Reinhard Kneser", "Volker Steinbiss"], "date": "1993", "abstract": "A simple and general scheme for the adaptation of stochastic language models to changing text styles is introduced. For each word in the running text, the adapted model is a linear combination of specific models, the interpolation parameters being estimated on the preceding text passage. Experiments on a 1.1-million English word corpus show the validity of the approach. The adaptation method improves a bigram language model by 10% in terms of test-set perplexity.<<ETX>> ", "references": ["c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "e1543e748c1d577f9b13d5debb06b7f2c9f7ad8c", "eadf7d20852caa92310d0cb582269b94226b1e58", "e1543e748c1d577f9b13d5debb06b7f2c9f7ad8c", "be1fed9544830df1137e72b1d2396c40d3e18365", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "be1fed9544830df1137e72b1d2396c40d3e18365", "e1543e748c1d577f9b13d5debb06b7f2c9f7ad8c"]},{"id": "6bbcfbd6d15ca72b7e5ef825b7ed8101da4798d8", "title": "A Discriminative Global Training Algorithm for Statistical MT", "authors": ["Christoph Tillmann", "Tong Zhang"], "date": "ACL", "abstract": "This paper presents a novel training algorithm for a linearly-scored block sequence translation model. The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder. No translation, language, or distortion model probabilities are used as in earlier work on SMT. Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches. Moreover, the training procedure treats the decoder as a… ", "references": ["ab7b5917515c460b90451e67852171a531671ab8", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "9c9548ac1705a48cc565c238c6102b7aa69101dc", "1f12451245667a85d0ee225a80880fc93c71cc8b", "ad53fda492f253d89e1f1649582706bdddca8acd", "ab7b5917515c460b90451e67852171a531671ab8", "ad53fda492f253d89e1f1649582706bdddca8acd", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "ad53fda492f253d89e1f1649582706bdddca8acd", "a4b828609b60b06e61bea7a4029cc9e1cad5df87"]},{"id": "26a6534f18879926ab0a921e6e93e246262e9066", "title": "Translingual Document Representations from Discriminative Projections", "authors": ["John C. Platt", "Kristina Toutanova", "Wen-tau Yih"], "date": "EMNLP", "abstract": "Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization. We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space. We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA). Both of these variants start with a basic model of documents (PCA and PLSA… ", "references": ["3709b6cb2ed14c04b60e38d5f75e89c41317e93d", "8df7a827e6df5519eaf54b8461692db45ce2d87e", "82402bf63b039073e2027246d1d4332781b15002", "15c28b0edbd2296324c07a0f218de83033781831", "18b8a90cd3668c826a10e01b2a9c680469ff28de", "3b141370b2f97c3db6b36ba270aff278b9edfc1f", "a6936964a235b30fd5970e7d7663c8a27a429411", "18b8a90cd3668c826a10e01b2a9c680469ff28de", "dd7edbb79b02e333145f645f897e55b9d387c29c", "dd7edbb79b02e333145f645f897e55b9d387c29c"]},{"id": "4b75d707eb3ffe4607c8cdd5436c8d7f8573fed9", "title": "Multilingual Models for Compositional Distributed Semantics", "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "date": "ACL", "abstract": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend… ", "references": ["79c0b2f44bbc2bc51de554b88ebe46204413f884", "d1f37d9cab68eb8cda669cc949394732f33264b4", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731", "6a4007e60346e4501acc936b49b7a476e73afa1e", "0d3233d858660aff451a6c2561a05378ed09725a", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "27e38351e48fe4b7da2775bf94341738bc4da07e"]},{"id": "343733a063e491d234a36d3e1090a739318b3566", "title": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections", "authors": ["Dipanjan Das", "Slav Petrov"], "date": "ACL", "abstract": "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised… ", "references": ["7744a4e9e59f8e43a01f464ec452bd216fd99688", "7744a4e9e59f8e43a01f464ec452bd216fd99688", "ce56527c77fd36c90b3e924a71fd9cd3db38e992", "1deed1a4a03e07aee3b8b8e4716f35033c715a57", "0eebaf8dbc32eabcf9c62e337ec7f947db650538", "48d704f56d074a72f23d4dea85c8202337d20a13", "53fd142f3c52ac68cedcfcdea3b0d350a6a75db6", "ce56527c77fd36c90b3e924a71fd9cd3db38e992", "7744a4e9e59f8e43a01f464ec452bd216fd99688", "48d704f56d074a72f23d4dea85c8202337d20a13"]},{"id": "0c67397ac4c61fd1b702d52a7bb6d42a5bf8d7aa", "title": "Phrasetable Smoothing for Statistical Machine Translation", "authors": ["George F. Foster", "Roland Kuhn", "Howard Johnson"], "date": "EMNLP", "abstract": "We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric. ", "references": ["b73dd8aeddcd109efc1857edf5afbd13f1ebd426", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "3888028ecc976f5a5786152ebc418559896da53c", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "3888028ecc976f5a5786152ebc418559896da53c", "d7da009f457917aa381619facfa5ffae9329a6e9"]},{"id": "d3e5c17d9a45b00d1923d256f3fd607553ce0231", "title": "Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based Adaptation", "authors": ["Radu Florian", "David Yarowsky"], "date": "1999", "abstract": "This paper presents a novel method of generating and applying hierarchical, dynamic topic-based language models. It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques. These combined models help capture long-distance lexical dependencies. Experiments on the Broadcast News corpus show significant improvement in perplexity (10.5% overall and 33.5% on target vocabulary). ", "references": ["9196ba50969fb39acbc88413d02700bd9707cad8", "673992da19d9209434615b12d55bdd36be706e9e", "9196ba50969fb39acbc88413d02700bd9707cad8", "9196ba50969fb39acbc88413d02700bd9707cad8", "fd646f7b2ceb85bf96e80d5c4a7a69e1ad4dfbe4", "6e58b5f825df9fb0b00465a66598f302c30b080a", "fd646f7b2ceb85bf96e80d5c4a7a69e1ad4dfbe4", "fd646f7b2ceb85bf96e80d5c4a7a69e1ad4dfbe4", "6e58b5f825df9fb0b00465a66598f302c30b080a", "673992da19d9209434615b12d55bdd36be706e9e"]},{"id": "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "title": "Improvements in Phrase-Based Statistical Machine Translation", "authors": ["Richard Zens", "Hermann Ney"], "date": "HLT-NAACL", "abstract": "In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set… ", "references": ["37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "32a9ba4a76d1e9948c1cb980800ad117531753f8", "c48a1cf13e186b492d1a6e798d07f65bca4950d2", "d7da009f457917aa381619facfa5ffae9329a6e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "d7da009f457917aa381619facfa5ffae9329a6e9", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "223dcd0e44532fc02444709e61327432c74fe46d"]},{"id": "5b9927bd8322b474360d364f295d56713f370485", "title": "UCB System Description for the WMT 2007 Shared Task", "authors": ["Preslav Nakov", "Marti A. Hearst"], "date": "WMT@ACL", "abstract": "For the WMT 2007 shared task, the UC Berkeley team employed three techniques of interest. First, we used monolingual syntactic paraphrases to provide syntactic variety to the source training set sentences. Second, we trained two language models: a small in-domain model and a large out-of-domain model. Finally, we made use of results from prior research that shows that cognate pairs can improve word alignments. We contributed runs translating English to Spanish, French, and German using various… ", "references": []},{"id": "195df0de3c4c181d26391dd73746c7aefe709ab6", "title": "Experiments in Domain Adaptation for Statistical Machine Translation", "authors": ["Philipp Koehn", "Josh Schroeder"], "date": "WMT@ACL", "abstract": "The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. ", "references": ["1f12451245667a85d0ee225a80880fc93c71cc8b", "1f12451245667a85d0ee225a80880fc93c71cc8b", "399da68d3b97218b6c80262df7963baa89dcc71b", "f5b1146b7ca79322aab124fd63825b9c175c02cf", "7603681a5dc08c132fcb77dfdfca96fea676ed81", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "7603681a5dc08c132fcb77dfdfca96fea676ed81", "99e8d34817ae10d7304521e89c5fbf908b9d856b", "7603681a5dc08c132fcb77dfdfca96fea676ed81", "a4b828609b60b06e61bea7a4029cc9e1cad5df87"]},{"id": "ca4497eafa13eca9df90f8de582efadc3b8c9d09", "title": "Constituent Boundary Parsing for Example-Based Machine Translation", "authors": ["Osamu Furuse", "Hitoshi Iida"], "date": "COLING", "abstract": "This paper proposes an effective parsing method for example-based machine translation. In this method, an input string is parsed by the top-down application of linguistic patterns consisting of variables and constituent boundaries. A constituent boundary is expressed by either a functional word or a part-of-speech bigram. When structural ambiguity occurs, the most plausible structure is selected using the total values of distance calculations in the example-based framework. Transfer-Driven… ", "references": []},{"id": "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "title": "Multiple Object Recognition with Visual Attention", "authors": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "date": "2015", "abstract": "We present an attention-based model for recognizing multiple objects in images.", "references": ["b3d8dffb73bc93de239998548386c84177caa2ad", "26cb14c9d22cf946314d685fe3541ef9f641e429", "0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "4816f0b6f0d05da3901441bfa5cc7be044b4da8b", "02227c94dd41fe0b439e050d377b0beb5d427cda", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "4816f0b6f0d05da3901441bfa5cc7be044b4da8b", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "5cb6700d94c6118ee13f4f4fecac99f111189812", "title": "BabyTalk: Understanding and Generating Simple Image Descriptions", "authors": ["Girish Kulkarni", "Visruth Premraj", "Tamara L. Berg"], "date": "2013", "abstract": "We present a system to automatically generate natural language descriptions from images. This system consists of two parts. The first part, content planning, smooths the output of computer vision-based detection and recognition algorithms with statistics mined from large pools of visually descriptive text to determine the best content words to use to describe an image. The second step, surface realization, chooses words to construct natural language sentences based on the predicted content and… ", "references": []},{"id": "0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6", "title": "Learning Generative Models with Visual Attention", "authors": ["Yichuan Tang", "Nitish Srivastava", "Ruslan Salakhutdinov"], "date": "NIPS", "abstract": "Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By… ", "references": ["1e80f755bcbf10479afd2338cec05211fdbd325c", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "a0ca579aeba44a57ae251ec5bfe513e197bed240", "5d90f06bb70a0a3dced62413346235c02b1aa086", "700bbcd3518ca8cb3dac50a89fc69cad3dc1a579", "5d90f06bb70a0a3dced62413346235c02b1aa086", "b46533abc845057d02e4bbd78effb46b340b4d70", "72829d537f0ec8b1cc0ced2f278bb56ce89f1b0c", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3"]},{"id": "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "authors": ["Xinlei Chen", "C. Lawrence Zitnick"], "date": "2014", "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions.", "references": ["82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9", "4aa4069693bee00d1b0759ca3df35e59284e9845", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "05e074abddd3fe987b9bebd46f6cf4bf8465c37e", "4aa4069693bee00d1b0759ca3df35e59284e9845", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "2a0d0f6c5a69b264710df0230696f47c5918e2f2", "05e074abddd3fe987b9bebd46f6cf4bf8465c37e", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11"]},{"id": "2e36ea91a3c8fbff92be2989325531b4002e2afc", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "authors": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "date": "2014", "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space.", "references": ["c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "fad611e35b3731740b4d8b754241e77add5a70b9", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "4aa4069693bee00d1b0759ca3df35e59284e9845", "4aa4069693bee00d1b0759ca3df35e59284e9845", "fad611e35b3731740b4d8b754241e77add5a70b9", "82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9"]},{"id": "0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3", "title": "Learning to combine foveal glimpses with a third-order Boltzmann machine", "authors": ["Hugo Larochelle", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations.", "references": ["804ea41e75758e34afd4db9458e8aa118e5ec49b", "2ac91e028cdc602695b46bd1f372c03b4d2776cf", "7ee4d5cd626cf1ba7dea02f4fad850acc113be12", "d0a98934162bb2be12846b5190e681be6994c0f4", "0d8685fb5add2378cae7baa3506ea22d06f092a6", "d0a98934162bb2be12846b5190e681be6994c0f4", "804ea41e75758e34afd4db9458e8aa118e5ec49b", "0eb2e4a205a628ab059cab41d3b772f614ad29f2", "fe862c49248d8898b4712ba58e6bec13ea586732", "320b36777d57e772d88d278ceeccd1f5e746304c"]},{"id": "44254390973b3d3e6a98f53f1da961addb4d352b", "title": "Learning Translation Templates From Bilingual Text", "authors": ["Hiroyuki Kaji", "Yuuko Kida", "Yasutsugu Morimoto"], "date": "COLING", "abstract": "This paper proposes a two-phase example-based machine translation methodology which develops translation templates from examples and then translates using template matching. This method improves translation quality and facilitates customization of machine translation systems. This paper focuses on the automatic learning of translation templates. A translation template is a bilingual pair of sentences in which corresponding units (words and pharases) are coupled and replaced with variables… ", "references": []},{"id": "34c71b6177f2daf1d1d726dd05941a545a149928", "title": "Acquisition of Phrase-level Bilingual Correspondence using Dependency Structure", "authors": ["Kaoru Yamamoto", "Yuji Matsumoto"], "date": "COLING", "abstract": "This paper describes a method to find phrase-level translation patterns from parallel corpora by applying dependency structure analysis. We use statistical dependency parsers to determine dependency relations between base phrases in a sentence. Our method is tested with a business expression corpus containing 10000 English-Japanese sentence pairs and achieved approximately 90% accuracy in extracting bilingual correspondences. The result shows that the use of dependency relation helps to acquire… ", "references": ["62fdfeb319a4dfef5752c78714ba03625d1019db", "a76563076016fb1cb813deba45db2409772a51da", "23e1993ad65ef984bd11d58463d59f1e3cec1341"]},{"id": "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "title": "Identifying Word Correspondences in Parallel Texts", "authors": ["William A. Gale", "Kenneth Ward Church"], "date": "HLT", "abstract": "Researchers in both machine translation (e.g., Brown et a/, 1990) arm bilingual lexicography (e.g., Klavans and Tzoukermarm, 1990) have recently become interested in studying parallel texts (also known as bilingual corpora), bodies of text such as the Canadian Hansards (parliamentary debates) which are available in multiple languages (such as French and English). Much of the current excitement surrounding parallel texts was initiated by Brown et aL (1990), who outline a selforganizing method… ", "references": ["a1066659ec1afee9dce586f6f49b7d44527827e1", "f853daccfcb2350f9adcd75331d148b04c21e5ef", "a1066659ec1afee9dce586f6f49b7d44527827e1", "b4c6cd6879df810fee431ae1cf16ac35d3b2c239", "983548e49ce6693103b68bb579d04cf61bcaac8c", "983548e49ce6693103b68bb579d04cf61bcaac8c", "a0b61625e60a419bd5ea1d892047a65a73d9f0c4", "a76563076016fb1cb813deba45db2409772a51da", "a1066659ec1afee9dce586f6f49b7d44527827e1", "b4c6cd6879df810fee431ae1cf16ac35d3b2c239"]},{"id": "c20b4068be640ebaffbc56382c3e4e0bcf62664e", "title": "Factored Language Models and Generalized Parallel Backoff", "authors": ["Jeff A. Bilmes", "Katrin Kirchhoff"], "date": "HLT-NAACL", "abstract": "We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff… ", "references": ["399da68d3b97218b6c80262df7963baa89dcc71b", "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "399da68d3b97218b6c80262df7963baa89dcc71b"]},{"id": "6a5f9307b8b8473b233432b0e8aa0b4bef311996", "title": "Improved Language Modeling for Statistical Machine Translation", "authors": ["Katrin Kirchhoff", "Mei Yang"], "date": "ParallelText@ACL", "abstract": "Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that… ", "references": ["50b8e8d48f4973cdeefa835807b4e1a8ca65ced3", "017ddb7e815236defd0566bc46f6ed8401cc6ba6", "2beb2197a8a03aea8d6e678b6cf8f4886f75315c", "2479cc774fcf8486a98c7b18d131ba7807e58dd6", "a651bb7cc7fc68ece0cc66ab921486d163373385", "8b7e3e2af584a5994eb459a20aa32b7f5ef35fd1", "017ddb7e815236defd0566bc46f6ed8401cc6ba6", "c20b4068be640ebaffbc56382c3e4e0bcf62664e", "ebfa1ac159607569053c087041023259e9d27541", "0fdb5de656cbcf521d121343bd21ab1731e76452"]},{"id": "f3402fafad543cb094cd2707a73f07119f6125c9", "title": "Beyond N-Grams: Can Linguistic Sophistication Improve Language Modeling?", "authors": ["Eric Brill", "Radu Florian", "Lidia Mangu"], "date": "COLING-ACL", "abstract": "It seems obvious that a successful model of natural language would incorporate a great deal of both linguistic and world knowledge. Interestingly, state of the art language models for speech recognition are based on a very crude linguistic model, namely conditioning the probability of a word on a small fixed number of preceding words. Despite many attempts to incorporate more sophisticated information into the models, the n-gram model remains the state of the art, used in virtually all speech… ", "references": ["617241818e8ddd6edcb4ee7682992673c18c6f3d", "b23c06a50c0fe8b7c5d10aaf29ccc79765e37f46", "56709865bd8fca7a5b12e667bc89f1f0e6ae1422", "a9ed6a300b5c813aecda938cf63f2221ab328c13", "56709865bd8fca7a5b12e667bc89f1f0e6ae1422", "56709865bd8fca7a5b12e667bc89f1f0e6ae1422", "b23c06a50c0fe8b7c5d10aaf29ccc79765e37f46", "f2d00cf1be1f129c88d0471263b90a3f3f06e942", "f2d00cf1be1f129c88d0471263b90a3f3f06e942", "851bce6405b781079359498bfd6237b95d3acc6c"]},{"id": "a19ceb1281b84d96abba03e973ba7274a8f0f8b0", "title": "A DP based Search Using Monotone Alignments in Statistical Translation", "authors": ["Christoph Tillmann", "Stephan E. Vogel", "A. Zubiaga"], "date": "ACL", "abstract": "In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results. The statistical translation uses two sources of information: a translation model and a language model. The language model used is a standard bigram model. For the translation model, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions. Thus, the approach amounts to a first-order… ", "references": ["358fccd9e2633679038ae8cf8e747adefb4dc214", "b66072f7dd9950d6ed2d53d41cb4656a5d7806c1", "4711ff01d8eff9b9d10deeb3b68f366f7944c208", "3491aa4a9a66ba3d1603230a70d82c7479666a7d", "ee61f8cd6a75e1a87f3e5d1aae49b9dbe95e97db", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "ab7b5917515c460b90451e67852171a531671ab8", "ee61f8cd6a75e1a87f3e5d1aae49b9dbe95e97db", "b66072f7dd9950d6ed2d53d41cb4656a5d7806c1"]},{"id": "f30a129113961242c6279436d60df17e9043ad08", "title": "A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm", "authors": ["Masaaki Nagata"], "date": "COLING", "abstract": "We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient two-pass N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. The proposed Japanese morphological analyzer achieved 95.1% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus. ", "references": ["c27913ce87476aa8a4a09621bcfb25a52647c84b"]},{"id": "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843", "title": "A Fully Statistical Approach to Natural Language Interfaces", "authors": ["Scott Miller", "David Stallard", "Richard M. Schwartz"], "date": "ACL", "abstract": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames. ", "references": ["55a1bf99b29436197fd4d872a78155a196a77a88", "ac8f1fd58be8a8c9f9599fc4da981ea3040945f6", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "a8fcc058607129590fa6ab692a38807efc7d7f1f", "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "70d2b25dfe4fbadfa3c816e01666f811dae9a4d1", "8df509919b31397e225280962c59384fbe83144e", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10"]},{"id": "83e89037edfa113cf15b01a218cfcf12c6463bcb", "title": "Clickthrough-based latent semantic models for web search", "authors": ["Jianfeng Gao", "Kristina Toutanova", "Wen-tau Yih"], "date": "SIGIR '11", "abstract": "This paper presents two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models are learned from this data. One is a bilingual topic model within the language modeling framework. It ranks… ", "references": ["c2872fb23b02597034a179f4adb82a00d6ffda8d", "a5a231cd0d6d3dc774ff9788fa39fdc2b79c6d70", "c2872fb23b02597034a179f4adb82a00d6ffda8d", "4e3cfdd4cc455b98ae3e661416550a656847825e", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "94a8ace25d5112e22f7235bbba26570b008a73e9", "3b141370b2f97c3db6b36ba270aff278b9edfc1f", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "4e3cfdd4cc455b98ae3e661416550a656847825e", "c2872fb23b02597034a179f4adb82a00d6ffda8d"]},{"id": "adfef97814b292a09520d8c78a141e7a4baf8726", "title": "Three New Probabilistic Models for Dependency Parsing: An Exploration", "authors": ["Jason Eisner"], "date": "1996", "abstract": "After presenting a novel O(n3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results… ", "references": ["617241818e8ddd6edcb4ee7682992673c18c6f3d", "9d3a863b71f093e1cbc9304a3287c1ddc48c6f31", "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "3764baa7465201f054083d02b58fa75f883c4461", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "9d3a863b71f093e1cbc9304a3287c1ddc48c6f31", "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "5752b8dcec5856b7ad6289bbe1177acce535fba4", "d607ed3aa8a1762e06988329aeb0c05b997023db", "3764baa7465201f054083d02b58fa75f883c4461"]},{"id": "4564666d8eb78a55d046144893407d312575979c", "title": "Training Phrase Translation Models with Leaving-One-Out", "authors": ["Joern Wuebker", "Arne Mauser", "Hermann Ney"], "date": "ACL", "abstract": "Several attempts have been made to learn phrase translation probabilities for phrase-based statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with over-fitting. We describe a novel leaving-one-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were… ", "references": ["c6a83c4fcc99ba6753109301949c5b7cfa978079", "da93c50f74d482e2f18e9ccdd0e5da5a81093b40", "da93c50f74d482e2f18e9ccdd0e5da5a81093b40", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "08930185a707e05f9f49112ba3152ec09a86dc7f", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "da93c50f74d482e2f18e9ccdd0e5da5a81093b40", "6a3819476b99a238d37beb00c878c5602cc639e8", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "c6a83c4fcc99ba6753109301949c5b7cfa978079"]},{"id": "a6936964a235b30fd5970e7d7663c8a27a429411", "title": "Automatic Cross-Language Retrieval Using Latent Semantic Indexing", "authors": ["Susan T. Dumais", "Todd A. Letsche", "Thomas K. Landauer"], "date": "1997", "abstract": "We describe a method for fully automated cross-language document retrieval in which no query translation is required.", "references": []},{"id": "59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b", "title": "The Penn Treebank: Annotating Predicate Argument Structure", "authors": ["Mitchell P. Marcus", "Grace Kim", "Britta Schasberger"], "date": "HLT", "abstract": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions… ", "references": ["d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "bdaf232c561f1f50e88b1d24097e214890b37e8b", "da838db79e7593018894ada44db35eee670941d6", "db52ec5e0c60d4f4cd16ec8fea822575bab17b57", "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a"]},{"id": "17ae3bda93abc40e758a1074c86baa041e977703", "title": "Head Automata and Bilingual Tiling: Translation with Minimal Representations", "authors": ["Hiyan Alshawi"], "date": "ACL", "abstract": "We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases.", "references": ["aad2b45d798c2c4f5fad6874c3944579af9afe3c", "2ef05e4f7891929b6e300b49880f45e63c66e454", "21e69bb8d3a36235a19da1408279290055b373bc", "ab7b5917515c460b90451e67852171a531671ab8", "5ce919d90ee41a8a6852677f7553e297de2b06a7", "aad2b45d798c2c4f5fad6874c3944579af9afe3c", "aad2b45d798c2c4f5fad6874c3944579af9afe3c", "5ce919d90ee41a8a6852677f7553e297de2b06a7", "f71ab539337bb1e496df363a3cc2a66849117314", "f1f4d5eba4b3638d62399a2580d48203c98903c6"]},{"id": "d18af6780f9242ec988c89ed0b67dc7d05a7785a", "title": "A Comparison of Alignment Models for Statistical Machine Translation", "authors": ["Franz Josef Och", "Hermann Ney"], "date": "COLING", "abstract": "In this paper, we present and compare various alignment models for statistical machine translation. We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a refined annotation scheme to produce suitable reference alignments. We also compare the impact of different alignment models on the translation quality of a statistical machine translation system. ", "references": ["8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "539036ab9e8f038c8a948596e77cc0dfcfa91fb3", "15315ed05451c88f83c50d56a66a0b85517c5f4f", "539036ab9e8f038c8a948596e77cc0dfcfa91fb3", "15315ed05451c88f83c50d56a66a0b85517c5f4f"]},{"id": "8adb6fafa7b1b373f33fa95f1ab4006578bdf022", "title": "Text Categorization Using Predicate-Argument Structures", "authors": ["Jacob Persson", "Richard Johansson", "Pierre Nugues"], "date": "NODALIDA", "abstract": "! Most text categorization methods use the vector space model in combination with a representation of documents based on bags of words. As its name indicates, bags of words ignore possible structures in the text and only take into account isolated, unrelated words. Although this limitation is widely acknowledged, most previous attempts to extend the bag-of-words model with more advanced approaches failed to produce conclusive improvements. We propose a novel method that extends the word-level… ", "references": ["7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "3d7ebfdff3b800d4b7fcba880b226a42538f47fb", "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc", "fd1901f34cc3673072264104885d70555b1a4cdc", "2b82514d73fd939a3097fa9a63002953a5283531", "14c146d457bbd201f3a117ee9c848300d341e5d0", "3d7ebfdff3b800d4b7fcba880b226a42538f47fb", "5a7beae22ce9f60fb1556ea20f9582bc3eb2dbe4", "3d7ebfdff3b800d4b7fcba880b226a42538f47fb", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93"]},{"id": "8a93cd1b6fbf7c8c86637bae18d979dafeb9a7c1", "title": "Factor-based Compositional Embedding Models", "authors": ["Mo Yu"], "date": "2014", "abstract": "Information about language structure is critical in many NLP tasks, where substructures of a sentence and its annotations inform downstream NLP task. Yet word representations alone do not capture such structure. For example, in relation extraction the sentence may be annotated with partof-speech tags, a dependency parse, and named entities, with the goal of predicting a relation label for a pair of target entities. Semantic role labeling has a similar form. In tasks such as these, it is… ", "references": ["687bac2d3320083eb4530bf18bb8f8f721477600", "745d86adca56ec50761591733e157f84cfb19671", "55b46e5b460ad3b15dd07b0e47bbc80fafcb37d0", "bc1022b031dc6c7019696492e8116598097a8c12", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "745d86adca56ec50761591733e157f84cfb19671", "bc1022b031dc6c7019696492e8116598097a8c12", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82"]},{"id": "c92970286c535992a86539b761357761e97a37ee", "title": "Towards Robust Linguistic Analysis using OntoNotes", "authors": ["Sameer Pradhan", "Alessandro Moschitti", "Zhi Zhong"], "date": "CoNLL", "abstract": "Large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic, semantic and discourse analyzers, and they serve as training data as well as evaluation benchmarks.", "references": ["44acd22bfb89f6e3d5529b2fa92b18b035e6bab5", "95067ce3aa49d52f684904a193d42e3aecf82ff2", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "86128b764ea86caa207298a092f70f8bedc8ba3f", "00cf902b27676cdc376e26567e70298b96c672a1", "00cf902b27676cdc376e26567e70298b96c672a1", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "a584e4b607e972783cea22daaaf1114ea94a8035", "95067ce3aa49d52f684904a193d42e3aecf82ff2", "95067ce3aa49d52f684904a193d42e3aecf82ff2"]},{"id": "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "title": "From captions to visual concepts and back", "authors": ["Hao Fang", "Saurabh Gupta", "Geoffrey Zweig"], "date": "2015", "abstract": "This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions.", "references": ["4aa4069693bee00d1b0759ca3df35e59284e9845", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11", "123b9de009865472c660192f8072493a48352dc2", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "f9791399e87bba3f911fd8f570443cf721cf7b1e", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11"]},{"id": "03b513b30b9d95e39285df1dc93be63e25f2744e", "title": "Decoding Algorithm in Statistical Machine Translation", "authors": ["Ye-Yi Wang", "Alexander H. Waibel"], "date": "ACL", "abstract": "Decoding algorithm is a crucial part in statistical machine translation. We describe a stack decoding algorithm in this paper. We present the hypothesis scoring method and the heuristics used in our algorithm. We report several techniques deployed to improve the performance of the decoder. We also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process. We evaluate and compare these techniques/models in our statistical machine translation system. ", "references": []},{"id": "f142c849ffef66f7520aff4e0b40ac964ccb8cc1", "title": "Language Models for Image Captioning: The Quirks and What Works", "authors": ["Jacob Devlin", "Hao Cheng", "Margaret Mitchell"], "date": "2015", "abstract": "Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare… ", "references": ["d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "d1275b2a2ab53013310e759e5c6878b96df643d4", "82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9", "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88"]},{"id": "82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9", "title": "Explain Images with Multimodal Recurrent Neural Networks", "authors": ["Junhua Mao", "Wei Xu", "Alan L. Yuille"], "date": "2014", "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images.", "references": ["fad611e35b3731740b4d8b754241e77add5a70b9", "85cb25e88d3b0548a26e7a70b6953e500d27eb9a", "eb42cf88027de515750f230b23b1a057dc782108", "fad611e35b3731740b4d8b754241e77add5a70b9", "eb42cf88027de515750f230b23b1a057dc782108", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "eb42cf88027de515750f230b23b1a057dc782108", "85cb25e88d3b0548a26e7a70b6953e500d27eb9a", "5726c7b40fcc454b77d989656c085520bf6c15fa"]},{"id": "4711ff01d8eff9b9d10deeb3b68f366f7944c208", "title": "A Polynomial-Time Algorithm for Statistical Machine Translation", "authors": ["Dekai Wu"], "date": "ACL", "abstract": "We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant… ", "references": ["79f20fb39a6e78352ebbb65b1737970837a420b5", "c9ff51bb4956e17911acce3c5984557c23152c47", "9828f03fe5c356e0b757284ff2286cfb4be990c9", "ab7b5917515c460b90451e67852171a531671ab8", "a1066659ec1afee9dce586f6f49b7d44527827e1", "9828f03fe5c356e0b757284ff2286cfb4be990c9", "ab7b5917515c460b90451e67852171a531671ab8", "7a7383b777a6b5b0c8b9b38eb5cb930df5ed243c", "7a7383b777a6b5b0c8b9b38eb5cb930df5ed243c", "c9ff51bb4956e17911acce3c5984557c23152c47"]},{"id": "ae9443b39a5abfbf3cc9776173c1ae4f94732408", "title": "Fast sequential decoding algorithm using a stack", "authors": ["Frederick Jelinek"], "date": "1969", "abstract": "In this paper a new sequential decoding algorithm is introduced that uses stack storage at the receiver. It is much simpler to describe and analyze than the Fano algorithm, and is about six times faster than the latter at transmission rates equal to Rcomp the rate below which the average number of decoding steps is bounded by a constant. Practical problems connected with implementing the stack algorithm are discussed and a scheme is described that facilitates satisfactory performance even with… ", "references": []},{"id": "f01fc808592ea7c473a69a6e7484040a435f36d9", "title": "Long-term recurrent convolutional networks for visual recognition and description", "authors": ["Jeff Donahue", "Lisa Anne Hendricks", "Trevor Darrell"], "date": "2015", "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video… ", "references": ["6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "5418b2a482720e013d487a385c26fae0f017c6a6", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "43795b7bac3d921c4e579964b54187bdbf6c6330", "5418b2a482720e013d487a385c26fae0f017c6a6", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "43795b7bac3d921c4e579964b54187bdbf6c6330", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745"]},{"id": "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "title": "Learning to Execute", "authors": ["Wojciech Zaremba", "Ilya Sutskever"], "date": "2014", "abstract": "Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass… ", "references": ["ded103d0613e1a8f51f586cc1678aee3ff26e811", "9819b600a828a57e1cde047bbe710d3446b30da5", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "cea967b59209c6be22829699f05b8b1ac4dc092d", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "c0b624c46b51920dfec5aa02cc86323c0beb0df5", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "533ee188324b833e059cb59b654e6160776d5812", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97"]},{"id": "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "title": "Gated Feedback Recurrent Neural Networks", "authors": ["Junyoung Chung", "Çaglar Gülçehre", "Yoshua Bengio"], "date": "ICML", "abstract": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with… ", "references": ["f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "cea967b59209c6be22829699f05b8b1ac4dc092d", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "cea967b59209c6be22829699f05b8b1ac4dc092d", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "8c571314311f507731296b21b56ab2c326b97392", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "c4916a5fb50bcc73213b6f054c42ad10c68c52cd", "title": "Dynamic Entity Representation with Max-pooling Improves Machine Reading", "authors": ["Sosuke Kobayashi", "Ran Tian", "Kentaro Inui"], "date": "HLT-NAACL", "abstract": "We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset (Hermann et al., 2015), our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests… ", "references": ["564257469fa44cdb57e4272f85253efb9acfd69d", "380169dfdf019dd77f3316ab14fefab337113652", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "380169dfdf019dd77f3316ab14fefab337113652", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "5082a1a13daea5c7026706738f8528391a1e6d59", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "d1505c6123c102e53eb19dff312cb25cea840b72", "564257469fa44cdb57e4272f85253efb9acfd69d", "ae5e6c6f5513613a161b2c85563f9708bf2e9178"]},{"id": "4a197ce36461849bcaee565b510a8ef71b7dcae3", "title": "Recurrent Memory Network for Language Modeling", "authors": ["Ke M. Tran", "Arianna Bisazza", "Christof Monz"], "date": "2016", "abstract": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling… ", "references": ["9819b600a828a57e1cde047bbe710d3446b30da5", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "cea967b59209c6be22829699f05b8b1ac4dc092d", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "759956bb98689dbcc891528636d8994e54318f85", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "9819b600a828a57e1cde047bbe710d3446b30da5", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250"]},{"id": "e86e81ad3fa4ab0b736f7fef721689e293ee788e", "title": "Broad Context Language Modeling as Reading Comprehension", "authors": ["Kevin Gimpel", "Hongxing Wang", "Zewei Chu"], "date": "EACL", "abstract": "Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they… ", "references": ["f2e50e2ee4021f199877c8920f1f984481c723aa", "99e763d3835686b8e1285c2a498766044fbc88ad", "564257469fa44cdb57e4272f85253efb9acfd69d", "77fb0b7aef619dfac650423d4677170df2158e0d", "b1e20420982a4f923c08652941666b189b11b7fe", "a39ffa57ef8e538b3c6a6c2bbc0b641f7cdc60dc", "a39ffa57ef8e538b3c6a6c2bbc0b641f7cdc60dc", "d1505c6123c102e53eb19dff312cb25cea840b72", "77fb0b7aef619dfac650423d4677170df2158e0d", "a39ffa57ef8e538b3c6a6c2bbc0b641f7cdc60dc"]},{"id": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks", "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "date": "2015", "abstract": "Abstract: We describe a new class of learning models called memory networks.", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "a584211768d49f80192f13b8ed2fda9c058dec34", "564257469fa44cdb57e4272f85253efb9acfd69d", "564257469fa44cdb57e4272f85253efb9acfd69d", "564257469fa44cdb57e4272f85253efb9acfd69d", "a584211768d49f80192f13b8ed2fda9c058dec34", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "9819b600a828a57e1cde047bbe710d3446b30da5", "97cc2fab13bf20d130de19dfffe6670aac2076b5", "2b776119a1347e1455dc498ff5078b3a94029ed9"]},{"id": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "title": "Dynamic Memory Networks for Visual and Textual Question Answering", "authors": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "date": "ICML", "abstract": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering.", "references": ["75ddc7ee15be14013a3462c01b38b0548486fbcb", "75ddc7ee15be14013a3462c01b38b0548486fbcb", "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60", "75ddc7ee15be14013a3462c01b38b0548486fbcb", "98bd5dd1740f585bf25320ba504e2c1ae57f2e5f", "452059171226626718eb677358836328f884298e", "75ddc7ee15be14013a3462c01b38b0548486fbcb", "75ddc7ee15be14013a3462c01b38b0548486fbcb", "71ae756c75ac89e2d731c9c79649562b5768ff39", "452059171226626718eb677358836328f884298e"]},{"id": "17de95a8ec3fe5917d91110b410ab64df33414bf", "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning", "authors": ["Fandong Meng", "Zhengdong Lu", "Qun Liu"], "date": "2015", "abstract": "We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English).", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "533ee188324b833e059cb59b654e6160776d5812", "93499a7c7f699b6630a86fad964536f9423bb6d0", "bc1022b031dc6c7019696492e8116598097a8c12", "cea967b59209c6be22829699f05b8b1ac4dc092d", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "5a829f63a9a03be652ed8568ab6ce77ef0f2a712", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b544dfe355a5070b60986319a3f51fb45d1348e", "5a829f63a9a03be652ed8568ab6ce77ef0f2a712"]},{"id": "a76563076016fb1cb813deba45db2409772a51da", "title": "Aligning Sentences in Parallel Corpora", "authors": ["Peter F. Brown", "Jennifer C. Lai", "Robert L. Mercer"], "date": "ACL", "abstract": "In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used… ", "references": ["6d9a739c2bdbadf0d355f9c19d09ed89117a9589", "2166fa493a8c6e40f7f8562d15712dd3c75f03df", "6d9a739c2bdbadf0d355f9c19d09ed89117a9589", "bdfb57141b2141095ed942b28be24808aeba8d54", "a0b61625e60a419bd5ea1d892047a65a73d9f0c4", "a0b61625e60a419bd5ea1d892047a65a73d9f0c4", "539036ab9e8f038c8a948596e77cc0dfcfa91fb3", "bdfb57141b2141095ed942b28be24808aeba8d54", "8b7e49e3ee16965d4bfdeeac03fbebb3308ebbec"]},{"id": "f314339651cb25e4234e0b96fe8bd87206847993", "title": "Iterative Alternating Neural Attention for Machine Reading", "authors": ["Alessandro Sordoni", "Philip Bachman", "Yoshua Bengio"], "date": "2016", "abstract": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the… ", "references": ["4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "9653d5c2c7844347343d073bbedd96e05d52f69b", "f2e50e2ee4021f199877c8920f1f984481c723aa", "d1505c6123c102e53eb19dff312cb25cea840b72", "f2e50e2ee4021f199877c8920f1f984481c723aa", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "d1505c6123c102e53eb19dff312cb25cea840b72", "d1505c6123c102e53eb19dff312cb25cea840b72", "f2e50e2ee4021f199877c8920f1f984481c723aa"]},{"id": "0858eb565308ef214a0b7e51f0aa204185adf430", "title": "An automatic technique to include grammatical and morphological information in a trigram-based statistical language model", "authors": ["Giulio Maltese", "Federico Mancini"], "date": "1992", "abstract": "A technique to take into account grammatical and morphological information in a trigram-based statistical language model is presented. This is automatically achieved by interpolating the trigram model (which uses sequences of words) with statistical models based on sequences of grammatical categories and/or lemmas. Such an approach reduces the effect of data sparseness in the trigram model due also to the way interpolation coefficients are chosen. With respect to trigrams, the authors obtained… ", "references": []},{"id": "c50cd7df4271ef94a0a60894f0e2cf4ef89fb912", "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "authors": ["Yichen Gong", "Samuel R. Bowman"], "date": "QA@ACL", "abstract": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct an query-aware context vector representation… ", "references": ["c636a2dd242908fe2e598a1077c0c57bfdea8633", "f314339651cb25e4234e0b96fe8bd87206847993", "62f88e8fc3b44c5627f2b4721b08498d78103893", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "f2e50e2ee4021f199877c8920f1f984481c723aa", "e978d832a4d86571e1b52aa1685dc32ccb250f50", "e978d832a4d86571e1b52aa1685dc32ccb250f50", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72"]},{"id": "86c2a7dc48445d75ec9bc71f7d9fdec622687e90", "title": "Machine Comprehension with Discourse Relations", "authors": ["Karthik Narasimhan", "Regina Barzilay"], "date": "ACL", "abstract": "This paper proposes a novel approach for incorporating discourse information into machine comprehension applications. Traditionally, such information is computed using off-the-shelf discourse analyzers. This design provides limited opportunities for guiding the discourse parser based on the requirements of the target task. In contrast, our model induces relations between sentences while optimizing a task-specific objective. This approach enables the model to benefit from discourse information… ", "references": ["d1d4712f267b2a65d5210a77e91d2a24888a4da0", "5985e41f8b1bdca631ff7bc95dde59b25e03f5c3", "27c18ff7782c227e9fdd972b749d9f9a8556305e", "9a4eda6e694a8b6f0d400e243e2416f9146709d9", "9a4eda6e694a8b6f0d400e243e2416f9146709d9", "27c18ff7782c227e9fdd972b749d9f9a8556305e", "9a4eda6e694a8b6f0d400e243e2416f9146709d9", "a6e0701de9fd89c1d74c997d9f264b0177c3c86f", "9a4eda6e694a8b6f0d400e243e2416f9146709d9", "38dd6e69be875011d9b73c1304777586e468e397"]},{"id": "9653d5c2c7844347343d073bbedd96e05d52f69b", "title": "Pointer Networks", "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "date": "2015", "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various… ", "references": ["47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f01fc808592ea7c473a69a6e7484040a435f36d9", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "2c1890864c1c2b750f48316dc8b650ba4772adc5", "title": "Stacked Attention Networks for Image Question Answering", "authors": ["Zichao Yang", "Xiaodong He", "Alexander J. Smola"], "date": "2016", "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images.", "references": ["2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1", "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60", "ac64fb7e6d2ddf236332ec9f371fe85d308c114d", "33261d252218007147a71e40f8367ed152fa2fe0", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1"]},{"id": "4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "title": "A Program for Aligning Sentences in Bilingual Corpora", "authors": ["William A. Gale", "Kenneth Ward Church"], "date": "ACL", "abstract": "Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper… ", "references": ["a76563076016fb1cb813deba45db2409772a51da", "a1066659ec1afee9dce586f6f49b7d44527827e1"]},{"id": "85b9eb556c211d954b31d9d58fed6891a07ab473", "title": "Word-Sense Disambiguation Using Statistical Methods", "authors": ["Peter F. Brown", "Stephen Della Pietra", "Robert L. Mercer"], "date": "ACL", "abstract": "We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent. ", "references": ["8a9b6828c5e4339025bb78af6b025d21b4830800", "f2f56ab99ea90301af95c2a9565e110792f645dd", "63d2bce4d0d1c1a61ea5d12ade750f558a57b8b6", "f2f56ab99ea90301af95c2a9565e110792f645dd", "8a9b6828c5e4339025bb78af6b025d21b4830800", "8a9b6828c5e4339025bb78af6b025d21b4830800", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "76e4e034c20bea86edcc6e71bbaddb47fafeecbc", "f2f56ab99ea90301af95c2a9565e110792f645dd", "63d2bce4d0d1c1a61ea5d12ade750f558a57b8b6"]},{"id": "09091c767a756001bf87205df25f1505354469a2", "title": "A Statistical Approach to Sense Disambiguation in Machine Translation", "authors": ["Peter F. Brown", "Stephen Della Pietra", "Robert L. Mercer"], "date": "HLT", "abstract": "We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the word's translations. ", "references": []},{"id": "2166fa493a8c6e40f7f8562d15712dd3c75f03df", "title": "A statistical approach to language translation", "authors": ["Peter F. Brown", "John Cocke", "Paul S. Roossin"], "date": "COLING", "abstract": "An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases.", "references": []},{"id": "a1066659ec1afee9dce586f6f49b7d44527827e1", "title": "A Statistical Approach to Machine Translation", "authors": ["Peter F. Brown", "John Cocke", "Paul S. Roossin"], "date": "1990", "abstract": "In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results. ", "references": ["6682e6d41d7c0c1125a65cbca172cb846ed216ea", "539036ab9e8f038c8a948596e77cc0dfcfa91fb3"]},{"id": "fac2ca048fdd7e848f0b9ba2f7be25bb49186770", "title": "The Microsoft Research Sentence Completion Challenge", "authors": ["G. Zweig", "Christopher J. C. Burges"], "date": "2011", "abstract": "Work on modeling semantics in text is progressing quickly, yet currently there are few public datasets which authors can use to measure and compare their systems. This work takes a step towards addressing this issue. We present the MSR Sentence Completion Challenge Data, which consists of 1,040 sentences, each of which has four impostor sentences, in which a single (fixed) word in the original sentence has been replaced by an impostor word with similar occurrence statistics. For each sentence… ", "references": ["cfa2646776405d50533055ceb1b7f050e9014dcb"]},{"id": "682b3dc0f4c46f96aa28358203c5013649e8dc62", "title": "Cutting Recursive Autoencoder Trees", "authors": ["Christian Scheible", "Hinrich Schütze"], "date": "2013", "abstract": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural… ", "references": ["3fc44ff7f37ec5585310666c183c65e0a0bb2446", "bc1022b031dc6c7019696492e8116598097a8c12", "932c8b99ef910bedd0f49d889230aba308004e0a", "5b60e4b182c1679eb788455586a3f8a4df300e3d", "cfa2646776405d50533055ceb1b7f050e9014dcb", "cfa2646776405d50533055ceb1b7f050e9014dcb", "8273bddfc16a5c2e3b8793b70929f8f6ef010b2a", "932c8b99ef910bedd0f49d889230aba308004e0a", "bc1022b031dc6c7019696492e8116598097a8c12", "6af58c061f2e4f130c3b795c21ff0c7e3903278f"]},{"id": "6e565308c8081e807709cb4a917443b737e6cdb4", "title": "Large-scale Simple Question Answering with Memory Networks", "authors": ["Antoine Bordes", "Nicolas Usunier", "Jason Weston"], "date": "2015", "abstract": "Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that… ", "references": ["a584211768d49f80192f13b8ed2fda9c058dec34", "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "a583af2696030bcf5f556edc74573fbee902be0b", "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "f86ec155cce6259e5230aaad3b762343757feb1d", "a584211768d49f80192f13b8ed2fda9c058dec34", "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "a584211768d49f80192f13b8ed2fda9c058dec34", "2fc760267d7e052e584cf55b641539e913df55ef", "71ae756c75ac89e2d731c9c79649562b5768ff39"]},{"id": "15de5528b04bf3d9cf741122677588140c25ebff", "title": "The neurobiology of semantic memory", "authors": ["Jeffrey R. Binder", "Rutvik H. Desai"], "date": "2011", "abstract": "Semantic memory includes all acquired knowledge about the world and is the basis for nearly all human activity, yet its neurobiological foundation is only now becoming clear. Recent neuroimaging studies demonstrate two striking results: the participation of modality-specific sensory, motor, and emotion systems in language comprehension, and the existence of large brain regions that participate in comprehension tasks but are not modality-specific. These latter regions, which include the inferior… ", "references": ["da33ab4ae1b8f53ca7eead1b1b55e676ee0e7a29", "088bd35f92192cd75f93464c50fc3280b2077a22", "da33ab4ae1b8f53ca7eead1b1b55e676ee0e7a29", "c9cbfdfafbb21a1858ac965389407a3c05bcfe4b", "e6d0b8426479070d6f080ef342cbaf6f24fa1082", "088bd35f92192cd75f93464c50fc3280b2077a22", "9021b7491a2669ae87ef336cd1ab8884e265c01c", "088bd35f92192cd75f93464c50fc3280b2077a22", "da33ab4ae1b8f53ca7eead1b1b55e676ee0e7a29", "c9cbfdfafbb21a1858ac965389407a3c05bcfe4b"]},{"id": "79c0b2f44bbc2bc51de554b88ebe46204413f884", "title": "The Role of Syntax in Vector Space Models of Compositional Semantics", "authors": ["Karl Moritz Hermann", "Phil Blunsom"], "date": "ACL", "abstract": "Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non… ", "references": ["57458bc1cffe5caa45a885af986d70f723f406b4", "57458bc1cffe5caa45a885af986d70f723f406b4", "27e38351e48fe4b7da2775bf94341738bc4da07e", "7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917", "27e38351e48fe4b7da2775bf94341738bc4da07e", "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "57458bc1cffe5caa45a885af986d70f723f406b4"]},{"id": "7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917", "title": "A Comparison of Vector-based Representations for Semantic Composition", "authors": ["William Blacoe", "Mirella Lapata"], "date": "EMNLP-CoNLL", "abstract": "In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to… ", "references": ["dac72f2c509aee67524d3321f77e97e8eff51de6", "6633814eb1e066d146bdae16d0c1c8344c60778c", "4662eea99a7850dc742cf02cee5a8e06d1b029d2", "d020eb83f03a9f9c97e728355c4a9010fa65d8ef", "6633814eb1e066d146bdae16d0c1c8344c60778c", "d020eb83f03a9f9c97e728355c4a9010fa65d8ef", "553fb89d5858826c02f26e94262e8958debc777e", "3c5126da7ce388c64b796c80d15a3c3629d6ad58", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "3c5126da7ce388c64b796c80d15a3c3629d6ad58"]},{"id": "95174b77e4a5856fb0b0283bb0cb8acd3429d946", "title": "The NXT-format Switchboard Corpus: a rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue", "authors": ["Sasha Calhoun", "Jean Carletta", "David Beaver"], "date": "2010", "abstract": "This paper describes a recently completed common resource for the study of spoken discourse, the NXT-format Switchboard Corpus. Switchboard is a long-standing corpus of telephone conversations (Godfrey et al. in SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings of ICASSP-92, pp. 517–520, 1992). We have brought together transcriptions with existing annotations for syntax, disfluency, speech acts, animacy, information status, coreference, and prosody; along with… ", "references": ["657a126f831d854c79a63f4b100c889ae23c25ac", "a87ca203325c7c2a57e319e069a36afde4a7d46a", "a77f8d99ab7d81ebe5208d0500e19753ec694fb5", "657a126f831d854c79a63f4b100c889ae23c25ac", "a87ca203325c7c2a57e319e069a36afde4a7d46a", "b19b37e28fc62f5c67bda243756e4e3a337bd1a7", "c1ae44eaab0a0c4a26e3377c55407ebe79517c63", "a77f8d99ab7d81ebe5208d0500e19753ec694fb5", "a87ca203325c7c2a57e319e069a36afde4a7d46a", "81200812fde4c423293c2b09fa198c093cc05c3e"]},{"id": "81bd1081df12c554f5b577677eb1a3975f728476", "title": "Recognizing Entailment and Contradiction by Tree-based Convolution", "authors": ["Lili Mou", "Rui Men", "Zhi Jin"], "date": "2015", "abstract": "Recognizing entailment and contradiction between two sentences has wide applications in NLP. Traditional methods include featurerich classifiers or formal reasoning. However, they are usually limited in terms of accuracy and scope. Recently, the renewed prosperity of neural networks has made many improvements in a variety of NLP tasks. In our previous work, the tree-based convolutional neural network (TBCNN) has achieved high performance in several sentence-level classification tasks. But… ", "references": ["27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "ae3e2451491f7d6ea1ee0c587fd8c811b4200c07", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "032fbcb58e2282f02426a0f09c6d5b42787936ec", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "757a192157d7587433db93a899d32c4c5e832489", "03db9da61a96e6d4e23af05130892ecfff01751f", "ae3e2451491f7d6ea1ee0c587fd8c811b4200c07"]},{"id": "c333778104f648c385b4631f7b4a859787e9d3d3", "title": "A SICK cure for the evaluation of compositional distributional semantic models", "authors": ["Marco Marelli", "Stefano Menini", "Roberto Zamparelli"], "date": "LREC", "abstract": "Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for… ", "references": ["27e38351e48fe4b7da2775bf94341738bc4da07e", "528fa9bb03644ba752fb9491be49b9dd1bce1d52", "aa8217237363ae94301b453ee2100957e6e15b68", "0165568bcc1a819c18564567f2ec15d859be2519", "528fa9bb03644ba752fb9491be49b9dd1bce1d52", "ef3b1e5f9cbf3a4f4724be2e3c5a85d4db3014c0", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "3c5126da7ce388c64b796c80d15a3c3629d6ad58", "11ec56898a9e7f401a2affe776b5297bd4e25025", "27e38351e48fe4b7da2775bf94341738bc4da07e"]},{"id": "581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "title": "A Phrase-Based Alignment Model for Natural Language Inference", "authors": ["Bill MacCartney", "Michel Galley", "Christopher D. Manning"], "date": "EMNLP", "abstract": "The alignment problem---establishing links between corresponding phrases in two related sentences---is as important in natural language inference (NLI) as it is in machine translation (MT). But the tools and techniques of MT alignment do not readily transfer to NLI, where one cannot assume semantic equivalence, and for which large volumes of bitext are lacking. We present a new NLI aligner, the MANLI system, designed to address these challenges. It uses a phrase-based alignment representation… ", "references": ["59c442932e9fcfcac6df5566c2bcd1ec331548c9", "2a7b85b09d91f9ae139584fc9b124f6b051bc8c5", "dce3bcaf68e10992a49189c1227a3ccd4cc8a11d", "318e9eb50bb879f01efb7b4ed0482c8e458b6e8c", "318e9eb50bb879f01efb7b4ed0482c8e458b6e8c", "c1d36654636df36165967c8a005520e170056209", "318e9eb50bb879f01efb7b4ed0482c8e458b6e8c", "757a192157d7587433db93a899d32c4c5e832489", "6a3819476b99a238d37beb00c878c5602cc639e8", "c1d36654636df36165967c8a005520e170056209"]},{"id": "f6d37a305fb900a56de5ddcd1095ec1ba5c8f157", "title": "An Inference-Based Approach to Recognizing Entailment", "authors": ["Peter Clark", "Philip Harrison"], "date": "2009", "abstract": "For this year's RTE challenge we have continued to pursue a (somewhat) \"logical\" approach to recognizing entailment, in which our system, called BLUE (Boeing Language Understanding Engine) first creates a logic-based representation of a text T and then performs simple inference (using WordNet and the DIRT inference rule database) to try and infer a hypothesis H. The overall system can be viewed as comprising of three main elements: parsing, WordNet, and DIRT, built on top of a simple baseline of bag-of-words comparison.", "references": ["3f16763a19a40190e35bf151b8afaf074665a725"]},{"id": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "Glove: Global Vectors for Word Representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "date": "EMNLP", "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque.", "references": ["330da625c15427c6e42ccfa3b747fb29e5835bf0", "500d570ce02abf42bc1bc535620741d4c5665e6a", "53ca064b9f1b92951c1997e90b776e95b0880e52", "e89f679710507e239775a1e9c81988c3f928cbed", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "53ab89807caead278d3deb7b6a4180b277d3cb77", "e89f679710507e239775a1e9c81988c3f928cbed", "e89f679710507e239775a1e9c81988c3f928cbed", "53ca064b9f1b92951c1997e90b776e95b0880e52", "dac72f2c509aee67524d3321f77e97e8eff51de6"]},{"id": "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "title": "Reasoning about Entailment with Neural Attention", "authors": ["Tim Rocktäschel", "Edward Grefenstette", "Phil Blunsom"], "date": "2015", "abstract": "While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural… ", "references": ["11ec56898a9e7f401a2affe776b5297bd4e25025", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5082a1a13daea5c7026706738f8528391a1e6d59", "fca1e631b8f93036065311eb92727c509423475a", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "0ce317d84086b8885ddbc7923ec00bedb64ab6dc", "title": "Measuring the Influence of Long Range Dependencies with Neural Network Language Models", "authors": ["Hai Son Le", "Alexandre Allauzen", "François Yvon"], "date": "WLM@NAACL-HLT", "abstract": "In spite of their well known limitations, most notably their use of very local contexts, n-gram language models remain an essential component of many Natural Language Processing applications, such as Automatic Speech Recognition or Statistical Machine Translation. This paper investigates the potential of language models using larger context windows comprising up to the 9 previous words. This study is made possible by the development of several novel Neural Network Language Model architectures… ", "references": ["cb45e9217fe323fbc199d820e7735488fca2a9b3", "e41498c05d4c68e4750fb84a380317a112d97b01", "e41498c05d4c68e4750fb84a380317a112d97b01", "9819b600a828a57e1cde047bbe710d3446b30da5", "ba786c46373892554b98df42df7af6f5da343c9d", "9819b600a828a57e1cde047bbe710d3446b30da5", "ba786c46373892554b98df42df7af6f5da343c9d", "d4f00f97581d30e7104d1fe924085248093aa472", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "ba786c46373892554b98df42df7af6f5da343c9d"]},{"id": "6396ab37641d36be4c26420e58adeb8665914c3b", "title": "Modeling Biological Processes for Reading Comprehension", "authors": ["Jonathan Berant", "Vivek Srikumar", "Christopher D. Manning"], "date": "EMNLP", "abstract": "Machine reading calls for programs that read and understand text, but most current work only attempts to extract facts from redundant web-scale corpora.", "references": ["5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "564257469fa44cdb57e4272f85253efb9acfd69d", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "74fe7ec751cd50295b15cfd46389a8fefb37c414", "564257469fa44cdb57e4272f85253efb9acfd69d", "687dce9ac01f5996601655035c34b449c27c3b6a", "687dce9ac01f5996601655035c34b449c27c3b6a", "564257469fa44cdb57e4272f85253efb9acfd69d", "e0d88edbe9c880b6f41c0cdf2c0f64763b58491f"]},{"id": "f26e088bc4659a9b7fce28b6604d26de779bcf93", "title": "Learning Answer-Entailing Structures for Machine Comprehension", "authors": ["Mrinmaya Sachan", "Kumar Avinava Dubey", "Matthew Richardson"], "date": "ACL", "abstract": "Understanding open-domain text is one of the primary challenges in NLP. Machine comprehension evaluates the system’s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hidden (latent) structure that explains the relation between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the… ", "references": ["f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "5380c0946aff2aa100c91ccbbe7ac034441e24a3", "5380c0946aff2aa100c91ccbbe7ac034441e24a3", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7"]},{"id": "75ddc7ee15be14013a3462c01b38b0548486fbcb", "title": "Learning to Compose Neural Networks for Question Answering", "authors": ["Jacob Andreas", "Marcus Rohrbach", "Dan Klein"], "date": "2016", "abstract": "We describe a question answering model that applies to both images and structured knowledge bases.", "references": ["df4f851e3c37017822a683b1356c6c390b5b5487", "d1505c6123c102e53eb19dff312cb25cea840b72", "0ac8f1a3c679b90d22c1f840cdc8d61ffef750ac", "af44f5db5b4396e1670cda07eff5ad84145ba843", "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "62a956d7600b10ca455076cd56e604dfd106072a", "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "62a956d7600b10ca455076cd56e604dfd106072a", "d1505c6123c102e53eb19dff312cb25cea840b72", "df4f851e3c37017822a683b1356c6c390b5b5487"]},{"id": "445406b0d88ae965fa587cf5c167374ff1bbc09a", "title": "A Rule-based Question Answering System for Reading Comprehension Tests", "authors": ["Ellen Riloff", "Michael Thelen"], "date": "Workshop On Reading…", "abstract": "We have developed a rule-based system, Quarc, that can read a short story and find the sentence in the story that best answers a given question. Quarc uses heuristic rules that look for lexical and semantic clues in the question and the story. We have tested Quarc on reading comprehension tests typically given to children in grades 3--6. Overall, Quarc found the correct sentence 40% of the time, which is encouraging given the simplicity of its rules. ", "references": ["5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f"]},{"id": "af44f5db5b4396e1670cda07eff5ad84145ba843", "title": "A Neural Network for Factoid Question Answering over Paragraphs", "authors": ["Mohit Iyyer", "Jordan Boyd-Graber", "Hal Daumé"], "date": "EMNLP", "abstract": "Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations. These methods are ineective when question text contains very few individual words (e.g., named entities) that are indicative of the answer. We introduce a recursive neural network (rnn) model that can reason over such input by modeling textual compositionality. We apply our model, qanta, to a dataset of questions from a trivia competition… ", "references": ["8bbb2a5b01067e252702b13e24cd8eccdce732d6", "5ab6ddd1d45302bf635cce5cb93fbaf4ea79458a", "73b1f03870fd196200c47beb8212bb881373f6e4", "0dad0da221dea30c3a0e90c45a0699aeb850af49", "5856de5c6f43b6562bc3e93a89e1456fa035ec5e", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "4e4c29f1638c63e294f9e29a20602d58e0c8e212", "5856de5c6f43b6562bc3e93a89e1456fa035ec5e", "0dad0da221dea30c3a0e90c45a0699aeb850af49", "0dad0da221dea30c3a0e90c45a0699aeb850af49"]},{"id": "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "authors": ["Jiasen Lu", "Jianwei Yang", "Devi Parikh"], "date": "2016", "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the… ", "references": ["5fa973b8d284145bf0ced9acf2913a74674260f6", "def584565d05d6a8ba94de6621adab9e301d375d", "7214daf035ab005b3d1e739750dd597b4f4513fa", "62a956d7600b10ca455076cd56e604dfd106072a", "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60", "2c1890864c1c2b750f48316dc8b650ba4772adc5", "2c1890864c1c2b750f48316dc8b650ba4772adc5", "5fa973b8d284145bf0ced9acf2913a74674260f6", "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60", "1cf6bc0866226c1f8e282463adc8b75d92fba9bb"]},{"id": "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "title": "Paraphrase-Driven Learning for Open Question Answering", "authors": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "date": "ACL", "abstract": "We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions.", "references": ["9c99620d7511c83a402ff3b4b3a2348a669e61e3", "bd298c1bcefcd7feb108111cd72758c265d16ee6", "2a5a179a70e98ee1c4b4b8b54f6c8aba629c02e4", "92fb5e045bb23f13c11d8bb277925013b24b5930", "9c99620d7511c83a402ff3b4b3a2348a669e61e3", "92fb5e045bb23f13c11d8bb277925013b24b5930", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "bd298c1bcefcd7feb108111cd72758c265d16ee6"]},{"id": "a583af2696030bcf5f556edc74573fbee902be0b", "title": "Weakly Supervised Memory Networks", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "date": "2015", "abstract": "In this paper we introduce a variant of Memory Networks (Weston et al., 2015b) that needs significantly less supervision to perform question and answering tasks. The original model requires that the sentences supporting the answer be explicitly indicated during training. In contrast, our approach only requires the answer to the question during training. We apply the model to the synthetic bAbI tasks, showing that our approach is competitive with the supervised approach, particularly when… ", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "b185742930fd959aaccdfdecdb31641839a787c4", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "d38e8631bba0720becdaf7b89f79d9f9dca45d82"]},{"id": "c1787db25af5614f41e56938aa594f2dbb1dca07", "title": "The Unreasonable Effectiveness of Data", "authors": ["Alon Y. Halevy", "Peter Norvig", "Fernando C Pereira"], "date": "2009", "abstract": "At Brown University, there is excitement of having access to the Brown Corpus, containing one million English words.", "references": ["fa96245255f18739dab9e70cc4ae5dda0765b509", "fa96245255f18739dab9e70cc4ae5dda0765b509", "fa96245255f18739dab9e70cc4ae5dda0765b509", "eeccec1d162c0b979e09d4b6396d1761443ff8c0", "fa96245255f18739dab9e70cc4ae5dda0765b509", "fa96245255f18739dab9e70cc4ae5dda0765b509", "fa96245255f18739dab9e70cc4ae5dda0765b509", "eeccec1d162c0b979e09d4b6396d1761443ff8c0", "eeccec1d162c0b979e09d4b6396d1761443ff8c0", "eeccec1d162c0b979e09d4b6396d1761443ff8c0"]},{"id": "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "title": "Learning Dependency-Based Compositional Semantics", "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein"], "date": "2013", "abstract": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical forxm and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.Our goal is to instead learn a semantic parser from question–answer… ", "references": ["8dd9fd6a45afd266d48255c398429e01ea4fd6db", "92fb5e045bb23f13c11d8bb277925013b24b5930", "74fe7ec751cd50295b15cfd46389a8fefb37c414", "7ae207cfaf01dc2b6799da67f454190b34994870", "b7c0e47f8b768258b7d536c21b218e6c46ab8791", "c7a40c3ef180d847bb3db40fd01990e08a6264f7", "7c7ba9df3ab69f0f3502a7a873c031c8244187ee", "687dce9ac01f5996601655035c34b449c27c3b6a", "92fb5e045bb23f13c11d8bb277925013b24b5930", "1f8a99790dd9124d528a718e55a0b27683685c77"]},{"id": "b75329489baf067e6f7bbb74f16ffd49fba80dfa", "title": "Freebase QA: Information Extraction or Semantic Parsing?", "authors": ["Xuchen Yao", "Jonathan Berant", "Benjamin Van Durme"], "date": "ACL", "abstract": "We contrast two seemingly distinct approaches to the task of question answering (QA) using Freebase: one based on information extraction techniques, the other on semantic parsing. Results over the same test-set were collected from two state-ofthe-art, open-source systems, then analyzed in consultation with those systems’ creators. We conclude that the differences between these technologies, both in task performance, and in how they get there, is not significant. This suggests that the semantic… ", "references": ["3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4", "74fe7ec751cd50295b15cfd46389a8fefb37c414", "74fe7ec751cd50295b15cfd46389a8fefb37c414", "3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4", "319e572fcddff77513eed8a25effbc7d9ff8ef85", "36d69fec4884389c1709d3ca74394cac814ce4a4", "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "80c2d8c691b09f8b4e53f512b9d2641b49fda935", "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "b29447ba499507a259ae9d8f685d60cc1597d7d3"]},{"id": "e08acdc9aa026931ef75909cd5f75955755d8021", "title": "Learning strategies for story comprehension: a reinforcement learning approach", "authors": ["Eugene Grois", "David C. Wilkins"], "date": "ICML '05", "abstract": "This paper describes the use of machine learning to improve the performance of natural language question answering systems. We present a model for improving story comprehension through inductive generalization and reinforcement learning, based on classified examples. In the process, the model selects the most relevant and useful pieces of lexical information to be used by the inference procedure. We compare our approach to three prior non-learning systems, and evaluate the conditions under… ", "references": ["f0a76c6275dba168f272e1679e92edb6f5957f23", "2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f", "2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f", "f0a76c6275dba168f272e1679e92edb6f5957f23", "f0a76c6275dba168f272e1679e92edb6f5957f23", "2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f", "f0a76c6275dba168f272e1679e92edb6f5957f23", "f0a76c6275dba168f272e1679e92edb6f5957f23", "f0a76c6275dba168f272e1679e92edb6f5957f23", "f0a76c6275dba168f272e1679e92edb6f5957f23"]},{"id": "c0e8b7b668d82afd5a7a90999d78c3a36d23d909", "title": "An Entailment-Based Approach to the QA4MRE Challenge", "authors": ["Peter Clark", "Philip Harrison", "Xuchen Yao"], "date": "CLEF", "abstract": "This paper describes our entry to the 2012 QA4MRE Main Task (English dataset). The QA4MRE task poses a significant challenge as the expression of knowledge in the question and answer (in the document) typically substantially differs. Ultimately, one would need a system that can perform full machine reading – creating an internal model of the document’s meaning – to achieve high performance. Our approach is a preliminary step toward this, based on estimating the likelihood of textual entailment… ", "references": ["c077b3791a7ea01ef487f48ebac943af2e8e79c4", "c077b3791a7ea01ef487f48ebac943af2e8e79c4", "4639cb3718e4eb698a0285548ed2bf23ad9908a9", "950a3c89dacbc3e7ddcd43d7ff6f985697e41cdb", "135ecd32a3a569efc7c7f24c9a95abc415b878f2", "74f61af390292fc197659ae698429df4a2de62df"]},{"id": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs", "authors": ["Jonathan Berant", "Andrew Chou", "Percy Liang"], "date": "EMNLP", "abstract": "In this paper, we train a semantic parser that scales up to Freebase.", "references": ["e7e7b9a731678bf0494fe29cbebb42a822224cc6", "92fb5e045bb23f13c11d8bb277925013b24b5930", "c0be2ac2f45681f1852fc1d298af5dceb85834f4", "c1052027ddbacc24fc4a244d2c073f86aca62747", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "92fb5e045bb23f13c11d8bb277925013b24b5930", "dd78a271722fb9e0eaade4208860398de1d80b7f", "c1052027ddbacc24fc4a244d2c073f86aca62747", "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c"]},{"id": "abba83b5747e98d10ab982df9ae94cc799668f16", "title": "Simple Coreference Resolution with Rich Syntactic and Semantic Features", "authors": ["Aria Haghighi", "Dan Klein"], "date": "EMNLP", "abstract": "Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and… ", "references": ["a20bfec3c95aad003dcb45a21a220c19cca8bb66", "890c1201d3edef7104ba9d75078ed90f9c30c93b", "d0e498810f28462ea4d172f1f6000b7c1cf2870c", "08c81389b3ac4b8253d718a7cebe04a5536efa78", "ed2f04fc0164e64533affc4acffe80bf934e3440", "890c1201d3edef7104ba9d75078ed90f9c30c93b", "a20bfec3c95aad003dcb45a21a220c19cca8bb66", "a20bfec3c95aad003dcb45a21a220c19cca8bb66", "08c81389b3ac4b8253d718a7cebe04a5536efa78", "ddde3c7371a8d3d8fbbf53a37aba96fa9b0ce1db"]},{"id": "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "title": "Deep Read: A Reading Comprehension System", "authors": ["Lynette Hirschman", "Marc Light", "John D. Burger"], "date": "ACL", "abstract": "This paper describes initial work on Deep Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it. We have acquired a corpus of 60 development and 60 test stories of 3rd to 6th grade material; each story is followed by short-answer questions (an answer key was also provided). We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated… ", "references": []},{"id": "3c22c2dc4805aa8b94761b5ff48c434138bb9855", "title": "What is left to be understood in ATIS?", "authors": ["Gökhan Tür", "Dilek Z. Hakkani-Tür", "Larry Heck"], "date": "2010", "abstract": "One of the main data resources used in many studies over the past two decades for spoken language understanding (SLU) research in spoken dialog systems is the airline travel information system (ATIS) corpus. Two primary tasks in SLU are intent determination (ID) and slot filling (SF). Recent studies reported error rates below 5% for both of these tasks employing discriminative machine learning techniques with the ATIS test set. While these low error rates may suggest that this task is close to… ", "references": ["766018bf388646e23bd53eb3692afb0f67915a14", "d7de46fb5eb895b4e1323c596715292239f45234", "b42e47f494583db7731585485cb0f5ab5081c151", "6443636a690b3595ec63321db5c4c95d8ea44d53", "b42e47f494583db7731585485cb0f5ab5081c151", "780999094f775ec6b8faa94cd496a4c560d31a04", "6443636a690b3595ec63321db5c4c95d8ea44d53", "9a592e2dd099aa42b5595b243b71a2c6fc6257f8", "b42e47f494583db7731585485cb0f5ab5081c151", "7a1620f36439cdf057133e2fe4d94dbe8f79e448"]},{"id": "7ec3a89dc9b2f51c5224eb020e86c86c8498da00", "title": "A Challenge Set for Advancing Language Modeling", "authors": ["Geoffrey Zweig", "Christopher J. C. Burges"], "date": "WLM@NAACL-HLT", "abstract": "In this paper, we describe a new, publicly available corpus intended to stimulate research into language modeling techniques which are sensitive to overall sentence coherence. The task uses the Scholastic Aptitude Test's sentence completion format. The test set consists of 1040 sentences, each of which is missing a content word. The goal is to select the correct replacement from amongst five alternates. In general, all of the options are syntactically valid, and reasonable with respect to local… ", "references": ["0e4d042b668805e19f097b7eb0f223babec68f67", "673992da19d9209434615b12d55bdd36be706e9e", "a63540c6eefdae0ac555bdd8a9bda7afea918974", "dae559d16c0795d1f76bf22a2690a30c7da08296", "a63540c6eefdae0ac555bdd8a9bda7afea918974", "dae559d16c0795d1f76bf22a2690a30c7da08296", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "dae559d16c0795d1f76bf22a2690a30c7da08296", "673992da19d9209434615b12d55bdd36be706e9e"]},{"id": "eaf71a1e1a2b186db899558921616888ed2f26de", "title": "Automatic Gap-fill Question Generation from Text Books", "authors": ["Manish Agarwal", "Prashanth Mannem"], "date": "BEA@ACL", "abstract": "In this paper, we present an automatic question generation system that can generate gap-fill questions for content in a document.", "references": []},{"id": "d2dece97743aabd8ecfe549804d4c9ad6f3bc2af", "title": "Automatic Multi-Layer Corpus Annotation for Evaluation Question Answering Methods: CBC4Kids", "authors": ["Jochen L. Leidner", "Tiphaine Dalmas", "Claire Grover"], "date": "LINC@EACL", "abstract": "Reading comprehension tests are receiving increased attention within the NLP community as a controlled test-bed for developing, evaluating and comparing robust question answering (NLQA) methods. To support this, we have enriched the MITRE CBC4Kids corpus with multiple XML annotation layers recording the output of various tokenizers, lemmatizers, a stemmer, a semantic tagger, POS taggers and syntactic parsers. Using this resource, we have built a baseline NLQA system for wordoverlap based answer… ", "references": ["33be02735525a3eb6111ee790ea2e15775019d21", "33be02735525a3eb6111ee790ea2e15775019d21", "33be02735525a3eb6111ee790ea2e15775019d21", "e979925b15861153a0e9ce8ace39a28d319e613d", "e979925b15861153a0e9ce8ace39a28d319e613d", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "49ade126d6cd6cf39f33e9f4c9f714d2e6a46d50", "49ade126d6cd6cf39f33e9f4c9f714d2e6a46d50", "4131cf43239d9ff7b27bbda7593dd48167a0d7ab", "4131cf43239d9ff7b27bbda7593dd48167a0d7ab"]},{"id": "30110856f45fde473f1903f686aa365cf70ed4c7", "title": "Learning Context-free Grammars: Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory", "authors": ["Independence WayPrinceton"], "date": "1992", "abstract": "This work describes an approach for inferring De-terministic Context-free (DCF) Grammars in a Connectionist paradigm using a Recurrent Neu-ral Network Pushdown Automaton (NNPDA). The NNPDA consists of a recurrent neural network connected to an external stack memory through a common error function. We show that the NNPDA is able to learn the dynamics of an underlying push-down automaton from examples of grammatical and non-grammatical strings. Not only does the network learn the state… ", "references": []},{"id": "f92272e33b11a0d2f47b5b65446c0f1a913cfd17", "title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension", "authors": ["Yiming Cui", "Ting Liu", "Guoping Hu"], "date": "COLING", "abstract": "Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension… ", "references": ["9653d5c2c7844347343d073bbedd96e05d52f69b", "ebd03c093df7ec30a2a360b377e85b6f00ecb96f", "f2e50e2ee4021f199877c8920f1f984481c723aa", "b1e20420982a4f923c08652941666b189b11b7fe", "f2e50e2ee4021f199877c8920f1f984481c723aa", "d6865d056a5a0882c4fcfa36b3da728da6ad0fc0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f2e50e2ee4021f199877c8920f1f984481c723aa", "b1e20420982a4f923c08652941666b189b11b7fe", "9653d5c2c7844347343d073bbedd96e05d52f69b"]},{"id": "9b30e7c50d8aa3d872a63d7ca2e18ebf6a23c031", "title": "Natural Language Processingwith Modular Neural Networks and Distributed Lexicon", "authors": ["Risto Miikkulainen", "Michael G. Dyer"], "date": "1991", "abstract": "An approach to connectionist natural language processing is proposed, which is based on hierarchically organized modular Parallel Distributed Processing (PDP) networks and a central lexicon of distributed input/output representations. The modules communicate using these representations, which are global and publicly available in the system. The representations are developed automatically by all networks while they are learning their processing tasks. The resulting representations reflect the… ", "references": []},{"id": "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "title": "Deep Recursive Neural Networks for Compositionality in Language", "authors": ["Ozan Irsoy", "Claire Cardie"], "date": "NIPS", "abstract": "Recursive neural networks comprise a class of architecture that can operate on structured input.", "references": ["b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "9c0ddf74f87d154db88d79c640578c1610451eec", "57458bc1cffe5caa45a885af986d70f723f406b4", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "cfa2646776405d50533055ceb1b7f050e9014dcb", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "c58dd287a476b4722c5b6b1316629e2874682219", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "9c0ddf74f87d154db88d79c640578c1610451eec"]},{"id": "15454e478cc826e195cb15732ea0db57ad8bd38c", "title": "Separating Answers from Queries for Neural Reading Comprehension", "authors": ["Dirk Weissenborn"], "date": "2016", "abstract": "We present a novel neural architecture for answering queries, designed to optimally leverage explicit support in the form of query-answer memories. Our model is able to refine and update a given query while separately accumulating evidence for predicting the answer. Its architecture reflects this separation with dedicated embedding matrices and loosely connected information pathways (modules) for updating the query and accumulating evidence. This separation of responsibilities effectively… ", "references": ["2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "f314339651cb25e4234e0b96fe8bd87206847993", "d1505c6123c102e53eb19dff312cb25cea840b72", "f314339651cb25e4234e0b96fe8bd87206847993", "f314339651cb25e4234e0b96fe8bd87206847993", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5", "13fe71da009484f240c46f14d9330e932f8de210", "d1505c6123c102e53eb19dff312cb25cea840b72", "f2e50e2ee4021f199877c8920f1f984481c723aa"]},{"id": "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "title": "A Clockwork RNN", "authors": ["Jan Koutník", "Klaus Greff", "Jürgen Schmidhuber"], "date": "2014", "abstract": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a… ", "references": ["c69201d091dd92699fd90a17b9e3407319726791", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "c69201d091dd92699fd90a17b9e3407319726791", "50c770b425a5bb25c77387f687a9910a9d130722", "50c770b425a5bb25c77387f687a9910a9d130722", "c69201d091dd92699fd90a17b9e3407319726791", "e141d68065ce638f9fc4f006eab2f66711e89768", "50c770b425a5bb25c77387f687a9910a9d130722", "f6e91c9e7e8f8a577a98ecfcfa998212a683195a"]},{"id": "9665247ea3421929f9b6ad721f139f11edb1dbb8", "title": "Learning Longer Memory in Recurrent Neural Networks", "authors": ["Armand Joulin", "Marc'Aurelio Ranzato"], "date": "2015", "abstract": "Recurrent neural network is a powerful model that learns temporal patterns in sequential data.", "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "1fac520eca9767bfe9a28302494c818642a2b2c7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "ded103d0613e1a8f51f586cc1678aee3ff26e811", "4ef03716945bd3907458efbe1bbf8928dafc1efc", "d0be39ee052d246ae99c082a565aba25b811be2d", "d1275b2a2ab53013310e759e5c6878b96df643d4", "d1275b2a2ab53013310e759e5c6878b96df643d4", "d1275b2a2ab53013310e759e5c6878b96df643d4", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97"]},{"id": "d5ddb30bf421bdfdf728b636993dc48b1e879176", "title": "Learning and development in neural networks: the importance of starting small", "authors": ["Jeffrey L. Elman"], "date": "1993", "abstract": "It is a striking fact that in humans the greatest learning occurs precisely at that point in time--childhood--when the most dramatic maturational changes also occur. This report describes possible synergistic interactions between maturational change and the ability to learn a complex domain (language), as investigated in connectionist networks. The networks are trained to process complex sentences involving relative clauses, number agreement, and several types of verb argument structure… ", "references": ["631bd89fd63b8facd759958b8f60ee23bb3c397e", "3e0aa477f5caa75d26dddaca01d6bc03141029dd", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "0ac8973f2d4e40509605cd2541e69d0c6d77e401", "06f73a9eb8154224ab5bf8ad0a68f6a8beeb0aa0", "0ac8973f2d4e40509605cd2541e69d0c6d77e401", "3e0aa477f5caa75d26dddaca01d6bc03141029dd", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "79c9caa8b2e87f2e079295d0422e566c2bf42fd1", "d3dba8be4a83cca022aa8c85d65094d5f7412e05"]},{"id": "c74e230a5a6fd5e2db6ace765ce38afe65f96214", "title": "Learning Multilevel Distributed Representations for High-Dimensional Sequences", "authors": ["Ilya Sutskever", "Geoffrey E. Hinton"], "date": "AISTATS", "abstract": "We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel… ", "references": ["8978cf7574ceb35f4c3096be768c7547b28a35d0", "9f63683b975d30eeef0d563e3464431efedef5c2", "607802a4067cb7738bac85d3ca3386f859e637b9", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "9f63683b975d30eeef0d563e3464431efedef5c2", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "607802a4067cb7738bac85d3ca3386f859e637b9", "4f7476037408ac3d993f5088544aab427bc319c1", "9c5a0951cea300222834497c8e12ac2be99cd11e"]},{"id": "a24508e65e599b5b20c33af96dbe7017d5caca37", "title": "Learning internal representations", "authors": ["Jonathan Baxter"], "date": "1995", "abstract": "Most machine learning theory and practice is concerned with learning a single task. In this thesis it is argued that in general there is insufficient information in a single task for a learner to generalise well and that what is required for good generalisation is information about many similar learning tasks. Similar learning tasks form a body of prior information that can be used to constrain the learner and make it generalise better. Examples of learning scenarios in which there are many… ", "references": ["25406e6733a698bfc4ac836f8e74f458e75dad4f"]},{"id": "3832057ac487f43e885cdb485a6ca1462834bb8d", "title": "Estimating or Propagating Gradients Through Stochastic Neurons", "authors": ["Yoshua Bengio"], "date": "2013", "abstract": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such s tochastic neurons, i.e., can we “back-propagate” through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrate d that a simple biologically plausible… ", "references": ["052b1d8ce63b07fec3de9dbb583772d860b7c769", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "f8c8619ea7d68e604e40b814b40c72888a755e95", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "72d32c986b47d6b880dad0c3f155fe23d2939038", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "f7410cd1afeba276f4479e8b5f04f12530b48d83", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "b7b915d508987b73b61eccd2b237e7ed099a2d29"]},{"id": "512ea8d0c5b5de896129e76d4276f7b996fe88d8", "title": "Learning Stochastic Feedforward Neural Networks", "authors": ["Yichuan Tang", "Ruslan Salakhutdinov"], "date": "NIPS", "abstract": "Multilayer perceptrons (MLPs) or neural networks are popular models used for nonlinear regression and classification tasks. As regressors, MLPs model the conditional distribution of the predictor variables Y given the input variables X. However, this predictive distribution is assumed to be unimodal (e.g. Gaussian). For tasks involving structured prediction, the conditional distribution should be multi-modal, resulting in one-to-many mappings. By using stochastic hidden variables rather than… ", "references": ["4cf3569e045993dfe090749f26a55a768684ab86", "2f59406cce55c7bb9a78521bd14755a0db0aee7d", "00cd1dab559a9671b692f39f14c1573ab2d1416b", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "94b0e8e97c19ad0977d26e3e355d3ae09ad49365", "94b0e8e97c19ad0977d26e3e355d3ae09ad49365", "94b0e8e97c19ad0977d26e3e355d3ae09ad49365", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "5a2668bf420d8509a4dfa28e1cdcdac14c649975", "00cd1dab559a9671b692f39f14c1573ab2d1416b"]},{"id": "4af41f4d838daa7ca6995aeb4918b61989d1ed80", "title": "Classes for fast maximum entropy training", "authors": ["Joshua Goodman"], "date": "2001", "abstract": "Maximum entropy models are considered by many to be one of the most promising avenues of language modeling research. Unfortunately, long training times make maximum entropy research difficult. We present a speedup technique: we change the form of the model to use classes. Our speedup works by creating two maximum entropy models, the first of which predicts the class of each word, and the second of which predicts the word itself. This factoring of the model leads to fewer nonzero indicator… ", "references": ["0555dccd4f243fe6d353e8d0af4f161882694b1f", "0555dccd4f243fe6d353e8d0af4f161882694b1f"]},{"id": "d6fb7546a29320eadad868af66835059db93d99f", "title": "Efficient training of large neural networks for language modeling", "authors": ["Helena Schwenk"], "date": "2004", "abstract": "Recently there has been increasing interest in using neural networks for language modeling. In contrast to the well-known backoff n-gram language models, the neural network approach tries to limit the data sparseness problem by performing the estimation in a continuous space, allowing by this means smooth interpolations. The complexity to train such a model and to calculate one n-gram probability is however several orders of magnitude higher than for the backoff models, making the new approach… ", "references": ["3d6036af971c1f11ab712cc41487376a94e63673", "c06b7af4128a2c7c1b9977585bc026c6916322e9", "bbc659924151c767f907de9b412d9b7619d26690", "6b388f0151ab37adb3d57738b8f52a3f943f86c8", "8f617510cfe7f8ba24b238f43b135aca60c1b7b1", "8f617510cfe7f8ba24b238f43b135aca60c1b7b1", "399da68d3b97218b6c80262df7963baa89dcc71b", "bbc659924151c767f907de9b412d9b7619d26690", "3d6036af971c1f11ab712cc41487376a94e63673", "7d7867776be77bdb9f467b30c385cad8393f5240"]},{"id": "a8ca92770bce439a207cc75fd28a749b51b5a516", "title": "Comparison of part-of-speech and automatically derived category-based language models for speech recognition", "authors": ["Thomas R Niesler", "Edward W. D. Whittaker", "Philip C. Woodland"], "date": "1998", "abstract": "This paper compares various category-based language models when used in conjunction with a word-based trigram by means of linear interpolation. Categories corresponding to parts-of-speech as well as automatically clustered groupings are considered. The category-based model employs variable-length n-grams and permits each word to belong to multiple categories. Relative word error rate reductions of between 2 and 7% over the baseline are achieved in N-best rescoring experiments on the Wall Street… ", "references": ["b0130277677e5b915d5cd86b3afafd77fd08eb2e", "2e6dd008f2837dea054cd968ae6a50325809833d", "2e6dd008f2837dea054cd968ae6a50325809833d", "0b26fa1b848ed808a0511db34bce2426888f0b68", "a208a5a99e2a4d8f48673e980e8cc0479df890f8", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "24653b3b33d48c409deb672f8d8ee0eff31cd418", "a208a5a99e2a4d8f48673e980e8cc0479df890f8", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "8648dbfff9662fa9c62a95622712dd2951b5b3a3"]},{"id": "09c76da2361d46689825c4efc37ad862347ca577", "title": "A bit of progress in language modeling", "authors": ["Joshua Goodman"], "date": "2001", "abstract": "In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n -grams, skipping, interpolated Kneser?Ney smoothing, and clustering.", "references": ["9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "2a562693856bd87c412721df6433044c7a59ed84", "21108df08a7d20d877c2b6c0f03a2b53a9cd537d", "2a562693856bd87c412721df6433044c7a59ed84", "f7463aad3b5182820995101602788ea4c9bb4d9f", "ae7f2177b485737883235b9cc4233e7fd98e1365", "ae7f2177b485737883235b9cc4233e7fd98e1365", "dae559d16c0795d1f76bf22a2690a30c7da08296", "29053eab305c2b585bcfbb713243b05646e7d62d", "2a562693856bd87c412721df6433044c7a59ed84"]},{"id": "608045cac6ba523c3141b646c220d7736b1398a9", "title": "Large scale recurrent neural network on GPU", "authors": ["Boxun Li", "Erjin Zhou", "Huazhong Yang"], "date": "2014", "abstract": "Large scale artificial neural networks (ANNs) have been widely used in data processing applications. The recurrent neural network (RNN) is a special type of neural network equipped with additional recurrent connections. Such a unique architecture enables the recurrent neural network to remember the past processed information and makes it an expressive model for nonlinear sequence processing tasks. However, the large computation complexity makes it difficult to effectively train a recurrent… ", "references": ["fc40ad1238fba787dd8a58a7aed57a8d020a6fdc", "3127190433230b3dc1abd0680bb58dced4bcd90e", "fb970e9fdd270f56bdd1bbda0621e6992eab9d1c", "0d6203718c15f137fda2f295c96269bc2b254644", "053912e76e50c9f923a1fc1c173f1365776060cc", "d1208ac421cf8ff67b27d93cd19ae42b8d596f95", "fb970e9fdd270f56bdd1bbda0621e6992eab9d1c", "d1208ac421cf8ff67b27d93cd19ae42b8d596f95", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "fb970e9fdd270f56bdd1bbda0621e6992eab9d1c"]},{"id": "0db6eb46ca9941660acc775e3ca39bf4434c18be", "title": "Hierarchical Phrase-Based Translation", "authors": ["David Chiang"], "date": "2007", "abstract": "We present a statistical machine translation model that uses hierarchical phrasesphrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system's training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU… ", "references": ["3e4681ece0316438b9984097894fab3ef56d3b7a", "8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "3e4681ece0316438b9984097894fab3ef56d3b7a", "ad3d2f463916784d0c14a19936c1544309a0a440", "d9f2186e2e3aab0e697532aeec0d1b88a61af02d", "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "5d4ff509583809d33d26bc531d341a73341986ec", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "f5b1146b7ca79322aab124fd63825b9c175c02cf", "9c9548ac1705a48cc565c238c6102b7aa69101dc"]},{"id": "ded103d0613e1a8f51f586cc1678aee3ff26e811", "title": "Advances in optimizing recurrent networks", "authors": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "date": "2013", "abstract": "After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle… ", "references": ["43c8a545f7166659e9e21c88fe234e0323855216", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "d1275b2a2ab53013310e759e5c6878b96df643d4", "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "43c8a545f7166659e9e21c88fe234e0323855216", "c5145b1d15fea9340840cc8bb6f0e46e8934827f", "0d2336389dff3031910bd21dd1c44d1b4cd51725", "0d2336389dff3031910bd21dd1c44d1b4cd51725", "07ca885cb5cc4328895bfaec9ab752d5801b14cd"]},{"id": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "title": "Bidirectional recurrent neural networks", "authors": ["Mike Schuster", "Kuldip K. Paliwal"], "date": "1997", "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN.", "references": ["34468c0aa95a7aea212d8738ab899a69b2fc14c6", "03bc854feaee144b54924b440eff02ed9082cc6b", "03bc854feaee144b54924b440eff02ed9082cc6b", "8a6d820385527df2183a36ae1615f426ba894c5d", "2a6f3efc7e27e2d873af9f358e32d4b38251792a", "2a6f3efc7e27e2d873af9f358e32d4b38251792a", "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "8a6d820385527df2183a36ae1615f426ba894c5d", "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "dbc0a468ab103ae29717703d4aa9f682f6a2b664"]},{"id": "1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5", "title": "Least angle regression", "authors": ["Bradley Efron", "Trevor J. Hastie", "Robert Tibshirani"], "date": "2004", "abstract": "The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional… ", "references": []},{"id": "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1", "title": "Independent component filters of natural images compared with simple cells in primary visual cortex", "authors": ["J. H. van Hateren", "Andries van der Schaaf"], "date": "1998", "abstract": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured… ", "references": ["4333c01c6bf12219cc2dd35ca8452e66ae9ee99f", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "aeb37769d72999bcbfb0582b73607fd8d23f4545", "e2f74bec30cc4e471919de4dfd27c45dbc7b4b9d", "aeb37769d72999bcbfb0582b73607fd8d23f4545", "4ea293ac6d42d09ccb9ffab7bd72dcf6102c3eab", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "4ea293ac6d42d09ccb9ffab7bd72dcf6102c3eab", "4ea293ac6d42d09ccb9ffab7bd72dcf6102c3eab", "c5f5311fa1f34159ab3a0a1d58da51cd0340a640"]},{"id": "75e50717070e82cdf3945265a75def6960b55a9d", "title": "Explanation-based neural network learning a lifelong learning approach", "authors": ["Sebastian Thrun"], "date": "1995", "abstract": "From the Publisher: \nLifelong learning addresses situations in which a learner faces a series of different learning tasks providing the opportunity for synergy among them. Explanation-based neural network learning (EBNN) is a machine learning algorithm that transfers knowledge across multiple learning tasks. When faced with a new learning task, EBNN exploits domain knowledge accumulated in previous learning tasks to guide generalization in the new one. As a result, EBNN generalizes more… ", "references": []},{"id": "42d906c733f273109c0ed716a5ef6e2a379beb26", "title": "Learning Overcomplete Representations", "authors": ["Michael S. Lewicki", "Terrence J. Sejnowski"], "date": "2000", "abstract": "In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual… ", "references": ["f4547e5e6ac2e1fcb0ba51edbbe5d65b5765b4ca", "a87e0d75a8c17e464cf8e95a0466533e14b97c5e", "2805537bec87a6177037b18f9a3a9d3f1038867b", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "156ed6cd90506cd1111dffcd8e80e33ff62ca5ae", "156ed6cd90506cd1111dffcd8e80e33ff62ca5ae", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "da9cee8647743711c1f8b4f61185bfbece0ad284", "da9cee8647743711c1f8b4f61185bfbece0ad284", "5478a91c183c3a460bd4098acb8927bfc671367c"]},{"id": "9af121fbed84c3484ab86df8f17f1f198ed790a0", "title": "Atomic Decomposition by Basis Pursuit", "authors": ["Scott Saobing Chen", "David L. Donoho", "Michael A. Saunders"], "date": "1998", "abstract": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for… ", "references": ["b1d28fb3b000a10d2a60740a4ad84fbaecf4d7ea", "7756c24837b1f9ca3fc5be4ce7b4de0fcf9de8e6", "7756c24837b1f9ca3fc5be4ce7b4de0fcf9de8e6", "2cc257b0c7db92f90c3224c35df7b8e85f57a090", "b1d28fb3b000a10d2a60740a4ad84fbaecf4d7ea", "54205667c1f65a320f667d73c354ed8e86f1b9d9", "2cc257b0c7db92f90c3224c35df7b8e85f57a090", "7756c24837b1f9ca3fc5be4ce7b4de0fcf9de8e6", "7756c24837b1f9ca3fc5be4ce7b4de0fcf9de8e6", "2acd8d8f8d2df5bb3f5350e3b009436544a5f907"]},{"id": "c221cc946425d85f93c86e3be2c31d4feb00faa1", "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "authors": ["Ronald J. Williams"], "date": "1992", "abstract": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which… ", "references": ["5e9dc8d71572719cec58ec815bbd331fbd07fa15", "e08da64c0175139d7094a9bfbb3ec38648f8457f", "5e9dc8d71572719cec58ec815bbd331fbd07fa15", "84cdfa79e6eb9bf9e625e3af38d9f968df18a880", "e08da64c0175139d7094a9bfbb3ec38648f8457f", "8c6ebb3ad0ec3839bae24912945e31804edd8bc7", "e08da64c0175139d7094a9bfbb3ec38648f8457f", "5e9dc8d71572719cec58ec815bbd331fbd07fa15", "84cdfa79e6eb9bf9e625e3af38d9f968df18a880", "954c5ffcdd876e6646e78c89be4ae5e4113a8728"]},{"id": "5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c", "title": "Readings in Machine Learning", "authors": ["Jude W. Shavlik", "Thomas E. Deitterich", "Thomas G. Dietterich"], "date": "1991", "abstract": "From the Publisher: \nThe ability to learn is a fundamental characteristic of intelligent behavior. Consequently, machine learning has been a focus of artificial intelligence since the beginnings of AI in the 1950s. The 1980s saw tremendous growth in the field, and this growth promises to continue with valuable contributions to science, engineering, and business. \n \nReadings in Machine Learning collects the best of the published machine learning literature, including papers that address a wide… ", "references": []},{"id": "55d9ae6dd0ce6f27e2b50d66d355bf6986f03c70", "title": "Inductive transfer with context-sensitive neural networks", "authors": ["Daniel L. Silver", "Ryan Poirier", "Duane Currie"], "date": "2008", "abstract": "Context-sensitive Multiple Task Learning, or csMTL, is presented as a method of inductive transfer which uses a single output neural network and additional contextual inputs for learning multiple tasks. Motivated by problems with the application of MTL networks to machine lifelong learning systems, csMTL encoding of multiple task examples was developed and found to improve predictive performance. As evidence, the csMTL method is tested on seven task domains and shown to produce hypotheses for… ", "references": ["ec5540f6da71eb79a18bbcacb48b8ea847cad120", "f1b648eaaa2fabdb688a40aebd87ec9b8f74f40b", "cb48a7a46f9bc3ba74ecd728e91866d259a7982f", "d37250b498f66b8d43a336b54fca8b83328e4ede", "47aaeb6dc682162dfe5659c2cad64e5d825ad910", "cb48a7a46f9bc3ba74ecd728e91866d259a7982f", "d37250b498f66b8d43a336b54fca8b83328e4ede", "77ece7d29fdfe73d4d24be75083bcc562c463a4d", "ec5540f6da71eb79a18bbcacb48b8ea847cad120", "ec5540f6da71eb79a18bbcacb48b8ea847cad120"]},{"id": "993d16e86ac8e263718a4269ed487bb6b027dddb", "title": "Knowledge-based cascade-correlation: Using knowledge to speed learning", "authors": ["Thomas R. Shultz", "François Rivest"], "date": "2001", "abstract": "Research with neural networks typically ignores the role of knowledge in learning by initializing the network with random connection weights. We examine a new extension of a well-known generative algorithm, cascade-correlation. Ordinary cascade-correlation constructs its own network topology by recruiting new hidden units as needed to reduce network error. The extended algorithm, knowledge-based cascade-correlation (KBCC), recruits previously learned sub-networks as well as single hidden units… ", "references": ["496c1654c406e1b467f6c65eff685032d9a1c64c", "b66fcdaca4c9e789bd4fae5dfd08a325bbb9fa48", "496c1654c406e1b467f6c65eff685032d9a1c64c", "496c1654c406e1b467f6c65eff685032d9a1c64c", "ac4a1baeed9414c3a23b6e83ab260d7dfc0f3d89", "9423f0d006dc006f160cdb46f34acf53e8440fec", "496c1654c406e1b467f6c65eff685032d9a1c64c", "9464d15f4f8d578f93332db4aa1c9c182fd51735", "ac4a1baeed9414c3a23b6e83ab260d7dfc0f3d89", "ac4a1baeed9414c3a23b6e83ab260d7dfc0f3d89"]},{"id": "2805537bec87a6177037b18f9a3a9d3f1038867b", "title": "Sparse coding with an overcomplete basis set: A strategy employed by V1?", "authors": ["Bruno A. Olshausen", "David J. Field"], "date": "1997", "abstract": "The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more… ", "references": ["841cd4a6cac86fa0cfb0e8542eac5ed164f23f50", "e5ae37805bc9e0291c4c2c64cb2435f91849de74", "e3a83c2ed3af29a23ab342212d1ae9650a0c64a1", "e5ae37805bc9e0291c4c2c64cb2435f91849de74", "ff1152582155acaa0e9d0ccbc900a4641504256d", "a2cc34e721c82eb64538c4bfeb53d3534440d857", "a2cc34e721c82eb64538c4bfeb53d3534440d857", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "841cd4a6cac86fa0cfb0e8542eac5ed164f23f50", "ff1152582155acaa0e9d0ccbc900a4641504256d"]},{"id": "90094dba07438121fb55220f241ab74e881f5cee", "title": "Conditioned Regression Models for Non-blind Single Image Super-Resolution", "authors": ["Gernot Riegler", "Samuel Schulter", "Horst Bischof"], "date": "2015", "abstract": "Single image super-resolution is an important task in the field of computer vision and finds many practical applications. Current state-of-the-art methods typically rely on machine learning algorithms to infer a mapping from low-to high-resolution images. These methods use a single fixed blur kernel during training and, consequently, assume the exact same kernel underlying the image formation process for all test images. However, this setting is not realistic for practical applications, because… ", "references": ["05fbbd5ca168950c65453a5771846905f5170c6f", "177bded5ff0c9a621f29ac1e7d920e8a3ee7bab8", "6d0096f31fca8948b255c72a683fe951954ef334", "177bded5ff0c9a621f29ac1e7d920e8a3ee7bab8", "0504945cc2d03550fecb6ff02e637f9421107c25", "89f62dd328cb3e0e7b1a7e03e5b39084b4a91d18", "b4018623293585e06e5ec159d11a9178f9f99380", "6d0096f31fca8948b255c72a683fe951954ef334", "6d0096f31fca8948b255c72a683fe951954ef334", "0504945cc2d03550fecb6ff02e637f9421107c25"]},{"id": "73c8acd433c33e09916eaa1b0311c3ea7b2610d2", "title": "Deep Stereo: Learning to Predict New Views from the World's Imagery", "authors": ["John Flynn", "Ivan Neulander", "Noah Snavely"], "date": "2016", "abstract": "Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision [22, 33], but their use in graphics problems has been limited ([23, 7] are notable recent exceptions). In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches, which consist of multiple complex stages of processing, each of which… ", "references": ["52b34b5387e5f8eee8f209a147c8fed7cde222b4", "f908d2fb9cfcaff17030a912e4811fb02aaeec03", "4a965ee6dcfc1ab12170af762e1d08e11b60ddb8", "10d5e52943d4aa5861fc43be4b5c264026fba5da", "10d5e52943d4aa5861fc43be4b5c264026fba5da", "52b34b5387e5f8eee8f209a147c8fed7cde222b4", "687e80eb70c7bbad6001006d9269b202650a3354", "10d5e52943d4aa5861fc43be4b5c264026fba5da", "817f38e6e9844d4513047916fe88561a758846e7", "10d5e52943d4aa5861fc43be4b5c264026fba5da"]},{"id": "17fa1c2a24ba8f731c8b21f1244463bc4b465681", "title": "Deep multi-scale video prediction beyond mean square error", "authors": ["Michaël Mathieu", "Camille Couprie", "Yann LeCun"], "date": "2015", "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics.", "references": ["4d790c8fae40357d24813d085fa74a436847fb49", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "3935cabb73d75939ade5fc8839cfd946fbdc8057", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "4d790c8fae40357d24813d085fa74a436847fb49", "0fb3b63090f95af97723efe565893eb25ea9188c", "bd243d77076b3b8fe046bd3dc6e8a02aa9b38d62", "e4257bc131c36504a04382290cbc27ca8bb27813", "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c", "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c"]},{"id": "b6e7d0ec83f2a40fadc99bb0f1ced8508f5cfee5", "title": "Spatio-temporal video autoencoder with differentiable memory", "authors": ["Viorica Patraucean", "Ankur Handa", "Roberto Cipolla"], "date": "2015", "abstract": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end… ", "references": ["8b3b8848a311c501e704c45c6d50430ab7068956", "ec8ad9784823cc27b1c4221d732392a4d0780a2d", "56493a503eecb659bbef2fe4dd1fb915d251cac0", "d25908309e3c40fe0beb434e2abd21813708a5a8", "6f9f143ec602aac743e07d092165b708fa8f1473", "6f9f143ec602aac743e07d092165b708fa8f1473", "6f9f143ec602aac743e07d092165b708fa8f1473", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "8b3b8848a311c501e704c45c6d50430ab7068956", "56493a503eecb659bbef2fe4dd1fb915d251cac0"]},{"id": "53ab89807caead278d3deb7b6a4180b277d3cb77", "title": "Better Word Representations with Recursive Neural Networks for Morphology", "authors": ["Thang Luong", "Richard Socher", "Christopher D. Manning"], "date": "CoNLL", "abstract": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks.", "references": ["b4fcb3921b42d156a812484dcabf60c3f4410d42", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "27e38351e48fe4b7da2775bf94341738bc4da07e", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "27e38351e48fe4b7da2775bf94341738bc4da07e", "bc1022b031dc6c7019696492e8116598097a8c12", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "b4fcb3921b42d156a812484dcabf60c3f4410d42"]},{"id": "77f0a39b8e02686fd85b01971f8feb7f60971f80", "title": "Identity Mappings in Deep Residual Networks", "authors": ["Kaiming He", "Xiangyu Zhang"], "date": "2016", "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these… ", "references": ["cd85a549add0c7c7def36aca29837efd24b24080", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "cd85a549add0c7c7def36aca29837efd24b24080", "f63e917638553414526a0cc8550de4ad2d83fe7a", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "b92aa7024b87f50737b372e5df31ef091ab54e62", "f63e917638553414526a0cc8550de4ad2d83fe7a"]},{"id": "c2fb5b39428818d7ec8cc78e152e19c21b7db568", "title": "FlowNet: Learning Optical Flow with Convolutional Networks", "authors": ["Alexey Dosovitskiy", "Philipp Fischer", "Thomas Brox"], "date": "2015", "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different… ", "references": ["317aee7fc081f2b137a85c4f20129007fd8e717e", "428db42e86f6d51292e23fa57797e35cecd0e2ee", "fdccc24d6771a12272515c5ef0fa89434cbdd697", "b437b5a0445f17b06b12791bc48aeb8110e95dc5", "317aee7fc081f2b137a85c4f20129007fd8e717e", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "a9ce496186120df8f9ed3367e76a4947419e992e", "b437b5a0445f17b06b12791bc48aeb8110e95dc5", "56493a503eecb659bbef2fe4dd1fb915d251cac0", "1a2a770d23b4a171fa81de62a78a3deb0588f238"]},{"id": "750f26d613d3bda4ce043944aa3ef358b0c5de68", "title": "Co-learning of Word Representations and Morpheme Representations", "authors": ["Siyu Qiu", "Qing Cui", "Tie-Yan Liu"], "date": "COLING", "abstract": "The techniques of using neural networks to learn distributed word representations (i.e., word embeddings) have been used to solve a variety of natural language processing tasks. The recently proposed methods, such as CBOW and Skip-gram, have demonstrated their effectiveness in learning word embeddings based on context information such that the obtained word embeddings can capture both semantic and syntactic relationships between words. However, it is quite challenging to produce high-quality… ", "references": ["9606ff5bdeb6b9bde63c5bf6ad22edeca51d35db", "bc1022b031dc6c7019696492e8116598097a8c12", "57458bc1cffe5caa45a885af986d70f723f406b4", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "53ab89807caead278d3deb7b6a4180b277d3cb77", "3d5236e301aaccf2906aabc85a4d2be9dfc0b815", "dac72f2c509aee67524d3321f77e97e8eff51de6", "57458bc1cffe5caa45a885af986d70f723f406b4", "9606ff5bdeb6b9bde63c5bf6ad22edeca51d35db", "53ab89807caead278d3deb7b6a4180b277d3cb77"]},{"id": "9213538b4f6f067e70bedc0709901b39481254c1", "title": "Active Long Term Memory Networks", "authors": ["Tommaso Furlanello", "Jiaping Zhao", "Bosco S. Tjan"], "date": "2016", "abstract": "Continual Learning in artificial neural networks suffers from interference and forgetting when different tasks are learned sequentially. This paper introduces the Active Long Term Memory Networks (A-LTM), a model of sequential multi-task deep learning that is able to maintain previously learned association between sensory input and behavioral output while acquiring knew knowledge. A-LTM exploits the non-convex nature of deep neural networks and actively maintains knowledge of previously learned… ", "references": ["7c61efd58584451b8988c42f2b7006eddbb291f1", "0407b605b8f55db72e2545586bfe8e946b691b70", "c213af6582c0d518a6e8e14217611c733eeb1ef1", "c213af6582c0d518a6e8e14217611c733eeb1ef1", "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d", "2722b9e5ab8da95f03e578bb65879c452c105385", "052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c", "052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c", "052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c", "3c3861c607fb79f3fbf79552018724617fc8ba1b"]},{"id": "5151d6cb3a4eaec14a56944d58338251fca344ab", "title": "Overcoming catastrophic forgetting in neural networks", "authors": ["James N Kirkpatrick", "Razvan Pascanu", "Raia Hadsell"], "date": "2017", "abstract": "Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in… ", "references": ["53c9443e4e667170acc60ca1b31a0ec7151fe753", "8d49d34fff05285cb9a148261caff57775eb4453", "1def5d3711ebd1d86787b1ed57c91832c5ddc90b", "53c9443e4e667170acc60ca1b31a0ec7151fe753", "591b52d24eb95f5ec3622b814bc91ac872acda9e", "84dccdefebec40941af9d0cd4d415d7f2e5f1959", "53c9443e4e667170acc60ca1b31a0ec7151fe753", "1c4927af526d5c28f7c2cfa492ece192d80a61d4", "53c9443e4e667170acc60ca1b31a0ec7151fe753", "2722b9e5ab8da95f03e578bb65879c452c105385"]},{"id": "ed06a9685bc8e50866a7bbda49fc5033c4088276", "title": "Knowledge Transfer in Deep Block-Modular Neural Networks", "authors": ["Alexander V. Terekhov", "Guglielmo Montone", "J. Kevin O'Regan"], "date": "2015", "abstract": "Although deep neural networks DNNs have demonstrated impressive results during the last decade, they remain highly specialized tools, which are trained --- often from scratch --- to solve each particular task. The human brain, in contrast, significantly re-uses existing capacities when learning to solve new tasks. In the current study we explore a block-modular architecture for DNNs, which allows parts of the existing network to be re-used to solve a new task without a decrease in performance… ", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "ccf415df5a83b343dae261286d29a40e8b80e6c6", "0407b605b8f55db72e2545586bfe8e946b691b70", "c213af6582c0d518a6e8e14217611c733eeb1ef1", "193edd20cae92c6759c18ce93eeea96afd9528eb", "57458bc1cffe5caa45a885af986d70f723f406b4", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "14cfd41e28c28c21e06a1583ef85d6fac321a557", "14cfd41e28c28c21e06a1583ef85d6fac321a557"]},{"id": "ea160ca21b2a8c4402b077d6338e6a679aa9d7b9", "title": "Less-forgetting Learning in Deep Neural Networks", "authors": ["Heechul Jung", "Jeongwoo Ju", "Junmo Kim"], "date": "2016", "abstract": "A catastrophic forgetting problem makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light conditions. This paper presents a new method for alleviating the catastrophic forgetting problem. Unlike previous research, our method does not use any information from the source domain. Surprisingly, our method is very effective to forget less of the information in the source domain, and we… ", "references": ["4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "0407b605b8f55db72e2545586bfe8e946b691b70", "b1d380e3aa0c958e1c8d9d0b2b2eae8b6f44baed", "34f25a8704614163c4095b3ee2fc969b60de4698", "b1d380e3aa0c958e1c8d9d0b2b2eae8b6f44baed", "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972", "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972", "b1d380e3aa0c958e1c8d9d0b2b2eae8b6f44baed", "0407b605b8f55db72e2545586bfe8e946b691b70", "34f25a8704614163c4095b3ee2fc969b60de4698"]},{"id": "0407b605b8f55db72e2545586bfe8e946b691b70", "title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "authors": ["Ian J. Goodfellow", "Mehdi Mirza", "Yoshua Bengio"], "date": "2014", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms.", "references": ["1f88427d7aa8225e47f946ac41a0667d7b69ac52", "2ebf18e7892e660a833152ddc6cf8f1d21a7b881", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "67107f78a84bdb2411053cb54e94fa226eea6d8e", "0d6203718c15f137fda2f295c96269bc2b254644", "188e247506ad992b8bc62d6c74789e89891a984f", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "d895647b4a80861703851ef55930a2627fe19492", "0d6203718c15f137fda2f295c96269bc2b254644", "591b52d24eb95f5ec3622b814bc91ac872acda9e"]},{"id": "83f200fdef3f1b1778a3b46eabd44d5e2b305e2e", "title": "Simultaneous Deep Transfer Across Domains and Tasks", "authors": ["Judy Hoffman", "Eric Tzeng", "Kate Saenko"], "date": "2015", "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias.", "references": ["2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1", "b8de958fead0d8a9619b55c7299df3257c624a96", "2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1", "b8de958fead0d8a9619b55c7299df3257c624a96", "b8de958fead0d8a9619b55c7299df3257c624a96", "5d9a3036181676e187c9c0ff995d8bed1db3557d", "b8de958fead0d8a9619b55c7299df3257c624a96", "5d9a3036181676e187c9c0ff995d8bed1db3557d", "7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7", "47b9ccff528dc4b104f13734f2f2f9b69a492553"]},{"id": "f14325ec3041a73118bc4d819204cbbca07d5a71", "title": "Cross-Stitch Networks for Multi-task Learning", "authors": ["Ishan Misra", "Abhinav Shrivastava", "Martial Hebert"], "date": "2016", "abstract": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Specifically, we… ", "references": ["b4482761879009635e04d170f9f9c70a74f4ba39", "3205e0c0cb44a40c1ea2ed1096f5778c147dd457", "c3b8367a80181e28c95630b9b63060d895de08ff", "b8de958fead0d8a9619b55c7299df3257c624a96", "c3b8367a80181e28c95630b9b63060d895de08ff", "e219a61354d972a28954e655a7c53373508a08b6", "e219a61354d972a28954e655a7c53373508a08b6", "dbb3342599c9b431a3152a0d5c813d3e56967a27", "d2d35fc47bfcd9bbcdf1905b23be6e5dcee05e9c", "3205e0c0cb44a40c1ea2ed1096f5778c147dd457"]},{"id": "69e76e16740ed69f4dc55361a3d319ac2f1293dd", "title": "Asynchronous Methods for Deep Reinforcement Learning", "authors": ["Volodymyr Mnih", "Adrià Puigdomènech Badia", "Koray Kavukcuoglu"], "date": "ICML", "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers.", "references": ["024006d4c2a89f7acacc6e4438d156525b60a98f", "2c327af2071a572b1cfa640c7e39524d78a3f012", "024006d4c2a89f7acacc6e4438d156525b60a98f", "024006d4c2a89f7acacc6e4438d156525b60a98f", "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d", "2319a491378867c7049b3da055c5df60e1671158", "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d", "024006d4c2a89f7acacc6e4438d156525b60a98f", "4eb082956ea3f9b2d83936c41893e385d8cf8918", "024006d4c2a89f7acacc6e4438d156525b60a98f"]},{"id": "3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "title": "Deep Learning of Representations for Unsupervised and Transfer Learning", "authors": ["Yoshua Bengio"], "date": "ICML Unsupervised and…", "abstract": "Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the… ", "references": ["e60ff004dde5c13ec53087872cfcdd12e85beb57", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "8db95dbd08e4ee64fb258e5380e78cfa507ed94d", "195d0a8233a7a46329c742eaff56c276f847fadc", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "843959ffdccf31c6694d135fad07425924f785b1", "843959ffdccf31c6694d135fad07425924f785b1", "8978cf7574ceb35f4c3096be768c7547b28a35d0"]},{"id": "f82e4ff4f003581330338aaae71f60316e58dd26", "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)", "authors": ["Marc G. Bellemare", "Yavar Naddaf", "Michael Bowling"], "date": "2013", "abstract": "In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer… ", "references": ["bd8dc869f71558f862a27421af1ddb70edf5cbd4", "25fd7e9ed8d1a669c7a8d28a8b620479899e6b53", "6c8f0f28bcbc358726035cfd243239fd32eb8cba", "78295084d9b67da83552c86642828c05eb99352a", "6c8f0f28bcbc358726035cfd243239fd32eb8cba", "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d", "97efafdb4a3942ab3efba53ded7413199f79c054", "97efafdb4a3942ab3efba53ded7413199f79c054", "50e9a441f56124b7b969e6537b66469a0e1aa707", "6c8f0f28bcbc358726035cfd243239fd32eb8cba"]},{"id": "082b1f5c791cadef18c4920ecc1396615a3fe7cb", "title": "Continual learning in reinforcement environments", "authors": ["Mark B. Ring"], "date": "GMD-Bericht", "abstract": "Continual learning is the constant development of complex behaviors with no nal end in mind. It is the process of learning ever more complicated skills by building on those skills already developed. In order for learning at one stage of development to serve as the foundation for later learning, a continual-learning agent should learn hierarchically. CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development is proposed, described, tested, and evaluated in this… ", "references": ["54c4cf3a8168c1b70f91cf78a3dc98b671935492", "11463e2a6ed218e87e22cba2c2f24fb5992d0293", "087d5ea60b0b15fb9d3396ad321a7f941f88e720", "1762fdd825a4b9b60cb070f1ed08e078bd7555f1", "1762fdd825a4b9b60cb070f1ed08e078bd7555f1", "54c4cf3a8168c1b70f91cf78a3dc98b671935492", "b559ad3d359b940e8e841f0e24465333d4b1c8bf", "54c4cf3a8168c1b70f91cf78a3dc98b671935492", "1678bd32846b1aded5b1e80a617170812e80f562", "8a7acaf6469c06ae5876d92f013184db5897bb13"]},{"id": "1c4927af526d5c28f7c2cfa492ece192d80a61d4", "title": "Policy Distillation", "authors": ["Andrei A. Rusu", "Sergio Gomez Colmenarejo", "Raia Hadsell"], "date": "2015", "abstract": "Abstract: Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance.", "references": ["bdfbba8d6b4561f8400a1ef0a7dc76feacb93c0a", "b6cc21b30912bdaecd9f178d700a4c545b1d0838", "fa0f916247b8f1f7ea24ac014b114ad4912fa352", "cd85a549add0c7c7def36aca29837efd24b24080", "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d", "b6cc21b30912bdaecd9f178d700a4c545b1d0838", "cd85a549add0c7c7def36aca29837efd24b24080", "3b9732bb07dc99bde5e1f9f75251c6ea5039373e", "bdfbba8d6b4561f8400a1ef0a7dc76feacb93c0a", "fa0f916247b8f1f7ea24ac014b114ad4912fa352"]},{"id": "1def5d3711ebd1d86787b1ed57c91832c5ddc90b", "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning", "authors": ["Emilio Parisotto", "Jimmy Ba", "Ruslan Salakhutdinov"], "date": "2015", "abstract": "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train… ", "references": ["1d47c13031e3beefcaaed071fe748261db52e454", "79ab3c49903ec8cb339437ccf5cf998607fc313e", "024006d4c2a89f7acacc6e4438d156525b60a98f", "b6cc21b30912bdaecd9f178d700a4c545b1d0838", "3cc71552a5b942b099786d683577a3b09a6e735a", "b6b8a1b80891c96c28cc6340267b58186157e536", "1d47c13031e3beefcaaed071fe748261db52e454", "79ab3c49903ec8cb339437ccf5cf998607fc313e", "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d", "79ab3c49903ec8cb339437ccf5cf998607fc313e"]},{"id": "084a54c5b76e10648e1d15985641baa5433ff893", "title": "rwthlm - the RWTH aachen university neural network language modeling toolkit", "authors": ["Martin Sundermeyer", "Ralf Schlüter", "Hermann Ney"], "date": "INTERSPEECH", "abstract": "We present a novel toolkit that implements the long short-term memory (LSTM) neural network concept for language modeling. The main goal is to provide a software which is easy to use, and which allows fast training of standard recurrent and LSTM neural network language models. The toolkit obtains state-of-the-art performance on the standard Treebank corpus. To reduce the training time, BLAS and related libraries are supported, and it is possible to evaluate multiple word sequences in parallel… ", "references": ["d1275b2a2ab53013310e759e5c6878b96df643d4", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "a17745f1d7045636577bcd5d513620df5860e9e5", "167ad306d84cca2455bc50eb833454de9f2dcd02", "fcfb39e64678fe9cb681f11b9a3314becec82bb2", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "86d62362d50fd3d26f0c049fc72d4cf40bd218b6", "d36b19b4c5977dd2a2796a5ad3508a3d8a087809", "167ad306d84cca2455bc50eb833454de9f2dcd02", "167ad306d84cca2455bc50eb833454de9f2dcd02"]},{"id": "878ba5458e9e51f0b341fd9117fa0b43ef4096d3", "title": "End-to-end attention-based large vocabulary speech recognition", "authors": ["Dzmitry Bahdanau", "Jan Chorowski", "Yoshua Bengio"], "date": "2016", "abstract": "Many state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are hybrids of neural networks and Hidden Markov Models (HMMs). Recently, more direct end-to-end methods have been investigated, in which neural architectures were trained to model sequences of characters [1,2]. To our knowledge, all these approaches relied on Connectionist Temporal Classification [3] modules. We investigate an alternative method for sequence modelling based on an attention mechanism that… ", "references": ["24741d280869ad9c60321f5ab6e5f01b7852507d", "79d1e429c241d0aa47a2194246256a5bc79585bc", "dc555e8156c956f823587ebbff018863e6d2a95e", "dc555e8156c956f823587ebbff018863e6d2a95e", "47d2dc34e1d02a8109f5c04bb6939725de23716d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f", "b624504240fa52ab76167acfe3156150ca01cf3b", "97acdfb3d247f8250d865ef8a9169f06e40f138b", "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f"]},{"id": "e957747f4f8600940be4c5bb001aa70c84e53a53", "title": "Latent Predictor Networks for Code Generation", "authors": ["Wang Ling", "Phil Blunsom", "Andrew W. Senior"], "date": "2016", "abstract": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs.", "references": ["d39d39a6246c13928bbd66d122f3e61e67b584ef", "d39d39a6246c13928bbd66d122f3e61e67b584ef", "d39d39a6246c13928bbd66d122f3e61e67b584ef", "8dd9fd6a45afd266d48255c398429e01ea4fd6db", "d4ab3e01c4d1308371c76fbc9665701100461e88", "d39d39a6246c13928bbd66d122f3e61e67b584ef", "484ed558a5863060b373e8a2cb7cb302b8c36116", "8dd9fd6a45afd266d48255c398429e01ea4fd6db", "8dd9fd6a45afd266d48255c398429e01ea4fd6db", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa"]},{"id": "2a76c2121eee30af82a24058b4e149f05bcda911", "title": "Language modeling with sum-product networks", "authors": ["Wei-Chen Cheng", "Stanley Kok", "Kian Ming Adam Chai"], "date": "INTERSPEECH", "abstract": "Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPN’s inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its… ", "references": ["9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "d1275b2a2ab53013310e759e5c6878b96df643d4", "bd7d93193aad6c4b71cc8942e808753019e87706", "d1b21442aa4a6af708d64082d61f6e63e1d4cdf1", "aa57ec2eef24f10d7e11a8a39a69d4e0a6cf095e", "d1275b2a2ab53013310e759e5c6878b96df643d4", "d1b21442aa4a6af708d64082d61f6e63e1d4cdf1", "bd7d93193aad6c4b71cc8942e808753019e87706", "d1b21442aa4a6af708d64082d61f6e63e1d4cdf1", "aa57ec2eef24f10d7e11a8a39a69d4e0a6cf095e"]},{"id": "e8cd37fbd8bd5e690eef5861cf92af8e002d4533", "title": "Translating Video Content to Natural Language Descriptions", "authors": ["Marcus Rohrbach", "Wei Qiu", "Bernt Schiele"], "date": "2013", "abstract": "Humans use rich natural language to describe and communicate visual perceptions.", "references": ["169b847e69c35cfd475eb4dcc561a24de11762ca", "169b847e69c35cfd475eb4dcc561a24de11762ca", "fbadf89b990acedf23e1df03d4869010d2dbc59e", "d53a97a3dd7760b193c0d9a5293b60feff239059", "a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908", "21b3007f967d39e1346bc91e0fc8b3f16121300c", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "092f57121e10dcb65a6c348dd8b529bb06ebfb89", "d53a97a3dd7760b193c0d9a5293b60feff239059", "169b847e69c35cfd475eb4dcc561a24de11762ca"]},{"id": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "title": "Recurrent Neural Network Regularization", "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "date": "2014", "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation. ", "references": ["9819b600a828a57e1cde047bbe710d3446b30da5", "9819b600a828a57e1cde047bbe710d3446b30da5", "4ef03716945bd3907458efbe1bbf8928dafc1efc", "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "c0b624c46b51920dfec5aa02cc86323c0beb0df5", "f9a1b3850dfd837793743565a8af95973d395a4e", "0894b06cff1cd0903574acaa7fcf071b144ae775", "0894b06cff1cd0903574acaa7fcf071b144ae775", "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d"]},{"id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank", "authors": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "date": "1993", "abstract": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant. ", "references": ["a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "820214a49dd8cae98506f08ac953074e6f66c715", "c2fbd6cead3815b8e7038fda6f0f0254a2218ca7", "c2fbd6cead3815b8e7038fda6f0f0254a2218ca7", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "91b52959c7731830f1518ed35e5ab1dabadf79ec", "73cb6c929026bf5475602bad3fdde033da3c86c9", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "91b52959c7731830f1518ed35e5ab1dabadf79ec", "f89268038d043ab1bc3aff0034202ba46e7acd84"]},{"id": "3a7011346ce939e3251915e92ae2f252e4c7f777", "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs", "authors": ["Jonathan Krause", "Johanna E. Johnson", "Li Fei-Fei"], "date": "2017", "abstract": "Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating… ", "references": ["9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "bf55591e09b58ea9ce8d66110d6d3000ee804bdd", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "f678a0041f2c6f931168010e7418c500c3f14cdb", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "d7ce5665a72c0b607f484c1b448875f02ddfac3b", "d7ce5665a72c0b607f484c1b448875f02ddfac3b", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54"]},{"id": "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "authors": ["Andrej Karpathy", "Armand Joulin", "Fei Fei Li"], "date": "NIPS", "abstract": "We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these… ", "references": ["2a0d0f6c5a69b264710df0230696f47c5918e2f2", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11", "fad611e35b3731740b4d8b754241e77add5a70b9", "4aa4069693bee00d1b0759ca3df35e59284e9845", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11", "4aa4069693bee00d1b0759ca3df35e59284e9845", "6eb3a15108dfdec25b46522ed94b866aeb156de9", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "6eb3a15108dfdec25b46522ed94b866aeb156de9", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d"]},{"id": "d7ce5665a72c0b607f484c1b448875f02ddfac3b", "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning", "authors": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "date": "2016", "abstract": "We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language.", "references": ["a72b8bbd039989db39769da836cdb287737deb92", "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "f01fc808592ea7c473a69a6e7484040a435f36d9", "317aee7fc081f2b137a85c4f20129007fd8e717e", "317aee7fc081f2b137a85c4f20129007fd8e717e", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0"]},{"id": "f678a0041f2c6f931168010e7418c500c3f14cdb", "title": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks", "authors": ["Haonan Yu", "Jiang Wang", "Wei Xu"], "date": "2016", "abstract": "We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video.", "references": ["f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4", "b21c78a62fbb945a19ae9a8935933711647e7d70", "43795b7bac3d921c4e579964b54187bdbf6c6330", "e58a110fa1e4ddf247d5c614d117d64bfbe135c4", "f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4", "b21c78a62fbb945a19ae9a8935933711647e7d70", "b21c78a62fbb945a19ae9a8935933711647e7d70", "f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4", "e58a110fa1e4ddf247d5c614d117d64bfbe135c4", "f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4"]},{"id": "efbc200feab74e5087c4005d8759e5dadb3a3077", "title": "Controllable Text Generation", "authors": ["Zhiting Hu", "Zichao Yang", "Eric P. Xing"], "date": "2017", "abstract": "Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain.", "references": ["d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "35da0a2001eea88486a5de677ab97868c93d0824", "fc3fb80cae9c41790da6454b928c9f794361a7c7", "d82b55c35c8673774a708353838918346f6c006f", "fc3fb80cae9c41790da6454b928c9f794361a7c7", "66ad2fbc8b73242a889699868611fcf239e3435d", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "687bac2d3320083eb4530bf18bb8f8f721477600", "687bac2d3320083eb4530bf18bb8f8f721477600", "66ad2fbc8b73242a889699868611fcf239e3435d"]},{"id": "b21c78a62fbb945a19ae9a8935933711647e7d70", "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "date": "2015", "abstract": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models.", "references": ["1fa3c7c81864bbdfab8fbefab470864646844ddb", "1956c239b3552e030db1b78951f64781101125ed", "6fd25d5a0ec7a39eceb23ee63f85ea7958a21cdf", "cea967b59209c6be22829699f05b8b1ac4dc092d", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "1956c239b3552e030db1b78951f64781101125ed", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "9204d5b82652ee69859b6de56eb9a189a458c97c", "1fa3c7c81864bbdfab8fbefab470864646844ddb", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "a72b8bbd039989db39769da836cdb287737deb92", "title": "Mind's eye: A recurrent visual representation for image caption generation", "authors": ["Xinlei Chen", "C. Lawrence Zitnick"], "date": "2015", "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. Critical to our approach is a recurrent neural network that attempts to dynamically build a visual representation of the scene as a caption is being generated or read. The representation automatically learns to remember long-term visual concepts. Our model is capable of both generating novel captions given an image, and reconstructing visual features given an image description. We evaluate… ", "references": ["f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b"]},{"id": "5656fa5aa6e1beeb98703fc53ec112ad227c49ca", "title": "Multi-Prediction Deep Boltzmann Machines", "authors": ["Ian J. Goodfellow", "Mehdi Mirza", "Yoshua Bengio"], "date": "NIPS", "abstract": "We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MP-DBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM… ", "references": ["de34c1f18da491c92cc8400cfdab72d2012bf22a", "ecf48759e605f744e56a358861f8bfb2eadcd325", "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d", "ecf48759e605f744e56a358861f8bfb2eadcd325", "2a8a076c26875208d52c66e07aa7f6db9a4f34b7", "de34c1f18da491c92cc8400cfdab72d2012bf22a", "e25ee57a060e18b09fd8834c2d4f437c4e65ce48", "04e9385a1267d75197f695acf83e314668f6ae52", "e25ee57a060e18b09fd8834c2d4f437c4e65ce48", "e25ee57a060e18b09fd8834c2d4f437c4e65ce48"]},{"id": "5726c7b40fcc454b77d989656c085520bf6c15fa", "title": "Multimodal learning with deep Boltzmann machines", "authors": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "date": "2012", "abstract": "Data often consists of multiple diverse modalities.", "references": ["869171b2f56cfeaa9b81b2626cb4956fea590a57", "e0f49caabbf79ffda35432219bb0ec9b41753dff", "b32de117302258dd29919435cd001a8bcdfee3b3", "969c47fe20e41b3331c8aec3b2b964396d914b2c", "b32de117302258dd29919435cd001a8bcdfee3b3", "969c47fe20e41b3331c8aec3b2b964396d914b2c", "80e9e3fc3670482c1fee16b2542061b779f47c4f", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "e0f49caabbf79ffda35432219bb0ec9b41753dff", "b32de117302258dd29919435cd001a8bcdfee3b3"]},{"id": "5507dc32b368c8afd3b9507e9b5888da7bd7d7cd", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "authors": ["Sam Wiseman", "Alexander M. Rush"], "date": "EMNLP", "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks.", "references": ["f67ec8d10f04442c55ada6821031cf39e06aaa8e", "0d24a0695c9fc669e643bad51d4e14f056329dec", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f67ec8d10f04442c55ada6821031cf39e06aaa8e", "cea967b59209c6be22829699f05b8b1ac4dc092d", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "93499a7c7f699b6630a86fad964536f9423bb6d0", "cea967b59209c6be22829699f05b8b1ac4dc092d", "93499a7c7f699b6630a86fad964536f9423bb6d0", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "96494e722f58705fa20302fe6179d483f52705b4", "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "authors": ["Alex Graves", "Santiago Fernández", "Jürgen Schmidhuber"], "date": "ICML '06", "abstract": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data.", "references": ["8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5"]},{"id": "54e325aee6b2d476bbbb88615ac15e251c6e8214", "title": "Generative Adversarial Nets", "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Yoshua Bengio"], "date": "NIPS", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake.", "references": ["5f5dc5b9a2ba710937e2c413b37b053cd673df02", "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "23b80dc704e25cf52b5a14935002fc083ce9c317", "23b80dc704e25cf52b5a14935002fc083ce9c317", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "23b80dc704e25cf52b5a14935002fc083ce9c317", "23b80dc704e25cf52b5a14935002fc083ce9c317", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "5d90f06bb70a0a3dced62413346235c02b1aa086"]},{"id": "dbee79ac1865cd42780215d8fb2da4bc2ab7f381", "title": "Revisiting Recurrent Neural Networks for robust ASR", "authors": ["Oriol Vinyals", "Suman V. Ravuri", "Daniel Povey"], "date": "2012", "abstract": "In this paper, we show how new training principles and optimization techniques for neural networks can be used for different network structures. In particular, we revisit the Recurrent Neural Network (RNN), which explicitly models the Markovian dynamics of a set of observations through a non-linear function with a much larger hidden state space than traditional sequence models such as an HMM. We apply pretraining principles used for Deep Neural Networks (DNNs) and second-order optimization… ", "references": ["d0be39ee052d246ae99c082a565aba25b811be2d", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "6658bbf68995731b2083195054ff45b4eca38b3a", "9819b600a828a57e1cde047bbe710d3446b30da5", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "6658bbf68995731b2083195054ff45b4eca38b3a", "6658bbf68995731b2083195054ff45b4eca38b3a"]},{"id": "c1bca434074c447a31fa227059baccee66b48387", "title": "Recurrent Neural Networks for Noise Reduction in Robust ASR", "authors": ["Andrew L. Maas", "Quoc V. Le", "Andrew Y. Ng"], "date": "INTERSPEECH", "abstract": "Recent work on deep neural networks as acoustic models for automatic speech recognition (ASR) have demonstrated substantial performance improvements. We introduce a model which uses a deep recurrent auto encoder neural network to denoise input features for robust ASR. The model is trained on stereo (noisy and clean) audio features to predict clean features given noisy input. The model makes no assumptions about how noise affects the signal, nor the existence of distinct noise environments… ", "references": ["dbee79ac1865cd42780215d8fb2da4bc2ab7f381", "2446e8f2012f23176ff602be633c0ed2b956d66c", "2446e8f2012f23176ff602be633c0ed2b956d66c", "41a0be8ede3fee193b69802d5ec0a044de66530f", "b77bfe687300f727c58efd28bac944808d7264e1", "dbee79ac1865cd42780215d8fb2da4bc2ab7f381", "2446e8f2012f23176ff602be633c0ed2b956d66c", "2446e8f2012f23176ff602be633c0ed2b956d66c", "02841e798c9e75983970ed6a2f4756aab704197e", "41a0be8ede3fee193b69802d5ec0a044de66530f"]},{"id": "9a9f4bf3bfe133e1c70f6b60654c238b677c66d0", "title": "Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition", "authors": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Gerald Penn"], "date": "2012", "abstract": "Convolutional Neural Networks (CNN) have showed success in achieving translation invariance for many image processing tasks. The success is largely attributed to the use of local filtering and max-pooling in the CNN architecture. In this paper, we propose to apply CNN to speech recognition within the framework of hybrid NN-HMM model. We propose to use local filtering and max-pooling in frequency domain to normalize speaker variance to achieve higher multi-speaker speech recognition performance… ", "references": ["d7174b0cf599408fb723e6702504e27dc9d6c203", "f42b865e20e61a954239f421b42007236e671f19", "2109f8f91301abec8497286160cd6b0f2e65ed05", "f42b865e20e61a954239f421b42007236e671f19", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "f354310098e09c1e1dc88758fca36767fd9d084d", "2109f8f91301abec8497286160cd6b0f2e65ed05", "2109f8f91301abec8497286160cd6b0f2e65ed05", "90b63e917d5737b06357d50aa729619e933d9614", "f42b865e20e61a954239f421b42007236e671f19"]},{"id": "7a35e306999fc619e7ecff1ad9c9b693df0ef65c", "title": "Semi-formal Evaluation of Conversational Characters", "authors": ["Ron Artstein", "Sudeep Gandhe", "David R. Traum"], "date": "Languages: From Formal to…", "abstract": "Conversational dialogue systems cannot be evaluated in a fully formal manner, because dialogue is heavily dependent on context and current dialogue theory is not precise enough to specify a target output ahead of time. Instead, we evaluate dialogue systems in a semi-formal manner, using human judges to rate the coherence of a conversational character and correlating these judgments with measures extracted from within the system. We present a series of three evaluations of a single… ", "references": ["445a9c8f689cf0314cedfa4544bba69b9e9b6d9a", "d3aca13c966bb22eed7086baeb287a64bc18c152", "ee50a0c398691f7ea7fc517751da093dc42be7d7", "445a9c8f689cf0314cedfa4544bba69b9e9b6d9a", "edfa9baa3ca75550b5c40901324a61358742d25f", "4b7dcca3de306591b54b6cba36cd4a4982860630", "ee50a0c398691f7ea7fc517751da093dc42be7d7", "4b7dcca3de306591b54b6cba36cd4a4982860630", "359600381c37cda9906c56665007f571f65e776d", "d3aca13c966bb22eed7086baeb287a64bc18c152"]},{"id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization", "authors": ["Diederik P. Kingma", "Jimmy Ba"], "date": "2015", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.", "references": ["f7cc843c318d8862357485488971b26527ef1a8e", "8729441d734782c3ed532a7d2d9611b438c0a09a", "af0ee019dcc1fe7eab918e3c670a6c47e48d17f6", "e8f95ccfd13689f672c39dca3eccf1c484533bcc", "8729441d734782c3ed532a7d2d9611b438c0a09a", "05aba481e8a221df5d8775a3bb749001e7f2525e", "5a767a341364de1f75bea85e0b12ba7d3586a461", "05aba481e8a221df5d8775a3bb749001e7f2525e", "f7cc843c318d8862357485488971b26527ef1a8e", "f7cc843c318d8862357485488971b26527ef1a8e"]},{"id": "404a7bb70a535de4ac3edf79543f00293523d486", "title": "Toward Learning and Evaluation of Dialogue Policies with Text Examples", "authors": ["David DeVault", "Anton Leuski", "Kenji Sagae"], "date": "SIGDIAL Conference", "abstract": "We present a dialogue collection and enrichment framework that is designed to explore the learning and evaluation of dialogue policies for simple conversational characters using textual training data. To facilitate learning and evaluation, our framework enriches a collection of role-play dialogues with additional training data, including paraphrases of user utterances, and multiple independent judgments by external referees about the best policy response for the character at each point. As a… ", "references": ["1373258b51ff8730e818a623190bf63119179b6f", "d2c34818a95f59f75a4465cac40a7fc7aff5c5b3", "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "35ea2ec1795bbc0e6ef4f06b8da60e7e37ff8937", "1373258b51ff8730e818a623190bf63119179b6f", "33fc7a58f2a4c924a5f3868eced1726ce961e559", "093ee62fb84b51ffc9e9d984183069c4396884e5", "093ee62fb84b51ffc9e9d984183069c4396884e5", "2d1f61f162a133fae1e13fa1d7c90f19401fe22d", "2d1f61f162a133fae1e13fa1d7c90f19401fe22d"]},{"id": "990237c429e93250bab66e3ac91b4c9e4b5df633", "title": "A Semi-automated Evaluation Metric for Dialogue Model Coherence", "authors": ["Sudeep Gandhe", "David R. Traum"], "date": "2016", "abstract": "We propose a new metric, Voted Appropriateness, which can be used to automatically evaluate dialogue policy decisions, once some wizard data has been collected. We show that this metric outperforms a previously proposed metric Weak agreement. We also present a taxonomy for dialogue model evaluation schemas, and orient our new metric within this taxonomy. ", "references": ["5940c8bfd16728c5f9bb66e807f78f43dc7a38e4", "404a7bb70a535de4ac3edf79543f00293523d486", "cfdef0cd7ec53868c600005ec74a4a34f063a004", "89d4df859b231d44f0bec3ed2cc2ac9432cf1592", "cfdef0cd7ec53868c600005ec74a4a34f063a004", "cfdef0cd7ec53868c600005ec74a4a34f063a004", "5940c8bfd16728c5f9bb66e807f78f43dc7a38e4", "1af24887a9a1b9d2aae14c4552ccd02ed758f20c", "5940c8bfd16728c5f9bb66e807f78f43dc7a38e4", "35ea2ec1795bbc0e6ef4f06b8da60e7e37ff8937"]},{"id": "8388f1be26329fa45e5807e968a641ce170ea078", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "authors": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "date": "2015", "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are… ", "references": ["47900aca2f0b50da3010ad59b394c870f0e6c02e", "1e80f755bcbf10479afd2338cec05211fdbd325c", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "182015c5edff1956cbafbcb3e7bbe294aa54f9fc", "c08f5fa876181fc040d76c75fe2433eee3c9b001", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "4dcdae25a5e33682953f0853ee4cf7ca93be58a9", "8bb86dde5c7e11918b19e75e6211b77bf2649951", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd"]},{"id": "26046351a69ce6d21ac34fd9d065672a35441397", "title": "Evaluating coherence in open domain conversational systems", "authors": ["Ryuichiro Higashinaka", "Toyomi Meguro", "Yoshihiro Matsuo"], "date": "INTERSPEECH", "abstract": "We propose a method for evaluating coherence between user utterances and those generated from open domain conversational systems. Our aim is to make it possible for such systems to ascertain whether utterances generated from them are appropriate to the context before generation so that possible breakdown in conversation arising from inappropriate utterances can be avoided. In our method, we train a classifier that distinguishes a pair of a user utterance and that generated from a system as… ", "references": ["da4398dd0f54972aa5eba0032b96677e14d7cea2", "48afab4bbd2fa40db8148fbdcec9621c5c6dd89b", "5b6f048840ded1a27b9830f78f4d48b3ededb3ea", "35d6117e582825dd3467c6106047eb50704e03e1", "ec6c09bbf13a08e8fcc901e334e2aae20821727b", "fd3b68c2d912e84878db726ac9eecca729e17660", "da4398dd0f54972aa5eba0032b96677e14d7cea2", "ec2d3c9ac2ad75095f8b5f53803959c95989facc", "ec2d3c9ac2ad75095f8b5f53803959c95989facc", "48afab4bbd2fa40db8148fbdcec9621c5c6dd89b"]},{"id": "129cbad01be98ee88a930e31898cb76be79c41c1", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "authors": ["Chia-Wei Liu", "Ryan Lowe", "Joelle Pineau"], "date": "EMNLP", "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results… ", "references": ["6d19d73909ffaa6c94cae6a2535ed52d138cb63b", "efc0ca3664cce452c20464d65cdd8dcee15228af", "916441619914101258c71669b5ccc36424b54a6c", "da6918ed87095d1313bd20606a934f899d4084b0", "4d4b46e545e1a3f6871b49cc69640ef2eb1a4654", "efc0ca3664cce452c20464d65cdd8dcee15228af", "efc0ca3664cce452c20464d65cdd8dcee15228af", "6d19d73909ffaa6c94cae6a2535ed52d138cb63b", "1b52411221d53be8602f0ebe8aa118e2dc062c62", "efc0ca3664cce452c20464d65cdd8dcee15228af"]},{"id": "916441619914101258c71669b5ccc36424b54a6c", "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "authors": ["Ryan Lowe", "Nissan Pow", "Joelle Pineau"], "date": "SIGDIAL Conference", "abstract": "This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from… ", "references": ["33fc7a58f2a4c924a5f3868eced1726ce961e559", "efc0ca3664cce452c20464d65cdd8dcee15228af", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "a36cba3f779e6624d1130f026174b31b0e596bdd", "737c30659890cd79be89803317e4f8ea2b0ab4d6", "33fc7a58f2a4c924a5f3868eced1726ce961e559", "5b6f048840ded1a27b9830f78f4d48b3ededb3ea", "5b6f048840ded1a27b9830f78f4d48b3ededb3ea", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d"]},{"id": "96c6bc559b79d8fd518f431c707e8b44ce3bc4de", "title": "Analysis of Representations for Domain Adaptation", "authors": ["Shai Ben-David", "John Blitzer", "Fernando C Pereira"], "date": "NIPS", "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classifier which performs well on a target domain with a different distribution. Under what conditions can we adapt a classifier trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success… ", "references": ["0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff", "385197d4c02593e2823c71e4f90a0993b703620e", "f93c6881afb3e527a2cb6ea953fd91e95d20d03b", "91fb3e2b1ac9e588037e37e4d9be485e5fd60b27", "05fb162abd00afc4cb453cad8685191fd199928c", "adcec120a442af0d58f7a1d2dc175b58ff5a32e2", "a24508e65e599b5b20c33af96dbe7017d5caca37", "385197d4c02593e2823c71e4f90a0993b703620e", "a24508e65e599b5b20c33af96dbe7017d5caca37", "f93c6881afb3e527a2cb6ea953fd91e95d20d03b"]},{"id": "6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach", "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "date": "ICML", "abstract": "The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research.", "references": ["12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "a07dbf1b94d4aec62b85bbb6e857b3b4c295139b", "a07dbf1b94d4aec62b85bbb6e857b3b4c295139b", "bae533297207bb64597afa3b86a699ed6c4c98b1", "96c6bc559b79d8fd518f431c707e8b44ce3bc4de", "ca781a2fc31a28f653b222ca8afdedc90a009266", "18862760ac708a589afa5848ab55931996db1b28", "b25663fa149be5286de193c13324098aedd7e2cc", "b25663fa149be5286de193c13324098aedd7e2cc", "d895647b4a80861703851ef55930a2627fe19492"]},{"id": "8db26a22942404bd435909a16bb3a50cd67b4318", "title": "Marginalized Denoising Autoencoders for Domain Adaptation", "authors": ["Minmin Chen", "Zhixiang Eddie Xu", "Fei Sha"], "date": "2012", "abstract": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high… ", "references": ["9f62067945d991cd78a62cf647de17f01d1b54d3", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "aa1762a629b31d254450e37ce8baa235d729d82b", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "00b69fcb15b6ddedd6a1b23a0e4ed3afc0b8ac49", "c059e65211f9b880798cc5dbd80ec2db5cf3b809", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "00b69fcb15b6ddedd6a1b23a0e4ed3afc0b8ac49"]},{"id": "66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde", "title": "A theory of learning from different domains", "authors": ["Shai Ben-David", "John Blitzer", "Jennifer Wortman Vaughan"], "date": "2009", "abstract": "Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution.", "references": ["727e1e16ede6eaad241bad11c525da07b154c688", "5f126695342ea2bab2aa6eb971563f5d91d00aa0", "5f126695342ea2bab2aa6eb971563f5d91d00aa0", "5800621821d4765e8f9afcaf0bd2ed35896d7c98", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "96c6bc559b79d8fd518f431c707e8b44ce3bc4de", "727e1e16ede6eaad241bad11c525da07b154c688", "99990944b8516536f4ff2e5af6d9ea1c5993c0bd", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "727e1e16ede6eaad241bad11c525da07b154c688"]},{"id": "d4d8ef65132928e7bbdf0945cf98ef192dd1aed4", "title": "Biased Representation Learning for Domain Adaptation", "authors": ["Fei Huang", "Alexander Yates"], "date": "EMNLP-CoNLL", "abstract": "Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains. We present a novel, formal statement of the representation learning task. We argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. Leveraging the Posterior Regularization… ", "references": ["b672ef69f60aea81220d658963445c41e60bb0e3", "07e739d9d019c7c7974c264567b35ab7849e5cc4", "619fd1de5aa0dc649841c8f7d5cb65965a3b7fc6", "e4f5c9d0ab8ea3a91b0f9ffa698fa79c43463115", "5db592bef4b5ff231e1de92588907808f00bfbb4", "e0e00f8a5c31403f8871a823f000ab38ac0fe2c0", "5db592bef4b5ff231e1de92588907808f00bfbb4", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "96c6bc559b79d8fd518f431c707e8b44ce3bc4de", "619fd1de5aa0dc649841c8f7d5cb65965a3b7fc6"]},{"id": "a3cbb2a295dba31d9c9af77cc177fe538de94d5e", "title": "Unsupervised Domain Adaptation by Domain Invariant Projection", "authors": ["Mahsa Baktash", "Mehrtash Tafazzoli Harandi", "Mathieu Salzmann"], "date": "2013", "abstract": "Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this… ", "references": ["7de1d1612debcbde32cd588fa607a408df79c717", "2280504cdb0082c1a78412d4656709f16ed2a761", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "cea9cfa21060a1a34a337abae81c75e7370f084c", "d3edbfee56884d2b6d9aa51a6c525f9a05248802", "cea9cfa21060a1a34a337abae81c75e7370f084c", "b7511b06ea23225714d81e48eb2297a91f758189", "2280504cdb0082c1a78412d4656709f16ed2a761", "2280504cdb0082c1a78412d4656709f16ed2a761", "cea9cfa21060a1a34a337abae81c75e7370f084c"]},{"id": "24741d280869ad9c60321f5ab6e5f01b7852507d", "title": "Deep Speech: Scaling up end-to-end speech recognition", "authors": ["Awni Y. Hannun", "Carl Case", "Andrew Y. Ng"], "date": "2014", "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning.", "references": ["709e40a2614846a48d8e25f114eb2fa6ece853cc", "acf4e90062ca28e12f9e3a8c8b117030469d3e4b", "b48168acba4a6ca33ad0f11bbf1c7d8106333822", "709e40a2614846a48d8e25f114eb2fa6ece853cc", "79d1e429c241d0aa47a2194246256a5bc79585bc", "79d1e429c241d0aa47a2194246256a5bc79585bc", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f", "acf4e90062ca28e12f9e3a8c8b117030469d3e4b", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "8dc1d5c47b8af57cbff36632318b4302706df6a3", "title": "Combining time- and frequency-domain convolution in convolutional neural network-based phone recognition", "authors": ["László Tóth"], "date": "2014", "abstract": "Convolutional neural networks have proved very successful in image recognition, thanks to their tolerance to small translations. They have recently been applied to speech recognition as well, using a spectral representation as input. However, in this case the translations along the two axes - time and frequency - should be handled quite differently. So far, most authors have focused on convolution along the frequency axis, which offers invariance to speaker and speaking style variations. Other… ", "references": ["9a9f4bf3bfe133e1c70f6b60654c238b677c66d0", "9a9f4bf3bfe133e1c70f6b60654c238b677c66d0", "9a9f4bf3bfe133e1c70f6b60654c238b677c66d0", "64da1980714cfc130632c5b92b9d98c2f6763de6", "2ec87a97d202d5e432ca96490e409d931bbead7f", "64da1980714cfc130632c5b92b9d98c2f6763de6", "655ae6f82c24e3e01b2b27c56512b06ba36d49c1", "1a3c74c7b11ad5635570932577cdde2a3f7a6a5c", "57a5fa22f10ce6ccf27286f74a050d2dac037e06", "2ec87a97d202d5e432ca96490e409d931bbead7f"]},{"id": "f0fb306a3ea4e31f59b18ffeb497054ea934ba6a", "title": "Mode Regularized Generative Adversarial Networks", "authors": ["Tong Che", "Yanran Li", "Wenjie Li"], "date": "2016", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes.", "references": ["571b0750085ae3d939525e62af510ee2cee9d5ea", "571b0750085ae3d939525e62af510ee2cee9d5ea", "571b0750085ae3d939525e62af510ee2cee9d5ea", "1db6e3078597386ac4222ba6c3f4f61b61f53539", "488bb25e0b1777847f04c943e6dbc4f84415b712", "571b0750085ae3d939525e62af510ee2cee9d5ea", "571b0750085ae3d939525e62af510ee2cee9d5ea", "2ba23d9b46027e47b4483243871760e315213ffe", "571b0750085ae3d939525e62af510ee2cee9d5ea", "47900aca2f0b50da3010ad59b394c870f0e6c02e"]},{"id": "9a700c7a7e7468e436f00c34551fbe3e0f70e42f", "title": "Towards Principled Methods for Training Generative Adversarial Networks", "authors": ["Martín Arjovsky", "Léon Bottou"], "date": "2017", "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks.", "references": ["39e0c341351f8f4a39ac890b96217c7f4bde5369", "76cee11c6a9f1424f03571378a966c1417ff2935", "db38edba294b7d2fd8ca3aad65721bd9dce32619", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "bee044c8e8903fb67523c1f8c105ab4718600cdb", "39e0c341351f8f4a39ac890b96217c7f4bde5369", "db38edba294b7d2fd8ca3aad65721bd9dce32619", "571b0750085ae3d939525e62af510ee2cee9d5ea", "8388f1be26329fa45e5807e968a641ce170ea078", "76cee11c6a9f1424f03571378a966c1417ff2935"]},{"id": "1b40fe1a9d25d5694c7ea40a57d0aaa2e2cd5dd1", "title": "Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space", "authors": ["Anh Nguyen", "Jeff Clune", "Jason Yosinski"], "date": "2017", "abstract": "Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [37] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading… ", "references": ["0936352b78a52bc5d2b5e3f04233efc56664af51", "0936352b78a52bc5d2b5e3f04233efc56664af51", "c8b509be29721ee6b12c880b4d97ed6b60bad217", "0936352b78a52bc5d2b5e3f04233efc56664af51", "c8b509be29721ee6b12c880b4d97ed6b60bad217", "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87", "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87", "39e0c341351f8f4a39ac890b96217c7f4bde5369", "0936352b78a52bc5d2b5e3f04233efc56664af51", "571b0750085ae3d939525e62af510ee2cee9d5ea"]},{"id": "0c3b69b5247ef18fd5bab1109d87a04184ea8f4b", "title": "A Recurrent Latent Variable Model for Sequential Data", "authors": ["Junyoung Chung", "Kyle Kastner", "Yoshua Bengio"], "date": "NIPS", "abstract": "In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder.", "references": ["bf38dfb13352449b965c08282b66d3ffc5a0539f", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "e3417538b569b21971f3fbbdf8ea5ae5ebedfb48", "cbec12feff814f4d4d11e892ecdbd93fd393cb64", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "bf38dfb13352449b965c08282b66d3ffc5a0539f", "0b544dfe355a5070b60986319a3f51fb45d1348e"]},{"id": "571b0750085ae3d939525e62af510ee2cee9d5ea", "title": "Improved Techniques for Training GANs", "authors": ["Tim Salimans", "Ian J. Goodfellow", "Xi Chen"], "date": "2016", "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework.", "references": ["b321e8eca4dbf424a9e30dd938fb423786c90b76", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "b321e8eca4dbf424a9e30dd938fb423786c90b76", "4e2f6b4bc889eed1afe5833d5190f6f02e501061", "4e2f6b4bc889eed1afe5833d5190f6f02e501061", "8388f1be26329fa45e5807e968a641ce170ea078", "543f21d81bbea89f901dfcc01f4e332a9af6682d", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "8388f1be26329fa45e5807e968a641ce170ea078"]},{"id": "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses", "authors": ["Alessandro Sordoni", "Michel Galley", "William B. Dolan"], "date": "HLT-NAACL", "abstract": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and… ", "references": ["167ad306d84cca2455bc50eb833454de9f2dcd02", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "d1275b2a2ab53013310e759e5c6878b96df643d4", "5b6f048840ded1a27b9830f78f4d48b3ededb3ea", "57458bc1cffe5caa45a885af986d70f723f406b4", "5b6f048840ded1a27b9830f78f4d48b3ededb3ea", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "167ad306d84cca2455bc50eb833454de9f2dcd02", "d1275b2a2ab53013310e759e5c6878b96df643d4", "34992ceb89e251f2ed5c1a792fbd594bcf8246c2"]},{"id": "651e5bcc14f14605a879303e97572a27ea8c7956", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "authors": ["Jiwei Li", "Michel Galley", "William B. Dolan"], "date": "2016", "abstract": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \"I don't know\") regardless of the input.", "references": ["7df22e88a86d7e7e914a9cf3ad5b8fbd62b35cb8", "17f5c7411eeeeedf25b0db99a9130aa353aee4ba", "7d0684b19ba46e739e28baa1e180c008226f793a", "7d0684b19ba46e739e28baa1e180c008226f793a", "bc1f09a48e173a9e0f516d7384acec49e0e9ba60", "3385397d3be400c3f4a6f79f9c47e67e50333b45", "7df22e88a86d7e7e914a9cf3ad5b8fbd62b35cb8", "c25e3fc7bb2c890c692b500dcd4e774e66081aab", "7d0684b19ba46e739e28baa1e180c008226f793a", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d"]},{"id": "d82b55c35c8673774a708353838918346f6c006f", "title": "Generating Sentences from a Continuous Space", "authors": ["Samuel R. Bowman", "Luke Vilnis", "Samy Bengio"], "date": "CoNLL", "abstract": "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation.", "references": ["0c3b69b5247ef18fd5bab1109d87a04184ea8f4b", "73e8633886dc380a46fc02f2e1ec5bf68dba0734", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "cea967b59209c6be22829699f05b8b1ac4dc092d", "452059171226626718eb677358836328f884298e", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "d41cfe9b2ada4e09d53262bc75c473d8043936fc", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "authors": ["Martín Abadi", "Ashish Agarwal", "Xiaoqiang Zheng"], "date": "2016", "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of… ", "references": ["4d376d6978dad0374edfa6709c9556b42d3594d3", "a41b826d23957d6ad4e9e794d20a583a9b567c5d", "a41b826d23957d6ad4e9e794d20a583a9b567c5d", "4d376d6978dad0374edfa6709c9556b42d3594d3", "4d376d6978dad0374edfa6709c9556b42d3594d3", "e69c8b5df8a4178b1c8c7f154a761147a6f030be", "4d376d6978dad0374edfa6709c9556b42d3594d3", "cea967b59209c6be22829699f05b8b1ac4dc092d", "31c36d445367ba204244bb74893c5654e31c3869", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "3385397d3be400c3f4a6f79f9c47e67e50333b45", "title": "A trainable generator for recommendations in multimodal dialog", "authors": ["Marilyn A. Walker", "Rashmi Prasad", "Amanda Stent"], "date": "INTERSPEECH", "abstract": "As the complexity of spoken dialogue systems has increased, there has been increasing interest in spoken language generation (SLG). SLG promises portability across application domains and dialogue situations through the development of applicationindependent linguistic modules. However in practice, rulebased SLGs often have to be tuned to the application. Recently, a number of research groups have been developing hybrid methods for spoken language generation, combining general linguistic modules… ", "references": ["d951a42bf4d2e769bcf14edc1f2d4c246da34e70", "87dea5e5cbc06e1437a9db67f55d8f1e93905330", "87dea5e5cbc06e1437a9db67f55d8f1e93905330", "0e90fdf91325a9cb51cd64c18ef19baad7c0881b", "0e90fdf91325a9cb51cd64c18ef19baad7c0881b", "81124d7b0b0d69f6bd08e159e7fd75ce55dd103c", "87dea5e5cbc06e1437a9db67f55d8f1e93905330", "d951a42bf4d2e769bcf14edc1f2d4c246da34e70", "81124d7b0b0d69f6bd08e159e7fd75ce55dd103c", "6701317b5cf29fdaea35a9cc53707479757d4e57"]},{"id": "74ec753c27a01e93380c148ba886f8e0317c61ee", "title": "The Fisher Corpus: a Resource for the Next Generations of Speech-to-Text", "authors": ["Christopher Cieri", "David Miller", "Kevin Walker"], "date": "LREC", "abstract": "This paper describes, within the context of the DARPA EARS program, the design and implementation of the Fisher protocol for collecting conversational telephone speech which has yielded more than 16,000 English conversations. It also discusses the Quick Transcription specification that allowed 2000 hours of Fisher audio to be transcribed in less than one year. Fisher data is already in use within the DARPA EARS programs and will be published via the Linguistic Data Consortium for general use… ", "references": []},{"id": "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "title": "Neural Responding Machine for Short-Text Conversation", "authors": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "date": "2015", "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows… ", "references": ["98445f4172659ec5e891e031d8202c102135c644", "9819b600a828a57e1cde047bbe710d3446b30da5", "d8a358fb026fda39546cf8e3cbf9e5d754d63463", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "9819b600a828a57e1cde047bbe710d3446b30da5", "98445f4172659ec5e891e031d8202c102135c644", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "d8a358fb026fda39546cf8e3cbf9e5d754d63463"]},{"id": "469a7d19c074c0d2df699340cbd5b105bdd0f7e6", "title": "Data Generation as Sequential Decision Making", "authors": ["Philip Bachman", "Doina Precup"], "date": "NIPS", "abstract": "We connect a broad class of generative models through their shared reliance on sequential decision making. Motivated by this view, we develop extensions to an existing model, and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct the… ", "references": ["59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e", "66ad2fbc8b73242a889699868611fcf239e3435d", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "d0c61536927c2f5dc2ddb74664268a3623580b9c", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "484ad17c926292fbe0d5211540832a8c8a8e958b", "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e"]},{"id": "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0", "title": "Generalized Denoising Auto-Encoders as Generative Models", "authors": ["Yoshua Bengio", "Li Yao", "Pascal Vincent"], "date": "NIPS", "abstract": "Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders… ", "references": ["74706fab48249b071e10615f8da60b8401fb9f3f", "41fef1a197fab9684a4608b725d3ae72e1ab4b39", "2d851f681f82c71a934aebd16e8112adf1239f85", "f8c8619ea7d68e604e40b814b40c72888a755e95", "2d851f681f82c71a934aebd16e8112adf1239f85", "31a2053ebda7f6f77afe8c3fc53269b73567e446", "872bae24c109f7c30e052ac218b17a8b028d08a0", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "195d0a8233a7a46329c742eaff56c276f847fadc", "195d0a8233a7a46329c742eaff56c276f847fadc"]},{"id": "208c94412e618afb08e760318caaa4526eec8d6d", "title": "Continuously Learning Neural Dialogue Management", "authors": ["Pei-hao Su", "Milica Gašić", "Steve J. Young"], "date": "2016", "abstract": "We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradient-based algorithms on one single model. The experiments demonstrate the supervised model's effectiveness in the corpus-based evaluation, with user simulation, and with paid human… ", "references": ["5a6326fca97641e449085cea50338e40bbdce0ae", "737c30659890cd79be89803317e4f8ea2b0ab4d6", "0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa", "7a1e07a3c889a1e87270cc1bde855218c35db4ae", "013ff354023700cec2f765e0fe1baf645957a654", "0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa", "1b472a261d183d31c40e1ad4bfb4f63abc2c25d6", "578828260315dbb36afb5c0d5d40d70acd7dd47f", "1b472a261d183d31c40e1ad4bfb4f63abc2c25d6", "013ff354023700cec2f765e0fe1baf645957a654"]},{"id": "4b7dcca3de306591b54b6cba36cd4a4982860630", "title": "An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email", "authors": ["Marilyn A. Walker"], "date": "2000", "abstract": "This paper describes a novel method by which a spoken dialogue system can learn to choose an optimal dialogue strategy from its experience interacting with human users. The method is based on a combination of reinforcement learning and performance modeling of spoken dialogue systems. The reinforcement learning component applies Q-learning (Watkins, 1989), while the performance modeling component applies the PARADISE evaluation framework (Walker et al., 1997) to learn the performance function… ", "references": ["cf4729d32b4478a2e343b22f67b9f47a7e38081a", "8332944fb0cd6571095c371d3a65621c6ae77cd4", "a38faf8bfc7ff634a63d27999d16bab21858710f", "911dcf64fddfda2b178bcba52007e7e3e2d7daef", "cf4729d32b4478a2e343b22f67b9f47a7e38081a", "839f1b61803e65f20f067b361d0ebf6db337172c", "911dcf64fddfda2b178bcba52007e7e3e2d7daef", "8332944fb0cd6571095c371d3a65621c6ae77cd4", "22404884bd6e2dc80691b968a1e571f873b0a774", "8332944fb0cd6571095c371d3a65621c6ae77cd4"]},{"id": "09eb7642b733dfcf1d9ac14a9435d3bc96d18f0c", "title": "POMDP-based dialogue manager adaptation to extended domains", "authors": ["Milica Gasic", "Catherine Breslin", "Steve J. Young"], "date": "SIGDIAL Conference", "abstract": "Existing spoken dialogue systems are typically designed to operate in a static and well-defined domain, and are not well suited to tasks in which the concepts and values change dynamically. To handle dynamically changing domains, techniques will be needed to transfer and reuse existing dialogue policies and rapidly adapt them using a small number of dialogues in the new domain. As a first step in this direction, this paper addresses the problem of automatically extending a dialogue system to… ", "references": ["1b472a261d183d31c40e1ad4bfb4f63abc2c25d6", "bf229e7f726a9f792abfa891f313cda328ba389a", "bf229e7f726a9f792abfa891f313cda328ba389a", "90bd2d7fe732ec04d1c8c73673cb96e10faec289", "0eccd175a114733c1bd42ee3f2d37f73af897c8a", "1b472a261d183d31c40e1ad4bfb4f63abc2c25d6", "1b472a261d183d31c40e1ad4bfb4f63abc2c25d6", "1b472a261d183d31c40e1ad4bfb4f63abc2c25d6", "0eccd175a114733c1bd42ee3f2d37f73af897c8a", "b0c61d60d10210d3a77f5966d18ed255bce9c6f5"]},{"id": "6719ef93142d64a69b52c916f9ee132b5339d9d1", "title": "Incremental on-line adaptation of POMDP-based dialogue managers to extended domains", "authors": ["Milica Gasic", "Dongho Kim", "Steve J. Young"], "date": "INTERSPEECH", "abstract": "Copyright © 2014 ISCA. An important property of open domain spoken dialogue systems is their ability to deal with a set of new, previously unseen, concepts introduced in the conversation. The dialogue manager must then quickly learn how to talk about the new concepts using its knowledge of the existing concepts. It has previously been shown that a single new concept could be accommodated by mapping the kernel function of a Gaussian process to incorporate an additional concept into the domain of… ", "references": ["b4300fbc44973301810017cf917bd6e5b1eadd3f", "b4300fbc44973301810017cf917bd6e5b1eadd3f", "cfdef0cd7ec53868c600005ec74a4a34f063a004", "09eb7642b733dfcf1d9ac14a9435d3bc96d18f0c", "b0c61d60d10210d3a77f5966d18ed255bce9c6f5", "b99d66f53eed00320320acce63a22f0c48465d8b", "59291fe64997728191c7d14385031066c162497b", "b0c61d60d10210d3a77f5966d18ed255bce9c6f5", "b4300fbc44973301810017cf917bd6e5b1eadd3f", "cfdef0cd7ec53868c600005ec74a4a34f063a004"]},{"id": "1e0ede6f60b1106070e041211133c634a2e4f991", "title": "On-line policy optimisation of Bayesian spoken dialogue systems via human interaction", "authors": ["Milica Gasic", "Catherine Breslin", "Steve J. Young"], "date": "2013", "abstract": "A partially observable Markov decision process has been proposed as a dialogue model that enables robustness to speech recognition errors and automatic policy optimisation using reinforcement learning (RL). However, conventional RL algorithms require a very large number of dialogues, necessitating a user simulator. Recently, Gaussian processes have been shown to substantially speed up the optimisation, making it possible to learn directly from interaction with human users. However, early… ", "references": ["04213b2da00939e9fb72efcd8b304c18b20b6f4a", "b4300fbc44973301810017cf917bd6e5b1eadd3f", "03a2accb5a2d5492baf40327f86fb724ced72cf9", "bf229e7f726a9f792abfa891f313cda328ba389a", "b4300fbc44973301810017cf917bd6e5b1eadd3f", "68eeadbe0004a1ae6f0d5864811f0e63c2535382", "e2753726fd32c1e4644ffb70e9af2784d4ba6c4f", "03a2accb5a2d5492baf40327f86fb724ced72cf9", "bf229e7f726a9f792abfa891f313cda328ba389a", "bf229e7f726a9f792abfa891f313cda328ba389a"]},{"id": "839f1b61803e65f20f067b361d0ebf6db337172c", "title": "Reinforcement Learning for Spoken Dialogue Systems", "authors": ["Satinder P. Singh", "Michael Kearns", "Marilyn A. Walker"], "date": "NIPS", "abstract": "Recently, a number of authors have proposed treating dialogue systems as Markov decision processes (MDPs). However, the practical application of MDP algorithms to dialogue systems faces a number of severe technical challenges. We have built a general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework, and have applied it to dialogue corpora gathered from two dialogue systems built at AT&T Labs. Our experiments demonstrate that RLDS holds promise as… ", "references": ["cf4729d32b4478a2e343b22f67b9f47a7e38081a", "97efafdb4a3942ab3efba53ded7413199f79c054", "47e544d692678bbe6bb1624a9dc177e886d4c7af", "90bd2d7fe732ec04d1c8c73673cb96e10faec289", "97efafdb4a3942ab3efba53ded7413199f79c054", "b42e47f494583db7731585485cb0f5ab5081c151", "47e544d692678bbe6bb1624a9dc177e886d4c7af"]},{"id": "41f1d50c85d3180476c4c7b3eea121278b0d8474", "title": "Pixel Recurrent Neural Networks", "authors": ["Aäron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "date": "ICML", "abstract": "Modeling the distribution of natural images is a landmark problem in unsupervised learning.", "references": ["f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "5d90f06bb70a0a3dced62413346235c02b1aa086", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "5d90f06bb70a0a3dced62413346235c02b1aa086", "0523e14247d74c4505cd5e32e1f0495f291ec432", "705fd4febe2fff810d2f72f48dcda20826eca77a", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864"]},{"id": "3d2c6941a9b4608ba52b328369a3352db2092ae0", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks", "authors": ["Tim Salimans", "Diederik P. Kingma"], "date": "2016", "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction.", "references": ["97dc8df45972e4ed7423fc992a5092ba25b33411", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "97dc8df45972e4ed7423fc992a5092ba25b33411", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "aa7bfd2304201afbb19971ebde87b17e40242e91", "97dc8df45972e4ed7423fc992a5092ba25b33411", "2c03df8b48bf3fa39054345bafabfeff15bfd11d"]},{"id": "eb53b7c13156e3acacb47c1e51d93cefeabfaeb0", "title": "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System", "authors": ["Matthew Kearns", "D. Litman", "Mike Walker"], "date": "2002", "abstract": "Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do… ", "references": []},{"id": "0936352b78a52bc5d2b5e3f04233efc56664af51", "title": "Conditional Image Generation with PixelCNN Decoders", "authors": ["Aäron van den Oord", "Nal Kalchbrenner", "Alex Graves"], "date": "NIPS", "abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture.", "references": ["0875fc92cce33df5cf7df169590dbf0ca00d2652", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "0523e14247d74c4505cd5e32e1f0495f291ec432", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "f4278dac2abafb66a0a824f1d59e72b67e7540a1", "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "0523e14247d74c4505cd5e32e1f0495f291ec432", "47900aca2f0b50da3010ad59b394c870f0e6c02e"]},{"id": "f958d4921951e394057a1c4ec33bad9a34e5dad1", "title": "A Convolutional Encoder Model for Neural Machine Translation", "authors": ["Jonas Gehring", "Michael Auli", "Yann Dauphin"], "date": "2017", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform… ", "references": ["eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "9486f640f90b7c3ddb0d8adff6fa16dd9758746a", "1a5ea605111eb3403868d4b679315e944beee8c6", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "9486f640f90b7c3ddb0d8adff6fa16dd9758746a", "6b1e805be7bc8c4a43dae6f310f7b355bc98605d", "5423e418053a585bbc35180f5719c25b36e160b7", "98445f4172659ec5e891e031d8202c102135c644", "b60abe57bc195616063be10638c6437358c81d1e", "9486f640f90b7c3ddb0d8adff6fa16dd9758746a"]},{"id": "7a67159fc7bc76d0b37930b55005a69b51241635", "title": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks", "authors": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush"], "date": "HLT-NAACL", "abstract": "Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning.", "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "0b544dfe355a5070b60986319a3f51fb45d1348e", "221ef0a2f185036c06f9fb089109ded5c888c4c6", "0b544dfe355a5070b60986319a3f51fb45d1348e", "a994ed8bac53bf7cf70d428d71cf1e2940fb4351", "5082a1a13daea5c7026706738f8528391a1e6d59", "b064b714107ffc724e0d477f4083e9e507c22fec", "0b544dfe355a5070b60986319a3f51fb45d1348e", "f56405a2e323af6dcbbc0a94c08ecc9f94740242", "5082a1a13daea5c7026706738f8528391a1e6d59"]},{"id": "489955574c435169abd72285cfe2f055f538a401", "title": "Efficient Summarization with Read-Again and Copy Mechanism", "authors": ["Wenyuan Zeng", "Wenjie Luo", "Raquel Urtasun"], "date": "2016", "abstract": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address… ", "references": ["5f176a929d9eaa569b430cb784280802cf8fca79", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "5f176a929d9eaa569b430cb784280802cf8fca79", "29a294eaec7b485245aa21d994f7300f6b5da8fc", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5f176a929d9eaa569b430cb784280802cf8fca79", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "a9614b05461bb306cc47c8cd645b9b67bb1227ba"]},{"id": "5ab72d44237533534de8402e30f3ccce25ce30de", "title": "Distraction-based neural networks for modeling documents", "authors": ["Qian Chen", "Xiao-Dan Zhu", "Hui Jiang"], "date": "IJCAI", "abstract": "Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization… ", "references": ["5209c6fb3efee27cddcf0203628c41776e42fe4c", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "b21c78a62fbb945a19ae9a8935933711647e7d70", "8d7f6dc8b0b9101580cc96f1f303d1eba3d590af", "bfcc02bf5435c05d4e1e959697c62e90702a1e93", "b21c78a62fbb945a19ae9a8935933711647e7d70", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de"]},{"id": "2f160ce71f01ac2043de67536ff0e413ff6f58c5", "title": "Temporal Attention Model for Neural Machine Translation", "authors": ["Baskaran Sankaran", "Haitao Mi", "Abe Ittycheriah"], "date": "2016", "abstract": "Attention-based Neural Machine Translation (NMT) models suffer from attention deficiency issues as has been observed in recent research. We propose a novel mechanism to address some of these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. We compare our approach against the baseline NMT model and… ", "references": ["ada937c9f51316c6ac87f9d1d4509383d23e0c21", "93499a7c7f699b6630a86fad964536f9423bb6d0", "93499a7c7f699b6630a86fad964536f9423bb6d0", "d5631abafe3003381d735d7385b07ab15c04182a", "93499a7c7f699b6630a86fad964536f9423bb6d0", "ada937c9f51316c6ac87f9d1d4509383d23e0c21", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "22358c1e6f371db45a0d237baff6052e0a50e498", "1af68821518f03568f913ab03fc02080247a27ff", "d5631abafe3003381d735d7385b07ab15c04182a"]},{"id": "f77a604410d88307ec5c6331c8b6133272fbaa10", "title": "Self-Critical Sequence Training for Image Captioning", "authors": ["Pratik P. Rane", "A. M. Sargar", "Faiza Shaikh"], "date": "2018", "abstract": "Image captioning aims at generating a natural language description of an image. Open domain captioning is a very challenging task, as it requires a fine-grained understanding of the global and the local entities in an image, as well as their attributes and relationships. The recently released MSCOCO challenge [1] provides a new, larger scale platform for evaluating image captioning systems, complete with an evaluation server for benchmarking competing methods. Deep learning approaches to… ", "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e", "62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99"]},{"id": "e06a68b26bde368883761c9dceb547914b2ecca8", "title": "Textual Entailment with Structured Attentions and Composition", "authors": ["Kai Zhao", "Liang Huang", "Mingbo Ma"], "date": "2016", "abstract": "Deep learning techniques are increasingly popular in the textual entailment task, overcoming the fragility of traditional discrete models with hard alignments and logics. In particular, the recently proposed attention models (Rockt\\\"aschel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art accuracy by computing soft word alignments between the premise and hypothesis sentences. However, there remains a major limitation: this line of work completely ignores syntax and recursion, which… ", "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "e0a1c613ed7297dd7b6fc7b031341e66285beacc", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "10f5c0397f44aed8a6244110b7707a7a82c2529b"]},{"id": "04d1a26c2516dc14a765112a63ec60dc3cb3de72", "title": "Tree-Structured Composition in Neural Networks without Tree-Structured Architectures", "authors": ["Samuel R. Bowman", "Christopher D. Manning", "Christopher Potts"], "date": "2015", "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional… ", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "7c05a4ffee7e159e34b2efea7e44d994333ec628", "c58dd287a476b4722c5b6b1316629e2874682219", "7c05a4ffee7e159e34b2efea7e44d994333ec628", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c58dd287a476b4722c5b6b1316629e2874682219", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "40be3888daa5c2e5af4d36ae22f690bcc8caf600", "8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092"]},{"id": "bc82b4f9f202062857958f0336fc28327a75563b", "title": "Structured Prediction Energy Networks", "authors": ["David Belanger", "Andrew McCallum"], "date": "ICML", "abstract": "We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features… ", "references": ["3c75f54c2ff675aeb207a93e4d1e1555758c8150", "3c75f54c2ff675aeb207a93e4d1e1555758c8150", "7fc604e1a3e45cd2d2742f96d62741930a363efa", "7fc604e1a3e45cd2d2742f96d62741930a363efa", "1e20f9de45d26950ecd11965989d2b15a5d0d86b", "317aee7fc081f2b137a85c4f20129007fd8e717e", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "317aee7fc081f2b137a85c4f20129007fd8e717e", "83361c7fef302dc023555ac5ab432013ae2adc54"]},{"id": "acec46ffd3f6046af97529127d98f1d623816ea4", "title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation", "authors": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "date": "2016", "abstract": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation.", "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "4d070993cb75407b285e14cb8aac0077624ef4d9", "4d070993cb75407b285e14cb8aac0077624ef4d9", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "1956c239b3552e030db1b78951f64781101125ed", "105146a7872835a52c8c5c55a3aae62c5d8852a1", "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"]},{"id": "4d070993cb75407b285e14cb8aac0077624ef4d9", "title": "Character-based Neural Machine Translation", "authors": ["Marta R. Costa-jussà", "José A. R. Fonollosa"], "date": "2016", "abstract": "Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of… ", "references": ["acec46ffd3f6046af97529127d98f1d623816ea4", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "733b821faeebe49b6efcf5369e3b9902b476529e", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "ff1577528a34a11c2a81d2451d346c412c674c02", "acec46ffd3f6046af97529127d98f1d623816ea4", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "1af68821518f03568f913ab03fc02080247a27ff"]},{"id": "1af68821518f03568f913ab03fc02080247a27ff", "title": "Neural Machine Translation of Rare Words with Subword Units", "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "date": "2016", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via… ", "references": ["93499a7c7f699b6630a86fad964536f9423bb6d0", "93a9694b6a4149e815c30a360347593b75860761", "1956c239b3552e030db1b78951f64781101125ed", "93a9694b6a4149e815c30a360347593b75860761", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "93499a7c7f699b6630a86fad964536f9423bb6d0", "4d070993cb75407b285e14cb8aac0077624ef4d9", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "ff1577528a34a11c2a81d2451d346c412c674c02", "dca029eafe302034f0e7784b9266403938c55263"]},{"id": "9cee45ef1212ebbc7d468f9b1d7df24f5005e64d", "title": "Highway long short-term memory RNNS for distant speech recognition", "authors": ["Yu Zhang", "Guoguo Chen", "James R. Glass"], "date": "2016", "abstract": "In this paper, we extend the deep long short-term memory (DL-STM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers.", "references": ["bf28afd4231b08e6458822306cc0f847f592de14", "6658bbf68995731b2083195054ff45b4eca38b3a", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "bf28afd4231b08e6458822306cc0f847f592de14", "067e07b725ab012c80aa2f87857f6791c1407f6d", "bf28afd4231b08e6458822306cc0f847f592de14", "bf28afd4231b08e6458822306cc0f847f592de14", "6658bbf68995731b2083195054ff45b4eca38b3a", "223e9b53fc271d451801fb63869945a0d8c2ed61", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"]},{"id": "733b821faeebe49b6efcf5369e3b9902b476529e", "title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models", "authors": ["Minh-Thang Luong", "Christopher D. Manning"], "date": "2016", "abstract": "Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words.", "references": ["93499a7c7f699b6630a86fad964536f9423bb6d0", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "53ab89807caead278d3deb7b6a4180b277d3cb77", "cea967b59209c6be22829699f05b8b1ac4dc092d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "93499a7c7f699b6630a86fad964536f9423bb6d0", "1af68821518f03568f913ab03fc02080247a27ff", "53ab89807caead278d3deb7b6a4180b277d3cb77", "93499a7c7f699b6630a86fad964536f9423bb6d0", "93499a7c7f699b6630a86fad964536f9423bb6d0"]},{"id": "654a3e53fb41d8168798ee0ee61dfab73739b1ed", "title": "Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks", "authors": ["Kyunghyun Cho", "Aaron C. Courville", "Yoshua Bengio"], "date": "2015", "abstract": "Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. In this paper we focus on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of… ", "references": ["43795b7bac3d921c4e579964b54187bdbf6c6330", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "8dc1d5c47b8af57cbff36632318b4302706df6a3", "43795b7bac3d921c4e579964b54187bdbf6c6330", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "0f84a81f431b18a78bd97f59ed4b9d8eda390970", "8dc1d5c47b8af57cbff36632318b4302706df6a3", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "8a756d4d25511d92a45d0f4545fa819de993851d", "5f425b7abf2ed3172ed060df85bb1885860a297e"]},{"id": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "authors": ["Yarin Gal", "Zoubin Ghahramani"], "date": "NIPS", "abstract": "Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning.", "references": ["f9a1b3850dfd837793743565a8af95973d395a4e", "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6", "5a9ef216bf11f222438fff130c778267d39a9564", "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6", "5a9ef216bf11f222438fff130c778267d39a9564", "5a9ef216bf11f222438fff130c778267d39a9564", "34f25a8704614163c4095b3ee2fc969b60de4698", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "34f25a8704614163c4095b3ee2fc969b60de4698", "31b7afa0a6d0968382804e2e7a1470f11b6fb1ac"]},{"id": "401d68e1a930b0f7e02030cab4c185fb1839cb11", "title": "Capacity and Trainability in Recurrent Neural Networks", "authors": ["Jasmine Collins", "Jascha Sohl-Dickstein", "David Sussillo"], "date": "2017", "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters… ", "references": ["510e26733aaff585d65701b9f1be7ca9d5afc586", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "1a35a88f24ce2364d137ee80c46964ae497f9524", "1a35a88f24ce2364d137ee80c46964ae497f9524", "82abca98f2b208d681bb681c518a79f4accbc9a4", "5dd52311ccce1a57473b55b0d36cbd40bf0f59f0", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "510e26733aaff585d65701b9f1be7ca9d5afc586", "82abca98f2b208d681bb681c518a79f4accbc9a4", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081"]},{"id": "58001259d2f6442b07cc0d716ff99899abbb2bc7", "title": "Gated Word-Character Recurrent Language Model", "authors": ["Yasumasa Onoe", "Kyunghyun Cho"], "date": "2016", "abstract": "We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs.", "references": ["891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "5999fd9b9712fee3184989d043bff899935b4208", "3da0d432d73d19c71fb72e8ff0cb431d1b3c080a", "3da0d432d73d19c71fb72e8ff0cb431d1b3c080a", "9819b600a828a57e1cde047bbe710d3446b30da5", "733b821faeebe49b6efcf5369e3b9902b476529e"]},{"id": "2012f32199adc88747d5a1b47c7b4ba1cb3cb995", "title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method", "authors": ["Yoav Goldberg", "Omer Levy"], "date": "2014", "abstract": "The word2vec software of Tomas Mikolov and colleagues (this https URL ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind… ", "references": ["330da625c15427c6e42ccfa3b747fb29e5835bf0", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "330da625c15427c6e42ccfa3b747fb29e5835bf0"]},{"id": "e9c771197a6564762754e48c1daafb066f449f2e", "title": "Unitary Evolution Recurrent Neural Networks", "authors": ["Martín Arjovsky", "Amar Shah", "Yoshua Bengio"], "date": "2016", "abstract": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of… ", "references": ["0d6203718c15f137fda2f295c96269bc2b254644", "84069287da0a6b488b8c933f3cb5be759cb6237e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "99c970348b8f70ce23d6641e201904ea49266b6e", "bc1022b031dc6c7019696492e8116598097a8c12", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "99c970348b8f70ce23d6641e201904ea49266b6e", "0d6203718c15f137fda2f295c96269bc2b254644", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"]},{"id": "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "title": "A fast and simple algorithm for training neural probabilistic language models", "authors": ["Andriy Mnih", "Yee Whye Teh"], "date": "ICML", "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. \n \nWe propose a fast and simple algorithm for training NPLMs based… ", "references": ["a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "bd7d93193aad6c4b71cc8942e808753019e87706", "9819b600a828a57e1cde047bbe710d3446b30da5", "8b395470a57c48d174c4216ea21a7a58bc046917", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "8b395470a57c48d174c4216ea21a7a58bc046917", "fac2ca048fdd7e848f0b9ba2f7be25bb49186770", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "699d5ab38deee78b1fd17cc8ad233c74196d16e9"]},{"id": "bf85a0cd645ad68919c0706741ab568a60a58af2", "title": "Neural Programmer: Inducing Latent Programs with Gradient Descent", "authors": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "date": "2016", "abstract": "Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to… ", "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "6f5a0fbb1473e95d0a14ef2081985d16bc063bfb", "d7b14e09afa995814a41b6e312b3fc9c16277567", "6f5a0fbb1473e95d0a14ef2081985d16bc063bfb", "b59d91e0699d4e1896a15bae13fd180bdaf77ea5", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "bff3ea999978e8c9503b62510bba11c2a5f24e51"]},{"id": "2e74e29298f0f71694ac21958996d147191fe4b0", "title": "Lattice-based Minimum Error Rate Training for Statistical Machine Translation", "authors": ["Wolfgang Macherey", "Franz Josef Och", "Jakob Uszkoreit"], "date": "EMNLP", "abstract": "Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking… ", "references": ["eaba49c37be13c5bfadb270e3308cda307f27d69", "8d125e6de1e77e9168ad0ed355551cf72062e1c2", "8d125e6de1e77e9168ad0ed355551cf72062e1c2", "f91b17e852774d80f4d11b9c7f5b99b1dd8aacf7", "7e0a9e64b4c838ff1ffb1075e3c2deceda0fa5a0", "e102637440b2144d9fea86eec99553354a9c44cc", "e2a68774f92d1e894cbbbef2c819e4592990eb4b", "f91b17e852774d80f4d11b9c7f5b99b1dd8aacf7", "8d125e6de1e77e9168ad0ed355551cf72062e1c2", "ad3d2f463916784d0c14a19936c1544309a0a440"]},{"id": "2887e349875dfc30a6e7666424d83abc40c7fd53", "title": "Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training", "authors": ["Xinyan Xiao", "Yang Liu", "Shouxun Lin"], "date": "EMNLP", "abstract": "Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With… ", "references": ["10d21ca7728cb3dd15731accedda9ea711d8a0f4", "bef0b80cbe5ba692aecba09a4557bbe1e02b0e8a", "8d125e6de1e77e9168ad0ed355551cf72062e1c2", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "02bcc68113cff36226eb9d977f7367f14e2157e5", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "8d125e6de1e77e9168ad0ed355551cf72062e1c2", "bef0b80cbe5ba692aecba09a4557bbe1e02b0e8a"]},{"id": "9c9548ac1705a48cc565c238c6102b7aa69101dc", "title": "A Localized Prediction Model for Statistical Machine Translation", "authors": ["Christoph Tillmann", "Tong Zhang"], "date": "ACL", "abstract": "In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT). The model predicts blocks with orientation to handle local phrase re-ordering. We use a maximum likelihood criterion to train a log-linear block bigram model which uses real-valued features (e.g. a language model score) as well as binary features based on the block identities themselves, e.g. block bigram features. Our training algorithm can easily handle… ", "references": ["8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "1f12451245667a85d0ee225a80880fc93c71cc8b", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "7181f7a664fbbf34c7c147c8a90f0343cdd1674c", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "d7da009f457917aa381619facfa5ffae9329a6e9"]},{"id": "cf76789618f5db929393c1187514ce6c3502c3cd", "title": "Recurrent Dropout without Memory Loss", "authors": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "date": "2016", "abstract": "This paper presents a novel approach to recurrent neural network (RNN) regularization.", "references": ["d46b81707786d18499f911b4ab72bb10c65406ba", "a946fa85ade4306d8912c75ed0186a83404ada0a", "a946fa85ade4306d8912c75ed0186a83404ada0a", "84069287da0a6b488b8c933f3cb5be759cb6237e", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "a946fa85ade4306d8912c75ed0186a83404ada0a", "4a5e68033940785468cbe34bd6249106c311acfb", "9665247ea3421929f9b6ad721f139f11edb1dbb8"]},{"id": "ab3cbc6eb94932c25b6fcd4851eacd5ef34c25ab", "title": "Discriminative Reranking for Machine Translation", "authors": ["Libin Shen", "Anoop Sarkar", "Franz Josef Och"], "date": "HLT-NAACL", "abstract": "This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental… ", "references": ["844db702be4bc149b06b822b47247e15f5894cc3", "fe638b5610475d4524684fb2c2b7b08c119c8700", "c447c0cb2673037633f71faf8ccf4f89806ba1b0", "fe638b5610475d4524684fb2c2b7b08c119c8700", "6c6f643afce42347ffd21d93c5d002dfe35cae0e", "c447c0cb2673037633f71faf8ccf4f89806ba1b0", "fe638b5610475d4524684fb2c2b7b08c119c8700", "fe638b5610475d4524684fb2c2b7b08c119c8700", "6c6f643afce42347ffd21d93c5d002dfe35cae0e", "1f12451245667a85d0ee225a80880fc93c71cc8b"]},{"id": "c3fdc954fa36b123da63a3d35a8eecfdaf1b298b", "title": "Hidden-Variable Models for Discriminative Reranking", "authors": ["Terry K Koo", "Michael Collins"], "date": "HLT/EMNLP", "abstract": "We describe a new method for the representation of NLP structures within reranking approaches. We make use of a conditional log-linear model, with hidden variables representing the assignment of lexical items to word clusters or word senses. The model learns to automatically make these assignments based on a discriminative training criterion. Training and decoding with the model requires summing over an exponential number of hidden-variable assignments: the required summations can be computed… ", "references": ["3fc44ff7f37ec5585310666c183c65e0a0bb2446", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "39fb0b6873c58ec182357721eaed75801505a9df", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "1c2b3f4bc53f6c0aef2a2296d737f532b985322c", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "39fb0b6873c58ec182357721eaed75801505a9df"]},{"id": "567dc4e26ece98e96c2e798ae8acafa5883945a9", "title": "Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm", "authors": ["Brian Roark", "Murat Saraçlar", "Mark Johnson"], "date": "ACL", "abstract": "This paper describes discriminative language modeling for a large vocabulary speech recognition task. We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs). The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer. The perceptron algorithm has the benefit of automatically selecting a relatively… ", "references": ["5a7958b418bceb48a315384568091ab1898b1640", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "a80a452e587bd7f06ece1be101d6775fcee0f7af", "4df160605e810fb4b7189e9967cf184ff5fd1aa4", "878783964ab23c97052ea82685368099d85c500d", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "a80a452e587bd7f06ece1be101d6775fcee0f7af", "aa321cbc2482116f32eaa54a2f393229afa48398", "2614fe6b2ae962ec863739173512b0093245851b"]},{"id": "d01379ebb53c66a4ccf5f4959d904dcf9e161e41", "title": "Order Matters: Sequence to sequence for sets", "authors": ["Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur"], "date": "2016", "abstract": "Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear… ", "references": ["44d2abe2175df8153f465f6c39b68b76a0d40ab9", "878ba5458e9e51f0b341fd9117fa0b43ef4096d3", "71ae756c75ac89e2d731c9c79649562b5768ff39", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "9653d5c2c7844347343d073bbedd96e05d52f69b", "a97b5db17acc731ef67321832dbbaf5766153135", "9653d5c2c7844347343d073bbedd96e05d52f69b", "71ae756c75ac89e2d731c9c79649562b5768ff39", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a"]},{"id": "8829e3873846c6bbad5aca111e64f9d2c1b24299", "title": "Deep Sequential Neural Network", "authors": ["Ludovic Denoyer", "Patrick Gallinari"], "date": "2014", "abstract": "Neural Networks sequentially build high-level features through their successive layers.", "references": ["cd7f63611d59ae32fb0a029f978d4b0c1168adf3", "c0b1589b029db1d0cbf37f24648f13c2c9281a3a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "327d3df8ea2020882827d6bace1e26c9d24309c2", "8a756d4d25511d92a45d0f4545fa819de993851d", "9b6204e5ef66f82bee891b1336eb5ae1f64e8c99", "327d3df8ea2020882827d6bace1e26c9d24309c2", "9b6204e5ef66f82bee891b1336eb5ae1f64e8c99", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "9b6204e5ef66f82bee891b1336eb5ae1f64e8c99"]},{"id": "b59d91e0699d4e1896a15bae13fd180bdaf77ea5", "title": "Neural Programmer-Interpreters", "authors": ["Scott E. Reed", "Nando de Freitas"], "date": "2016", "abstract": "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity… ", "references": ["47225c992d7086cf5d113942212edb4a57401130", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f8b1722c252cf6ef384e6e1614ed631e9255e6db", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "a2499dd426c46c645ee805d7594b6687547c72d4", "47225c992d7086cf5d113942212edb4a57401130", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f10e071292d593fef939e6ef4a59baf0bb3a6c2b", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "d38e8631bba0720becdaf7b89f79d9f9dca45d82"]},{"id": "754d6ad9c36406bfb7f48e0a7b3cac430edc0648", "title": "Dynamic Cortex Memory: Enhancing Recurrent Neural Networks for Gradient-Based Sequence Learning", "authors": ["Sebastian Otte", "Marcus Liwicki", "Andreas Zell"], "date": "ICANN", "abstract": "In this paper a novel recurrent neural network (RNN) model for gradient-based sequence learning is introduced. The presented dynamic cortex memory (DCM) is an extension of the well-known long short term memory (LSTM) model. The main innovation of the DCM is the enhancement of the inner interplay of the gates and the error carousel due to several new and trainable connections. These connections enable a direct signal transfer from the gates to one another. With this novel enhancement the… ", "references": ["8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c", "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c", "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c", "b158a006bebb619e2ea7bf0a22c27d45c5d19004", "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c", "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c", "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c", "b158a006bebb619e2ea7bf0a22c27d45c5d19004", "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c", "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c"]},{"id": "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "title": "Conditional Computation in Neural Networks for faster models", "authors": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Doina Precup"], "date": "2015", "abstract": "Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the… ", "references": ["b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "70155488a49d51755c1dfea728e03a6dd72703a1", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "8829e3873846c6bbad5aca111e64f9d2c1b24299", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "8a756d4d25511d92a45d0f4545fa819de993851d", "d0be39ee052d246ae99c082a565aba25b811be2d", "cf3229e74f912ef365d67d1954441b32ce2573ee", "5d90f06bb70a0a3dced62413346235c02b1aa086"]},{"id": "92ba9c288cbd0089cf6e9d988c9672f095a67109", "title": "Using Hidden Markov Modeling to Decompose Human-Written Summaries", "authors": ["Hongyan Jing"], "date": "2002", "abstract": "Professional summarizers often reuse original documents to generate summaries. The task of summary sentence decomposition is to deduce whether a summary sentence is constructed by reusing the original text and to identify reused phrases. Specifically, the decomposition program needs to answer three questions for a given summary sentence: (1) Is this summary sentence constructed by reusing the text in the original document? (2) If so, what phrases in the sentence come from the original document… ", "references": []},{"id": "067e07b725ab012c80aa2f87857f6791c1407f6d", "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "authors": ["Hasim Sak", "Andrew W. Senior", "Françoise Beaufays"], "date": "INTERSPEECH", "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs.", "references": ["1149888d75af4ed5dffc25731b875651c3ccdeb2", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "f9a1b3850dfd837793743565a8af95973d395a4e", "f9a1b3850dfd837793743565a8af95973d395a4e", "2f83f6e1afadf0963153974968af6b8342775d82", "2f83f6e1afadf0963153974968af6b8342775d82", "2f83f6e1afadf0963153974968af6b8342775d82", "6658bbf68995731b2083195054ff45b4eca38b3a"]},{"id": "11540131eae85b2e11d53df7f1360eeb6476e7f4", "title": "Learning to Forget: Continual Prediction with LSTM", "authors": ["Felix A. Gers", "Jürgen Schmidhuber", "Fred A. Cummins"], "date": "2000", "abstract": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel… ", "references": ["44d2abe2175df8153f465f6c39b68b76a0d40ab9", "8c571314311f507731296b21b56ab2c326b97392", "8c571314311f507731296b21b56ab2c326b97392", "d0be39ee052d246ae99c082a565aba25b811be2d", "dbe8c61628896081998d1cd7d10343a45b7061bd", "dbe8c61628896081998d1cd7d10343a45b7061bd", "7fb4d10f6d2ee3133135958aefd50bf22dcced9d", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"]},{"id": "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "title": "Learning to Transduce with Unbounded Memory", "authors": ["Edward Grefenstette", "Karl Moritz Hermann", "Phil Blunsom"], "date": "NIPS", "abstract": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as… ", "references": ["30110856f45fde473f1903f686aa365cf70ed4c7", "c3823aacea60bc1f2cabb9283144690a3d015db5", "1811f708b8b7456a3708fabd2fd638da36bd7ba0", "f10e071292d593fef939e6ef4a59baf0bb3a6c2b", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "30110856f45fde473f1903f686aa365cf70ed4c7", "a583af2696030bcf5f556edc74573fbee902be0b", "30110856f45fde473f1903f686aa365cf70ed4c7", "30110856f45fde473f1903f686aa365cf70ed4c7", "2a1483397106807e74ab422dd8330d56a3cc6db5"]},{"id": "9d08213ede54c4e205d18b4400288831af918ec8", "title": "Headline Generation Based on Statistical Translation", "authors": ["Michele Banko", "Vibhu O. Mittal", "Michael J. Witbrock"], "date": "ACL", "abstract": "Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required. An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding. A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation. The issue then becomes one of generating a target… ", "references": ["1e3cd66275ba8b8a0cb6e3531a6bbf54ed93adb7", "7f284ed30a2024ee64f6dda1746dc8617b451986", "15281680463698dca403697bd627af4efebc98a2", "ab7b5917515c460b90451e67852171a531671ab8", "15281680463698dca403697bd627af4efebc98a2", "ab7b5917515c460b90451e67852171a531671ab8", "673992da19d9209434615b12d55bdd36be706e9e", "ab7b5917515c460b90451e67852171a531671ab8", "673992da19d9209434615b12d55bdd36be706e9e", "25f51f4132626a645924b3c8b3edcbdcc35c48a3"]},{"id": "1ec86811a79fb02a1c551b8f418314a00f5f5a99", "title": "Global inference for sentence compression : an integer linear programming approach", "authors": ["J. Clarke", "Mirella Lapata"], "date": "2008", "abstract": "Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts… ", "references": ["5aa70188f70d349580aed96c10a68f57dace2d33", "27891ad3fbd8420c1a5a7459bf72b2e299487aab", "5aa70188f70d349580aed96c10a68f57dace2d33", "5aa70188f70d349580aed96c10a68f57dace2d33", "5aa70188f70d349580aed96c10a68f57dace2d33", "5aa70188f70d349580aed96c10a68f57dace2d33", "9f0e98c17674031a055551f65951c1caad822d1b", "17708302b7e611608da42feda0f35fb8b92a9ee4", "d9da9be266b1739dea7691302dc7eb4d33677802", "a89ac9902ccb889d096ec74f980fe8fc00940390"]},{"id": "0be949cc24188ef7205bdaaeb7df2508344b8d5a", "title": "DUC in context", "authors": ["Paul Over", "Hoa Dang", "Donna K. Harman"], "date": "2007", "abstract": "Recent years have seen increased interest in text summarization with emphasis on evaluation of prototype systems. Many factors can affect the design of such evaluations, requiring choices among competing alternatives. This paper examines several major themes running through three evaluations: SUMMAC, NTCIR, and DUC, with a concentration on DUC. The themes are extrinsic and intrinsic evaluation, evaluation procedures and methods, generic versus focused summaries, single- and multi-document… ", "references": ["f88a8e3c8097a98346d6dc6ffb0fc0567a3d0936", "dc954000617ae982f83b7f1ed4bd72b95e38aa88", "6ca6b510517c1876f4fb27ba425a4cd507b91efd", "f88a8e3c8097a98346d6dc6ffb0fc0567a3d0936", "dc954000617ae982f83b7f1ed4bd72b95e38aa88", "4cfc137546ef9a9bee9924bf97fe3c16da5b8912", "3b856c6567fe6f7770306f8cc20cd480c44f513c", "8974a2851439f916a4f3ce09535a65c80982cb8e", "f88a8e3c8097a98346d6dc6ffb0fc0567a3d0936", "70cb232a6e391bfa49b0441a9956820c52ec32f2"]},{"id": "9c818fe59a76b242dcca62579bd353fe9cf01c0d", "title": "Sentence Compression Beyond Word Deletion", "authors": ["Trevor Cohn", "Mirella Lapata"], "date": "COLING", "abstract": "In this paper we generalise the sentence compression task.", "references": ["2a5a179a70e98ee1c4b4b8b54f6c8aba629c02e4", "0db6eb46ca9941660acc775e3ca39bf4434c18be", "2a5a179a70e98ee1c4b4b8b54f6c8aba629c02e4", "b4d4304597b6a6c56e71846f68d5eb64385837d3", "25956623c2ad64895fa31294a094b20188121837", "b4d4304597b6a6c56e71846f68d5eb64385837d3", "9f0e98c17674031a055551f65951c1caad822d1b", "25956623c2ad64895fa31294a094b20188121837", "36e8b8b35e51e5b39fcafdb5c2bc763796d0672e", "fa07fa673d8c908e91d22c4566572a72548ccee9"]},{"id": "50dfb7358cc85cc7ab0eda68c517164ebd205d42", "title": "Sentence Simplification by Monolingual Machine Translation", "authors": ["Sander Wubben", "Antal van den Bosch", "Emiel Krahmer"], "date": "ACL", "abstract": "In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by… ", "references": ["2a5a179a70e98ee1c4b4b8b54f6c8aba629c02e4", "da47c8256aca536ffd7e59212f7c7a3361c9ff73", "5c37dbdfe70cf7b08aecb2b438685b61ebf09753", "5a4f7c894f4560c3205978d9277d744a910560f6", "da47c8256aca536ffd7e59212f7c7a3361c9ff73", "da47c8256aca536ffd7e59212f7c7a3361c9ff73", "da47c8256aca536ffd7e59212f7c7a3361c9ff73", "36e8b8b35e51e5b39fcafdb5c2bc763796d0672e", "7ecf212e3ab4413dfea6a179f36565fe0d028616", "da47c8256aca536ffd7e59212f7c7a3361c9ff73"]},{"id": "c0787db6b91bf8d7c2b175eccaf6ab27be031c81", "title": "OOV detection by joint word/phone lattice alignment", "authors": ["Hui Lin", "Jeff A. Bilmes", "Katrin Kirchhoff"], "date": "2007", "abstract": "We propose a new method for detecting out-of-vocabulary (OOV) words for large vocabulary continuous speech recognition (LVCSR) systems. Our method is based on performing a joint alignment between independently generated word and phone lattices, where the word-lattice is aligned via a recognition lexicon. Based on a similarity measure between phones, we can locate highly mis-aligned regions of time, and then specify those regions as candidate OOVs. This novel approach is implemented using the… ", "references": ["7afbd4786634bca104632265c37f1662dda883b5", "27c63939daf043f810942296bd98f1cac3510846", "9b470d0b7a3d0417fb44983071ce38628d406fa3", "26208cdcf49948cddb6438fc1c70c26b4a8b4926", "9bb497d05ac4e2a12ac3e3abb08cef477c286532", "325e90957eb2797e6a0e9f85331e19bc59b077a4", "b21ed328b414d3b72e5b688200bc90e942126948", "9d403e0cad5cb80f3f6bd42bfb0a53c80a159085", "7afbd4786634bca104632265c37f1662dda883b5", "d3a51176b45a6b40b15d096a5cee605cddbe5644"]},{"id": "27c63939daf043f810942296bd98f1cac3510846", "title": "Using word confidence measure for OOV words detection in a spontaneous spoken dialog system", "authors": ["Hui Sun", "Guoliang Zhang", "Mingxing Xu"], "date": "INTERSPEECH", "abstract": "Developing a real-life spoken dialogue system must face with many practical issues, where the out-of-vocabulary (OOV) words problem is one of the key difficulties. This paper presents the OOV detection mechanism based on the word confidence scoring developed for the d-Ear Attendant system, a spontaneous spoken dialogue system. In the d-Ear Attendant system, an explicit filler model is originally used to detect the presence of OOV words [1]. Although this approach has a satisfactory OOV… ", "references": ["86c351076631c802bcb6c468b6cd1750d2257ed9"]},{"id": "9b470d0b7a3d0417fb44983071ce38628d406fa3", "title": "Detection of OOV words using generalized word models and a semantic class language model", "authors": ["Thomas Schaaf"], "date": "INTERSPEECH", "abstract": "This paper describes an approach to detect out-of-vocabulary words in spontaneous speech using a language model built on semantic categories and a new type of generalized word models consisting of a mixture of specific and general acoustic units. We demonstrate the construction of the generalized word models as replacements for surnames in a German spontaneous travel planning task GSST [1]. We show that the use of our generalized word models improves recognition accuracy in cases where out-of… ", "references": ["2109f8f91301abec8497286160cd6b0f2e65ed05", "2109f8f91301abec8497286160cd6b0f2e65ed05", "0a7965e68396f64df8be7c01ced8453b090cc3b6", "0208557fa5450c2fb5efde65be8cec9ae4a40ecd", "b4bc615e1f41f10853b9406dc35173715e99dda5", "8a0a14d8f84fcd8003cc0417cb97b2455dac3eee", "0d8c323cf5af3ed2ce85cd78692fcf53b6d0635b", "b4bc615e1f41f10853b9406dc35173715e99dda5", "a39bd4f6291f2bda9f29f6b02a5a43e0b997f923", "d07d86cc5163e26e94fb1bc70b4c8629f0b08643"]},{"id": "1ad72930186136b693799f07f7d605bebfa26752", "title": "Morpheme Based Language Models for Speech Recognition of Czech", "authors": ["William J. Byrne", "Jan Hajic", "Josef Psutka"], "date": "TSD", "abstract": "In our paper we propose new technique for language modelling of highly inflectional languages such as Czech, Russian an other Slavic languages. Our aim is to alleviate main problem encountered in these languages, which is enormous vocabulary growth caused by great number of different word forms derived from one word (lemma). We reduced the size of the vocabulary by decomposing words into stems and endings and storing these sub-word units (morphemes) in the vocabulary separately. Then we trained… ", "references": []},{"id": "9d87ca8779b968b5fe08e074c6aa5c2bc6e5a4cc", "title": "Syllable-based Language Models in Speech Recognition for English Spoken Document Retrieval", "authors": ["Christian Schrumpf", "Martha Larson", "Stefan Eickeler"], "date": "2005", "abstract": "The spoken content of audio/visual collections such as TV or radio archives is an information resource of enormous potential. The challenge is to develop methods that will make it possible to browse or search these collections. The experimental results presented in this paper demonstrate that syllable-level transcripts provide an important supple- ment to conventional word-level transcripts for the task of unlimited vocabulary American English spoken document retrieval. Recognition is performed… ", "references": []},{"id": "bf7d940ea625da995e6f577d5249379f82e1c004", "title": "Learning units for domain-independent out-of- vocabulary word modelling", "authors": ["Issam Bazzi", "James R. Glass"], "date": "INTERSPEECH", "abstract": "This paper describes our recent work on detecting and recognizing out-of-vocabulary (OOV) words for robust speech recognition and understanding. To allow for OOV recognition within a word-based recognizer, the in-vocabulary (IV) word network is augmented with an OOV word model so that OOV words are considered simultaneously with IV words during recognition. We explore several configurations for the OOV model, the best of which utilizes a set of domain-independent, automatically derived… ", "references": ["e22ffd1dbbf9dca915ee167c502788029ef19bba", "ca12089834fec3c839e96f21699d1e43510c9d75", "d07d86cc5163e26e94fb1bc70b4c8629f0b08643", "e22ffd1dbbf9dca915ee167c502788029ef19bba", "e22ffd1dbbf9dca915ee167c502788029ef19bba", "e22ffd1dbbf9dca915ee167c502788029ef19bba", "d07d86cc5163e26e94fb1bc70b4c8629f0b08643", "e22ffd1dbbf9dca915ee167c502788029ef19bba", "e22ffd1dbbf9dca915ee167c502788029ef19bba", "e22ffd1dbbf9dca915ee167c502788029ef19bba"]},{"id": "ec5f929b57cf12b4d624ab125f337c14ad642ab1", "title": "Modelling out-of-vocabulary words for robust speech recognition", "authors": ["Issam Bazzi", "James R. Glass"], "date": "2002", "abstract": "This thesis concerns the problem of unknown or out-of-vocabulary (OOV) words in continuous speech recognition. \nWe propose a novel approach for handling OOV words within a single-stage recognition framework. To achieve this goal, an explicit and detailed model of OOV words is constructed and then used to augment the closed-vocabulary search space of a standard speech recognizer. This OOV model achieves open-vocabulary recognition through the use of more flexible subword units that can be… ", "references": ["bf7d940ea625da995e6f577d5249379f82e1c004", "4a7c88b57233913475946fb14f54d978cd38e5d6", "865f65edc9684745458f871544287573f4719a7e", "b7c86c4465762001b45a8cb2c42da091643141a5", "e9536fd33edada8cf4252039c9348fb041840aed", "c1e0096ee5a8ea07a9f3fb459299b7fb0eb2cd7f", "c1e0096ee5a8ea07a9f3fb459299b7fb0eb2cd7f", "f23f9fc2619d7b4609bf10c4ce861640a3eaa724", "9cb0042802b9e68ae952ccf081eccabd2b6eefa3", "c1e0096ee5a8ea07a9f3fb459299b7fb0eb2cd7f"]},{"id": "16e987de3d97c8fe93a10cee9bb4df3ce7b72696", "title": "Large vocabulary continuous speech recognition of an inflected language using stems and endings", "authors": ["Tomaz Rotovnik", "Mirjam Sepesy Maucec", "Zdravko Kacic"], "date": "2007", "abstract": "In this article, we focus on creating a large vocabulary speech recognition system for the Slovenian language. Currently, state-of-the-art recognition systems are able to use vocabularies with sizes of 20,000 to 100,000 words. These systems have mostly been developed for English, which belongs to a group of uninflectional languages. Slovenian, as a Slavic language, belongs to a group of inflectional languages. Its rich morphology presents a major problem in large vocabulary speech recognition… ", "references": ["96427f7d5b99d0b73184c46f9b22a0be654e46a6", "96427f7d5b99d0b73184c46f9b22a0be654e46a6", "1ad72930186136b693799f07f7d605bebfa26752", "aa26e84fb6fb58530f43d923621811343c3389db", "1ad72930186136b693799f07f7d605bebfa26752", "0c64ff6840e4c484684d51e6e694290e1c721b04", "96427f7d5b99d0b73184c46f9b22a0be654e46a6", "e45cd442fe3718f33fbe830c2ce840015e68ae40", "8d9e68f9c54ec37fe53441d0ea2854348cc6e60c", "dabee1d8bb444bf5b16e94a360846cf730226ec9"]},{"id": "94c904bb29ba60b00a06ca92ce47acc9b3e6a4fe", "title": "Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches", "authors": ["Martha Larson", "Daniel Willett", "Gerhard Rigoll"], "date": "INTERSPEECH", "abstract": "This paper proposes a novel combined compound split ting and phrase recombination method that optimizes the comp osition of the speech recognition lexicon for a given domain. Data-driven compound word splitting is followed by iterative re combination of high frequency combinations. Language model perp lexity and size are the criteria used to identify a balance be twe n compound decomposition, which reduces OOV, and lexical unit recombination, which packs additional context into a fixed-size… ", "references": ["2e15ed7e412663fac7a8def5df754cb968704c3c", "3b4588bb760cb646aa33ef60460935d5e3838216", "295439a6b75615537830c2c1106be91a42da22f5", "e8a6f95a573265f5ea2220d02f0a49025f724a54", "1773b7aea547f0aa2ced67b0017696bfefe77e2b", "51ccee33db1890a11970eced8cea3d7e39f9af75", "51ccee33db1890a11970eced8cea3d7e39f9af75", "1773b7aea547f0aa2ced67b0017696bfefe77e2b", "3b4588bb760cb646aa33ef60460935d5e3838216", "3b4588bb760cb646aa33ef60460935d5e3838216"]},{"id": "db3c3008ced47d307337b66d73d7f61ff8ab6a47", "title": "A corpus-based decompounding algorithm for German lexical modeling in LVCSR", "authors": ["Martine Adda-Decker"], "date": "INTERSPEECH", "abstract": "In this paper a corpus-based decompounding algorithm is described and applied for German LVCSR. The decompounding algorithm contributes to address two major problems for LVCSR: lexical coverage and letter-to-sound conversion. The idea of the algorithm is simple: given a word start of length only few different characters can continue an admissible word in the language. But concerning compounds, if word start reaches a constituent word boundary, the set of successor characters can theoretically… ", "references": ["ac23d187938df6434f9ace42526783614755d49e", "ac23d187938df6434f9ace42526783614755d49e", "c500129168ba54d7eeb63b34b874c9262761956f", "505c61ea3b6b73b0e554baaf755b8a86d0d9553d", "ac23d187938df6434f9ace42526783614755d49e", "5c70c217a23e224b4eea289152035d136afa1e43", "51ccee33db1890a11970eced8cea3d7e39f9af75"]},{"id": "a97b5db17acc731ef67321832dbbaf5766153135", "title": "Supervised Sequence Labelling with Recurrent Neural Networks", "authors": ["Alex Graves"], "date": "Studies in Computational…", "abstract": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks. Its two main contributions are (1) a new type of output layer that allows… ", "references": ["d0be39ee052d246ae99c082a565aba25b811be2d", "96494e722f58705fa20302fe6179d483f52705b4", "d0be39ee052d246ae99c082a565aba25b811be2d", "f70ae50828e3b6166628f5e8edb239b1cca6b471", "50c770b425a5bb25c77387f687a9910a9d130722", "0cd938d610d57d056f8c980cba9ae9f5dff21eca", "12496bf48ebdb5ab3c92bc911d6ee42369fa70bc", "bb404305aca0c9ea6195bf1d918efe13e5301a90", "8a51dc0b5694af3e54393e20e05f42fc3cbe476b", "f70ae50828e3b6166628f5e8edb239b1cca6b471"]},{"id": "df040243cadd17c8c5ef1e0edc8e5aabb9613688", "title": "Sub-lexical language models for German LVCSR", "authors": ["Amr El-Desoky Mousa", "M. Ali Basha Shaik", "Hermann Ney"], "date": "2010", "abstract": "One of the major difficulties related to German LVCSR is the rich morphology nature of German, leading to high out-of-vocabulary (OOV) rates, and high language model (LM) perplexities. Normally, compound words make up an essential fraction of the German vocabulary. Most compound OOVs are composed of frequent in-vocabulary words. Here, we investigate the use of sub-lexical LMs based on different approaches for word decomposition, namely supervised and unsupervised decomposition, as well as… ", "references": ["94c904bb29ba60b00a06ca92ce47acc9b3e6a4fe", "b42abb25f97fa729a4bd22cb09d3c49aebe6dce3", "26208cdcf49948cddb6438fc1c70c26b4a8b4926", "94c904bb29ba60b00a06ca92ce47acc9b3e6a4fe", "5ff5f0138e9b059072b76a1988d33b90e52dac50", "6f0c72f25d413eb8cc2dfbda38afbeeae74e6f87", "26208cdcf49948cddb6438fc1c70c26b4a8b4926", "bbbe9571c6717dfdc52fc6041c48a9e13a9c43b5", "82956efe47488f17c893e6ef608d80ad74aa1089", "26208cdcf49948cddb6438fc1c70c26b4a8b4926"]},{"id": "47d2dc34e1d02a8109f5c04bb6939725de23716d", "title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "authors": ["Jan Chorowski", "Dzmitry Bahdanau", "Yoshua Bengio"], "date": "2014", "abstract": "We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results… ", "references": ["2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "0b544dfe355a5070b60986319a3f51fb45d1348e", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "96494e722f58705fa20302fe6179d483f52705b4", "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "0b544dfe355a5070b60986319a3f51fb45d1348e", "b48168acba4a6ca33ad0f11bbf1c7d8106333822", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "b48168acba4a6ca33ad0f11bbf1c7d8106333822", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85"]},{"id": "c6629770cb6a00ad585918e71fe6dbad829ad0d1", "title": "An application of recurrent nets to phone probability estimation", "authors": ["Anthony J. Robinson"], "date": "1994", "abstract": "This paper presents an application of recurrent networks for phone probability estimation in large vocabulary speech recognition. The need for efficient exploitation of context information is discussed; a role for which the recurrent net appears suitable. An overview of early developments of recurrent nets for phone recognition is given along with the more recent improvements that include their integration with Markov models. Recognition results are presented for the DARPA TIMIT and Resource… ", "references": ["a08c99425ad94eed67d059813511fe9ca55e73eb", "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "cd0568b4faa03910ae3c07d00c627666f404305d", "b6c94cc324f585bd6c004f2b99b5589568643e45", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "c91c2ac02e33caff601b2e4d62a6841b33ca3929", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c91c2ac02e33caff601b2e4d62a6841b33ca3929"]},{"id": "d5d46991c7e92352865dbf442be7c74d0d560dd8", "title": "Improving Multi-Step Prediction of Learned Time Series Models", "authors": ["Arun Venkatraman", "Martial Hebert", "J. Andrew Bagnell"], "date": "AAAI", "abstract": "Most typical statistical and machine learning approaches to time series modeling optimize a single-step prediction error. In multiple-step simulation, the learned model is iteratively applied, feeding through the previous output as its new input. Any such predictor however, inevitably introduces errors, and these compounding errors change the input distribution for future prediction steps, breaking the train-test i.i.d assumption common in supervised learning. We present an approach that reuses… ", "references": ["d9252af1d91686d9f5b6e10923e3600e86186235", "3b76c51beceb5057b1285bd7d709817cda17adc0", "3ddaeeaebf5f8942c4b3fb38175245a92b42429b", "685df8bc3c5f5e055d8aab1248791a1117680cb9", "78396e535101308d4431c08f0e85b18c920ee44f", "5bbc287a26674127dcba8a285855d649d4ceb70b", "78396e535101308d4431c08f0e85b18c920ee44f", "3ddaeeaebf5f8942c4b3fb38175245a92b42429b", "d0be39ee052d246ae99c082a565aba25b811be2d", "f43840dc1638a18eb6178f1060dc5f41af1c5ac7"]},{"id": "79ab3c49903ec8cb339437ccf5cf998607fc313e", "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning", "authors": ["Stéphane Ross", "Geoffrey J. Gordon", "J. Andrew Bagnell"], "date": "AISTATS", "abstract": "Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d.", "references": ["e40a0969aac73d50485c05de7f1c0ab081d77028", "45372f73a0e40da428595597816ac4cae1469cec", "45372f73a0e40da428595597816ac4cae1469cec", "78396e535101308d4431c08f0e85b18c920ee44f", "c883f38d202548c1d89ef5de8892d53227842092", "e40a0969aac73d50485c05de7f1c0ab081d77028", "f65020fc3b1692d7989e099d6b6e698be5a50a93", "e40a0969aac73d50485c05de7f1c0ab081d77028", "78396e535101308d4431c08f0e85b18c920ee44f", "78396e535101308d4431c08f0e85b18c920ee44f"]},{"id": "4d376d6978dad0374edfa6709c9556b42d3594d3", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "authors": ["Sergey Ioffe", "Christian Szegedy"], "date": "2015", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.", "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4", "a538b05ebb01a40323997629e171c91aa28b8e2f", "73e1644ad4aab9e11c1e46eea513141c4930602d", "04f16203f1e66e8d2151dc359fd0405a0f482da7", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "162d958ff885f1462aeda91cd72582323fd6a1f4", "a538b05ebb01a40323997629e171c91aa28b8e2f", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f"]},{"id": "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks", "authors": ["Tu Munich", "Germany"], "date": "2008", "abstract": "Offline handwriting recognition—the automatic transcription of images of handwritten text—is a challenging task that combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks—multidimensional recurrent neural networks and connectionist temporal… ", "references": ["96494e722f58705fa20302fe6179d483f52705b4", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "541208b0de55cefe571a8e1c7a6b004d2ba9245b", "047655e733a9eed9a500afd916efa566915b9110", "047655e733a9eed9a500afd916efa566915b9110", "1f462943c8d0af69c12a09058251848324135e5a", "1b7a0048801f9d43dc48a8f04367be813146b05a", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "047655e733a9eed9a500afd916efa566915b9110", "1b7a0048801f9d43dc48a8f04367be813146b05a"]},{"id": "346fbcffe4237aa60e8bcb3d4294a8b99436f1d0", "title": "Factored conditional restricted Boltzmann Machines for modeling motion style", "authors": ["Graham W. Taylor", "Geoffrey E. Hinton"], "date": "ICML '09", "abstract": "The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the… ", "references": ["576104ba976841628c67d2d794c9a64ba876eb87", "576104ba976841628c67d2d794c9a64ba876eb87", "c5fdc6f420651ae075559be3c009e4f6f42039ff", "576104ba976841628c67d2d794c9a64ba876eb87", "099e18956a689878c62a27e7f8775c4d2fb89ea1", "099e18956a689878c62a27e7f8775c4d2fb89ea1", "099e18956a689878c62a27e7f8775c4d2fb89ea1", "099e18956a689878c62a27e7f8775c4d2fb89ea1", "099e18956a689878c62a27e7f8775c4d2fb89ea1", "c5fdc6f420651ae075559be3c009e4f6f42039ff"]},{"id": "8f3e1bbbdc1190ae6320c0520f539337a5ca5927", "title": "Improving Chinese Tokenization With Linguistic Filters On Statistical Lexical Acquisition", "authors": ["Dekai Wu", "Pascale Fung"], "date": "ANLP", "abstract": "The first step in Chinese NLP is to tokenize or segment character sequences into words, since the text contains no word delimiters. Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date have been based on dictionary lookup (e.g., Chang & Chen 1993; Chiang et al. 1992; Lin et al. 1993; Wu & Tseng 1993; Sproat et al. 1994).We present empirical evidence for four points concerning tokenization of… ", "references": []},{"id": "66ff19af88c81e0f3582ac65359a0543a16e1ac8", "title": "Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization", "authors": ["Bishan Yang", "Claire Cardie"], "date": "ACL", "abstract": "This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences. Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture nonlocal contextual cues that are important for sentiment interpretation. In contrast, our approach allows structured modeling of sentiment while taking into account both local and global contextual information. Specifically, we… ", "references": ["9ed75761a8608ee4b44a5216538a320f4c7a6327", "8ec5c6abf7b8c64d5939285bfe97b56d9bd2c6f4", "b35e340a4356eb8953b04c17375ffb631da68916", "8ec5c6abf7b8c64d5939285bfe97b56d9bd2c6f4", "9ed75761a8608ee4b44a5216538a320f4c7a6327", "cfa2646776405d50533055ceb1b7f050e9014dcb", "1303eacac5a3f2784fd0ab053344d85f1749861e", "9b89b67277b5d0c7e4aed9b69041473955be43bd", "0416849413c2038ccfc8d8353c182f9da5bce4a5", "b35e340a4356eb8953b04c17375ffb631da68916"]},{"id": "4052d4c2e36c0ddf78a1a47eee19452bb287a427", "title": "Syntactic features for Arabic speech recognition", "authors": ["Hong-Kwang Jeff Kuo", "Lidia Mangu", "Young-Suk Lee"], "date": "2009", "abstract": "We report word error rate improvements with syntactic features using a neural probabilistic language model through N-best re-scoring. The syntactic features we use include exposed head words and their non-terminal labels both before and after the predicted word. Neural network LMs generalize better to unseen events by modeling words and other context features in continuous space. They are suitable for incorporating many different types of features, including syntactic features, where there is… ", "references": ["54c846ee00c6132d70429cc279e8577f63ed05e4", "54c846ee00c6132d70429cc279e8577f63ed05e4", "673992da19d9209434615b12d55bdd36be706e9e", "e493205bc5151b02fe2712d813dd38f5f9ea9ba2", "ac2351860c3cc77883b5ff7b7e50295b163b75a4", "54c846ee00c6132d70429cc279e8577f63ed05e4", "e6140a793a4554806eb39d15c018d8f782d2ac1e", "54c846ee00c6132d70429cc279e8577f63ed05e4", "673992da19d9209434615b12d55bdd36be706e9e", "ac2351860c3cc77883b5ff7b7e50295b163b75a4"]},{"id": "941f318e41147773ae69d9da4f8de9b8dbea70f4", "title": "Learning semantic representations using convolutional neural networks for web search", "authors": ["Yelong Shen", "Xiaodong He", "Grégoire Mesnil"], "date": "WWW '14 Companion", "abstract": "This paper presents a series of new latent semantic models based on a convolutional neural network (CNN) to learn low-dimensional semantic vectors for search queries and Web documents.", "references": []},{"id": "377110ffbaf595badd15943a3c214b32c91d934e", "title": "Exploiting Chinese character models to improve speech recognition performance", "authors": ["James Hieronymus", "Xunying Liu", "Philip C. Woodland"], "date": "INTERSPEECH", "abstract": "The Chinese language is based on characters which are syllabic in nature. Since languages have syllabotactic rules which govern the construction of syllables and their allowed sequences, Chinese character sequence models can be used as a first level approximation of allowed syllable sequences. N-gram character sequence models were trained on 4.3 billion characters. Characters are used as a first level recognition unit with multiple pronunciations per character. For comparison the CU-HTK… ", "references": ["6769e78616bc3a332a1829d1a3c2220e5b94555d", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "439a75954a129ecc30f17b62a979b9af230d83d4", "f4cc5563c694355ddcf746ff9a55ccdb22d86a98", "96d683d0eaf2d866664b8f4ab214ec2c843e3221", "6250a40253e32f0edbc00b98776226bc6f4ce5c2", "fd7f688e004e8133b3f0239cd8aad7e9707065e9", "8f3e1bbbdc1190ae6320c0520f539337a5ca5927", "fd7f688e004e8133b3f0239cd8aad7e9707065e9", "5bf65452ae566a052b00d919404f462470869600"]},{"id": "0861015b3c89d68749548c19c6f056eee34eafc6", "title": "A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)", "authors": ["Jonathan G. Fiscus"], "date": "1997", "abstract": "Describes a system developed at NIST to produce a composite automatic speech recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has a lower error rate than any of the individual systems.", "references": []},{"id": "a5f8135cf356a80e13241b5b36a5836eaad85fd1", "title": "Improved neural network based language modelling and adaptation", "authors": ["Junho Park", "Xunying Liu", "Philip C. Woodland"], "date": "INTERSPEECH", "abstract": "Neural network language models (NNLM) have become an increasingly popular choice for large vocabulary continuous speech recognition (LVCSR) tasks, due to their inherent generalisation and discriminative power. This paper present two techniques to improve performance of standard NNLMs. First, the form of NNLM is modelled by introduction an additional output layer node to model the probability mass of out-of-shortlist (OOS) words. An associated probability normalisation scheme is explicitly… ", "references": ["3f1a28b0d8f15e8765708f88a18173ce84d8eabf", "26080f0969a520ebd82f252dd060f5a4948bbd6e", "4f2f5b96410f7333fcfde027658e86bc1e18dc87", "26080f0969a520ebd82f252dd060f5a4948bbd6e", "792bfb7b24bcfd12fd2c15089d99df8740817956", "26080f0969a520ebd82f252dd060f5a4948bbd6e", "792bfb7b24bcfd12fd2c15089d99df8740817956", "f83615a105d602fefb4076293a3c95d3659f26b6", "8b395470a57c48d174c4216ea21a7a58bc046917", "aae5c396338ad6c304cd2ed7a40650131c5fc4c3"]},{"id": "28b98600e363025d66d5b9a529330a808acc6c6f", "title": "In Defense of Word Embedding for Generic Text Representation", "authors": ["Guy Lev", "Benjamin Klein", "Lior Wolf"], "date": "NLDB", "abstract": "Statistical methods have shown a remarkable ability to capture semantics. The word2vec method is a frequently cited method for capturing meaningful semantic relations between words from a large text corpus. It has the advantage of not requiring any tagging while training. The prevailing view is, however, that it lacks the ability to capture semantics of word sequences and is virtually useless for most purposes, unless combined with heavy machinery. This paper challenges that view, by showing… ", "references": ["4cfad7889dc12825309325cd4b4f3febed424e36", "bfc214ce7ab5b101425e5cabd631176bb427adff", "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "f2bc2c9ca182def54be95f32ba82807c33d30b6b", "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "5c97a26fd66d7413681cb74041347a47d0c97178", "5c97a26fd66d7413681cb74041347a47d0c97178", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "4cfad7889dc12825309325cd4b4f3febed424e36"]},{"id": "20b5c6af6182cf078b10fd30be9548cf36e9ed83", "title": "Words versus Character n-Grams for Anti-Spam Filtering", "authors": ["Ioannis Kanaris", "Konstantinos Kanaris", "Efstathios Stamatatos"], "date": "2007", "abstract": "The increasing number of unsolicited e-mail messages (spam) reveals the need for the development of reliable anti-spam filters. The vast majority of content-based techniques rely on word-based representation of messages. Such approaches require reliable tokenizers for detecting the token boundaries. As a consequence, a common practice of spammers is to attempt to confuse tokenizers using unexpected punctuation marks or special characters within the message. In this paper we explore an… ", "references": []},{"id": "7e8d5a108c28cdfb92f419ce919fbf7993dfebfc", "title": "A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval", "authors": ["Yelong Shen", "Xiaodong He", "Grégoire Mesnil"], "date": "CIKM '14", "abstract": "In this paper, we propose a new latent semantic model that incorporates a convolutional-pooling structure over word sequences to learn low-dimensional, semantic vector representations for search queries and Web documents.", "references": ["e5305866d701a2c102c5f81fbbf48bf6ac29f252", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "58bd7dfb4c7c8dec78fe722bd67b02565eb1b0b7", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "ec7018ef4a99a5a5ed6547eb16fe856e3f2e60c6", "ec7018ef4a99a5a5ed6547eb16fe856e3f2e60c6", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "c2872fb23b02597034a179f4adb82a00d6ffda8d", "3464374899e799cbd516d00f75e425efd495150e", "e5305866d701a2c102c5f81fbbf48bf6ac29f252"]},{"id": "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "title": "Learning Character-level Representations for Part-of-Speech Tagging", "authors": ["Cícero Nogueira dos Santos", "Bianca Zadrozny"], "date": "ICML", "abstract": "Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose… ", "references": ["330da625c15427c6e42ccfa3b747fb29e5835bf0", "bc1022b031dc6c7019696492e8116598097a8c12", "53ab89807caead278d3deb7b6a4180b277d3cb77", "acc4e56c44771ebf69302a06af51498aeb0a6ac8", "eb42a490cf4f186d3383c92963817d100afd81e2", "d6d931fe9bfe7223165b14ec85d79d56e406e38a", "d6d931fe9bfe7223165b14ec85d79d56e406e38a", "acc4e56c44771ebf69302a06af51498aeb0a6ac8", "53ab89807caead278d3deb7b6a4180b277d3cb77", "be7e395709bd801c3d93a7c07ef1ea42146d82cb"]},{"id": "fbf417c83ae5b895fc645346e4efbf3a0aabeac9", "title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks", "authors": ["Rie Johnson", "Tong Zhang"], "date": "HLT-NAACL", "abstract": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data.", "references": ["6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "ab001d508fbb4160e53686e05b800ab4baeb9728", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "667939f67523db023d8465e18a96d0198905df72", "54e840c8973db7665a6388b2d992ef08ed7f0260", "649d03490ef72c5274e3bccd03d7a299d2f8da91"]},{"id": "b0aca3e7877c3c20958b0fae5cbf2dd602104859", "title": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts", "authors": ["Cícero Nogueira dos Santos", "Maíra A. de C. Gatti"], "date": "COLING", "abstract": "Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain.", "references": ["b8fd561d280336e6bd0af75c3468c93b0d623677", "5ee9bed324a4f0716554409dc367df3beeb27b44", "5ee9bed324a4f0716554409dc367df3beeb27b44", "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "b8fd561d280336e6bd0af75c3468c93b0d623677", "876e51a39ae8bf9b1d48f4e219bbc4e0705ee1cd", "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "687bac2d3320083eb4530bf18bb8f8f721477600", "cfa2646776405d50533055ceb1b7f050e9014dcb", "b8fd561d280336e6bd0af75c3468c93b0d623677"]},{"id": "56172b6fe2613c37d9790bde8ab6ccda14b35678", "title": "Persistent RNNs: Stashing Recurrent Weights On-Chip", "authors": ["Greg Diamos", "Shubho Sengupta", "Sanjeev Satheesh"], "date": "ICML", "abstract": "This paper introduces a new technique for mapping Deep Recurrent Neural Networks (RNN) efficiently onto GPUs.", "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "067e07b725ab012c80aa2f87857f6791c1407f6d", "31c36d445367ba204244bb74893c5654e31c3869", "067e07b725ab012c80aa2f87857f6791c1407f6d", "31c36d445367ba204244bb74893c5654e31c3869", "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "31c36d445367ba204244bb74893c5654e31c3869", "965c9aec5e68d49142c5af6a9f0a984f6c2c743a", "8ff840a40d3f1557c55c19d4d636da77103168ce", "31c36d445367ba204244bb74893c5654e31c3869"]},{"id": "d1b78d136e9e6be0aeb814027f0f3fd843606155", "title": "A Neural Autoregressive Topic Model", "authors": ["Hugo Larochelle", "Stanislas Lauly"], "date": "NIPS", "abstract": "We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents.", "references": ["aa1762a629b31d254450e37ce8baa235d729d82b", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "b32de117302258dd29919435cd001a8bcdfee3b3", "b32de117302258dd29919435cd001a8bcdfee3b3", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "b32de117302258dd29919435cd001a8bcdfee3b3", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "9360e5ce9c98166bb179ad479a9d2919ff13d022"]},{"id": "40be3888daa5c2e5af4d36ae22f690bcc8caf600", "title": "Visualizing and Understanding Recurrent Networks", "authors": ["Andrej Karpathy", "Johanna E. Johnson", "Fei-Fei Li"], "date": "2015", "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data.", "references": ["71ae756c75ac89e2d731c9c79649562b5768ff39", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "533ee188324b833e059cb59b654e6160776d5812", "533ee188324b833e059cb59b654e6160776d5812", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "adfcf065e15fd3bc9badf6145034c84dfb08f204"]},{"id": "48b4524a3b1207157b1b2f87885c434c96fc7a19", "title": "Joint Parsing and Semantic Role Labeling", "authors": ["Charles A. Sutton", "Andrew McCallum"], "date": "CoNLL", "abstract": "A striking feature of human syntactic processing is that it is context-dependent, that is, it seems to take into account semantic information from the discourse context and world knowledge. In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses. To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser. Our current… ", "references": ["f2124f8995a9cdf4e168baba426472d2d811c0f1"]},{"id": "790ecefeaf2b471b439743a772ccce026131bef5", "title": "Simple Semi-supervised Dependency Parsing", "authors": ["Terry K Koo", "Xavier Carreras", "Michael Collins"], "date": "ACL", "abstract": "We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions… ", "references": ["713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "ad269ba941949a1d66b6649a71d752784c576dc3", "10a9abb4c78f0be5cc85847f248d3e8277b3c810", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "8d1d98807843fad7de1734629edb0c873015c14a", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "f52de7242e574b70410ca6fb70b79c811919fc00", "1deed1a4a03e07aee3b8b8e4716f35033c715a57", "3de5d40b60742e3dfa86b19e7f660962298492af"]},{"id": "31b4c03d721dc10b87c178277c1d369f91db8f0e", "title": "Semi-Supervised Learning for Natural Language", "authors": ["Percy Liang"], "date": "2005", "abstract": "Statistical supervised learning techniques have been successful for many natural language processing tasks, but they require labeled datasets, which can be expensive to obtain. On the other hand, unlabeled data (raw text) is often available \"for free\" in large quantities. Unlabeled data has shown promise in improving the performance of a number of tasks, e.g. word sense disambiguation, information extraction, and natural language parsing. In this thesis, we focus on two segmentation tasks… ", "references": ["1c0ece611643cfb8f3a23e4802c754ea583ebe37", "9f6c28b20458f7f7de0989c2f8296d67162610f2", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "df3520f52fcf42b4f10ed4b35b3b3f9cd050f290", "08e39912a54fc46f25f9e79bfa06ee44311b051a", "df3520f52fcf42b4f10ed4b35b3b3f9cd050f290", "4614650c3bb3e835c80612d3bca9586f81db95a3", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "08e39912a54fc46f25f9e79bfa06ee44311b051a"]},{"id": "f95adc1d8daaa07a0c956826ec274ca9e2515ddc", "title": "Batch normalized recurrent neural networks", "authors": ["César Laurent", "Gabriel Pereyra", "Yoshua Bengio"], "date": "2016", "abstract": "Recurrent Neural Networks (RNNs) are powerful models for sequential data that have the potential to learn long-term dependencies. However, they are computationally expensive to train and difficult to parallelize. Recent work has shown that normalizing intermediate representations of neural networks can significantly improve convergence rates in feed-forward neural networks [1]. In particular, batch normalization, which uses mini-batch statistics to standardize features, was shown to… ", "references": ["ac973bbfd62a902d073a85ca621fd297e8660a82", "34f25a8704614163c4095b3ee2fc969b60de4698", "4d376d6978dad0374edfa6709c9556b42d3594d3", "4d376d6978dad0374edfa6709c9556b42d3594d3", "ac973bbfd62a902d073a85ca621fd297e8660a82", "533ee188324b833e059cb59b654e6160776d5812", "941e30afcae061a115301c65a1afe49d8856f14e", "34f25a8704614163c4095b3ee2fc969b60de4698", "941e30afcae061a115301c65a1afe49d8856f14e", "ac973bbfd62a902d073a85ca621fd297e8660a82"]},{"id": "8250ecbaef057bdb5390ef4e4be798f1523a23f6", "title": "Deep Networks With Large Output Spaces", "authors": ["Sudheendra Vijayanarasimhan", "Jonathon Shlens", "Jay Yagnik"], "date": "2015", "abstract": "Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data.", "references": ["774f67303ea4a3a94874f08cf9a9dacc69b40782", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "3127190433230b3dc1abd0680bb58dced4bcd90e", "348b0133bd5d0d885ce7e0f8e5d6bb394a03b2b2", "774f67303ea4a3a94874f08cf9a9dacc69b40782", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "348b0133bd5d0d885ce7e0f8e5d6bb394a03b2b2"]},{"id": "84069287da0a6b488b8c933f3cb5be759cb6237e", "title": "On the difficulty of training recurrent neural networks", "authors": ["Razvan Pascanu", "Yoshua Bengio"], "date": "ICML", "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994.", "references": ["266e07d0dd9a75b61e3632e9469993dbaf063f1c", "a10ec7cc6c42c7780ef631c038b16c49ed865038", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "69e5339c0c3928a354e848b9ccf5349f6397e60b", "ded103d0613e1a8f51f586cc1678aee3ff26e811", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "69e5339c0c3928a354e848b9ccf5349f6397e60b", "0d6203718c15f137fda2f295c96269bc2b254644", "ded103d0613e1a8f51f586cc1678aee3ff26e811", "ded103d0613e1a8f51f586cc1678aee3ff26e811"]},{"id": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization", "authors": ["Jimmy Ba", "Jamie Ryan Kiros", "Geoffrey E. Hinton"], "date": "2016", "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feedforward neural… ", "references": ["a6cb366736791bcccc5c8639de5a8f9636bf87e8", "31868290adf1c000c611dfc966b514d5a34e8d23", "8ff840a40d3f1557c55c19d4d636da77103168ce", "cea967b59209c6be22829699f05b8b1ac4dc092d", "952454718139dba3aafc6b3b67c4f514ac3964af", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "6b570069f14c7588e066f7138e1f21af59d62e61", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f95adc1d8daaa07a0c956826ec274ca9e2515ddc", "a6cb366736791bcccc5c8639de5a8f9636bf87e8"]},{"id": "6503a3d9fb204c2a08ecfcfe6ba5b815fc65a030", "title": "Guided Learning for Bidirectional Sequence Classification", "authors": ["Libin Shen", "Giorgio Satta", "Aravind K. Joshi"], "date": "ACL", "abstract": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using… ", "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "eb42a490cf4f186d3383c92963817d100afd81e2", "b3bf257416306c9d77ac36614850a44fa92a6327", "5a7958b418bceb48a315384568091ab1898b1640", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "eb42a490cf4f186d3383c92963817d100afd81e2", "eb42a490cf4f186d3383c92963817d100afd81e2", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "eb42a490cf4f186d3383c92963817d100afd81e2", "4402eb69536144313321f446802b0971b07ff589"]},{"id": "0cea6b034ee949d10604ba163270b699e711ded8", "title": "Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch", "authors": ["Xie Chen", "Yongqiang Wang", "Philip C. Woodland"], "date": "INTERSPEECH", "abstract": "Copyright © 2014 ISCA.", "references": ["80554518a78d10dd9591de31a6f1ec46844a8cc4", "9819b600a828a57e1cde047bbe710d3446b30da5", "f89f4a93b8b8e034830e62fbb6eb86da66832576", "9819b600a828a57e1cde047bbe710d3446b30da5", "80554518a78d10dd9591de31a6f1ec46844a8cc4", "cda39ac12fd8cce796d98b344f1076d2da834ebd", "cda39ac12fd8cce796d98b344f1076d2da834ebd", "d6fb7546a29320eadad868af66835059db93d99f", "f89f4a93b8b8e034830e62fbb6eb86da66832576", "cda39ac12fd8cce796d98b344f1076d2da834ebd"]},{"id": "9f08b01251cb99f4ffae8c7b3e4468d3af9c98d3", "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences", "authors": ["Baotian Hu", "Zhengdong Lu", "Qingcai Chen"], "date": "NIPS", "abstract": "Semantic matching is of central importance to many natural language tasks [2,28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer… ", "references": ["eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "941f318e41147773ae69d9da4f8de9b8dbea70f4", "9c0ddf74f87d154db88d79c640578c1610451eec", "4aba54ea82bf99ed4690d45051f1b25d8b9554b5", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba"]},{"id": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "title": "Improved backing-off for M-gram language modeling", "authors": ["Reinhard Kneser", "Hermann Ney"], "date": "1995", "abstract": "In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem.", "references": ["974a522a4b09f65df68e1e0025d68f1c896e457c", "a2c9aea19862e26810fd8113254ab82914014d0c", "b2986b25f50babd536dd0ecf2237d9eabf5843c2", "b2986b25f50babd536dd0ecf2237d9eabf5843c2", "11d74704fe3bb078f81e32e61857e082794c829d", "11d74704fe3bb078f81e32e61857e082794c829d", "974a522a4b09f65df68e1e0025d68f1c896e457c", "a2c9aea19862e26810fd8113254ab82914014d0c", "b2986b25f50babd536dd0ecf2237d9eabf5843c2", "974a522a4b09f65df68e1e0025d68f1c896e457c"]},{"id": "1a3c74c7b11ad5635570932577cdde2a3f7a6a5c", "title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "authors": ["George E. Dahl", "Tara N. Sainath", "Geoffrey E. Hinton"], "date": "2013", "abstract": "Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks.", "references": ["c25f3a963f62165a8fc46bc63865e6bec1477e59", "843959ffdccf31c6694d135fad07425924f785b1", "d7174b0cf599408fb723e6702504e27dc9d6c203", "d7174b0cf599408fb723e6702504e27dc9d6c203", "843959ffdccf31c6694d135fad07425924f785b1", "375d7b8a70277d5d7b5e0cc999b03ba395c42901", "a538b05ebb01a40323997629e171c91aa28b8e2f", "a538b05ebb01a40323997629e171c91aa28b8e2f", "6658bbf68995731b2083195054ff45b4eca38b3a", "46af78834358337447001241cd2e18828ed926f0"]},{"id": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "title": "Efficient softmax approximation for GPUs", "authors": ["Edouard Grave", "Armand Joulin", "Hervé Jégou"], "date": "2017", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations… ", "references": ["e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "d46b81707786d18499f911b4ab72bb10c65406ba", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "d46b81707786d18499f911b4ab72bb10c65406ba", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "d46b81707786d18499f911b4ab72bb10c65406ba"]},{"id": "aa1762a629b31d254450e37ce8baa235d729d82b", "title": "Large-Scale Learning of Embeddings with Reconstruction Sampling", "authors": ["Yann Dauphin", "Xavier Glorot", "Yoshua Bengio"], "date": "ICML", "abstract": "In this paper, we present a novel method to speed up the learning of embeddings for large-scale learning tasks involving very sparse data, as is typically the case for Natural Language Processing tasks. Our speed-up method has been developed in the context of Denoising Auto-encoders, which are trained in a purely unsupervised way to capture the input distribution, and learn embeddings for words and text similar to earlier neural language models. The main contribution is a new method to… ", "references": ["c19fbefdeead6a4154a22a9c8551a18b1530033a", "843959ffdccf31c6694d135fad07425924f785b1", "b8012351bc5ebce4a4b3039bbbba3ce393bc3315", "872bae24c109f7c30e052ac218b17a8b028d08a0", "d895647b4a80861703851ef55930a2627fe19492", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "57458bc1cffe5caa45a885af986d70f723f406b4", "1c46943103bd7b7a2c7be86859995a4144d1938b", "699d5ab38deee78b1fd17cc8ad233c74196d16e9"]},{"id": "f5e23b61597f3d2b51515c557a213be1f0e6ed6f", "title": "Semantic Vector Models and Functional Models for Pregroup Grammars", "authors": ["Anne Preller", "Mehrnoosh Sadrzadeh"], "date": "2011", "abstract": "We show that vector space semantics and functional semantics in two-sorted first order logic are equivalent for pregroup grammars. We present an algorithm that translates functional expressions to vector expressions and vice-versa. The semantics is compositional, variable free and invariant under change of order or multiplicity. It includes the semantic vector models of Information Retrieval Systems and has an interior logic admitting a comprehension schema. A sentence is true in the interior… ", "references": ["44da6a8fc4aafad47db93bc164de9513b6adc7e5", "aee59e44815aecc75811f62767e79fdf13cf267e", "44da6a8fc4aafad47db93bc164de9513b6adc7e5", "3fda6c4be5121027af34f88b6cfa055d045c5905", "a159172bdfccd744030be226884d12b7cd97b7ca", "314782b81f04378471c27e1a91a2ea0bd670e1ab", "3fda6c4be5121027af34f88b6cfa055d045c5905", "de5e318b0045b7df732f74a772eee314c8e8d2be", "3fda6c4be5121027af34f88b6cfa055d045c5905", "a159172bdfccd744030be226884d12b7cd97b7ca"]},{"id": "0d3233d858660aff451a6c2561a05378ed09725a", "title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "authors": ["Will Y. Zou", "Richard Socher", "Christopher D. Manning"], "date": "EMNLP", "abstract": "We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models.", "references": ["8326ff39e8fe4272cfaa444767b6592c522e48ec", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "87caba83ac5844118ed39bdb15e723017a459bce", "98b98125240a44847489ce366b8e1891bd4caaaa", "b79b12fb29ce3f47f9544551aa4cd1f49776ef03", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "87caba83ac5844118ed39bdb15e723017a459bce", "88bb2faf928cbc95ce5dbf94da8c1b96e22577c4", "330da625c15427c6e42ccfa3b747fb29e5835bf0"]},{"id": "4471e3117cdac2fae74d305d54b237bb3addd749", "title": "Explorations In Automatic Thesaurus Discovery", "authors": ["Gregory Grefenstette"], "date": "1994", "abstract": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index. ", "references": []},{"id": "73e897104540642698321c106cc9c35af369fe12", "title": "Combining Symbolic and Distributional Models of Meaning", "authors": ["Stephen Clark", "Stephen G. Pulman"], "date": "AAAI Spring Symposium…", "abstract": "The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful… ", "references": ["b9eea85e590f6e522e3681b8e45012684c60b0fd", "b54bcfca3fddc26b8889739a247a25e445818149", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "b54bcfca3fddc26b8889739a247a25e445818149", "913c0fe31e99379dcc3e9e8c584c596bf6113941", "99614334a3e9b809e43384777409af7eccde3db6", "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac", "36eff99a7f23cec395e4efc80ff7f937934c7be6", "e6c7adc28e20d361d5c35aa9808094b10f6a34d1", "e6c7adc28e20d361d5c35aa9808094b10f6a34d1"]},{"id": "d3cf28ab36ff7f7601a55c1e832736b2473a07f0", "title": "Representing word meaning and order information in a composite holographic lexicon.", "authors": ["Michael N. Jones", "Douglas J. K. Mewhort"], "date": "2007", "abstract": "The authors present a computational model that builds a holographic lexicon representing both word meaning and word order from unsupervised experience with natural language. The model uses simple convolution and superposition mechanisms (cf. B. B. Murdock, 1982) to learn distributed holographic representations for words. The structure of the resulting lexicon can account for empirical data from classic experiments studying semantic typicality, categorization, priming, and semantic constraint in… ", "references": ["a2f77fe7b6c7685ab2d7d635f21b31b80d093a78", "a2f77fe7b6c7685ab2d7d635f21b31b80d093a78", "9c001260f8cb704347be78af98a41da571eb416c", "45313a157dcaa1103eaa53f099204c66c7dd9cd9", "34c34f6366e7337396c904dd68f1213817299da4", "756fc297eb767565f3f0f130d351824159b7ec47", "17f278d11d3667d0facd518252c2c80d046eee2c", "756fc297eb767565f3f0f130d351824159b7ec47", "bffe67de0a4548ecac1d318284f4b055036af972", "a2f77fe7b6c7685ab2d7d635f21b31b80d093a78"]},{"id": "00162f43964fd457a9158408c1ac0e8990489782", "title": "SemEval-2007 Task 10: English Lexical Substitution Task", "authors": ["Diana McCarthy", "Roberto Navigli"], "date": "SemEval@ACL", "abstract": "In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find an alternative substitute word or phrase for a target word in context. The task involves both finding the synonyms and disambiguating the context. Participating systems are free to use any lexical resource. There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is. ", "references": ["f9a25e0dc776857fc24ebc7115c980312f2719b1", "cc0c3033ea7d4e19e1f5ac71934759507e126162", "0bd6fed9d6f9f5bde5703eb7d24340a3b4992b1b", "74795ed7f04541e6df82eab47158d5d3c88a1aad", "6f3250ba47fdb413a0c113cc16d274517864f8ab", "e4f0dd983d6406a7a2930a4743b85c57d6e57008", "5ae41a2ee6a8a18823441fa2f8ddf7fac3149846", "6f3250ba47fdb413a0c113cc16d274517864f8ab", "0bd6fed9d6f9f5bde5703eb7d24340a3b4992b1b", "5ae41a2ee6a8a18823441fa2f8ddf7fac3149846"]},{"id": "2f55b6959997e8464c19140644c6e3d6781a55b4", "title": "A Distributional Model of Semantic Context Effects in Lexical Processing", "authors": ["Scott McDonald", "Chris Brew"], "date": "ACL", "abstract": "One of the most robust findings of experimental psycholinguistics is that the context in which a word is presented influences the effort involved in processing that word. We present a computational model of contextual facilitation based on word co-occurrence vectors, and empirically validate the model through simulation of three representative types of context manipulation: single word priming, multiple-priming and contextual constraint. The aim of our study is to find out whether special… ", "references": ["e1d108d3a250272860492eda88ab8e53a1c0d183", "41072aa1d9c3ae206d4a12cd1636bd84a40a4141", "41072aa1d9c3ae206d4a12cd1636bd84a40a4141", "6d8018bd8b288baca0c55522877efd1b49258747", "b09032e56e27177cc6aeceb1d1f29e0f45b68bb4", "3df57e4f860804872fdd0ec66b0ff6c547498c87", "6d8018bd8b288baca0c55522877efd1b49258747", "6d8018bd8b288baca0c55522877efd1b49258747", "41072aa1d9c3ae206d4a12cd1636bd84a40a4141", "37875a22dce6664d33a343bcaa7cd41a5bf5f30b"]},{"id": "fd1901f34cc3673072264104885d70555b1a4cdc", "title": "Automatic Retrieval and Clustering of Similar Words", "authors": ["Dekang Lin"], "date": "COLING-ACL", "abstract": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is. ", "references": ["4966f2d75734b4abd4ad105b85eff675cb781b5d", "a3ff7801bcf72fea30117c88d397403a570c5c68", "c80bcd31f077f9a632ce959278d1c2fc095131a8", "8b8e5d7da8441f0d44dac7ac3b87050d653bfa1a", "30a8f7577c1ccdbd14dcea85efdfae5420e1ccb6", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "4966f2d75734b4abd4ad105b85eff675cb781b5d", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "8b8e5d7da8441f0d44dac7ac3b87050d653bfa1a"]},{"id": "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "title": "Vector-based Models of Semantic Composition", "authors": ["Jeff Mitchell", "Mirella Lapata"], "date": "ACL", "abstract": "This paper proposes a framework for representing the meaning of phrases and sentences in vector space.", "references": ["0c074681e891ecd764bb844666011fa41cf7c8f1", "1ea1a0e87169f9ac95293d77eb0bbcd79506bd09", "6797b320c62999eb6fac3ed2874e9e7ef44d6cb7", "495f3405da229b903797472c64d09d83659fdb34", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "f76807536e7f6542a72609929a9630de802a597f", "0c074681e891ecd764bb844666011fa41cf7c8f1", "495f3405da229b903797472c64d09d83659fdb34", "f76807536e7f6542a72609929a9630de802a597f", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4"]},{"id": "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "title": "Producing high-dimensional semantic spaces from lexical co-occurrence", "authors": ["Kevin P. Lund", "Curt Burgess"], "date": "1996", "abstract": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional… ", "references": []},{"id": "6c6c5cfa01c57f2015c8c923b1404727ad3330fc", "title": "Cross Language Text Classification by Model Translation and Semi-Supervised Learning", "authors": ["Lei Shi", "Rada Mihalcea", "Mingjun Tian"], "date": "EMNLP", "abstract": "In this paper, we introduce a method that automatically builds text classifiers in a new language by training on already labeled data in another language. Our method transfers the classification knowledge across languages by translating the model features and by using an Expectation Maximization (EM) algorithm that naturally takes into account the ambiguity associated with the translation of a word. We further exploit the readily available unlabeled data in the target language via semi… ", "references": ["a3e39e183cf46a49320be761242c42e02e530e80", "4e14aa89e2a0d9ac2e6e80f9d618b2fdf07defd4", "bae533297207bb64597afa3b86a699ed6c4c98b1", "6b9ac7558d2dacbc51cb303f8f685498a16f90a5", "ed28062f9b267d2fd3261293d91c52d7ff72edf2", "3a2c3cf32c8c214cccebc45ebc2a2a1dd7d1e658", "76ad5e0fcb1ed7380ac990277e064715f229e6ce", "f06afab33a35608bb7959d995ba2e8e4f46cfc73", "6b9ac7558d2dacbc51cb303f8f685498a16f90a5", "a3e39e183cf46a49320be761242c42e02e530e80"]},{"id": "fc9344147e3ddd8dd8ec86452961745df50e5c1e", "title": "Cross-lingual relevance models", "authors": ["Victor Lavrenko", "Martin Choquette", "W. Bruce Croft"], "date": "SIGIR '02", "abstract": "We propose a formal model of Cross-Language Information Retrieval that does not rely on either query translation or document translation. Our approach leverages recent advances in language modeling to directly estimate an accurate topic model in the target language, starting with a query in the source language. The model integrates popular techniques of disambiguation and query expansion in a unified formal framework. We describe how the topic model can be estimated with either a parallel… ", "references": ["406640217b61473357142aaf92d7b20454ff54ff", "406640217b61473357142aaf92d7b20454ff54ff", "d6f53f91fedcf7b8271b54541642438ffe8333e3", "406640217b61473357142aaf92d7b20454ff54ff", "406640217b61473357142aaf92d7b20454ff54ff", "406640217b61473357142aaf92d7b20454ff54ff", "d6f53f91fedcf7b8271b54541642438ffe8333e3", "406640217b61473357142aaf92d7b20454ff54ff", "d6f53f91fedcf7b8271b54541642438ffe8333e3", "406640217b61473357142aaf92d7b20454ff54ff"]},{"id": "78d515c5b01fb34f5013e453f3ed852badfa31eb", "title": "Cross-Lingual Latent Topic Extraction", "authors": ["Duo Zhang", "Qiaozhu Mei", "ChengXiang Zhai"], "date": "ACL", "abstract": "Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deficiency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models… ", "references": ["dcb426cd5f9ca9a0d58e852fb8757202bfc36d37", "355b86dafd852e4df905f6ad9402c7d03831d618", "c18cf392c7d645d9d9e6cbadb8f50a4dbbabdf75", "dcb426cd5f9ca9a0d58e852fb8757202bfc36d37", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "dcb426cd5f9ca9a0d58e852fb8757202bfc36d37", "98b98125240a44847489ce366b8e1891bd4caaaa", "e6dd83b2aa34c806596fc619ff3fbccf5f9830ab", "355b86dafd852e4df905f6ad9402c7d03831d618", "f198043a866e9187925a8d8db9a55e3bfdd47f2c"]},{"id": "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "authors": ["Richard Socher", "Eric H. Huang", "Christopher D. Manning"], "date": "NIPS", "abstract": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning.", "references": ["9c0ddf74f87d154db88d79c640578c1610451eec", "9c0ddf74f87d154db88d79c640578c1610451eec", "7acfdc905f734abf966aed58abb983bc015ff7fe", "6d1792f1871a99cb89dcf713abd24592630f32b7", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "57458bc1cffe5caa45a885af986d70f723f406b4", "57458bc1cffe5caa45a885af986d70f723f406b4", "10b8f21e57b3392ce623c374c2c039f811ce5f69", "7acfdc905f734abf966aed58abb983bc015ff7fe", "c6afe8a8aa13de8e3f2710ef07b22ce86a005419"]},{"id": "e0f5c8b7925ab500714634ba74dfd47a047ac279", "title": "Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation", "authors": ["Ivan Titov"], "date": "ACL", "abstract": "We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific… ", "references": ["66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde", "d895647b4a80861703851ef55930a2627fe19492", "66d398aeaeb7ec24ededb1adaa4b4f09a6c1bcde", "1e1542a0f9ce1e3807aad47eb387f24ab47def9c", "117910c39dea67791cbff545ceba9b43dd98a263", "1300162100517d5b1c8089a5a3c0f65106400833", "1e1542a0f9ce1e3807aad47eb387f24ab47def9c", "1300162100517d5b1c8089a5a3c0f65106400833", "d6bfc777d9753ea18f994f70fe5dbc24c9ec3df7", "9fa8d73e572c3ca824a04a5f551b602a17831bc5"]},{"id": "f29bee88bcf08c805bbef6e43bcf9ac022a3e713", "title": "A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks", "authors": ["Hermann Georg Mayer", "Faustino J. Gomez", "Jürgen Schmidhuber"], "date": "2008", "abstract": "Tying suture knots is a time-consuming task performed frequently during Minimally Invasive Surgery (MIS). Automating this task could greatly reduce total surgery time for patients. Current solutions to this problem replay manually programmed trajectories, but a more general and robust approach is to use supervised machine learning to smooth surgeon-given training trajectories and generalize from them. Since knottying generally requires a controller with internal memory to distinguish between… ", "references": []},{"id": "d4f00f97581d30e7104d1fe924085248093aa472", "title": "Large Vocabulary SOUL Neural Network Language Models", "authors": ["Hai Son Le", "Ilya Oparin", "François Yvon"], "date": "INTERSPEECH", "abstract": "This paper presents continuation of research on Structured OUtput Layer Neural Network language models (SOUL NNLM) for automatic speech recognition. As SOUL NNLMs allow estimating probabilities for all in-vocabulary words and not only for those pertaining to a limited shortlist, we investigate its performance on a large-vocabulary task. Significant improvements both in perplexity and word error rate over conventional shortlist-based NNLMs are shown on a challenging Arabic GALE task… ", "references": ["47e3d8a1f8e92923e739ca34bea17004a40514e9", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "bcf7767ea4c39bb68ffad4edd7355e46dae92181", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "9819b600a828a57e1cde047bbe710d3446b30da5", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb"]},{"id": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "title": "Linguistic Regularities in Continuous Space Word Representations", "authors": ["Wen-tau Yih", "Geoffrey Zweig"], "date": "HLT-NAACL", "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks.", "references": ["5eb1a272f9933a11d113cf63fe659e073942bce5", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "dbb3b9c94129fe7a29cfdbd97f0ad5b5224ae246", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "f2f72cfb48d15d4d2bd1e91a92e7f3ac8635d433", "c808559b91ff60019b5a8cd49fbe3b2be1a7264b", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "c808559b91ff60019b5a8cd49fbe3b2be1a7264b"]},{"id": "030ba5a03666bf4c3a17c64699f8de8ec13d623b", "title": "Bridging Long Time Lags by Weight Guessing and \\long Short Term Memory\"", "authors": ["Corso Elvezia"], "date": "1996", "abstract": "Numerous recent papers (including many NIPS papers) focus on standard recurrent nets' inability to deal with long time lags between relevant input signals and teacher signals. Rather sophisticated, alternative methods were proposed. We rst show: problems used to promote certain algorithms in numerous previous papers can be solved more quickly by random weight guessing than by the proposed algorithms. This does not mean that guessing is a good algorithm. It just casts doubt on whether the other… ", "references": ["44d2abe2175df8153f465f6c39b68b76a0d40ab9", "13369d124474b5f8dcbc70d12296a185832192b2", "b158a006bebb619e2ea7bf0a22c27d45c5d19004", "13369d124474b5f8dcbc70d12296a185832192b2", "f91154c0d159a3f2dd3638915db32c5914544273", "f91154c0d159a3f2dd3638915db32c5914544273", "13369d124474b5f8dcbc70d12296a185832192b2", "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "32e97eef94beacace020e79322cef0e1e5a76ee0", "2d422e22085be42216bd4f1b5840600567d8bf4a"]},{"id": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality", "authors": ["Ilya Sutskever", "Jeffrey Dean"], "date": "2013", "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the… ", "references": ["330da625c15427c6e42ccfa3b747fb29e5835bf0", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "27e38351e48fe4b7da2775bf94341738bc4da07e", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "9606ff5bdeb6b9bde63c5bf6ad22edeca51d35db"]},{"id": "aed054834e2c696807cc8b227ac7a4197196e211", "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies", "authors": ["Sepp Hochreiter", "Yoshua Bengio"], "date": "2001", "abstract": "D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\\M[ X N@]_^O\\`JaNcb V RcQ W d EGKeL(^(QgfhKeLOE?i)^(QSj ETNPfPQkRl[ V R)m\"[ X ^(KeLOEG^ npo qarpo m\"[ X ^(KeLOEG^tsAu EGNPb V ^ v wyx zlwO{(|(}<~OC}((xp{ay.~A}_~ Cl3#|<Azw#|l6 (|  JpfhL XV EG^O QgJ  ETFOR] ^O\\JNPb V RcQ X E)ETR 6EGKeLOETNcKMLOE F ETN V RcQgJp^(^OE ZgZ E i ^(Qkj EGNPfhQSRO E OE2m1Jp^ RcNY E VZ sO! ¡ q.n sCD X KGKa8¢EG^ RPNhE¤£ ¥¦Q ZgZ Es m§J^ RPNO E VZ s( ̈ X  EG©#EKas# V ^ V  V… ", "references": ["e141d68065ce638f9fc4f006eab2f66711e89768", "b1ae0fb208fd389d2ff723e5442f9ca7896cb0a4", "4e2b855834df1f800414aa4950a22deae63577d6", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "b1ae0fb208fd389d2ff723e5442f9ca7896cb0a4", "4e2b855834df1f800414aa4950a22deae63577d6", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "4e2b855834df1f800414aa4950a22deae63577d6", "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "4e2b855834df1f800414aa4950a22deae63577d6"]},{"id": "c6867b6b564462d6b902f68e0bfa58f4717ca1cc", "title": "Fast Exact Multiplication by the Hessian", "authors": ["Barak A. Pearlmutter"], "date": "1994", "abstract": "Just storing the Hessian H (the matrix of second derivatives 2E/wiwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv{f(w)} = (/r)f(w rv)|r=0, note that Rv{w} = Hv and Rv{w} = v, and then apply Rv{} to the equations used… ", "references": ["a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "384094cff75cfa240d5acfe24cf340242c364847", "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "2a1e1da81b535e1bead3fc2ab6af8b07877823b9", "2f4a097b2131784d7ac3fc3c47d1e9283e9ac207", "934e49dac717a924bfda841bf6e54c32e900f0d1", "debfde8d6cd86cf61b50e9824cb2ff6bafecd507", "8fb8286c3652a46e601b6050deb0654339ab997d", "abc8a30694deda46c150d4da277aec291878cfeb", "abc8a30694deda46c150d4da277aec291878cfeb"]},{"id": "4c46347fbc272b21468efe3d9af34b4b2bad6684", "title": "Deep learning via Hessian-free optimization", "authors": ["James Martens"], "date": "ICML", "abstract": "We develop a 2nd-order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible… ", "references": ["43c8a545f7166659e9e21c88fe234e0323855216", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e"]},{"id": "14cb95cdc52b783fb8972a13b48081d8cdf81453", "title": "Projecting Parameters for Multilingual Word Sense Disambiguation", "authors": ["Mitesh M. Khapra", "Sapan Shah", "Pushpak Bhattacharyya"], "date": "EMNLP", "abstract": "We report in this paper a way of doing Word Sense Disambiguation (WSD) that has its origin in multilingual MT and that is cognizant of the fact that parallel corpora, wordnets and sense annotated corpora are scarce resources. With respect to these resources, languages show different levels of readiness; however a more resource fortunate language can help a less resource fortunate language. Our WSD method can be applied to a language even when no sense tagged corpora for that language is… ", "references": ["8504e70d21c443a538f2758a3e6f8f8fd2c4a173", "80b9c99a6dea12b6d0cf016c523b4c8ab698574e", "ab7b5917515c460b90451e67852171a531671ab8", "312e6b42a75b2dc510aa30924da4ea7018697deb", "b376435766a763fdc112d313430ca3facac057b2", "d4d8ba3d397cdf3bf4fbac430165d573dfbecae9", "8504e70d21c443a538f2758a3e6f8f8fd2c4a173", "b376435766a763fdc112d313430ca3facac057b2", "ab7b5917515c460b90451e67852171a531671ab8", "8504e70d21c443a538f2758a3e6f8f8fd2c4a173"]},{"id": "c45d9d930543934efef0b7501596c5effd1857dd", "title": "Report of NEWS 2010 Transliteration Mining Shared Task", "authors": ["A. Kumaran", "Mitesh M. Khapra", "Haizhou Li"], "date": "NEWS@ACL", "abstract": "This report documents the details of the Transliteration Mining Shared Task that was run as a part of the Named Entities Workshop (NEWS 2010), an ACL 2010 workshop. The shared task featured mining of name transliterations from the paired Wikipedia titles in 5 different language pairs, specifically, between English and one of Arabic, Chinese, Hindi Russian and Tamil. Totally 5 groups took part in this shared task, participating in multiple mining tasks in different languages pairs. The… ", "references": ["a0f9f3c338dd84b054dabcbd50b725f2b3609ad9", "a0f9f3c338dd84b054dabcbd50b725f2b3609ad9", "dab80f4725ce56d67d51969d6d50a3f58c901c12", "dab80f4725ce56d67d51969d6d50a3f58c901c12", "0bb396e509b2b6c324c4c2eda0a13c70fcdd3274", "dab80f4725ce56d67d51969d6d50a3f58c901c12", "3092a4929bdb3d6a8fe53f162586b7431b5ff8a4", "a8c74d482809ca8da4c406068d2f3d466ffba730", "9c81d86517e76a1f38fb6e126d09385aaa8ba8bc", "12e3151e1231537f25e38144d6350cb09bfb3586"]},{"id": "70f89905a695b7a3bde394f1ebac4a28e5458f8b", "title": "Transliteration Equivalence Using Canonical Correlation Analysis", "authors": ["Raghavendra Udupa", "Mitesh M. Khapra"], "date": "ECIR", "abstract": "We address the problem of Transliteration Equivalence, i.e. determining whether a pair of words in two different languages (e.g.Auden, ऑडन) are name transliterations or not. This problem is at the heart of Mining Name Transliterations (MINT) from various sources of multilingual text data including parallel, comparable, and non-comparable corpora and multilingual news streams. MINT is useful in several cross-language tasks including Cross-Language Information Retrieval (CLIR), Machine… ", "references": []},{"id": "6bf0414dae4f10c7e54fb9e5e8af5d0d0cab290b", "title": "A Generative Process for Contractive Auto-Encoders", "authors": ["Salah Rifai", "Yann Dauphin", "Yoshua Bengio"], "date": "ICML", "abstract": "The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating… ", "references": ["f93844c68d96f2f01da973b2ed3c236c8a369e57", "ff32cebbdb8a436ccd8ae797647428615ae32d74", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "195d0a8233a7a46329c742eaff56c276f847fadc", "ff32cebbdb8a436ccd8ae797647428615ae32d74", "ff32cebbdb8a436ccd8ae797647428615ae32d74", "dc1d132c79a72dba386dc47750a49b0c29b54568", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "f93844c68d96f2f01da973b2ed3c236c8a369e57"]},{"id": "382c5a5f938d426fb7734994933804b5b8b3fddd", "title": "Statistical Language Modeling Using Leaving-One-Out", "authors": ["Hermann Ney", "Sven Martin", "Frank Wessel"], "date": "1997", "abstract": "The need for a stochastic language model in speech recognition arises from Bayes’ decision rule for minimum error rate (Bahl et al., 1983). The word sequence w1 ... w N to be recognized from the sequence of acoustic observations x 1 ... x T is determined as that word sequence w 1 ... w N for which the posterior probability Pr(w 1 ... w N |x 1 ... x T ) attains its maximum. This rule can be rewritten in the form: \n \n$$ \\mathop {\\arg \\,\\max }\\limits_{{\\omega _1} \\cdot \\cdot \\cdot {\\omega _N… ", "references": []},{"id": "80e9e3fc3670482c1fee16b2542061b779f47c4f", "title": "Multimodal Deep Learning", "authors": ["Jiquan Ngiam", "Aditya Khosla", "Andrew Y. Ng"], "date": "ICML", "abstract": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio.", "references": ["b3852f0113fcf8a3913c55ae92393ae6ccde347e", "704d9fbe2b36b5187abc6982f666558bdd4cf386", "48c6b1d7fd3d9b5e170481c255aaab8bd66d40ab", "dd70cd5c716b7c484f4a9833cf61453e1e70d387", "b3852f0113fcf8a3913c55ae92393ae6ccde347e", "fa9816564cdb09ee7a581ec962e8a348421802a0", "9408ff9e3e826c22461f2e3b488dbfc760e9981e", "9408ff9e3e826c22461f2e3b488dbfc760e9981e", "fa9816564cdb09ee7a581ec962e8a348421802a0", "704d9fbe2b36b5187abc6982f666558bdd4cf386"]},{"id": "4d0050f220b755fe3319fe1e7011b6796c7ad2ea", "title": "Experiments on the implementation of recurrent neural networks for speech phone recognition", "authors": ["Ruxin Chen", "L. Jamieson"], "date": "1996", "abstract": "This paper reports on an extensive set of experiments that explore training methods and criteria for recurrent neural networks (RNNs) used for speech phone recognition. Seven different criterion functions are evaluated for speech recognition. A new criterion function that allows direct minimization of the frame error rate is proposed. Two new optimization methods for RNN weight updating are investigated. Experiments have been carried out on the Intel Paragon parallel processing system. The… ", "references": []},{"id": "8a51dc0b5694af3e54393e20e05f42fc3cbe476b", "title": "Rapid Retraining on Speech Data with LSTM Recurrent Networks.", "authors": ["Alex Graves", "Nicole Beringer", "Jürgen Schmidhuber"], "date": "2005", "abstract": "A system that could be quickly retrained on different corpora would be of great benefit to speech recognition. Recurrent Neural Networks (RNNs) are able to transfer knowledge by simply storing and then retraining their weights. In this report, we partition the TIDIGITS database into utterances spoken by men, women, boys and girls, and successively retrain a Long Short Term Memory (LSTM) RNN on them. We find that the network rapidly adapts to new subsets of the data, and achieves greater… ", "references": ["71b93e4fd006c09ad127d27e963021fcbdbd95d8", "4e8b257ec0d4d6ea9fc7eb9bcac98f7012720f32"]},{"id": "884689d18aea6ac49f2fa13361637eb9092cca00", "title": "Use of contexts in language model interpolation and adaptation", "authors": ["Xunying Liu", "Mark John Francis Gales", "Philip C. Woodland"], "date": "2009", "abstract": "Language models (LMs) are often constructed by building multiple individual component models that are combined using context independent interpolation weights. By tuning these weights, using either perplexity or discriminative approaches, it is possible to adapt LMs to a particular task. This paper investigates the use of context dependent weighting in both interpolation and test-time adaptation of language models. Depending on the previous word contexts, a discrete history weighting function… ", "references": ["7fa7e3109c481a5f7dc753f8bf4beaa908776b87", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "0f8ecdb1083d6b1a41a42ff6c4f253c26acb03bb", "792bfb7b24bcfd12fd2c15089d99df8740817956", "792bfb7b24bcfd12fd2c15089d99df8740817956", "aca5ff2f303899f99337e76214a912fb49201b35", "dae559d16c0795d1f76bf22a2690a30c7da08296", "0f8ecdb1083d6b1a41a42ff6c4f253c26acb03bb", "e4efb4b4c07d02b4031111b1cb97a7a13b5c928a", "bb503b24836b9bcbd231352c336302595104d881"]},{"id": "93c1268cc00bf0fe4ed7a7a5e2d2f272988baadf", "title": "Phoneme boundary estimation using bidirectional recurrent neural networks and its applications", "authors": ["Toshiaki Fukada", "Mike Schuster", "Yoshinori Sagisaka"], "date": "1999", "abstract": "This paper describes a phoneme boundary estimation method based on bidirectional recurrent neural networks (BRNNs). Experimental results showed that the proposed method could estimate segment boundaries significantly better than an HMM or a multilayer perceptron-based method. Furthermore, we incorporated the BRNN-based segment boundary estimator into the HMM-based and segment model-based recognition systems. As a result, we confirmed that (1) BRNN outputs were effective for improving the… ", "references": ["29abffb7bf0e428c8b48532181ad85d435c22b16", "60e352a65bfe8d10dac98313dc944d7120754acd", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "96df58d41e7c9ecc711fcab4cb2610867772d1bc", "cde29fc637c39d41e5448bc6a290359d5479e2ef", "8658d645b716d59c5edc80e33e1936e33574bc26", "29abffb7bf0e428c8b48532181ad85d435c22b16", "8658d645b716d59c5edc80e33e1936e33574bc26", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "96df58d41e7c9ecc711fcab4cb2610867772d1bc"]},{"id": "4c7fc0ca5bec117b75c7f4fc9c8b45579569abda", "title": "Biologically Plausible Speech Recognition with LSTM Neural Nets", "authors": ["Alex Graves", "Douglas Eck", "Jürgen Schmidhuber"], "date": "BioADIT", "abstract": "Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) are local in space and time and closely related to a biological model of memory in the prefrontal cortex. Not only are they more biologically plausible than previous artificial RNNs, they also outperformed them on many artificially generated sequential processing tasks. This encouraged us to apply LSTM to more realistic problems, such as the recognition of spoken digits. Without any modification of the underlying algorithm, we… ", "references": ["a4ffd2f5dd98ee744c013060c5bc06503336d931", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "f828b401c86e0f8fddd8e77774e332dfd226cb05", "71b93e4fd006c09ad127d27e963021fcbdbd95d8", "3d82e058a5c40954b8f5db170a298a889a254c37", "4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "71b93e4fd006c09ad127d27e963021fcbdbd95d8", "f828b401c86e0f8fddd8e77774e332dfd226cb05", "71b93e4fd006c09ad127d27e963021fcbdbd95d8"]},{"id": "bec62424fdb3633f3478216638671a2c78f6d833", "title": "Phase-Space Learning", "authors": ["Fu-Sheng Tsung", "Garrison W. Cottrell"], "date": "NIPS", "abstract": "Existing recurrent net learning algorithms are inadequate. We introduce the conceptual framework of viewing recurrent training as matching vector fields of dynamical systems in phase space. Phase-space reconstruction techniques make the hidden states explicit, reducing temporal learning to a feed-forward problem. In short, we propose viewing iterated prediction [LF88] as the best way of training recurrent networks on deterministic signals. Using this framework, we can train multiple… ", "references": ["8f3754263c918ef8e385e3ddba92584ed7592207", "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "03ac166407838478d33a4603f6f27e047a513d64", "5146d7902132bcb0b2e6fe5f607358768fc47323", "cbd1ade5b869b13d1853aa0753b82fb35c26bcba", "9b6c5f9cd78725a3534132eb01a3b3adb8d6bb39", "9b6c5f9cd78725a3534132eb01a3b3adb8d6bb39", "8f3754263c918ef8e385e3ddba92584ed7592207", "2942bccd646603453ba53c07d5091d9bcd644377", "8f3754263c918ef8e385e3ddba92584ed7592207"]},{"id": "685d42a668413422615519a52ac75d66fded4611", "title": "Framewise phoneme classification with bidirectional LSTM networks", "authors": ["Alec Graves", "Jürgen Schmidhuber"], "date": "2005", "abstract": "In this paper, we apply bidirectional training to a long short term memory (LSTM) network for the first time. We also present a modified, full gradient version of the LSTM learning algorithm. We discuss the significance of framewise phoneme classification to continuous speech recognition, and the validity of using bidirectional networks for online causal tasks. On the TIMIT speech database, we measure the framewise phoneme classification scores of bidirectional and unidirectional variants of… ", "references": ["e65406cbc62880767eb4a4ba050799da989661b6", "4d0050f220b755fe3319fe1e7011b6796c7ad2ea", "c49019464e326899e76be358a86b8706ee20d0ef", "4d0050f220b755fe3319fe1e7011b6796c7ad2ea", "4d0050f220b755fe3319fe1e7011b6796c7ad2ea", "4c7fc0ca5bec117b75c7f4fc9c8b45579569abda", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "4d0050f220b755fe3319fe1e7011b6796c7ad2ea", "047655e733a9eed9a500afd916efa566915b9110", "e65406cbc62880767eb4a4ba050799da989661b6"]},{"id": "f6cde498cfaf7b862da3fa358caee9749e65ed3d", "title": "Learning the Initial State of a Second-Order Recurrent Neural Network during Regular-Language Inference", "authors": ["Mikel L. Forcada", "Rafael C. Carrasco"], "date": "1995", "abstract": "Recent work has shown that second-order recurrent neural networks (2ORNNs) may be used to infer regular languages. This paper presents a modified version of the real-time recurrent learning (RTRL) algorithm used to train 2ORNNs, that learns the initial state in addition to the weights. The results of this modification, which adds extra flexibility at a negligible cost in time complexity, suggest that it may be used to improve the learning of regular languages when the size of the network is… ", "references": []},{"id": "f828b401c86e0f8fddd8e77774e332dfd226cb05", "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages", "authors": ["Felix A. Gers", "Jürgen Schmidhuber"], "date": "2001", "abstract": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n). ", "references": []},{"id": "850e994c086e95c8d8c2ba3c90e53104a0fa709e", "title": "Learning the Long-Term Structure of the Blues", "authors": ["Douglas Eck", "Jürgen Schmidhuber"], "date": "ICANN", "abstract": "In general music composed by recurrent neural networks (RNNs) suffers from a lack of global structure.", "references": ["0b54a31620ff2bc2f73d761932906d5d3be5bd1f", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "81e0a57abde1bf2cc4b7cd772e0573e92069e8ef"]},{"id": "da336ed15bd24c268cb2a09efe2aa4f298cda3ba", "title": "Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information", "authors": ["Sonja Nießen", "Hermann Ney"], "date": "2004", "abstract": "In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models. In particular, existing statistical systems for machine translation often treat different inflected forms of the same lemma as if they were independent of one another. The bilingual training data can be better exploited by explicitly taking into account the… ", "references": ["fc593d91a7974bb1d3fac1ffe47b787ce1853a88", "7526e9f65a24804a1686ab4a39f0e3a5598f4dae", "3dffe5ebf00f10dd137beff00d94952f1af658c3", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "e4d3bf856ce5259360a8033d50abcdd22873bcd6", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "e4d3bf856ce5259360a8033d50abcdd22873bcd6", "fc593d91a7974bb1d3fac1ffe47b787ce1853a88"]},{"id": "fc43ce300875906274ae9f40a1b437374703d0cf", "title": "Toward hierarchical models for statistical machine translation of inflected languages", "authors": ["Sonja Nießen", "Hermann Ney"], "date": "DDMMT@ACL", "abstract": "In statistical machine translation, correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models. Existing statistical systems for MT often treat different derivatives of the same lemma as if they were independent of each other. In this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the interdependencies of the different derivatives… ", "references": ["e7feffbba87f227e3de9244dbb75d39bd707d70a", "e7feffbba87f227e3de9244dbb75d39bd707d70a", "15315ed05451c88f83c50d56a66a0b85517c5f4f", "e7feffbba87f227e3de9244dbb75d39bd707d70a", "15315ed05451c88f83c50d56a66a0b85517c5f4f", "fc593d91a7974bb1d3fac1ffe47b787ce1853a88", "fc593d91a7974bb1d3fac1ffe47b787ce1853a88", "0991b1a89f9046cdc37e1db1f3f8d2d56b00162c", "c9214ebe91454e6369720136ab7dd990d52a07d4", "2b91a3954cf5c0d44344eb5b87111e95cfa7a6c5"]},{"id": "368a08ef108193ebe12f568b79594e4840a47444", "title": "Investigation into bottle-neck features for meeting speech recognition", "authors": ["Frantisek Grézl", "Martin Karafiát", "Lukás Burget"], "date": "INTERSPEECH", "abstract": "This work investigates into recently proposed Bottle-Neck features for ASR. The bottle-neck ANN structure is imported into Split Context architecture gaining significant WER reduction. Further, Universal Context architecture was developed which simplifies the system by using only one universal ANN for all temporal splits. Significant WER reduction can be obtained by applying fMPE on top of our BN features as a technique for discriminative feature extraction and further gain is also obtained by… ", "references": ["2e798df4d35beaac0b8968f44e142ae50bc43ca9", "f5e1e7df1497a133e478d2756742033187a180cf", "9d6589c63f0e5697f698904824006b0801174119", "2e798df4d35beaac0b8968f44e142ae50bc43ca9", "22b496b7c63e89b51ca2f905195339e03c9deb46", "547fbce9f33d6944970a2e523f713782f2f7332c", "22b496b7c63e89b51ca2f905195339e03c9deb46", "137b572ca406ba8fc86e985c185233b8cd6517d2", "137b572ca406ba8fc86e985c185233b8cd6517d2", "01d90d7dea0a7fbf67f8ba935e4bcdb0576902f9"]},{"id": "4b7ec490154397c2691d3404eccd412665fa5e6a", "title": "N-gram-based Machine Translation", "authors": ["José B. Mariño", "Rafael E. Banchs", "Marta R. Costa-jussà"], "date": "2006", "abstract": "This article describes in detail an n-gram approach to statistical machine translation. This approach consists of a log-linear combination of a translation model based on n-grams of bilingual units, which are referred to as tuples, along with four specific feature functions. Translation performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS). ", "references": ["74dfa2408023ac8033dbea83fad1c312f644a960", "c9214ebe91454e6369720136ab7dd990d52a07d4", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "35d51d505dccc70846ff73e616f32b0819577903", "223dcd0e44532fc02444709e61327432c74fe46d", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "a8515ffcd5818b68b33762fb7987c9d583f8a582", "a8515ffcd5818b68b33762fb7987c9d583f8a582", "74dfa2408023ac8033dbea83fad1c312f644a960", "35d51d505dccc70846ff73e616f32b0819577903"]},{"id": "223dcd0e44532fc02444709e61327432c74fe46d", "title": "Phrase-Based Statistical Machine Translation", "authors": ["Richard Zens", "Franz Josef Och", "Hermann Ney"], "date": "KI", "abstract": "This paper is based on the work carried out in the framework of the VERBMOBIL project, which is a limited-domain speech translation task (German-English.", "references": ["e7feffbba87f227e3de9244dbb75d39bd707d70a", "8a1fdc0d36ef7fc3b8ce18d496776e948a047dc3", "e5dbe9cccaf045d959a8e466803d4aaa58ac3fcf", "8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "c447c0cb2673037633f71faf8ccf4f89806ba1b0", "ab7b5917515c460b90451e67852171a531671ab8", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "8a1fdc0d36ef7fc3b8ce18d496776e948a047dc3", "e5dbe9cccaf045d959a8e466803d4aaa58ac3fcf"]},{"id": "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "title": "Structured language modeling", "authors": ["Ciprian Chelba", "Frederick Jelinek"], "date": "2000", "abstract": "This paper presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood re-estimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal and… ", "references": ["8648dbfff9662fa9c62a95622712dd2951b5b3a3", "5bfa91e7ec19c6401a763c73f2a2007c04836609", "de92006681796ca5a0b5ed044cff47488e98be92", "851bce6405b781079359498bfd6237b95d3acc6c", "673992da19d9209434615b12d55bdd36be706e9e", "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "d58244ed9b86e9ad7f90cb302d32e5f96a72d040", "8648dbfff9662fa9c62a95622712dd2951b5b3a3", "3764baa7465201f054083d02b58fa75f883c4461", "673992da19d9209434615b12d55bdd36be706e9e"]},{"id": "b158a006bebb619e2ea7bf0a22c27d45c5d19004", "title": "LSTM can Solve Hard Long Time Lag Problems", "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber"], "date": "NIPS", "abstract": "Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of. ", "references": ["32e97eef94beacace020e79322cef0e1e5a76ee0", "50c770b425a5bb25c77387f687a9910a9d130722", "9c5943e53c334a5dff1bb08a62cd2f64ab1fac41", "50c770b425a5bb25c77387f687a9910a9d130722", "29085cdffb3277c1c8fd10ac09e0d89452c8db83", "e141d68065ce638f9fc4f006eab2f66711e89768", "50c770b425a5bb25c77387f687a9910a9d130722", "50c770b425a5bb25c77387f687a9910a9d130722", "e141d68065ce638f9fc4f006eab2f66711e89768", "29085cdffb3277c1c8fd10ac09e0d89452c8db83"]},{"id": "5a03eea43e128f49218ed95b909da1136c757e57", "title": "A Study on Richer Syntactic Dependencies for Structured Language Modeling", "authors": ["Peng Xu", "Ciprian Chelba", "Frederick Jelinek"], "date": "ACL", "abstract": "We study the impact of richer syntactic dependencies on the performance of the structured language model (SLM) along three dimensions: parsing accuracy (LP/LR), perplexity (PPL) and word-error-rate (WER, N-best re-scoring). We show that our models achieve an improvement in LP/LR, PPL and/or WER over the reported baseline results using the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively. Analysis of parsing performance shows correlation between the quality of the… ", "references": ["54c846ee00c6132d70429cc279e8577f63ed05e4", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "6c9f553e723a40a6713453b734b552c1928bf52b", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "a63540c6eefdae0ac555bdd8a9bda7afea918974", "8db39e88142e9d539f1b6554764e400768ade9af", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "6c9f553e723a40a6713453b734b552c1928bf52b"]},{"id": "0ccb664faaf4221dfe20c5b321d017ce33af3fec", "title": "Just-in-time language modelling", "authors": ["Adam Berger", "Robert Miller"], "date": "1998", "abstract": "Traditional approaches to language modelling have relied on a fixed corpus of text to inform the parameters of a probability distribution over word sequences. Increasing the corpus size often leads to better-performing language models, but no matter how large, the corpus is a static entity, unable to reflect information about events which postdate it. We introduce an online paradigm which interleaves the estimation and application of a language model. We present a Bayesian approach to online… ", "references": ["b951b9f78b98a186ba259027996a48e4189d37e5", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715", "b951b9f78b98a186ba259027996a48e4189d37e5", "fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87", "b951b9f78b98a186ba259027996a48e4189d37e5", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715", "fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87", "fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87", "835cbdde59ed3c7e8c4bb8d9e4bb242b0e475286", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715"]},{"id": "d4e81f4fd59723dcb08b1ec6ef30cb115eafda1c", "title": "Variational approximation of long-span language models for lvcsr", "authors": ["Anoop Deoras", "Sanjeev Khudanpur"], "date": "2011", "abstract": "Long-span language models that capture syntax and semantics are seldom used in the first pass of large vocabulary continuous speech recognition systems due to the prohibitive search-space of sentence-hypotheses. Instead, an N-best list of hypotheses is created using tractable n-gram models, and rescored using the long-span models. It is shown in this paper that computationally tractable variational approximations of the long-span models are a better choice than standard n-gram models for first… ", "references": ["9819b600a828a57e1cde047bbe710d3446b30da5", "927c25cdd384e8f39ed7db7ab1558eb7fd8f048c", "fc999072ce188ee1d57b6bb744cb276b09a491bb", "9819b600a828a57e1cde047bbe710d3446b30da5", "f2d7c5b0412ce5da4cb49b138d277a1c345912eb", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "927c25cdd384e8f39ed7db7ab1558eb7fd8f048c", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "927c25cdd384e8f39ed7db7ab1558eb7fd8f048c", "927c25cdd384e8f39ed7db7ab1558eb7fd8f048c"]},{"id": "3bb45466dfb9770e706d1e63205e266e7761f915", "title": "Training Connectionist Models for the Structured Language Model", "authors": ["Peng Xu", "Ahmad Emami", "Frederick Jelinek"], "date": "EMNLP", "abstract": "We investigate the performance of the Structured Language Model (SLM) in terms of perplexity (PPL) when its components are modeled by connectionist models. The connectionist models use a distributed representation of the items in the history and make much better use of contexts than currently used interpolated or back-off models, not only because of the inherent capability of the connectionist model in fighting the data sparseness problem, but also because of the sublinear growth in the model… ", "references": ["09c76da2361d46689825c4efc37ad862347ca577", "09c76da2361d46689825c4efc37ad862347ca577", "5a03eea43e128f49218ed95b909da1136c757e57", "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "5a03eea43e128f49218ed95b909da1136c757e57", "09c76da2361d46689825c4efc37ad862347ca577", "5a03eea43e128f49218ed95b909da1136c757e57", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "09c76da2361d46689825c4efc37ad862347ca577"]},{"id": "343c8af478f7703459b0e390e888efe723f15e31", "title": "Probabilistic Models of Short and Long Distance Word Dependencies in Running Text", "authors": ["Julian Kupiec"], "date": "HLT", "abstract": "This article describes two complementary models that represent dependencies between words in local and non-local contexts. The type of local dependencies considered are sequences of part of speech categories for words. The non-local context of word dependency considered here is that of word recurrence, which is typical in a text. Both are models of phenomena that are to a reasonable extent domain independent, and thus are useful for doing prediction in systems using large vocabularies. ", "references": ["6a923c9f89ed53b6e835b3807c0c1bd8d532687b", "090f3ea5bc188bbb03aec02aba9ed9c7b38ff870", "9b5c1d0844714588cf59629cbbc8e5f2e01f4a15", "aca56c819acc4561f008efa21a32ae2ed6d3c820", "157babe1832490bce4f46d7302f0d9d6a74a2a5b", "9b5c1d0844714588cf59629cbbc8e5f2e01f4a15", "aca56c819acc4561f008efa21a32ae2ed6d3c820", "157babe1832490bce4f46d7302f0d9d6a74a2a5b", "9b5c1d0844714588cf59629cbbc8e5f2e01f4a15", "aca56c819acc4561f008efa21a32ae2ed6d3c820"]},{"id": "491566891addc26134c617ab026f5548de39401a", "title": "Speech recongnition and the frequency of recently used words: a modified Markov model for natural language", "authors": ["Roland Kuhn"], "date": "COLING", "abstract": "Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary. A class of Markov language models identified by Jelinek has achieved considerable success in this domain. A modification of the Markov approach, which assigns higher probabilities to recently used words, is proposed and tested against a pure Markov model. Parameter calculation and comparison of the two models both involve use… ", "references": []},{"id": "5feb2c61b04532869e44d1ca4e48c7108aee5fd3", "title": "An extended model of natural logic", "authors": ["Bill MacCartney", "Christopher D. Manning"], "date": "IWCS", "abstract": "We propose a model of natural language inference which identifies valid inferences by their lexical and syntactic features, without full semantic interpretation.", "references": ["757a192157d7587433db93a899d32c4c5e832489", "3d0f57ebc7de2b83802f8633c21152100493c7c5", "1d9a43c277434ebdba3256db2b3b6695d2620a4f", "291e2f2b3f4dd1007d72b66256810344d0f0f74b", "1446d3f9b912a46ee1211e88408843745c751a4b", "6fb0fe906c6d90f8111f5026770e1bbf12975241", "757a192157d7587433db93a899d32c4c5e832489", "1d9a43c277434ebdba3256db2b3b6695d2620a4f", "3d0f57ebc7de2b83802f8633c21152100493c7c5", "1446d3f9b912a46ee1211e88408843745c751a4b"]},{"id": "360899707ca19389e1d33d9499591c371c755286", "title": "A Statistical Approach to Continuous Speech Recognition", "authors": ["Lalit R. Bahl", "Frederick Jelinek", "Robert L. Mercer"], "date": "1983", "abstract": null, "references": []},{"id": "455afd748e8834ef521e4b67c7c056d3c33429e2", "title": "Hierarchical Attention Networks for Document Classification", "authors": ["Zichao Yang", "Diyi Yang", "Eduard H. Hovy"], "date": "HLT-NAACL", "abstract": "We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that… ", "references": ["2c1890864c1c2b750f48316dc8b650ba4772adc5", "ecb5336bf7b54a62109f325e7152bb74c4c7f527", "b21c78a62fbb945a19ae9a8935933711647e7d70", "f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "b21c78a62fbb945a19ae9a8935933711647e7d70", "ecb5336bf7b54a62109f325e7152bb74c4c7f527", "b21c78a62fbb945a19ae9a8935933711647e7d70", "7e8d5a108c28cdfb92f419ce919fbf7993dfebfc", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba"]},{"id": "11ec56898a9e7f401a2affe776b5297bd4e25025", "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment", "authors": ["Marco Marelli", "Luisa Bentivogli", "Roberto Zamparelli"], "date": "SemEval@COLING", "abstract": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness… ", "references": ["ac5637276be614f5c89bd8fe2ed82f412568eba1", "5f3c2557d8ae7b9025535ee021c545e2d6a92cf0", "5f3c2557d8ae7b9025535ee021c545e2d6a92cf0", "d9daa826f5989be59be27d7ca9f05401e12ab9da", "3276b9487b2336f662488f2a180622f3bcac6e82", "ac5637276be614f5c89bd8fe2ed82f412568eba1", "5f3c2557d8ae7b9025535ee021c545e2d6a92cf0", "ac5637276be614f5c89bd8fe2ed82f412568eba1", "3276b9487b2336f662488f2a180622f3bcac6e82", "3276b9487b2336f662488f2a180622f3bcac6e82"]},{"id": "287d5571dbf255a7ccbdc2bcfe9211fd8f0b2a7c", "title": "Finding Contradictions in Text", "authors": ["Marie-Catherine de Marneffe", "Anna N. Rafferty", "Christopher D. Manning"], "date": "ACL", "abstract": "Detecting conflicting statements is a foundational text understanding task with applications in information analysis.", "references": ["37c5bd3db1444d6ad331b876fbccb6e009eef44d", "757a192157d7587433db93a899d32c4c5e832489", "5ffa3aea748533186b6638d97eafe80d86b208b2", "095fc8aac545cc280199fd2770b46ea54ed6c720", "89789c61fa76ff34d5b265f04b4fb04b04b1413d", "89789c61fa76ff34d5b265f04b4fb04b04b1413d", "89789c61fa76ff34d5b265f04b4fb04b04b1413d", "89789c61fa76ff34d5b265f04b4fb04b04b1413d", "095fc8aac545cc280199fd2770b46ea54ed6c720", "318e9eb50bb879f01efb7b4ed0482c8e458b6e8c"]},{"id": "ea2563467c1c472a346d165b7f97c86317d63ca4", "title": "Recognising Textual Entailment with Logical Inference", "authors": ["Johan Bos", "Katja Markert"], "date": "HLT/EMNLP", "abstract": "We use logical inference techniques for recognising textual entailment.", "references": ["d9dc7d27ad31e977dec4f703bc75f699ae456f76", "084c55d6432265785e3ff86a2e900a49d501c00a", "77836624ea89c3abf54a2be4f8e7feef40eb7b45", "d9dc7d27ad31e977dec4f703bc75f699ae456f76", "77836624ea89c3abf54a2be4f8e7feef40eb7b45", "37c5bd3db1444d6ad331b876fbccb6e009eef44d", "3f32ceed415b05b6e0bd46c21239a1638b5b7e38", "dd5bfb029fb5fce0816fb606e33a9e7dff552154", "e333828f5aca95fcdb796c7924d16cbf1a5efebb", "26ae952599aa9ba5815a80356024258247fc2b10"]},{"id": "bfccb2d6e3d9f9b6bd8b14b2d4c6efa36c79341b", "title": "LSTM-based Deep Learning Models for non-factoid answer selection", "authors": ["Ming Tan", "Bing Xiang", "Bowen Zhou"], "date": "2015", "abstract": "In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools.", "references": ["4cfad7889dc12825309325cd4b4f3febed424e36", "84d2e6eb7772e2fa4fc0b14c28446d00370a45d2", "4cfad7889dc12825309325cd4b4f3febed424e36", "4cfad7889dc12825309325cd4b4f3febed424e36", "7be07436db9ad6f7aaf859aded74767e7d7e308d", "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "f33e86970c11f9bb6d0abb60acdc9274d5c3f342", "84d2e6eb7772e2fa4fc0b14c28446d00370a45d2", "828dbeb7cf922dc9b6657dd169b8d26d2b58eedb"]},{"id": "36c097a225a95735271960e2b63a2cb9e98bff83", "title": "A Fast Unified Model for Parsing and Sentence Understanding", "authors": ["Samuel R. Bowman", "Jon Gauthier", "Christopher Potts"], "date": "2016", "abstract": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suer from two key technical problems that make them slow and unwieldyforlarge-scaleNLPtasks: theyusually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducingtheStack-augmentedParser-Interpreter NeuralNetwork(SPINN),whichcombines parsing and interpretation within a single tree-sequence hybrid… ", "references": ["b78b881dec3334abf5b8c6390c40a2d93a177294", "d8ac015407cf68c695043b23d905cddd880e5844", "94960fcdf0ea3b346fca77ae8c63ae7943eb0d28", "df77714269f1f88182092f8535b1bc290fcd835d", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "04d1a26c2516dc14a765112a63ec60dc3cb3de72", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "04d1a26c2516dc14a765112a63ec60dc3cb3de72", "94960fcdf0ea3b346fca77ae8c63ae7943eb0d28", "32de44f01a96d4473d21099d15e25bc2b9f08e2f"]},{"id": "7c05a4ffee7e159e34b2efea7e44d994333ec628", "title": "Recursive Neural Networks Can Learn Logical Semantics", "authors": ["Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning"], "date": "2014", "abstract": "Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models---plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)---can correctly learn to identify logical relationships such as entailment and contradiction using these… ", "references": ["3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "687bac2d3320083eb4530bf18bb8f8f721477600", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "27e38351e48fe4b7da2775bf94341738bc4da07e", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "687bac2d3320083eb4530bf18bb8f8f721477600", "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "74fe7ec751cd50295b15cfd46389a8fefb37c414", "0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "32de44f01a96d4473d21099d15e25bc2b9f08e2f"]},{"id": "af56e6d4901dcd0f589bf969e604663d40f1be5d", "title": "A Block-sorting Lossless Data Compression Algorithm", "authors": ["Mike Burrows", "David J. Wheeler"], "date": "1994", "abstract": "The charter of SRC is to advance both the state of knowledge and the state of the art in computer systems.", "references": ["59c9f2036e673d8bc9713eed851d12c6c9fe53cb", "1337a72073e2c80da6e53b776ccb469d5abb0579", "1337a72073e2c80da6e53b776ccb469d5abb0579", "d7d07b2b2ca6e28561fa142fd6f2fd020bcb40a7", "5be4e0eccca2892d31406a03b0c485f7a395fe5a", "d7d07b2b2ca6e28561fa142fd6f2fd020bcb40a7", "1337a72073e2c80da6e53b776ccb469d5abb0579", "1337a72073e2c80da6e53b776ccb469d5abb0579", "59c9f2036e673d8bc9713eed851d12c6c9fe53cb", "59c9f2036e673d8bc9713eed851d12c6c9fe53cb"]},{"id": "79521a6d8814f9162ed1f7028e9e007c4df7181a", "title": "Sequential neural text compression", "authors": ["Jürgen Schmidhuber", "Stefan Heil"], "date": "1996", "abstract": "The purpose of this paper is to show that neural networks may be promising tools for data compression without loss of information. We combine predictive neural nets and statistical coding techniques to compress text files. We apply our methods to certain short newspaper articles and obtain compression ratios exceeding those of the widely used Lempel-Ziv algorithms (which build the basis of the UNIX functions \"compress\" and \"gzip\"). The main disadvantage of our methods is that they are about… ", "references": ["ac91740ae76ed9dbd853bddd6d1d9dc43bc55179", "2f7c4048a03281e976f28d35c2f9fef3a58346e6", "fd23c9168418324e81881365f297fb6a1caa3a07", "ae92215bff23ced8a5b8c631fefad2bf598d8d83", "ae92215bff23ced8a5b8c631fefad2bf598d8d83", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "63b6835c8fb31d91f503b8e08dff4bac8966c8cf", "ae92215bff23ced8a5b8c631fefad2bf598d8d83", "ff9d11a4745763bc1f70e5e5521641cc5ea44157", "ac91740ae76ed9dbd853bddd6d1d9dc43bc55179"]},{"id": "1f5d21625f8264f455591b3c7cbdac18b983b3c0", "title": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression", "authors": ["Ian H. Witten", "Timothy C. Bell"], "date": "1991", "abstract": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical… ", "references": []},{"id": "d98343b130f6a63b6276b3a3bdfa60dfdd17e859", "title": "The ISL meeting corpus: the impact of meeting type on speech style", "authors": ["Susanne Burger", "Victoria MacLaren", "Hua Yu"], "date": "INTERSPEECH", "abstract": "Speech research is becoming very interested in new application domains, such as meeting summarization and automatic transcription, and has thus begun to work with recorded meeting data. The following paper gives an overview of the meeting data collection at Interactive Systems Laboratories. There are currently over 100 meetings of different types recorded. An experiment is described which aimed at testing the possibility of controlling issues of speaking style by meeting type. Results show that… ", "references": ["03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395", "03856fbdb681403200dcb122ac519be9eaf8d395"]},{"id": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb", "title": "A maximum entropy approach to adaptive statistical language modelling", "authors": ["Ronald Rosenfeld"], "date": "1996", "abstract": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple… ", "references": ["b0130277677e5b915d5cd86b3afafd77fd08eb2e", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "914b06746d7305bcb5a38b6b4234e1b08f30a94b", "6e58b5f825df9fb0b00465a66598f302c30b080a", "0687165a9f0360bde0469fd401d966540e0897c3", "01a9a9686d45a3dc8182f59a1a77f1ac4f233761", "be9d87d39517729ba7e24c5fa46ed263e5b2f627", "49b2862ab73be40bf69ac3f457039f18d12df0ae", "6e58b5f825df9fb0b00465a66598f302c30b080a", "914b06746d7305bcb5a38b6b4234e1b08f30a94b"]},{"id": "ff7804a13efc26bf23ce813319641db69ddbb969", "title": "The ICSI Meeting Corpus", "authors": ["Adam Janin", "Don Baron", "Chuck Wooters"], "date": "2003", "abstract": "We have collected a corpus of data from natural meetings that occurred at the International Computer Science Institute (ICSI) in Berkeley, California over the last three years. The corpus contains audio recorded simultaneously from head-worn and table-top microphones, word-level transcripts of meetings, and various metadata on participants, meetings, and hardware. Such a corpus supports work in automatic speech recognition, noise robustness, dialog modeling, prosody, rich transcription… ", "references": ["dffbafb5c8d0ca39279f7f463765c991cd208895", "fe4ae6d90b0cdbedac3a9ca501d0d226508afaff", "8eb7e28ac41dcd32dbe839727889ecc54d236f5c", "8eb7e28ac41dcd32dbe839727889ecc54d236f5c", "84eb734fed7fb46359a4f6bd3f7482b742da0dc6", "df1694beda91747a28ae8528c1f85042c63b37d0", "df1694beda91747a28ae8528c1f85042c63b37d0", "84eb734fed7fb46359a4f6bd3f7482b742da0dc6", "dffbafb5c8d0ca39279f7f463765c991cd208895", "fe4ae6d90b0cdbedac3a9ca501d0d226508afaff"]},{"id": "f54aa90f53d81b43c1b8dd8ff38300cbd47cfac1", "title": "Hidden Markov Model Based Speech Activity Detection for the ICSI Meeting Project", "authors": ["Thilo Pfau", "Daniel P. W. Ellis"], "date": "2001", "abstract": "As part of a project into speech recognition in meeting environments, we have collected a corpus of multi-channel meeting recordings. We expected the identification of speaker activity to be straightforward given that the participants had individual microphones, but simple approaches yielded unacceptably erroneous labelings, mainly due to crosstalk between nearby speakers and wide variations in channel characteristics. We have therefore developed a more sophisticated approach for multichannel… ", "references": []},{"id": "084c55d6432265785e3ff86a2e900a49d501c00a", "title": "Foundations of statistical natural language processing", "authors": ["Christopher D. Manning", "Hinrich Schütze"], "date": "SGMD", "abstract": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own… ", "references": ["cb289d47d9333dbdb469375255ef12c17b76e860", "0c0eab87d4855c42ae6395bf2e27eefe55003b4a", "0f5524fd936d692beac6ab0bfcfcfa656cc3178b"]},{"id": "f9f91e7bac46b13444eddeb2438b01089e73b786", "title": "Tailoring Continuous Word Representations for Dependency Parsing", "authors": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "date": "ACL", "abstract": "Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of… ", "references": ["6833e89ffa90c7235b75e460b1563019918649a3", "dac72f2c509aee67524d3321f77e97e8eff51de6", "6833e89ffa90c7235b75e460b1563019918649a3", "dac72f2c509aee67524d3321f77e97e8eff51de6", "dac72f2c509aee67524d3321f77e97e8eff51de6", "e8db2b3051cc39af668a80b2ffd719d91820c58c", "e8db2b3051cc39af668a80b2ffd719d91820c58c", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "e8db2b3051cc39af668a80b2ffd719d91820c58c", "790ecefeaf2b471b439743a772ccce026131bef5"]},{"id": "1564b8ff799983f335dbf3bb7a5c92b689c4a19f", "title": "The AMI System for the Transcription of Speech in Meetings", "authors": ["Thomas Hain", "Lukás Burget", "Steve Renals"], "date": "2005", "abstract": "This paper describes the AMI transcription system for speech in meetings developed in collaboration by five research groups. The system includes generic techniques such as discriminative and speaker adaptive training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, maximum likelihood linear regression, and phone posterior based features, as well as techniques specifically designed for meeting data. These include segmentation and cross-talk suppression, beam… ", "references": ["5524e2559d4fd5dcf2f32caf5f15e6e064aec78b", "70e9033af1f8b32bcecd086714135018e5e8dce8", "5524e2559d4fd5dcf2f32caf5f15e6e064aec78b", "5524e2559d4fd5dcf2f32caf5f15e6e064aec78b", "107d72c63ecba241a2e3a9b01931e2c373550b88", "fc999072ce188ee1d57b6bb744cb276b09a491bb", "70e9033af1f8b32bcecd086714135018e5e8dce8", "df7619df1e8e41093f2fe3c8af7d532beae6aed8", "6cdd7b68e0cae329664c3f35e89015ca091b65ea", "6994e794f70a80d31ffa4e3816443ca81aae28e6"]},{"id": "22f8b93d37d4b620fa13ed1a50a28b2d3eb5cc3a", "title": "Implicit modelling of pronunciation variation in automatic speech recognition", "authors": ["Thomas Hain"], "date": "2005", "abstract": "Modelling of pronunciation variability is an important task for the acoustic model of an automatic speech recognition system. Good pronunciation models contribute to the robustness and generic applicability of a speech recogniser. Usually pronunciation modelling is associated with a lexicon that allows to explicitly control the selection of appropriate HMMs for a particular word. However, the use of data-driven clustering techniques or specific parameter tying techniques has considerable impact… ", "references": []},{"id": "2c5eec22c4dabb124b2bbf85966d88dd38e62193", "title": "Mean and variance adaptation within the MLLR framework", "authors": ["Mark John Francis Gales", "Philip C. Woodland"], "date": "1996", "abstract": "Abstract One of the key issues for adaptation algorithms is to modify a large number of parameters with only a small amount of adaptation data. Speaker adaptation techniques try to obtain near speaker-dependent (SD) performance with only small amounts of speaker-specific data, and are often based on initial speaker-independent (SI) recognition systems. Some of these speaker adaptation techniques may also be applied to the task of adaptation to a new acoustic environment. In this case an SI… ", "references": ["e368a35ae6b1c4bd6d2b49ba3962d9146b824a84", "e368a35ae6b1c4bd6d2b49ba3962d9146b824a84", "e368a35ae6b1c4bd6d2b49ba3962d9146b824a84", "b1c057eed83f4ab82c0400303d7b9ca14d09aa91", "b1c057eed83f4ab82c0400303d7b9ca14d09aa91", "88009fb591e3477c2cacfdd71efef0091576a1e0", "1c6d7d30d95a869e2a7dca975a2405b2d8795242", "778015de7b81dfde54367dd57fb76c86faa72be4", "b1c057eed83f4ab82c0400303d7b9ca14d09aa91", "b1c057eed83f4ab82c0400303d7b9ca14d09aa91"]},{"id": "8941fe8899d17ae1dd9626f6945516cc68a3c43d", "title": "Solving the Problems of Context Modelling", "authors": ["Clive Bloom"], "date": "1998", "abstract": "This invention provides a reinforcing structural element interconnecting existing structures at the floor level or rear wall of a vehicle body interior space and existing front transverse elements at the instrument panel or windshield base. The reinforcing element, which may be of box-or U-section, strengthens the body against longitudinal deformation in the event of a frontal collision. ", "references": []},{"id": "d812b8b86470f5cc6b7c0f7554007e9459abd089", "title": "Introducing MegaHAL", "authors": ["Jason L. Hutchens", "Michael D. Alder"], "date": "CoNLL", "abstract": "Conversation simulators are computer programs which give the appearance of conversing with a user in natural language. Alan Turing devised a simple test in order to decide whether such programs are intelligent. In 1991, the Cambridge Centre for Behavioural Studies held the first formal instantiation of the Turing Test. In this incarnation the test was known as the Loebner contest, as Dr. Hugh Loebner pledged a $100,000 grand prize for the first computer program to pass the test. In this paper… ", "references": ["5b99122ba4352893a756ad6732948ebd84762104", "5e5073fa1b723b8861050f2dba5f38e19574740e", "5e5073fa1b723b8861050f2dba5f38e19574740e", "3012f2504ddb9d527ad57a56561f4f72aa04285c", "2d5673caa9e6af3a7b82a43f19ee920992db07ad", "5e5073fa1b723b8861050f2dba5f38e19574740e"]},{"id": "e34aa3e43470489964e1ec71631485a57ba0d2eb", "title": "On the entropy of the Malay language", "authors": ["Choon Peng Tan"], "date": "1981", "abstract": "Cover and King's gambling approach for the estimation of the entropy of language is used to estimate the entropy of the Malay language. Our experimental results are shown to support the claim by Cover that committee betting can do better than that of individual subjects. ", "references": []},{"id": "b080dd6ce069c7eac8b89e9f50eb121ef6f06b90", "title": "Language Acquisition and Data Compression", "authors": ["Jason L. Hutchens", "Michael D. Alder"], "date": "1997", "abstract": "Statistical data compression requires a stochastic language model which must rapidly adapt to new data as it is encountered. A grammatical inference engine is introduced which satisfies this requirement; it is able to discover structure in arbitrary data using nothing more than the predictions of a simple trigram model. We show that compression may be used as an alternative to perplexity for language model evaluation, and that the information processing techniques employed by our system may… ", "references": []},{"id": "0509bf552a0d1fe895c019e4e8f1b1599c7112e4", "title": "Minimum Phone Error and I-smoothing for improved discriminative training", "authors": ["Daniel Povey", "Philip C. Woodland"], "date": "2002", "abstract": "In this paper we introduce the Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria for the discriminative training of HMM systems. The MPE/MWE criteria are smoothed approximations to the phone or word error rate respectively. We also discuss I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE). Experiments have been performed on the Switchboard/Call Home corpora of telephone conversations with… ", "references": ["74de4b349d65bd80714507ca3d2512fcc704d3ea", "b5f09ce0dd760857e0d0e4879f6e2543f04c5d33", "a27fc84fb14188f7149bd5cae9b24b7eaed3e588", "647edf2d639da0649f056db89c8ac0e20ff61f8c", "cdc3542552842d826a661f21e87917b976b4f7ee", "067120574d64e37be5fa66591a6d0115d9a6d561", "647edf2d639da0649f056db89c8ac0e20ff61f8c", "067120574d64e37be5fa66591a6d0115d9a6d561", "a27fc84fb14188f7149bd5cae9b24b7eaed3e588", "b3f258f923da64f99c11610ed13fedd3827ea5f6"]},{"id": "d7d07b2b2ca6e28561fa142fd6f2fd020bcb40a7", "title": "Modeling for text compression", "authors": ["Timothy C. Bell", "Ian H. Witten", "John G. Cleary"], "date": "1989", "abstract": "The best schemes for text compression use large models to help them predict which characters will come next. The actual next characters are coded with respect to the prediction, resulting in compression of information. Models are best formed adaptively, based on the text seen so far. This paper surveys successful strategies for adaptive modeling that are suitable for use in practical text compression systems.\nThe strategies fall into three main classes: finite-context modeling, in which the… ", "references": ["71568a29d90ca5dca4e2cf45ab235832fc773149"]},{"id": "4aa4069693bee00d1b0759ca3df35e59284e9845", "title": "DeViSE: A Deep Visual-Semantic Embedding Model", "authors": ["Andrea Frome", "Gregory S. Corrado"], "date": "NIPS", "abstract": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories.", "references": ["a3ea706f6604a1e6e87c33d7a3b4b97b1bb338ef", "aea0f946e8dcddb65cc2e907456c42453f246a50", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "0f6911bc1e6abee8bbf9dd3f8d54d40466429da7", "755e9f43ce398ae8737366720c5f82685b0c253e", "6ce77f485677387a791f41c0f2130e84ce8a8a1d", "0f6911bc1e6abee8bbf9dd3f8d54d40466429da7", "3a4a53fe47036ac89dad070ab87a9d8795b139b1", "a3ea706f6604a1e6e87c33d7a3b4b97b1bb338ef", "a3ea706f6604a1e6e87c33d7a3b4b97b1bb338ef"]},{"id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "authors": ["Slava M. Katz"], "date": "1987", "abstract": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from… ", "references": ["3888028ecc976f5a5786152ebc418559896da53c", "226a6dff9ccc1c2db9f09db644b13eb9d04322e7", "226a6dff9ccc1c2db9f09db644b13eb9d04322e7", "fdb0f957510bb1264ce67a00e133fa4ac71f12da", "226a6dff9ccc1c2db9f09db644b13eb9d04322e7", "3888028ecc976f5a5786152ebc418559896da53c", "fdb0f957510bb1264ce67a00e133fa4ac71f12da", "fdb0f957510bb1264ce67a00e133fa4ac71f12da", "b2986b25f50babd536dd0ecf2237d9eabf5843c2", "b2986b25f50babd536dd0ecf2237d9eabf5843c2"]},{"id": "2e12a485325776d3c23eae2b488d4812d86b4052", "title": "Training Restricted Boltzmann Machines on Word Observations", "authors": ["George E. Dahl", "Ryan P. Adams", "Hugo Larochelle"], "date": "2012", "abstract": "The restricted Boltzmann machine (RBM) is a flexible model for complex data. However, using RBMs for high-dimensional multinomial observations poses significant computational difficulties. In natural language processing applications, words are naturally modeled by K-ary discrete distributions, where K is determined by the vocabulary size and can easily be in the hundred thousands. The conventional approach to training RBMs on word observations is limited because it requires sampling the states… ", "references": ["85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "bc1022b031dc6c7019696492e8116598097a8c12", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "e7c64258997838087c9ba4e87225627b015122b2", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "bd7d93193aad6c4b71cc8942e808753019e87706", "bd7d93193aad6c4b71cc8942e808753019e87706", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "57458bc1cffe5caa45a885af986d70f723f406b4"]},{"id": "bd46c1b5948abe04e565a8bae6454da63a1b021e", "title": "Finite State Automata and Simple Recurrent Networks", "authors": ["Axel Cleeremans", "David Servan-Schreiber", "James L. McClelland"], "date": "1989", "abstract": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to… ", "references": []},{"id": "50d53cc562225549457cbc782546bfbe1ac6f0cf", "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "authors": ["Richard Socher", "Danqi Chen", "Andrew Y. Ng"], "date": "NIPS", "abstract": "Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships.", "references": ["473b3f2cc2c942c0116d980fe5b36a338f6017de", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "57458bc1cffe5caa45a885af986d70f723f406b4", "27e38351e48fe4b7da2775bf94341738bc4da07e", "27e38351e48fe4b7da2775bf94341738bc4da07e", "00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "f2f72cfb48d15d4d2bd1e91a92e7f3ac8635d433", "27e38351e48fe4b7da2775bf94341738bc4da07e", "00a3f6924f90fcd77e6e7e6534b957a75d0ced07", "81bbe42e3ec09c28b8864956148e58f4cb5aa860"]},{"id": "649d03490ef72c5274e3bccd03d7a299d2f8da91", "title": "Learning Word Vectors for Sentiment Analysis", "authors": ["Andrew L. Maas", "Raymond E. Daly", "Christopher Potts"], "date": "ACL", "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as… ", "references": ["0470303953de1d19765423f719d8314e3cb91278", "e14609a3a6c6f8ef3269d3e0728f88da57826698", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "746c085477ec83bee8ca28cb4c1482439905ee15", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "5ef2edfe51d3d768b1d89ad7e74e4ce8e55d1d49", "e14609a3a6c6f8ef3269d3e0728f88da57826698", "5ef2edfe51d3d768b1d89ad7e74e4ce8e55d1d49"]},{"id": "57dbba9281cbd1b4dd5d1932f0dd605b8f498322", "title": "A Recurrent Neural Network that Learns to Count", "authors": ["Paul Rodríguez"], "date": "1999", "abstract": "Parallel distributed processing (PDP) architectures demonstrate a potentially radical alternative to the traditional theories of language processing that are based on serial computational models.", "references": ["25ed21dfe1573a137eae9241c2318de968f7b76a", "0eaf1af77f16a9b166556e4cc40d282303b854db", "aa7cb86ebebbd7d0c165f3ea65e8f9dd716c7246", "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4", "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4", "225cfcc2bdb0fd6ab1b831257009032232688c2f", "f1d563993f8b61a6e592bf24fa38be9576fb2892", "aa7cb86ebebbd7d0c165f3ea65e8f9dd716c7246", "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4", "f1d563993f8b61a6e592bf24fa38be9576fb2892"]},{"id": "872cdc269f3cb59f8a227818f35041415091545f", "title": "Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks", "authors": ["C. Lee Giles", "Clifford B. Miller", "Yee-Chun Lee"], "date": "1992", "abstract": "We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization… ", "references": ["20b335039270bb4bd11fb21dbd9b9b85936c3012", "1ae1dd016502b853908038a6dfd97d967e2dd185", "2f180a3421d99348936fada0d735d0303f79cfd1", "f88829e90b096908758debdcf55eb1ee7eb784c9", "cea0fe8c2a77c84f9bbb172a9f65f2cb9ba9d24d", "2f180a3421d99348936fada0d735d0303f79cfd1", "1ae1dd016502b853908038a6dfd97d967e2dd185", "25c19c8c1d6778a16f8b27beac4d9c6a55357580", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "b185742930fd959aaccdfdecdb31641839a787c4"]},{"id": "9a8ebbfc26eb959e111c5c4ad8159aad142cccbc", "title": "Helping Our Own: The HOO 2011 Pilot Shared Task", "authors": ["Robert Dale", "Adam Kilgarriff"], "date": "ENLG", "abstract": "The aim of the Helping Our Own (HOO) Shared Task is to promote the development of automated tools and techniques that can assist authors in the writing task, with a specific focus on writing within the natural language processing community. This paper reports on the results of a pilot run of the shared task, in which six teams participated. We describe the nature of the task and the data used, report on the results achieved, and discuss some of the things we learned that will guide future… ", "references": ["6476f695cb4601ce9c6db3f354d6932af3fc9736", "737e7b7b695755bb86f9c89a215b162ec836cff9", "d00d6ddeaa9f9d8793fa571ee6beac2400545eb9", "737e7b7b695755bb86f9c89a215b162ec836cff9", "737e7b7b695755bb86f9c89a215b162ec836cff9", "737e7b7b695755bb86f9c89a215b162ec836cff9", "737e7b7b695755bb86f9c89a215b162ec836cff9", "737e7b7b695755bb86f9c89a215b162ec836cff9", "6476f695cb4601ce9c6db3f354d6932af3fc9736", "6476f695cb4601ce9c6db3f354d6932af3fc9736"]},{"id": "2ebc224d52761d5b76704c7b8f51369247a73d5f", "title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing", "authors": ["Marek Rei", "Helen Yannakoudakis"], "date": "2016", "abstract": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment… ", "references": ["4e5b3910084e75967d2cb627c19d8f0e60154b1e", "f8f92f791d67883f77525c5aecdb7b10b3488b29", "34e0754a745f462d4c4d2877ba9f9136764ef6e8", "18dd1c1bebe937a66296c1ad5561bb6fa9527ebe", "18dd1c1bebe937a66296c1ad5561bb6fa9527ebe", "67f01bc468c11ce77b9ce6c5f82ee02083f03d99", "499c230c0abb7c6c0faced0925991d9c6ed85eca", "499c230c0abb7c6c0faced0925991d9c6ed85eca", "4e5b3910084e75967d2cb627c19d8f0e60154b1e", "aa6984ade6f40535fc4eec549b142c06f9738c85"]},{"id": "f1e8f81a776ddefb621007370c56454f47c0cdce", "title": "Automated Grammatical Error Correction for Language Learners", "authors": ["Joel R. Tetreault", "Claudia Leacock"], "date": "COLING", "abstract": "It has been estimated that over a billion people are using or learning English as a second or foreign language, and the numbers are growing not only for English but for other languages as well. These language learners provide a burgeoning market for tools that help identify and correct learners' writing errors. Unfortunately, the errors targeted by typical commercial proofreading tools do not include those aspects of a second language that are hardest to learn. This volume describes the types… ", "references": ["9a8ebbfc26eb959e111c5c4ad8159aad142cccbc"]},{"id": "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc", "title": "Better Evaluation for Grammatical Error Correction", "authors": ["Daniel Dahlmeier", "Hwee Tou Ng"], "date": "HLT-NAACL", "abstract": "We present a novel method for evaluating grammatical error correction. The core of our method, which we call MaxMatch (M2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation. This optimal edit sequence is subsequently scored using F1 measure. We test our M2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more… ", "references": ["9a8ebbfc26eb959e111c5c4ad8159aad142cccbc", "b2f8876482c97e804bb50a5e2433881ae31d0cdd"]},{"id": "6dab1c6491929d396e9e5463bc2e87af88602aa2", "title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "authors": ["Wang Ling", "Chris Dyer", "Tiago Luís"], "date": "2015", "abstract": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form‐function relationship in language, our “composed” word… ", "references": ["b92513dac9d5b6a4683bcc625b94dd1ced98734e", "b92513dac9d5b6a4683bcc625b94dd1ced98734e", "46f418bf6fab132f193661226c5c27d67f870ea5", "53ab89807caead278d3deb7b6a4180b277d3cb77", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "53ab89807caead278d3deb7b6a4180b277d3cb77", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "1e662c87a36779dbe9f56f7ffd3ade756059d094", "46f418bf6fab132f193661226c5c27d67f870ea5", "944a1cfd79dbfb6fef460360a0765ba790f4027a"]},{"id": "a563c6f1564b1e6d1a643c544379abf5069f8000", "title": "A Beam-Search Decoder for Grammatical Error Correction", "authors": ["Daniel Dahlmeier", "Hwee Tou Ng"], "date": "EMNLP-CoNLL", "abstract": "We present a novel beam-search decoder for grammatical error correction. The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while… ", "references": ["1e3e191dd5c546f2cd9b4ce7da171787a1ffcddd", "35c55879cff5267c55d306a0195911663b748ada", "f1e8f81a776ddefb621007370c56454f47c0cdce", "35c55879cff5267c55d306a0195911663b748ada", "794e03473140760db0681fda938fad93e6d159bd", "794e03473140760db0681fda938fad93e6d159bd", "990036282d6a6377ed550b197a3b4fd97ddc631f", "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc", "990036282d6a6377ed550b197a3b4fd97ddc631f", "a4b828609b60b06e61bea7a4029cc9e1cad5df87"]},{"id": "71efd9633bef36d837ecc6753e8c6e0c5f87c839", "title": "Generating Confusion Sets for Context-Sensitive Error Correction", "authors": ["Alla Rozovskaya", "Dan Roth"], "date": "EMNLP", "abstract": "In this paper, we consider the problem of generating candidate corrections for the task of correcting errors in text. We focus on the task of correcting errors in preposition usage made by non-native English speakers, using discriminative classifiers. The standard approach to the problem assumes that the set of candidate corrections for a preposition consists of all preposition choices participating in the task. We determine likely preposition confusions using an annotated corpus of non-native… ", "references": ["1ae244ee046e6e025ed7fe6d7672f6f18096f5f7", "3846e4f18182fd8991197422ad22865d990e8ae8", "58dba823fc1d5f5d45c6fb7414aba7fed7011fb8", "8f715f46fa230a7e97dc351bdd6a09f2720ca0b7", "92b4cb4bff83118d1e2eb66d27a71e96b369c7f6", "ee455537e727cf921263540134949b7042ba6521", "8f715f46fa230a7e97dc351bdd6a09f2720ca0b7", "ee455537e727cf921263540134949b7042ba6521", "8f715f46fa230a7e97dc351bdd6a09f2720ca0b7", "ee455537e727cf921263540134949b7042ba6521"]},{"id": "794e03473140760db0681fda938fad93e6d159bd", "title": "Using Mostly Native Data to Correct Errors in Learners' Writing", "authors": ["Michael Gamon"], "date": "HLT-NAACL", "abstract": "We present results from a range of experiments on article and preposition error correction for non-native speakers of English. We first compare a language model and errorspecific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction. We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier. The… ", "references": ["3b1f687d1c5feffe13272c09e0e6d82320ba9bd7", "64887b841f12d71e1385bc792fa4126a7b97db91", "0a1b54e9b2532af9495e7e8afcc2159107263830", "ba4632eb5f2e40bd705fdda46f4c2f25029e07df", "63ed85c4555334f83b9b586408e92c2ed027aea6", "35c55879cff5267c55d306a0195911663b748ada", "ba4632eb5f2e40bd705fdda46f4c2f25029e07df", "35c55879cff5267c55d306a0195911663b748ada", "63ed85c4555334f83b9b586408e92c2ed027aea6", "0e640a9fc1535cb349430f57507db0bee22c6d28"]},{"id": "9755f9993553131e5cc796d34ecaa624fe0ddffa", "title": "A probability analysis on the value of unlabeled data for classification problems", "authors": ["Tong Zhang"], "date": "ICML", "abstract": "Recently, there has been increasing interest in using unlabeled data for classiica-tion. However, whether these unlabeled data are truly useful is still under debate. In order to have a better understanding of relevant issues, it is worthwhile to precisely formulate the problem and carefully analyze the value of unlabeled data under certain learning models. In this paper, we approach this problem from the statistical point of view, where we assume that a correct model of the underlying… ", "references": ["941ef255d31b5becbf0a3281bcf7ac0122e4c833", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "334867ed99a0af07d8a53dae4f7fdeffffdecc09", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8", "22834aa74138de7f4da42fb9dfb480cef4e7b177", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "3b3b54848c1bc6ffea2625ce79302abed8e8deb9"]},{"id": "161ffb54a3fdf0715b198bb57bd22f910242eb49", "title": "Multitask Learning", "authors": ["Rich Caruana"], "date": "1997", "abstract": "Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias.", "references": ["2f42a55da3a222184eee20c67d374a9134b77bdc", "47aaeb6dc682162dfe5659c2cad64e5d825ad910", "052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c", "a0ee1219f78e5b53938718e9a8f140491cef1523", "9464d15f4f8d578f93332db4aa1c9c182fd51735", "9464d15f4f8d578f93332db4aa1c9c182fd51735", "47aaeb6dc682162dfe5659c2cad64e5d825ad910", "d221bbcbd20c7157e4500f942de8ceec490f8936", "d221bbcbd20c7157e4500f942de8ceec490f8936", "052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c"]},{"id": "df3520f52fcf42b4f10ed4b35b3b3f9cd050f290", "title": "Limitations of Co-Training for Natural Language Learning from Large Datasets", "authors": ["David R. Pierce", "Claire Cardie"], "date": "EMNLP", "abstract": "Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase… ", "references": ["1c0ece611643cfb8f3a23e4802c754ea583ebe37", "b69e0cce79eb288ffb43ad7ae3b99b8dea9ac5ac", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "944cba683d10d8c1a902e05cd68e32a9f47b372e", "7fad831935254c9c9ec39ffb03752a3f736c3f76", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "d9c71db75046473f0e3d3229950d7c84c09afd5e", "7acb7b3e7ad16adbc68626cddd4bcba515b86e23", "b69e0cce79eb288ffb43ad7ae3b99b8dea9ac5ac", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4"]},{"id": "ed3c324be93f30797e0f71d5f5fb5417cdd790bc", "title": "Semi-Supervised Learning on Riemannian Manifolds", "authors": ["Mikhail Belkin", "Partha Niyogi"], "date": "2004", "abstract": "We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classification functions are naturally defined only on the submanifold in question rather than the total ambient space. Using the Laplace-Beltrami operator one… ", "references": ["afcd6da7637ddeef6715109aca248da7a24b1c65", "e2de29049d62de925cf709024b92774cd82b0a5a", "f67e9a6bd7c688f1c9c653584a4fa1f9c7fda2a6", "8b4a99762d33927e4db312082f9552cce1df9182", "49b8dff62cccc26023c876460234bf29084a382f", "125842668eab7decac136db8a59d392dc5e4e395", "53f2bf254c530c4412a8892896422511bc2cee45", "8b4a99762d33927e4db312082f9552cce1df9182", "53f2bf254c530c4412a8892896422511bc2cee45", "125842668eab7decac136db8a59d392dc5e4e395"]},{"id": "b4299baa815ca5a815a70fba94a9f6f2b42fff19", "title": "A High-Performance Semi-Supervised Learning Method for Text Chunking", "authors": ["Rie Kubota Ando", "Tong Zhang"], "date": "ACL", "abstract": "In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning. The idea is to find \"what good classifiers are like\" by learning from thousands of automatically generated auxiliary… ", "references": ["ad269ba941949a1d66b6649a71d752784c576dc3", "df3520f52fcf42b4f10ed4b35b3b3f9cd050f290", "207df123e3c24e6e25019d4b86f8efaad5d6f13c", "1bef43d460465b25fc8de5534a09fe0a849357bc", "207df123e3c24e6e25019d4b86f8efaad5d6f13c", "207df123e3c24e6e25019d4b86f8efaad5d6f13c", "ae41e4e74f9d35b9a5384faf77a4466d20879515", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "ae41e4e74f9d35b9a5384faf77a4466d20879515", "ae41e4e74f9d35b9a5384faf77a4466d20879515"]},{"id": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "title": "Grammar as a Foreign Language", "authors": ["Oriol Vinyals", "Lukasz Kaiser", "Geoffrey E. Hinton"], "date": "NIPS", "abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated… ", "references": ["f52de7242e574b70410ca6fb70b79c811919fc00", "053f1cf10ced2321c1853f307075f0a6a83b6840", "1956c239b3552e030db1b78951f64781101125ed", "053f1cf10ced2321c1853f307075f0a6a83b6840", "174bbdb96252454cbb40a9c4e53335996235a008", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "5bfd8d40bc071fffaf93685a46974b122ee4239d", "54c846ee00c6132d70429cc279e8577f63ed05e4", "f52de7242e574b70410ca6fb70b79c811919fc00", "053f1cf10ced2321c1853f307075f0a6a83b6840"]},{"id": "bd243d77076b3b8fe046bd3dc6e8a02aa9b38d62", "title": "C3D: Generic Features for Video Analysis", "authors": ["Du Tran", "Lubomir D. Bourdev", "Manohar Paluri"], "date": "2014", "abstract": "Videos have become ubiquitous due to the ease of capturing and sharing via social platforms like Youtube, Facebook, Instagram, and others. The computer vision community has tried to tackle various video analysis problems independently. As a consequence, even though some really good hand-crafted features have been proposed there is a lack of generic features for video analysis. On the other hand, the image domain has progressed rapidly by using features from deep convolutional networks. These… ", "references": ["9667f8264745b626c6173b1310e2ff0298b09cfc", "89d5b41b7fb0a122f811be270e6d5f72fc59d680", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "02ce7baee9d67bb3dff10fea2005017c7ffa066a", "89d5b41b7fb0a122f811be270e6d5f72fc59d680", "facbedfe90956c720f70aab14767b5e25dcc6478", "80bfcf1be2bf1b95cc6f36d229665cdf22d76190", "facbedfe90956c720f70aab14767b5e25dcc6478", "89d5b41b7fb0a122f811be270e6d5f72fc59d680", "a9ce496186120df8f9ed3367e76a4947419e992e"]},{"id": "67dccc9a856b60bdc4d058d83657a089b8ad4486", "title": "Two-Stream Convolutional Networks for Action Recognition in Videos", "authors": ["Karen Simonyan", "Andrew Zisserman"], "date": "NIPS", "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video.", "references": ["42269d0438c0ae4ca892334946ed779999691074", "facbedfe90956c720f70aab14767b5e25dcc6478", "34f2f2dbaca68eb05426b51620673e71b69e1b37", "80bfcf1be2bf1b95cc6f36d229665cdf22d76190", "42269d0438c0ae4ca892334946ed779999691074", "6d77482b5e3478f4616f7467054ad50505207958", "0f86767732f76f478d5845f2e59f99ba106e9265", "14d9be7962a4ec5a6e55755f4c7588ea00793652", "0f86767732f76f478d5845f2e59f99ba106e9265", "4d476b96be73fccc61f2076befbf5a468caa4180"]},{"id": "43cfb0a633ebd33d5a67a6f8103b754b94d72ec9", "title": "Hidden conditional random field with distribution constraints for phone classification", "authors": ["Dong Yu", "Li Deng", "Alex Acero"], "date": "INTERSPEECH", "abstract": "We advance the recently proposed hidden conditional random field (HCRF) model by replacing the moment constraints (MCs) with the distribution constraints (DCs). We point out that the distribution constraints are the same as the traditional moment constraints for the binary features but are able to better regularize the probability distribution of the continuousvalued features than the moment constraints. We show that under the distribution constraints the HCRF model is no longer log-linear but… ", "references": ["82bca4ea8410e1303d13d56f91e217f65fc379b0", "8142355a9b964fc270f16e5111ba3d969b01c3db", "ad4cfdb0c0bb62bf1fa99cb3d0519cadd9cbc160", "ad4cfdb0c0bb62bf1fa99cb3d0519cadd9cbc160", "82bca4ea8410e1303d13d56f91e217f65fc379b0", "0305c5c6b8ec2a3c6a70ddc55391b7b4af9be656", "817c418bc9c8ed4b6b13df9b82758de225eac0d5", "e084bbb9cbbce7c0d282df263cf70cba4042f067", "62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417", "3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8"]},{"id": "8142355a9b964fc270f16e5111ba3d969b01c3db", "title": "Evaluation of a long-contextual-Span hidden trajectory model and phonetic recognizer using a* lattice search", "authors": ["Dong Yu", "Li Deng", "Alex Acero"], "date": "INTERSPEECH", "abstract": "A long-contextual-span Hidden Trajectory Model (HTM) developed recently captures underlying dynamic structure of speech coarticulation and reduction using a highly compact set of context-independent parameters. However, the longspan nature of the HTM makes it difficult to develop efficient search algorithms for its full evaluation. In this paper, we describe our initial effort in meeting this challenge. The basic search algorithm is time-asynchronous A*. Given the structural complexity of the… ", "references": ["4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799", "4917c57f8e070553c24209d40a4b39a05edef799"]},{"id": "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c", "title": "Video (language) modeling: a baseline for generative models of natural videos", "authors": ["Marc'Aurelio Ranzato", "Arthur Szlam", "Sumit Chopra"], "date": "2014", "abstract": "We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We… ", "references": ["65dbbd5844bc5d3a5cb325fde48e87a688556a33", "da9e411fcf740569b6b356f330a1d0fc077c8d7c", "0228810a988f6b8f06337e14f564e2fd3f6e1056", "e3417538b569b21971f3fbbdf8ea5ae5ebedfb48", "0228810a988f6b8f06337e14f564e2fd3f6e1056", "32ef19e90e7834ec09ef19fcef7cd2aa6eff85a9", "0228810a988f6b8f06337e14f564e2fd3f6e1056", "32ef19e90e7834ec09ef19fcef7cd2aa6eff85a9", "67dccc9a856b60bdc4d058d83657a089b8ad4486", "65dbbd5844bc5d3a5cb325fde48e87a688556a33"]},{"id": "64595fd3db21e4e07252d8a2a5d640d2d7dd916d", "title": "Backpropagation training for multilayer conditional random field based phone recognition", "authors": ["Rohit Prabhavalkar", "Eric Fosler-Lussier"], "date": "2010", "abstract": "Conditional random fields (CRFs) have recently found increased popularity in automatic speech recognition (ASR) applications.", "references": ["2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "dbc0a468ab103ae29717703d4aa9f682f6a2b664", "ad4cfdb0c0bb62bf1fa99cb3d0519cadd9cbc160", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8", "2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "db6924b193c89337c8de17262a02e7d27eed8077"]},{"id": "f30da12f78987cd18e007a1a5312605081c5ac62", "title": "Comparison of Large Margin Training to Other Discriminative Methods for Phonetic Recognition by Hidden Markov Models", "authors": ["Fei Sha", "Lawrence K. Saul"], "date": "2007", "abstract": "In this paper we compare three frameworks for discriminative training of continuous-density hidden Markov models (CD-HMMs). Specifically, we compare two popular frameworks, based on conditional maximum likelihood (CML) and minimum classification error (MCE), to a new framework based on margin maximization. Unlike CML and MCE, our formulation of large margin training explicitly penalizes incorrect decodings by an amount proportional to the number of mislabeled hidden states. It also leads to a… ", "references": ["fd6387ca1949d61356adee35708dcdbee1e4fd05", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "dc34c3b927882a55fb2c5c39af265f96499f1a67", "7bdb332ce9a601cdd1d0f12685eab6d8b8374250", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "fd6387ca1949d61356adee35708dcdbee1e4fd05", "e084bbb9cbbce7c0d282df263cf70cba4042f067", "fd6387ca1949d61356adee35708dcdbee1e4fd05", "dc34c3b927882a55fb2c5c39af265f96499f1a67", "fd6387ca1949d61356adee35708dcdbee1e4fd05"]},{"id": "930bde26f600dade443e88af0e81c9695a96294e", "title": "Language recognition using deep-structured conditional random fields", "authors": ["Dong Yu", "Shizhen Wang", "Li Deng"], "date": "2010", "abstract": "We present a novel language identification technique using our recently developed deep-structured conditional random fields (CRFs). The deep-structured CRF is a multi-layer CRF model in which each higher layer's input observation sequence consists of the lower layer's observation sequence and the resulting lower layer's frame-level marginal probabilities. In this paper we extend the original deep-structured CRF by allowing for distinct state representations at different layers and demonstrate… ", "references": ["5fe9425d25e527db1b5d63872a09fb17a0d2e3c3", "d227c0161c9e8c5855bce989b001320cefe43ba2", "5fe9425d25e527db1b5d63872a09fb17a0d2e3c3", "5e9082caea65c76bfd23b8763872804473ee7872", "ea961bcae44945ee69cfce546ee7ca72bcbcf37c", "5e9082caea65c76bfd23b8763872804473ee7872", "ea961bcae44945ee69cfce546ee7ca72bcbcf37c", "5e9082caea65c76bfd23b8763872804473ee7872", "ea961bcae44945ee69cfce546ee7ca72bcbcf37c", "5c05a3a8d13668adbd2cec2c4ff08989ad87cfe7"]},{"id": "ad4cfdb0c0bb62bf1fa99cb3d0519cadd9cbc160", "title": "Hidden conditional random fields for phone classification", "authors": ["Asela Gunawardana", "Milind Mahajan", "John C. Platt"], "date": "INTERSPEECH", "abstract": "In this paper, we show the novel application of hidden conditional random fields (HCRFs) – conditional random fields with hidden state sequences – for modeling speech.", "references": ["3f10804d53644a47ce141a5ac273081b1ec6e62c", "5a7958b418bceb48a315384568091ab1898b1640", "b3f258f923da64f99c11610ed13fedd3827ea5f6", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "0feeec2feaaf832516a16cce2cf5a034564ca8fa", "b3f258f923da64f99c11610ed13fedd3827ea5f6", "3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8", "ce254353245326fe2f5a660e7bceda40e2333fbb", "bece46ed303f8eaef2affae2cba4e0aef51fe636", "0509bf552a0d1fe895c019e4e8f1b1599c7112e4"]},{"id": "652a7e6090a6b10cbeb0883ddac2002620aa8e69", "title": "Deep Multi-Task Learning with Shared Memory for Text Classification", "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "date": "EMNLP", "abstract": "Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text… ", "references": ["57458bc1cffe5caa45a885af986d70f723f406b4", "bc1022b031dc6c7019696492e8116598097a8c12", "57458bc1cffe5caa45a885af986d70f723f406b4", "cdd2906f29d8103632dba24484571a8a05c09076", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "cdd2906f29d8103632dba24484571a8a05c09076", "6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "bc1022b031dc6c7019696492e8116598097a8c12", "c3b8367a80181e28c95630b9b63060d895de08ff"]},{"id": "9405d0388f90ba1432ef13c21309d8363860e22e", "title": "When is multitask learning effective? Semantic sequence prediction under varying data conditions", "authors": ["Barbara Plank", "Héctor Martínez Alonso"], "date": "EACL", "abstract": "Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on when MTL works and whether there are data characteristics that help to determine its success. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary tasks, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, significant… ", "references": ["79c6f1f5f1342cf0852c72b37217e448ae538575", "acd87e4f672f0b92ea4164414c213560c23bee52", "acd87e4f672f0b92ea4164414c213560c23bee52", "d76c07211479e233f7c6a6f32d5346c983c5598f", "6b8da36fe5cfb19243b59c1725e0f1d96d4aee29", "bfda4772029faf4dd66086469cf44a6c96d16e5c", "bfda4772029faf4dd66086469cf44a6c96d16e5c", "bfda4772029faf4dd66086469cf44a6c96d16e5c", "6b8da36fe5cfb19243b59c1725e0f1d96d4aee29", "6b8da36fe5cfb19243b59c1725e0f1d96d4aee29"]},{"id": "83cf4b2f39bcc802b09fd59b69e23068447b26b7", "title": "Multi-Task Learning for Multiple Language Translation", "authors": ["Daxiang Dong", "Hua Wu", "Haifeng Wang"], "date": "ACL", "abstract": "In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages.", "references": ["1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "bc1022b031dc6c7019696492e8116598097a8c12", "d29cf0f457ec2089fd4d776ef9a246de810be689", "58456578b9256e24e5a78edd9fe90cea1ac1d806", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "d29cf0f457ec2089fd4d776ef9a246de810be689", "34992ceb89e251f2ed5c1a792fbd594bcf8246c2", "5d43224147a5bb8b17b6a6fc77bf86490e86991a", "cea967b59209c6be22829699f05b8b1ac4dc092d"]},{"id": "af88ce6116c2cd2927a4198745e99e5465173783", "title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "date": "2015", "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging.", "references": ["44d2abe2175df8153f465f6c39b68b76a0d40ab9", "c782f5a99254725517e5bd526dcc63fb59210589", "eb42a490cf4f186d3383c92963817d100afd81e2", "c782f5a99254725517e5bd526dcc63fb59210589", "c782f5a99254725517e5bd526dcc63fb59210589", "d2da66238e3f9ea48c63d65d1b7f7854a956cd10", "024d970eaa264e0ca17c4d6a8fdfcde81839a2a9", "024d970eaa264e0ca17c4d6a8fdfcde81839a2a9", "eb42a490cf4f186d3383c92963817d100afd81e2", "667939f67523db023d8465e18a96d0198905df72"]},{"id": "318fb72e53eb43667a78f32ec22f7e3135036e1b", "title": "Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network", "authors": ["Peilu Wang", "Yao Qian", "Zhao Hai"], "date": "2015", "abstract": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents.", "references": ["f9a1b3850dfd837793743565a8af95973d395a4e", "9819b600a828a57e1cde047bbe710d3446b30da5", "4d1d7562e077e593f985c6a9103a414e0deb5b4c", "4d1d7562e077e593f985c6a9103a414e0deb5b4c", "d29cf0f457ec2089fd4d776ef9a246de810be689", "d29cf0f457ec2089fd4d776ef9a246de810be689", "bc1022b031dc6c7019696492e8116598097a8c12", "eb42a490cf4f186d3383c92963817d100afd81e2", "8cb72cf5490c2a532d52237f688f915a92afe04c", "d29cf0f457ec2089fd4d776ef9a246de810be689"]},{"id": "fbdbe747c6aa8b35b981d21e475ff1506a1bae66", "title": "Composing Simple Image Descriptions using Web-scale N-grams", "authors": ["Siming Li", "Girish Kulkarni", "Yejin Choi"], "date": "CoNLL", "abstract": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our… ", "references": ["f7d618ae6fec67f981587da56ce39f01e49d92be", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "5cb6700d94c6118ee13f4f4fecac99f111189812", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "f7d618ae6fec67f981587da56ce39f01e49d92be", "e8dbc756ea246f599250c09e3efd9bba9909a842", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "60be767a255fd13f73ed4e64d9901b30cf6081e8"]},{"id": "b6c94cc324f585bd6c004f2b99b5589568643e45", "title": "An Alphanet approach to optimising input transformations for continuous speech recognition", "authors": ["John S. Bridle", "Lyn Dodd"], "date": "1991", "abstract": "The authors extend to continuous speech recognition (CSR) the Alphanet approach to integrating backprop networks and HMM (hidden Markov model)-based isolated word recognition. They present the theory of a method for discriminative training of components of a CSR system, using training data in the form of complete sentences. The derivatives of the discriminative score with respect to the parameters are expressed in terms of the posterior probabilities of state occupancies (gammas) under two… ", "references": []},{"id": "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics (Extended Abstract)", "authors": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "date": "2013", "abstract": "The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search.", "references": ["f773aa2936dde24327704e1d8542702bdd5b9c1c", "18f8820e2a5ca6273a39123c27c0745870cda057", "4bb51966222accaa2b28d93284095a76bb17f659", "18f8820e2a5ca6273a39123c27c0745870cda057", "6eb3a15108dfdec25b46522ed94b866aeb156de9", "6eb3a15108dfdec25b46522ed94b866aeb156de9", "0ba87571341beaf6a5c9a30e049be7b1fc9a4c60", "4bb51966222accaa2b28d93284095a76bb17f659", "3c05a47b72d6faec7f1b9da94f6dde985e11527f", "0ba87571341beaf6a5c9a30e049be7b1fc9a4c60"]},{"id": "1603a40b7bb56d563d9401f0d24c67d428e509f2", "title": "Phone recognition using Restricted Boltzmann Machines", "authors": ["Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "date": "2010", "abstract": "For decades, Hidden Markov Models (HMMs) have been the state-of-the-art technique for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. Conditional Restricted Boltzmann Machines (CRBMs) have recently proved to be very effective for modeling motion capture sequences and this paper investigates the application of this more powerful type of generative model to acoustic modeling. On the standard TIMIT corpus… ", "references": ["a0125b014ff5171c74bd6d8365f4cffe3714c0b0", "1626c940a64ad96a7ed53d7d6c0df63c6696956b", "774c6a9405b6d52d5d30a15e82601ca4635369c6", "a0125b014ff5171c74bd6d8365f4cffe3714c0b0", "774c6a9405b6d52d5d30a15e82601ca4635369c6", "774c6a9405b6d52d5d30a15e82601ca4635369c6", "c6629770cb6a00ad585918e71fe6dbad829ad0d1", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "c6629770cb6a00ad585918e71fe6dbad829ad0d1", "182c9ba291d97dc8d7482533044416869cb15f23"]},{"id": "d9d2ba2003d7324ae3d5ff7423a13f13efc79ca5", "title": "Pushing the envelope - aside [speech recognition]", "authors": ["Nelson Morgan", "Qifeng Zhu", "Marios Athineos"], "date": "2005", "abstract": "Despite successes, there are still significant limitations to speech recognition performance, particularly for conversational speech and/or for speech with significant acoustic degradations from noise or reverberation. For this reason, authors have proposed methods that incorporate different (and larger) analysis windows, which are described in this article. Note in passing that we and many others have already taken advantage of processing techniques that incorporate information over long time… ", "references": ["289e8512c1463c67aad3ff659613d6f8617e5691", "289e8512c1463c67aad3ff659613d6f8617e5691", "891610da0e97caa3f5f2ce96156b5b7c2a4dc961", "289e8512c1463c67aad3ff659613d6f8617e5691", "dad79ed0c04b95231735db9b020e253ec6ab95b9", "c058c0609c3f254465281ce3e3a6ea945ae6bdd9", "289e8512c1463c67aad3ff659613d6f8617e5691", "a2b439b063874df0a074449c7c8616ac0880c9c5", "dad79ed0c04b95231735db9b020e253ec6ab95b9", "1a70ac6680d319905d6bfca4cea0b4dc6c15f420"]},{"id": "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "title": "Every Picture Tells a Story: Generating Sentences from Images", "authors": ["Ali Farhadi", "Mohsen Hejrati", "David A. Forsyth"], "date": "ECCV", "abstract": "Humans can prepare concise descriptions of pictures, focusing on what they find important.", "references": ["bf60322f83714523e2d7c1d39983151fe9db7146", "259706f1fd85e2e900e757d2656ca289363e74aa", "0a2b5d227f70780c24ca379fadda2644dbc39b94", "bf60322f83714523e2d7c1d39983151fe9db7146", "59b374f75c4de5e344205a4eb7737ec61c02a33e", "259706f1fd85e2e900e757d2656ca289363e74aa", "259706f1fd85e2e900e757d2656ca289363e74aa", "df70db146b07ce173476be3877a5a3ae3ca06aa5", "59b374f75c4de5e344205a4eb7737ec61c02a33e", "c6e517eb85bc6c68dff5d3fadb2d817e839c966b"]},{"id": "8f72052f0d67a63756b79fe12d7e36ad338b616c", "title": "Statistical Modeling in Continuous Speech Recognition (CSR)", "authors": ["Steve J. Young"], "date": "UAI", "abstract": "Automatic continuous speech recognition (CSR) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues. This paper reviews the evolution of the statistical modelling techniques which underlie current-day systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a description of the speech signal and its parameterisation, the various modelling assumptions and their consequences… ", "references": []},{"id": "bf38dfb13352449b965c08282b66d3ffc5a0539f", "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "authors": ["Honglak Lee", "Peter T. Pham", "Andrew Y. Ng"], "date": "NIPS", "abstract": "In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data.", "references": ["e7a29a6a8b25c59c87ae87fb06cdcc34d62538b5", "19c8a76bc0a1de5e321573405422406bacfe3e2e", "e7a29a6a8b25c59c87ae87fb06cdcc34d62538b5", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "202cbbf671743aefd380d2f23987bd46b9caaf97", "355d44f53428b1ac4fb2ab468d593c720640e5bd", "e7a29a6a8b25c59c87ae87fb06cdcc34d62538b5", "19c8a76bc0a1de5e321573405422406bacfe3e2e", "355d44f53428b1ac4fb2ab468d593c720640e5bd"]},{"id": "e3c1bf806c325f306e5084c3bd332b83d2077e2a", "title": "Binary coding of speech spectrograms using a deep auto-encoder", "authors": ["Li Deng", "Michael L. Seltzer", "Geoffrey E. Hinton"], "date": "INTERSPEECH", "abstract": "This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms.", "references": ["8978cf7574ceb35f4c3096be768c7547b28a35d0", "4211ce21fcc03165020854cecb2f9da21dbf3f0d", "5c05a3a8d13668adbd2cec2c4ff08989ad87cfe7", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "622a40854f79fb385b61f1a3de1cdce4999e9f4b", "62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417", "62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417", "62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417", "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1", "15754483b95c5386fb145751ad3b054761207451"]},{"id": "28c322dc17c8d1dcf1a868f210e0276b288aaa34", "title": "Minimum phone error training of precision matrix models", "authors": ["Khe Chai Sim", "Mark John Francis Gales"], "date": "2006", "abstract": "Gaussian mixture models (GMMs) are commonly used as the output density function for large-vocabulary continuous speech recognition (LVCSR) systems. A standard problem when using multivariate GMMs to classify data is how to accurately represent the correlations in the feature vector. Full covariance matrices yield a good model, but dramatically increase the number of model parameters. Hence, diagonal covariance matrices are commonly used. Structured precision matrix approximations provide an… ", "references": ["5b2e1f044aea0ccb214df9a37805b9ce03b4d88f", "2a6ae3d667a5c2601c1852a0753c8b1c749fec1e", "5d4c1f55815b8c17f364dbe1f8fcd9218256c240", "a27fc84fb14188f7149bd5cae9b24b7eaed3e588", "683dff6dbb7070307a011194eababff3faa28533", "5d4c1f55815b8c17f364dbe1f8fcd9218256c240", "039b2ac703dbc4a807f9218ac4a2935787ab1881", "e771f4c899837af5d83e17e42b024835edb2497b", "683dff6dbb7070307a011194eababff3faa28533", "039b2ac703dbc4a807f9218ac4a2935787ab1881"]},{"id": "df53e0dc66eb13bb51c6e4803ceae56d3ebe6f23", "title": "Context-dependent modeling for acoustic-phonetic recognition of continuous speech", "authors": ["Richard M. Schwartz", "Yen-Lu Chow", "John Makhoul"], "date": "1985", "abstract": "This paper describes the results of our work in designing a system for phonetic recognition of unrestricted continuous speech. We describe several algorithms used to recognize phonemes using context-dependent Hidden Markov Models of the phonemes. We present results for several variations of the parameters of the algorithms. In addition, we propose a technique that makes it possible to integrate traditional acoustic-phonetic features into a hidden Markov process. The categorical decisions… ", "references": ["c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58"]},{"id": "e084bbb9cbbce7c0d282df263cf70cba4042f067", "title": "Large Margin Gaussian Mixture Modeling for Phonetic Classification and Recognition", "authors": ["Fei Sha", "Lawrence K. Saul"], "date": "2006", "abstract": "We develop a framework for large margin classification by Gaussian mixture models (GMMs). Large margin GMMs have many parallels to support vector machines (SVMs) but use ellipsoids to model classes instead of half-spaces. Model parameters are trained discriminatively to maximize the margin of correct classification, as measured in terms of Mahalanobis distances. The required optimization is convex over the model's parameter space of positive semidefinite matrices and can be performed… ", "references": ["ad4cfdb0c0bb62bf1fa99cb3d0519cadd9cbc160", "e309a155425162cf3b363b9393291b4f568416a0", "cc0d79c23e7692bbe781e2c041a9f28584294cae", "d5051890e501117097eeffbd8ded87694f0d8063", "cc0d79c23e7692bbe781e2c041a9f28584294cae", "3034afcd45fc190ed71982828b77f6e4154bdc5c", "2e854865394ccc46e46b415235a67ce8617c9f04", "3b3525fb0edd4cb5e0ca507486b3ce9c6ec3a4a8", "e309a155425162cf3b363b9393291b4f568416a0", "d5051890e501117097eeffbd8ded87694f0d8063"]},{"id": "8644ef2fd2cb0ab150cc13eb601f8353c6a306bf", "title": "State-based Gaussian selection in large vocabulary continuous speech recognition using HMMs", "authors": ["Mark John Francis Gales", "Kate Knill", "Steve J. Young"], "date": "1999", "abstract": "This paper investigates the use of Gaussian selection (GS) to increase the speed of a large vocabulary speech recognition system. Typically, 30-70% of the computational time of a continuous density hidden Markov model-based (HMM-based) speech recognizer is spent calculating probabilities. The aim of CS is to reduce this load by selecting the subset of Gaussian component likelihoods that should be computed given a particular input vector. This paper examines new techniques for obtaining \"good… ", "references": ["68f54fc2c50e1d53ba4177f53f2ac9d09fd5aef0", "9226b20bf8fb7467b0dac0acf0f0e39e6c3cff23", "17a33add9e1f5d1e2191f38d86f7e0e520e468ba", "8a0a14d8f84fcd8003cc0417cb97b2455dac3eee", "f9fee0d8c9d7f8c697b96718cb40aec820590313", "c058c0609c3f254465281ce3e3a6ea945ae6bdd9", "f9fee0d8c9d7f8c697b96718cb40aec820590313", "c058c0609c3f254465281ce3e3a6ea945ae6bdd9", "68f54fc2c50e1d53ba4177f53f2ac9d09fd5aef0", "618c54f2ee1ebffdb8dc8fc501166b01f8731496"]},{"id": "ecb6e57ae893cb1b6eda8908940f23787415eaa8", "title": "Simultaneous ANN feature and HMM recognizer design using string-based minimum classification error (MCE) training", "authors": ["Mazin G. Rahim", "Chin-Hui Lee"], "date": "1996", "abstract": "Conventional features used in state of the art hidden Markov model (HMM) based speech recognition systems are commonly inspired by scientific knowledge and expertise of the human vocal and auditory system. Although the intent when performing feature analysis is to extract \"relevant\" and \"discriminative\" information from the signal that is useful for speech recognition, this information may not be consistent with the objective of minimizing error rate in the recognition process. We utilize… ", "references": ["10de1f04f5f6c3548a1c66d13624b31d0a5f25df", "10de1f04f5f6c3548a1c66d13624b31d0a5f25df", "4e20d5130db07d16968b7f9ef26561a5c4dca652", "b6c94cc324f585bd6c004f2b99b5589568643e45", "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776"]},{"id": "5bc06dbc8715b0348192bc545ac9037f98608ed0", "title": "Augmented Statistical Models for Speech Recognition", "authors": ["Martin I. Layton", "Mark John Francis Gales"], "date": "2006", "abstract": "Recently there has been significant interest in developing new acoustic models for speech recognition. One such model, that allows complex dependencies to be represented, is the augmented statistical model. This incorporates additional dependencies by constructing a local exponential expansion of a standard HMM. Unfortunately, the resulting model often has an intractable normalisation term, rendering training difficult for all but binary classification tasks. In this paper, conditional… ", "references": ["b5f09ce0dd760857e0d0e4879f6e2543f04c5d33"]},{"id": "a27fc84fb14188f7149bd5cae9b24b7eaed3e588", "title": "Hidden Markov models, maximum mutual information estimation, and the speech recognition problem", "authors": ["Yves Normandin"], "date": "1992", "abstract": "Hidden Markov Models (HMMs) are one of the most powerful speech recognition tools available today. Even so, the inadequacies of HMMs as a \"correct\" modeling framework for speech are well known. In that context, we argue that the maximum mutual information estimation (MMIE) formulation for training is more appropriate vis-a-vis maximum likelihood estimation (MLE) for reducing the error rate. We also show how MMIE paves the way for new training possibilities. \nWe introduce Corrective MMIE… ", "references": []},{"id": "aa321cbc2482116f32eaa54a2f393229afa48398", "title": "Large scale discriminative training for speech recognition", "authors": ["Philip C. Woodland", "Daniel Povey"], "date": "2000", "abstract": "This paper describes, and evaluates on a large scale, the lattice based framework for discriminative training of large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models (HMMs). The paper concentrates on the maximum mutual information estimation (MMIE) criterion which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data. These experiments represent the largest-scale application of… ", "references": ["b3fa9a9a9652ab70dec27b0f4d333bdbbe9a31b1", "211b3af71105051cb45f491bd96a1fd792c747e0", "8037000339b6f96d3e81e5a66580ee91993a2c30", "1e526daa5cbe919c731e071c78467b52abcf2fcc", "6a3b8d6aaa38a8cbac71935b03560c79cc89f8c1", "60e352a65bfe8d10dac98313dc944d7120754acd", "1e526daa5cbe919c731e071c78467b52abcf2fcc", "a27fc84fb14188f7149bd5cae9b24b7eaed3e588", "b3fa9a9a9652ab70dec27b0f4d333bdbbe9a31b1", "6a3b8d6aaa38a8cbac71935b03560c79cc89f8c1"]},{"id": "6f9f143ec602aac743e07d092165b708fa8f1473", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling", "authors": ["Vijay Badrinarayanan", "Ankur Handa", "Roberto Cipolla"], "date": "2015", "abstract": "We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling.", "references": ["1a9658c0b7bea22075c0ea3c229b8c70c1790153", "0504945cc2d03550fecb6ff02e637f9421107c25", "06feba1ffd596b41884cea6e8ef0da89b6dd2233", "eb42cf88027de515750f230b23b1a057dc782108", "eb42cf88027de515750f230b23b1a057dc782108", "eb42cf88027de515750f230b23b1a057dc782108", "0504945cc2d03550fecb6ff02e637f9421107c25", "366a03d11c2402e7e83c17480ba24ca1f220333e", "1ecc15c576b54d17f7900b073e19edb2db2554f1", "e15cf50aa89fee8535703b9f9512fca5bfc43327"]},{"id": "9f48616039cb21903132528c0be5348b3019db50", "title": "Attention to Scale: Scale-Aware Semantic Image Segmentation", "authors": ["Liang-Chieh Chen", "Yi Yang", "Alan L. Yuille"], "date": "2016", "abstract": "Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation.", "references": ["317aee7fc081f2b137a85c4f20129007fd8e717e", "317aee7fc081f2b137a85c4f20129007fd8e717e", "602174ad642bae2e77d89ad0e973e123f8821d0d", "602174ad642bae2e77d89ad0e973e123f8821d0d", "317aee7fc081f2b137a85c4f20129007fd8e717e", "a9ce496186120df8f9ed3367e76a4947419e992e", "d3025efeaad55b444e568a4790acc56abc1a8f36", "e56bb892c581f682052ddd3896c65a2b29e64612", "4cef5476f9da50c1a8fefdcb7114863966f61d67", "e56bb892c581f682052ddd3896c65a2b29e64612"]},{"id": "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "title": "Conditional Random Fields as Recurrent Neural Networks", "authors": ["Shuai Zheng", "Sadeep Jayasumana", "Philip H. S. Torr"], "date": "2015", "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional… ", "references": ["416a242f24246f8ee2c00f4d3d1561443dc65b59", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "f084f0126d48a0793cf7e60830089b93ef09c844", "321c3a3da10d4b327b44f50c46f3305acb3bbaec", "f566b1f24e63151ddae652826638af054973a27f", "f084f0126d48a0793cf7e60830089b93ef09c844", "4cef5476f9da50c1a8fefdcb7114863966f61d67", "4cef5476f9da50c1a8fefdcb7114863966f61d67", "f084f0126d48a0793cf7e60830089b93ef09c844"]},{"id": "48adff169c044c674e7cbcc033c81d77c7ac9b43", "title": "Learning Hierarchical Features for Scene Labeling", "authors": ["Clément Farabet", "Camille Couprie", "Yann LeCun"], "date": "2013", "abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods… ", "references": ["05d0340695c89384ebfd929c6fe46dc6023a0238", "599c8d460575ddfea702075b8ccde01b6fe987e8", "ef2448cf2eae2bc2fc1d83f1da0fbaba188ecec7", "914f6db28db0de5f439e891dac38f09bbdc5a452", "914f6db28db0de5f439e891dac38f09bbdc5a452", "5e796ec9be0009b8f6ae7b1ccba1c9c055328d14", "914f6db28db0de5f439e891dac38f09bbdc5a452", "05d0340695c89384ebfd929c6fe46dc6023a0238", "05d0340695c89384ebfd929c6fe46dc6023a0238", "ef2448cf2eae2bc2fc1d83f1da0fbaba188ecec7"]},{"id": "d4cede3acfd94fccc927519e04384a8debfec705", "title": "Image Classification with the Fisher Vector: Theory and Practice", "authors": ["Jorge Sánchez", "Florent Perronnin", "Jakob J. Verbeek"], "date": "2013", "abstract": "A standard approach to describe an image for classification and retrieval purposes is to extract a set of local patch descriptors, encode them into a high dimensional vector and pool them into an image-level signature. The most common patch encoding strategy consists in quantizing the local descriptors into a finite set of prototypical elements. This leads to the popular Bag-of-Visual words representation. In this work, we propose to use the Fisher Kernel framework as an alternative patch… ", "references": ["23694b6d61668e62bb11f17c1d75dde3b4951948", "23694b6d61668e62bb11f17c1d75dde3b4951948", "3b282b22975e7220059616d6b08eb87482926db3", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "9791f1e47a48fa05387cb8dd93da53bf8f43c1f4", "aa1fa18231b8c6b35a21796af446899fc681a107", "aa1fa18231b8c6b35a21796af446899fc681a107", "aa1fa18231b8c6b35a21796af446899fc681a107", "aa1fa18231b8c6b35a21796af446899fc681a107"]},{"id": "34f2f2dbaca68eb05426b51620673e71b69e1b37", "title": "Action Recognition with Stacked Fisher Vectors", "authors": ["Xiaojiang Peng", "Changqing Zou", "Qiang Peng"], "date": "ECCV", "abstract": "Representation of video is a vital problem in action recognition.", "references": ["0f86767732f76f478d5845f2e59f99ba106e9265", "d90cb88d89408daf4a0fe5ac341a6b9db747a556", "a39e6968580762ac5ae3cd064e86e1849f3efb7f", "d90cb88d89408daf4a0fe5ac341a6b9db747a556", "a39e6968580762ac5ae3cd064e86e1849f3efb7f", "21e8f3344170a8f133b69308c178518df8a27274", "0f86767732f76f478d5845f2e59f99ba106e9265", "a39e6968580762ac5ae3cd064e86e1849f3efb7f", "a39e6968580762ac5ae3cd064e86e1849f3efb7f", "facbedfe90956c720f70aab14767b5e25dcc6478"]},{"id": "342786659379879f58bf5c4ff43c84c83a6a7389", "title": "Simultaneous Detection and Segmentation", "authors": ["Bharath Hariharan", "Pablo Arbeláez", "Jitendra Malik"], "date": "ECCV", "abstract": "We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it.", "references": ["1109b663453e78a59e4f66446d71720ac58cec25", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "1395f0561db13cad21a519e18be111cbe1e6d818", "4131a2862d9f926c6727da6dc75c8fda25f4a9e5", "1109b663453e78a59e4f66446d71720ac58cec25", "a9ce496186120df8f9ed3367e76a4947419e992e", "06b2739f4daba0382870f7e34ab653fa444993c0", "0385a8b21b50ba67e1b0e72d94cfc35675299afa", "1109b663453e78a59e4f66446d71720ac58cec25"]},{"id": "b49c25e6e40fffb10d3e90d54b876a54e6725962", "title": "Towards unified depth and semantic prediction from a single image", "authors": ["Peng Wang", "Xiaohui Shen", "Alan L. Yuille"], "date": "2015", "abstract": "Depth estimation and semantic segmentation are two fundamental problems in image understanding. While the two tasks are strongly correlated and mutually beneficial, they are usually solved separately or sequentially. Motivated by the complementary properties of the two tasks, we propose a unified framework for joint depth and semantic prediction. Given an image, we first use a trained Convolutional Neural Network (CNN) to jointly predict a global layout composed of pixel-wise depth values and… ", "references": ["2c0b510ebf995ef172cb64ed7ce24aa7903dced8", "39ad6c911f3351a3b390130a6e4265355b4d593b", "2c0b510ebf995ef172cb64ed7ce24aa7903dced8", "893ebdaeb66b2d0c5b5be100e2d19e54a3c0824e", "a9ce496186120df8f9ed3367e76a4947419e992e", "dc08847b65953ef2ae3542e47b08b57a46b5ba34", "bcd7182acb232e7beee5f84f1a5f6f5793495e25", "e36111cc8acd864f047ff138cd6edc668891c32c", "39ad6c911f3351a3b390130a6e4265355b4d593b", "4b66ace3a248a9d6c1298340749f1d6ca51d5af1"]},{"id": "9533de7fb7c0cbc78b00c5f6181a8378acd9ddc1", "title": "Symbolic and Neural Learning for Named-Entity Recognition", "authors": ["George Petasis", "Sergios Petridis", "Constantine D. Spyropoulos"], "date": "2000", "abstract": "Named-entity recognition involves the identification and classification of named entities in text. This is an important subtask in most language engineering applications, in particular information extraction, where different types of named entity are associated with specific roles in events. The manual construction of rules for the recognition of named entities is a tedious and time-consuming task. For this reason, we present in this paper two approaches to learning named-entity recognition… ", "references": ["e9292ba3230f01b9f6990362fdf06783b9347bf6", "879a760cd5fcc1de36422afbcdd05bd73ba0ee4f", "3137a2997991b1360f51b7a60b606d629eda3c54", "8b9e4698ae2c62b9c078a62c468732f7311a4fb2", "3137a2997991b1360f51b7a60b606d629eda3c54", "163149bcc09128f970302eb10414c08ce06a910c", "8b9e4698ae2c62b9c078a62c468732f7311a4fb2", "879a760cd5fcc1de36422afbcdd05bd73ba0ee4f", "e21a479d03bb90b307582421560fde5bf9d9631c", "3137a2997991b1360f51b7a60b606d629eda3c54"]},{"id": "0959ef8fefe9e7041f508c2448fc026bc9e08393", "title": "Material recognition in the wild with the Materials in Context Database", "authors": ["Sean Bell", "Paul Upchurch", "Kavita Bala"], "date": "2015", "abstract": "Recognizing materials in real-world images is a challenging task.", "references": ["61a92bfaffd7fd213127545ba316a4f47889bcc4", "0d5adc63a50fd78a68914cfd889595fd6b14b22c", "eb42cf88027de515750f230b23b1a057dc782108", "0d5adc63a50fd78a68914cfd889595fd6b14b22c", "eb42cf88027de515750f230b23b1a057dc782108", "866942b26cd59dfad2dbef030d4f76d6b6660fea", "392e3f9ab733264f938cdf7c4ebbfb4dc94ff919", "a9ce496186120df8f9ed3367e76a4947419e992e", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "c08f5fa876181fc040d76c75fe2433eee3c9b001"]},{"id": "e56bb892c581f682052ddd3896c65a2b29e64612", "title": "Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation", "authors": ["George Papandreou", "Liang-Chieh Chen", "Alan L. Yuille"], "date": "2015", "abstract": "Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation.", "references": ["d46bc623f5eecb44c5a053587f841dac5cc9b743", "3ad998a9b2c071c4a1971048f8a2d754530f08e8", "f341bfc9284972bd7e470082caf3c805a94662b4", "d46bc623f5eecb44c5a053587f841dac5cc9b743", "bda6bb50ef3ee4d086d502b42383e80eda633918", "f1f74c935b999cd0a4faf35af8b3d6f507876a83", "39ad6c911f3351a3b390130a6e4265355b4d593b", "39ad6c911f3351a3b390130a6e4265355b4d593b", "bda6bb50ef3ee4d086d502b42383e80eda633918", "bda6bb50ef3ee4d086d502b42383e80eda633918"]},{"id": "317aee7fc081f2b137a85c4f20129007fd8e717e", "title": "Fully Convolutional Networks for Semantic Segmentation", "authors": ["Evan Shelhamer", "Jonathan Long", "Trevor Darrell"], "date": "2017", "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features.", "references": ["74baf0185659ef0e1f8d412d3e906f6e73a6a873", "a9ce496186120df8f9ed3367e76a4947419e992e", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c", "3ad998a9b2c071c4a1971048f8a2d754530f08e8", "1a9658c0b7bea22075c0ea3c229b8c70c1790153", "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a", "39ad6c911f3351a3b390130a6e4265355b4d593b", "39ad6c911f3351a3b390130a6e4265355b4d593b", "cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c"]},{"id": "aa9efc8b2737eac0675ba5abb5feab8305482c12", "title": "Design Challenges and Misconceptions in Named Entity Recognition", "authors": ["Lev-Arie Ratinov", "Dan Roth"], "date": "CoNLL", "abstract": "We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system.", "references": ["2559417f8a3d6ab922cfa824b43f9f0c642a1dae", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "207df123e3c24e6e25019d4b86f8efaad5d6f13c", "2559417f8a3d6ab922cfa824b43f9f0c642a1dae", "03c03dec975554cb02aca1e076106178dbe0a8a0", "1fc360befecd3ca2fa9bddd799e4c16211299fa3", "def1f9b9457640bf267b1ce4efc99b82a3f0f1e7", "2559417f8a3d6ab922cfa824b43f9f0c642a1dae", "207df123e3c24e6e25019d4b86f8efaad5d6f13c", "03c03dec975554cb02aca1e076106178dbe0a8a0"]},{"id": "4bb8107199208080123d1bf5d5ddf233cf530adf", "title": "VecShare: A Framework for Sharing Word Representation Vectors", "authors": ["Jared Fernandez", "Zhaocheng Yu", "Doug Downey"], "date": "EMNLP", "abstract": "Many Natural Language Processing (NLP) models rely on distributed vector representations of words. Because the process of training word vectors can require large amounts of data and computation, NLP researchers and practitioners often utilize pre-trained embeddings downloaded from the Web. However, finding the best embeddings for a given task is difficult, and can be computationally prohibitive. We present a framework, called VecShare, that makes it easy to share and retrieve word embeddings on… ", "references": ["b4b78f1823cc0cfce13d52faf7e68f1f7e46f993", "b4b78f1823cc0cfce13d52faf7e68f1f7e46f993"]},{"id": "f8cdf754fb7c08caf6e2f82b176819230910be5b", "title": "CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes", "authors": ["Sameer Pradhan", "Alessandro Moschitti", "Yuchen Zhang"], "date": "EMNLP-CoNLL Shared Task", "abstract": "The CoNLL-2012 shared task involved predicting coreference in three languages -- English, Chinese and Arabic -- using OntoNotes data.", "references": ["a340eadd28c01de25b49d6b75285a6d7ab5073ab", "c7d3f610b528226f1c862c4f9cd6b37623f7390f", "bc0b93f9320d7c8eb03132652363f4135d9fd4dd", "bc0b93f9320d7c8eb03132652363f4135d9fd4dd", "5cb6ce78710b22cda87d423a9e54235804a01321", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "c7d3f610b528226f1c862c4f9cd6b37623f7390f", "548a54142456254709ba927660bd976a29b6fdb8", "a340eadd28c01de25b49d6b75285a6d7ab5073ab"]},{"id": "00cf902b27676cdc376e26567e70298b96c672a1", "title": "Joint Parsing and Named Entity Recognition", "authors": ["Jenny Rose Finkel", "Christopher D. Manning"], "date": "HLT-NAACL", "abstract": "For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser.", "references": ["5cb6ce78710b22cda87d423a9e54235804a01321", "f2124f8995a9cdf4e168baba426472d2d811c0f1", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "146b3649b693ae591c8953c0aae5264512c26ea3", "146b3649b693ae591c8953c0aae5264512c26ea3", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0ffa423a5283396c88ff3d4033d541796bd039cc", "48b4524a3b1207157b1b2f87885c434c96fc7a19", "5cb6ce78710b22cda87d423a9e54235804a01321", "5cb6ce78710b22cda87d423a9e54235804a01321"]},{"id": "6053d1693c0fab0c21ebbea0ba5408b441a3542b", "title": "cdec: A Decoder, Alignment, and Learning Framework for Finite- State and Context-Free Translation Models", "authors": ["Chris Dyer", "Adam Lopez", "Philip Resnik"], "date": "ACL", "abstract": "We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not… ", "references": ["c95986660453782451491d658de11feb847b15b8", "31d4504cc3281367abdd41f6af64aa56ab4991f9", "5f1baa6d2881fbefec199fa82062157e0a8c7d2a", "394c6c50445ab4ccd1c79fbc2db8f35994ef9f15", "02bcc68113cff36226eb9d977f7367f14e2157e5", "31d4504cc3281367abdd41f6af64aa56ab4991f9", "1e7371b2680cd1116065a34fe027104fe2a8d638", "860c1880c5a98cc6a5aeb331113eeda0d84e725c", "1e7371b2680cd1116065a34fe027104fe2a8d638", "394c6c50445ab4ccd1c79fbc2db8f35994ef9f15"]},{"id": "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc", "title": "RCV1: A New Benchmark Collection for Text Categorization Research", "authors": ["David D. Lewis", "Yiming Yang", "Fan Li"], "date": "2004", "abstract": "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes.", "references": ["9bccdbb61e07b0d0630ceb0ddaad8cf5219d2df1", "890c16ca29a781a7b793c603822ffd57aee9f57f", "590cfa38094122f42412d747350229f79b7e6412", "6160d37a7871cef2d6450832507b53201aa66682", "890c16ca29a781a7b793c603822ffd57aee9f57f", "01d6d53fce6fac2a33d92ddf096290d6b99c2d13", "91cbbe24c807473b7b935d39b63df5b15da9bb32", "890c16ca29a781a7b793c603822ffd57aee9f57f", "23354987095a8a9a283ce4c9a690522d6b11e2dd", "890c16ca29a781a7b793c603822ffd57aee9f57f"]},{"id": "a925892c520f2bda9d274bd64789130106392242", "title": "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding", "authors": ["Rie Johnson", "Tong Zhang"], "date": "2015", "abstract": "This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data… ", "references": ["ab001d508fbb4160e53686e05b800ab4baeb9728", "6100169e81d2172375279dcc9700a9a4b186d481", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "5db592bef4b5ff231e1de92588907808f00bfbb4", "6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "57458bc1cffe5caa45a885af986d70f723f406b4", "57458bc1cffe5caa45a885af986d70f723f406b4", "54e840c8973db7665a6388b2d992ef08ed7f0260", "fbf417c83ae5b895fc645346e4efbf3a0aabeac9"]},{"id": "86151fd48b2578ac1232bd927e07a8815144496a", "title": "Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models", "authors": ["Michael Auli", "Jianfeng Gao"], "date": "ACL", "abstract": "Neural network language models are often trained by optimizing likelihood, but we would prefer to optimize for a task specific metric, such as BLEU in machine translation. We show how a recurrent neural network language model can be optimized towards an expected BLEU loss instead of the usual cross-entropy criterion. Furthermore, we tackle the issue of directly integrating a recurrent network into firstpass decoding under an efficient approximation. Our best results improve a phrasebased… ", "references": ["25a4fb7025453ce73feef36eeaa45dbd0eb215e5", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "a17745f1d7045636577bcd5d513620df5860e9e5", "9819b600a828a57e1cde047bbe710d3446b30da5", "25a4fb7025453ce73feef36eeaa45dbd0eb215e5", "fcfb39e64678fe9cb681f11b9a3314becec82bb2", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "9819b600a828a57e1cde047bbe710d3446b30da5", "a17745f1d7045636577bcd5d513620df5860e9e5"]},{"id": "ecb5336bf7b54a62109f325e7152bb74c4c7f527", "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification", "authors": ["Duyu Tang", "Bing Qin", "Ting Liu"], "date": "EMNLP", "abstract": "Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document.", "references": ["649d03490ef72c5274e3bccd03d7a299d2f8da91", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "6f4065f0cc99a0839b0248ffb4457e5f0277b30d", "ab001d508fbb4160e53686e05b800ab4baeb9728", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "ab001d508fbb4160e53686e05b800ab4baeb9728", "6f4065f0cc99a0839b0248ffb4457e5f0277b30d"]},{"id": "7b5e31257f01aba987f16e175a3e49e00a5bd3bb", "title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "authors": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "date": "HLT-NAACL", "abstract": "We present a simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1’s strong assumptions and Model 2’s overparameterization. Efficient inference, likelihood evaluation, and parameter estimation algorithms are provided. Training the model is consistently ten times faster than Model 4. On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4. An open-source implementation of the alignment model described in… ", "references": ["de2df29b0a0312de7270c3f5a0af6af5645cf91a", "0db6eb46ca9941660acc775e3ca39bf4434c18be", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "ab7b5917515c460b90451e67852171a531671ab8", "6a730aff0b3e23423a00cb3407eb04e7f6e83878", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "c043b7516f32877f3f511e2d83605560797544f3", "6a730aff0b3e23423a00cb3407eb04e7f6e83878"]},{"id": "46f418bf6fab132f193661226c5c27d67f870ea5", "title": "Compositional Morphology for Word Representations and Language Modelling", "authors": ["Jan A. Botha", "Phil Blunsom"], "date": "ICML", "abstract": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological… ", "references": ["0db6eb46ca9941660acc775e3ca39bf4434c18be", "b4fcb3921b42d156a812484dcabf60c3f4410d42", "b4fcb3921b42d156a812484dcabf60c3f4410d42", "53ab89807caead278d3deb7b6a4180b277d3cb77", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "bac1b5d5767c4f78f8e7ec99d059bc82d5ca154a", "b4fcb3921b42d156a812484dcabf60c3f4410d42", "79c0b2f44bbc2bc51de554b88ebe46204413f884"]},{"id": "6aaa677da0e22fefae29182c2562de790902631f", "title": "Converting Continuous-Space Language Models into N-Gram Language Models for Statistical Machine Translation", "authors": ["Rui Wang", "Masao Utiyama", "Bao-Liang Lu"], "date": "EMNLP", "abstract": "Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that… ", "references": ["a4b828609b60b06e61bea7a4029cc9e1cad5df87", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "953dbe4541b82b6ac54dce85ab83734a02b6d30b", "0fbdddacd3e231d5b4d4c011eb7b7fe7732e631e", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "d4e81f4fd59723dcb08b1ec6ef30cb115eafda1c", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "0fcc184b3b90405ec3ceafd6a4007c749df7c363"]},{"id": "15c28b0edbd2296324c07a0f218de83033781831", "title": "Improving Machine Translation Performance by Exploiting Non-Parallel Corpora", "authors": ["Dragos Stefan Munteanu", "Daniel Marcu"], "date": "2005", "abstract": "We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine… ", "references": ["0ce9c261a7f69668da2066da0ad736e6eccdcd36", "7e6d09530490561f1fc4dbfbd82fc4ff456f046c", "71bdabaa757f2c6d31ac119c7f55dab03b60802f", "327c88dd06722a967be9c6b1176fbd79554967e7", "bf6cefab34c62596d486a86a0187c427d38f0b29", "bf6cefab34c62596d486a86a0187c427d38f0b29", "84ad45bfdc278ec8ec9c5d3db2f77b916e93627c", "0ce9c261a7f69668da2066da0ad736e6eccdcd36", "1fe7d6343bfa5684f4dfb24ce496e167b8a8d48b", "b83dbea361cd583f9fdfb134bb55c48f0335d297"]},{"id": "5d43224147a5bb8b17b6a6fc77bf86490e86991a", "title": "A Recursive Recurrent Neural Network for Statistical Machine Translation", "authors": ["Shujie Liu", "Nan Yang", "Ming Zhou"], "date": "ACL", "abstract": "In this paper, we propose a novel recursive recurrent neural network (R 2 NN) to model the end-to-end decoding process for statistical machine translation. R 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built… ", "references": ["6658bbf68995731b2083195054ff45b4eca38b3a", "7c51326b7efed781adc4c01f1f457a714af3cb79", "9c0ddf74f87d154db88d79c640578c1610451eec", "7c51326b7efed781adc4c01f1f457a714af3cb79", "9c0ddf74f87d154db88d79c640578c1610451eec", "7c51326b7efed781adc4c01f1f457a714af3cb79", "167ad306d84cca2455bc50eb833454de9f2dcd02", "9c0ddf74f87d154db88d79c640578c1610451eec", "9c0ddf74f87d154db88d79c640578c1610451eec", "7c51326b7efed781adc4c01f1f457a714af3cb79"]},{"id": "eddc57c88fcef195538035eb205355db656cba98", "title": "Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval", "authors": ["Almut Silja Hildebrand", "Matthias Eck", "Alex H. Waibel"], "date": "2005", "abstract": "In this paper we present experiments concerning translation model adaptation for statistical machine translation.", "references": ["bcf4ced6b5a253c7aec557c95e32d802eac9c414", "79794914adfbe844151b01bd0216c275cc33de4f", "e902237b56a259d46b2327f4936d1c775e78f5b1", "0ed207a86d7eb2381127ef5cf26f0c2952ae3133", "11e797026686de58fc44125c1152e1254baead7c", "0ee764f93bf2be7302d1f333dc19219eec8146ba", "bcf4ced6b5a253c7aec557c95e32d802eac9c414", "bcf4ced6b5a253c7aec557c95e32d802eac9c414", "79794914adfbe844151b01bd0216c275cc33de4f", "bcf4ced6b5a253c7aec557c95e32d802eac9c414"]},{"id": "ad351d3e221882950ece7eca9f13fbcf6973f29d", "title": "Direct Translation Model 2", "authors": ["Abraham Ittycheriah", "Salim Roukos"], "date": "HLT-NAACL", "abstract": "This paper presents a maximum entropy machine translation system using a minimal set of translation blocks (phrase-pairs). While recent phrase-based statistical machine translation (SMT) systems achieve significant improvement over the original source-channel statistical translation models, they 1) use a large inventory of blocks which have significant overlap and 2) limit the use of training to just a few parameters (on the order of ten). In contrast, we show that our proposed minimalist… ", "references": ["ad3d2f463916784d0c14a19936c1544309a0a440", "1f12451245667a85d0ee225a80880fc93c71cc8b", "9ab1b0907011792ad510abb9fb505beb4ae65502", "9ab1b0907011792ad510abb9fb505beb4ae65502", "f630bbd08dc16e8e61deef8183eaf80d03590d28", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "6bbcfbd6d15ca72b7e5ef825b7ed8101da4798d8", "1f12451245667a85d0ee225a80880fc93c71cc8b", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "198831b811624a86a88045bfc6c4006da86e1f79"]},{"id": "bcf4ced6b5a253c7aec557c95e32d802eac9c414", "title": "Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval", "authors": ["Matthias Eck", "Stephan E. Vogel", "Alexander H. Waibel"], "date": "LREC", "abstract": "Language modeling is an important part for both speech recognition and machine translation systems. Adaptation has been successfully applied to language models for speech recognition. In this paper we present experiments concerning language model adaptation for statistical machine translation. We develop a method to adapt language models using information retrieval methods. The adapted language models drastically reduce perplexity over a general language model and we can show that it is… ", "references": ["339ccf2ce210260d0a5f36f623859525f089a38a", "7f047880a19e2e363143cdf3bf3f8e4feb031edf", "227f1a1d39e3b816179f7fd02a5d75454c179a8d", "44400ff71d6fa0d7d102c15d71cfea6e9d853ee3", "0ee764f93bf2be7302d1f333dc19219eec8146ba", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715", "227f1a1d39e3b816179f7fd02a5d75454c179a8d", "404fffebcdb9b597489f62735d8ce59eff41f623", "b125e4be372a9f96400433c895a9961ba09ca1d2", "32db403dc12a8fdce90252f64752f1b0ab05da93"]},{"id": "028a35d6835b2717c02a7acd5be1c71e0749df26", "title": "Continuous space language models for the IWSLT 2006 task", "authors": ["Holger Schwenk", "Marta R. Costa-jussà", "José A. R. Fonollosa"], "date": "IWSLT", "abstract": "The language model of the target language plays an important role in statistical machine translation systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. This kind of approach is in particular promising for tasks where a very limited amount of resources are available, like the BTEC corpus of tourism related questions… ", "references": ["916fe670faac711e0b459a0737601d9ddc1e7d3c", "916fe670faac711e0b459a0737601d9ddc1e7d3c", "426baf47d1792113d4418c587a3a5317d7a73757", "7f21e8979a2da873b5f861910ae79a6e6692351d", "10fb8a7ef3811a037d38913fa6261f344115d7cd", "6a5f9307b8b8473b233432b0e8aa0b4bef311996", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "916fe670faac711e0b459a0737601d9ddc1e7d3c"]},{"id": "61a559a5ab77b449758795c86c6ff8a42b389987", "title": "Discriminative Learning for Label Sequences via Boosting", "authors": ["Yasemin Altun", "Thomas Hofmann", "Mark Johnson"], "date": "NIPS", "abstract": "This paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function. The proposed method combines many of the advantages of boosting schemes with the efficiency of dynamic programming methods and is attractive both, conceptually and computationally. In addition, we also discuss alternative approaches based on the Hamming loss for label sequences. The sequence boosting algorithm offers an interesting alternative to methods based on… ", "references": ["6805f60d07f4618de4a1d1e2b9c266863ed8311e", "277c2139eb4e11455a0b16759b7249c3b95b479e", "844db702be4bc149b06b822b47247e15f5894cc3"]},{"id": "07b27e79099f00a8d50f9e529e6b325ed827ead2", "title": "Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets", "authors": ["Robert K. Bryll", "Ricardo Gutierrez-Osuna", "Francis K. H. Quek"], "date": "2003", "abstract": "We present attribute bagging (AB), a technique for improving the accuracy and stability of classifier ensembles induced using random subsets of features. AB is a wrapper method that can be used with any learning algorithm. It establishes an appropriate attribute subset size and then randomly selects subsets of features, creating projections of the training set on which the ensemble classifiers are built. The induced classifiers are then used for voting. This article compares the performance of… ", "references": ["75e56ef7924972fde2ffc32d7071cd182d0f0f21", "9fad1ee8b6ed596f72b81f61f01def620a4ee997", "62340a4425b96aad503bf93e9009f83be374c8bd", "f7cc43c9dbf6991a91db6314523bec861d087b86", "b06eb99293fa73ee932140c6f1e4e508bdcc6651", "9fad1ee8b6ed596f72b81f61f01def620a4ee997", "c5491cc42ec80a59e843790393e2b6117efd78a1", "9fad1ee8b6ed596f72b81f61f01def620a4ee997", "62340a4425b96aad503bf93e9009f83be374c8bd", "9fad1ee8b6ed596f72b81f61f01def620a4ee997"]},{"id": "6a07215a1790f7e0656e5d4ebedb4ba8570a6c36", "title": "FeatureBoost: A Meta-Learning Algorithm that Improves Model Robustness", "authors": ["Joseph O'Sullivan", "John Langford", "Avrim Blum"], "date": "ICML", "abstract": "Most machine learning algorithms are lazy: they extract from the training set the minimum information needed to predict its labels. Unfortunately, this often leads to models that are not robust when features are removed or obscured in future test data. For example, a backprop net trained to steer a car typically learns to recognize the edges of the road, but does not learn to recognize other features such as the stripes painted on the road which could be useful when road edges disappear in… ", "references": ["4d031e39474f2b622e87316314cb6c33eeda0786", "4d031e39474f2b622e87316314cb6c33eeda0786", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "4d031e39474f2b622e87316314cb6c33eeda0786", "3cd2676dfc34f3a7963a93604576ebf0adda4959", "ccf5208521cb8c35f50ee8873df89294b8ed7292", "c67f0f00eaab360db1b9bd377e783c27c922dc86", "3cd2676dfc34f3a7963a93604576ebf0adda4959", "956871a2b60812cf6688fecf9654078d2148af48"]},{"id": "fd0060b30c2741375ed45e8a29f22931c7f42d75", "title": "Logarithmic Opinion Pools for Conditional Random Fields", "authors": ["Allen Charles Smith", "Trevor Cohn", "Miles Osborne"], "date": "ACL", "abstract": "Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overfit.", "references": ["34dc22dcbdf1e09fb48691ee1fc6fe4bb8f834c3", "d919e2bf6e87607ea7de4cde1da6c77f0ea46fa3", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "d919e2bf6e87607ea7de4cde1da6c77f0ea46fa3", "34dc22dcbdf1e09fb48691ee1fc6fe4bb8f834c3", "1ed5f7ff2ea4bfae8f93b17546a71631880277d0", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b"]},{"id": "0cae1bcefe1edb5c7ecfe04b94bb2b21285d3c6e", "title": "Generalized expectation criteria for lightly supervised learning", "authors": ["Andrew McCallum", "Gregory Druck"], "date": "2011", "abstract": "Machine learning has facilitated many recent advances in natural language processing and information extraction. Unfortunately, most machine learning methods rely on costly labeled data, which impedes their application to new problems. Even in the absence of labeled data we often have a wealth of prior knowledge about these problems. For example, we may know which labels particular words are likely to indicate for a sequence labeling task, or we may have linguistic knowledge suggesting probable… ", "references": ["dc9f999632bf6d82882cc54e2d2cc2d32eaed932", "5122b6a11e3b167ff2569049b821c71695132d4e", "dc9f999632bf6d82882cc54e2d2cc2d32eaed932", "806bac8a6adf3ba6af3366e77618cab488a81d9c", "806bac8a6adf3ba6af3366e77618cab488a81d9c", "6fa38ef70e64f592ca67d0c4e35abda72996a613", "92f45c0ccd5d3c00c2adcd250481234eba194620", "2c532112f1aa1af1faa0be0dee48f1b017f40b45", "3e6cf2877af468c7e6dac2d38bbe6ef91126f0b3", "68ff793c31dad287b52e389da1c5b0122bf0b244"]},{"id": "1e19a94d547ee023837c14c361139185e2353fc0", "title": "Max-Margin Parsing", "authors": ["Ben Taskar", "Dan Klein", "Christopher D. Manning"], "date": "EMNLP", "abstract": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of… ", "references": ["0c450531e1121cfb657be5195e310217a4675397", "435245be302b3dc8ed244b1e6b2dba0b92baacf8", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "b9eea85e590f6e522e3681b8e45012684c60b0fd", "0c450531e1121cfb657be5195e310217a4675397", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "0c450531e1121cfb657be5195e310217a4675397", "a46152d8ad27ae47086334c33c8376185b40340d", "1a6cda5c73b3da91ce4260b2b70ca5c226b39edf", "29899547c993f30a9afcc3514ef55358d45d6b97"]},{"id": "7744a4e9e59f8e43a01f464ec452bd216fd99688", "title": "Unsupervised Multilingual Grammar Induction", "authors": ["Benjamin Snyder", "Tahira Naseem", "Regina Barzilay"], "date": "ACL/IJCNLP", "abstract": "We investigate the task of unsupervised constituency parsing from bilingual parallel corpora. Our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data. We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters. To this end, we adapt a formalism known as unordered tree alignment to our probabilistic setting… ", "references": ["9e02b32e8b2b97b613fad9b384ae9fc6c7e8f0f6", "77021fb48704b860fa850dd103b79db4dcf920ee", "5c087c11ff147dc2304719554a479ff30cd4a914", "13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "b2e03d014ac0bf8df47821a2a3e10015c87ceda5", "13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "77021fb48704b860fa850dd103b79db4dcf920ee", "13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d", "9e02b32e8b2b97b613fad9b384ae9fc6c7e8f0f6", "9e02b32e8b2b97b613fad9b384ae9fc6c7e8f0f6"]},{"id": "02c8a0bc8bab9920e6615cfacf1df2ab3f2b1f68", "title": "Information Extraction with HMM Structures Learned by Stochastic Optimization", "authors": ["Dayne Freitag", "Andrew McCallum"], "date": "AAAI/IAAI", "abstract": "Recent research has demonstrated the strong performance of hidden Markov models applied to information extraction—the task of populating database slots with corresponding phrases from text documents. A remaining problem, however, is the selection of state-transition structure for the model. This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization. Our algorithm… ", "references": ["a36835241b44cda9253d86ddaf67f84ffc1d9a89", "204f6148bc6aba37eb5a7c5686d80547a99425b1", "ef93d44ad1df8a5c417b3f74c9b2f96169f2873e", "e9292ba3230f01b9f6990362fdf06783b9347bf6", "204f6148bc6aba37eb5a7c5686d80547a99425b1", "a36835241b44cda9253d86ddaf67f84ffc1d9a89", "e9292ba3230f01b9f6990362fdf06783b9347bf6", "204f6148bc6aba37eb5a7c5686d80547a99425b1"]},{"id": "c70550f81e3d582da97f82777ac502cf4652d6e1", "title": "Dependency Grammar Induction via Bitext Projection Constraints", "authors": ["Kuzman Ganchev", "Jennifer Gillenwater", "Ben Taskar"], "date": "ACL/IJCNLP", "abstract": "Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches… ", "references": ["20a05c9397e048a0ebc66379a78085e8050f54d1", "10a9abb4c78f0be5cc85847f248d3e8277b3c810", "64455eb3fd378965589a69aa8472c39af6d332d2", "64455eb3fd378965589a69aa8472c39af6d332d2", "b92c0e898b1fee243864176e18a2b50105be3e54", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "20a05c9397e048a0ebc66379a78085e8050f54d1", "4e14aa89e2a0d9ac2e6e80f9d618b2fdf07defd4", "0ce9c261a7f69668da2066da0ad736e6eccdcd36", "adfef97814b292a09520d8c78a141e7a4baf8726"]},{"id": "d9c71db75046473f0e3d3229950d7c84c09afd5e", "title": "Text Chunking using Transformation-Based Learning", "authors": ["Lance A. Ramshaw", "Mitchell P. Marcus"], "date": "1995", "abstract": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy.", "references": ["ffd2b7930210df0ad54d1fcc2846a116c95f34cc", "2acd1e59ef22c88b3a5efaed8f75662531e73853", "ffd2b7930210df0ad54d1fcc2846a116c95f34cc", "4f11e1b9d2656d8738329458ef3d325e6c6194a5", "b1bf3d314ad996c394949f88c4091a4832ce0c9b", "b1bf3d314ad996c394949f88c4091a4832ce0c9b", "d8ccc0846df41e225aede7a7c90ac5ccd899fe11", "d8ccc0846df41e225aede7a7c90ac5ccd899fe11", "ffd2b7930210df0ad54d1fcc2846a116c95f34cc", "4f11e1b9d2656d8738329458ef3d325e6c6194a5"]},{"id": "3dd63c9b5ea5df6ec020a34bf8ea7c1bf15997c7", "title": "Named Entity Recognition with Bilingual Constraints", "authors": ["Wanxiang Che", "Mengqiu Wang", "Ting Liu"], "date": "HLT-NAACL", "abstract": "Different languages contain complementary cues about entities, which can be used to improve Named Entity Recognition (NER) systems. We propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple Integer Linear Program, which encourages entity tags to agree via bilingual constraints. Bilingual NER experiments on the large OntoNotes 4.0 Chinese-English corpus show that the proposed method can improve strong baselines for both Chinese and… ", "references": ["2670373b02767574c97dee9c5e304c3dc44dd740", "327c88dd06722a967be9c6b1176fbd79554967e7", "eea9018ad34bf601e8e910ca1819e0deba684518", "327c88dd06722a967be9c6b1176fbd79554967e7", "7155e574d600e3f191eca05e439f65ea8f3bb8f4", "327c88dd06722a967be9c6b1176fbd79554967e7", "a5c8dd981a5d432fbf10aba6841a258b21da4713", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "7155e574d600e3f191eca05e439f65ea8f3bb8f4", "7155e574d600e3f191eca05e439f65ea8f3bb8f4"]},{"id": "a3d3f7f8bbe1ebf0eeb68b85ca34b34791d3a23e", "title": "Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging", "authors": ["Oscar Täckström", "Dipanjan Das", "Joakim Nivre"], "date": "2013", "abstract": "We consider the construction of part-of-speech taggers for resource-poor languages. Recently, manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting. In this paper, we show that additional token constraints can be projected from a resource-rich source language to a resource-poor target language via word-aligned bitext. We present several models to this end; in… ", "references": ["69164782f9b3b34acb07423151716b47c89d11b4", "e388d24f72988f5b62cf42598e638b4f3c184fcb", "98945ad67a5a66f8afc57a5105e04bafbd7fbd37", "0eebaf8dbc32eabcf9c62e337ec7f947db650538", "0eebaf8dbc32eabcf9c62e337ec7f947db650538", "0eebaf8dbc32eabcf9c62e337ec7f947db650538", "327c88dd06722a967be9c6b1176fbd79554967e7", "e388d24f72988f5b62cf42598e638b4f3c184fcb", "e388d24f72988f5b62cf42598e638b4f3c184fcb", "327c88dd06722a967be9c6b1176fbd79554967e7"]},{"id": "7f8f8f33187e20768ae0177780ac5ef78b77feca", "title": "Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques", "authors": ["Stefan Riezler", "Tracy Holloway King", "Mark Johnson"], "date": "ACL", "abstract": "We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.", "references": ["b3b86bfc97ec254e4ed4ad75a955cdaaec3e42b0", "9442a24d6b6d28f35602b504be4336beda365002", "e6c7adc28e20d361d5c35aa9808094b10f6a34d1", "b3b86bfc97ec254e4ed4ad75a955cdaaec3e42b0", "9442a24d6b6d28f35602b504be4336beda365002", "b3b86bfc97ec254e4ed4ad75a955cdaaec3e42b0", "b3b86bfc97ec254e4ed4ad75a955cdaaec3e42b0", "b361b5941d79d0568efef14145417274240017ad", "ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50", "b361b5941d79d0568efef14145417274240017ad"]},{"id": "ba93f0b457de350e5bb8fcc9e647c85eaa046991", "title": "Memory-Based Shallow Parsing", "authors": ["Erik F. Tjong Kim Sang"], "date": "2002", "abstract": "We present memory-based learning approaches to shallow parsing and apply these to five tasks: base noun phrase identification, arbitrary base phrase recognition, clause detection, noun phrase parsing and full parsing. We use feature selection techniques and system combination methods for improving the performance of the memory-based learner. Our approach is evaluated on standard data sets and the results are compared with that of other systems. This reveals that our approach works well for base… ", "references": ["111b870e874ded07f4af222b4754e36202c70d8e", "7fad831935254c9c9ec39ffb03752a3f736c3f76", "8b9e4698ae2c62b9c078a62c468732f7311a4fb2", "8b9e4698ae2c62b9c078a62c468732f7311a4fb2", "e4911f479ea3eebd882d9f6371c3055f4517db8b", "8b9e4698ae2c62b9c078a62c468732f7311a4fb2", "8b9e4698ae2c62b9c078a62c468732f7311a4fb2", "e4911f479ea3eebd882d9f6371c3055f4517db8b", "ef764bcca3bf9737bbba476d0cb33194b4ed1c3f", "ef764bcca3bf9737bbba476d0cb33194b4ed1c3f"]},{"id": "cb0690094be9d21334745f917b0adc4d87e0e898", "title": "Efficient Training of Conditional Random Fields", "authors": ["Hanna M. Wallach"], "date": "2002", "abstract": "This thesis explores a number of parameter estimation techniques for conditional random fields, a recently introduced [31] probabilistic model for labelling and segmenting sequential data. Theoretical and practical disadvantages of the training techniques reported in current literature on CRFs are discussed. We hypothesise that general numerical optimisation techniques result in improved performance over iterative scaling algorithms for training CRFs. Experiments run on a a subset of a well… ", "references": ["ca88f3fc9f50c645c43dd37b6cf637e18c30e0cf", "352dbd26580856ba4b9877d43aeba304343af66d", "878783964ab23c97052ea82685368099d85c500d", "352dbd26580856ba4b9877d43aeba304343af66d", "878783964ab23c97052ea82685368099d85c500d", "b951b9f78b98a186ba259027996a48e4189d37e5", "878783964ab23c97052ea82685368099d85c500d", "ca88f3fc9f50c645c43dd37b6cf637e18c30e0cf", "f903dba9799d1f7f8414891bdc8a5a79b1ecb6fb", "878783964ab23c97052ea82685368099d85c500d"]},{"id": "97cedf99252026f58e8154bc61d49cf885d42030", "title": "Edinburgh's Phrase-based Machine Translation Systems for WMT-14", "authors": ["Nadir Durrani", "Barry Haddow", "Kenneth Heafield"], "date": "WMT@ACL", "abstract": "This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English… ", "references": ["56fcf886de43d431590c5617546f75c4c16f3ad4", "dd8a63a1ed6ab755e0a860f0086416af13dceaa2", "7de66a09cd23f05859a95fa55616b515acab71e9", "6e039e1acca1b91e3dc417811ebbf56c060c6b02", "22ec0fb11ab16d0ad1c41be81648a82348364162", "dd8a63a1ed6ab755e0a860f0086416af13dceaa2", "dd8a63a1ed6ab755e0a860f0086416af13dceaa2", "22ec0fb11ab16d0ad1c41be81648a82348364162", "22ec0fb11ab16d0ad1c41be81648a82348364162", "22ec0fb11ab16d0ad1c41be81648a82348364162"]},{"id": "54c846ee00c6132d70429cc279e8577f63ed05e4", "title": "A Linear Observed Time Statistical Parser Based on Maximum Entropy Models", "authors": ["Adwait Ratnaparkhi"], "date": "1997", "abstract": "This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser… ", "references": ["fb486e03369a64de2d5b0df86ec0a7b55d3907db", "c43d3fbca6086a5c096ea69675928d8dbcff5a4b", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "c43d3fbca6086a5c096ea69675928d8dbcff5a4b", "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "3764baa7465201f054083d02b58fa75f883c4461", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a"]},{"id": "2648926c90f6cc9c9fa2e9590bc9b3df88ac9cd2", "title": "Training Conditional Random Fields with Multivariate Evaluation Measures", "authors": ["Jun Suzuki", "Erik McDermott", "Hideki Isozaki"], "date": "ACL", "abstract": "This paper proposes a framework for training Conditional Random Fields (CRFs) to optimize multivariate evaluation measures, including non-linear measures such as F-score. Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure. Specifically focusing on sequential segmentation tasks, i.e. text chunking and named entity recognition, we introduce a loss function that closely reflects the target evaluation… ", "references": ["8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "e06f1e6498908b64cd70de4156e51e11c558b872", "4477a7902f86c6c1354eaa5d77cafc5ab50b94c7", "22cb0959aaa8ea9a7adfd08c73167c9d902ff045", "22cb0959aaa8ea9a7adfd08c73167c9d902ff045", "22cb0959aaa8ea9a7adfd08c73167c9d902ff045", "a5231f75f9749678f8ad75a4d4afa3461d05060a", "a5231f75f9749678f8ad75a4d4afa3461d05060a", "1f12451245667a85d0ee225a80880fc93c71cc8b", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b"]},{"id": "1e834157de8b7a090747842359fbb7060680c45a", "title": "A Hybrid Generative/Discriminative Approach to Semi-Supervised Classifier Design", "authors": ["Akinori Fujino", "Naonori Ueda", "Kazumi Saito"], "date": "AAAI", "abstract": "Semi-supervised classifier design that simultaneously utilizes both labeled and unlabeled samples is a major research issue in machine learning. Existing semisupervised learning methods belong to either generative or discriminative approaches. This paper focuses on probabilistic semi-supervised classifier design and presents a hybrid approach to take advantage of the generative and discriminative approaches. Our formulation considers a generative model trained on labeled samples and a newly… ", "references": ["656859af2ed88cfa23f2bd063c1816a8fc04c47e", "06a154b63c9e49840ded076ebe9e9915ea672e99", "90929a6aa901ba958eb4960aeeb594c752e08369", "06a154b63c9e49840ded076ebe9e9915ea672e99", "ad67ccee45b801b0138016e2f44a566344e77320", "90929a6aa901ba958eb4960aeeb594c752e08369", "df7f8e4deade0da8428f9d6f38e2d2400cb77f07", "0e0801da1a187d90862cd00ce7f12222ff965ef0", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "06a154b63c9e49840ded076ebe9e9915ea672e99"]},{"id": "d5f5110d65eda0d2df7329582a232a86bf9a3a65", "title": "Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling", "authors": ["Feng Jiao", "Shaojun Wang", "Dale Schuurmans"], "date": "ACL", "abstract": "We present a new semi-supervised training procedure for conditional random fields (CRFs) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data. Our approach is based on extending the minimum entropy regularization framework to the structured prediction case, yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood. Although the training objective is no longer concave, it can still… ", "references": ["b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "e2de29049d62de925cf709024b92774cd82b0a5a", "8bce728b23956b5fdaa8d70d01ff40ee1f007082", "ad67ccee45b801b0138016e2f44a566344e77320", "b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "df95ae968cb0b722143f6000fa0dc7ce21cc35e2", "747618bde9d3155c0d7ccc024732044bce907bd6", "e2de29049d62de925cf709024b92774cd82b0a5a", "f83288f12b1ea85f3166be427d04a77c04de0330", "f83288f12b1ea85f3166be427d04a77c04de0330"]},{"id": "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33", "title": "Context-sensitive learning methods for text categorization", "authors": ["William W. Cohen", "Yoram Singer"], "date": "SIGIR '96", "abstract": "Two recently implemented machine-learning algorithms, RIPPER and sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the “context” of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different… ", "references": ["35582a30685083c62dca992553eec44123be9d07", "35582a30685083c62dca992553eec44123be9d07", "35582a30685083c62dca992553eec44123be9d07", "8f80ae7531ae8c8e06f53ca78d5ad8a2dfbc8697", "8f80ae7531ae8c8e06f53ca78d5ad8a2dfbc8697", "8f80ae7531ae8c8e06f53ca78d5ad8a2dfbc8697", "8f80ae7531ae8c8e06f53ca78d5ad8a2dfbc8697", "35582a30685083c62dca992553eec44123be9d07", "35582a30685083c62dca992553eec44123be9d07", "8f80ae7531ae8c8e06f53ca78d5ad8a2dfbc8697"]},{"id": "80ef14d2a1b8c7efbf45bedae9d001fe5446c7de", "title": "Active Learning with Committees for Text Categorization", "authors": ["Ray Liere", "Prasad Tadepalli"], "date": "AAAI/IAAI", "abstract": "In many real-world domains, supervised learning requires a large number of training examples. In this paper, we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning. Our approach is similar to the Query by Committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label. Our experiments… ", "references": ["5f78d6f79b3ef103cb2d8d170632eb74d9496412", "bbda209682dd2b1b1d0006a41cf84ca1b2487c71", "64f7d9fcdf5158ea91dfddf114eb97986177c067", "a1dace286582d91916fe470d08f30381cf453f20", "64f7d9fcdf5158ea91dfddf114eb97986177c067", "9257779eed46107bcdce9f4dc86298572ff466ce", "69859be3ea6cb8eb38434c80fef5d4997eaec2dc", "bbda209682dd2b1b1d0006a41cf84ca1b2487c71", "69859be3ea6cb8eb38434c80fef5d4997eaec2dc", "69859be3ea6cb8eb38434c80fef5d4997eaec2dc"]},{"id": "ad67ccee45b801b0138016e2f44a566344e77320", "title": "Semi-supervised Learning by Entropy Minimization", "authors": ["Yves Grandvalet", "Yoshua Bengio"], "date": "CAP", "abstract": "We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data.", "references": ["ca8219c2a7753872ac5343c68140014d57470fef", "f31d46fdeb84cb848c1e3f19c3182f0b481963d2", "22834aa74138de7f4da42fb9dfb480cef4e7b177", "847546aa1cbcb017cb16041cba4927b76f57461a", "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3", "f31d46fdeb84cb848c1e3f19c3182f0b481963d2", "22834aa74138de7f4da42fb9dfb480cef4e7b177", "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3", "e2de29049d62de925cf709024b92774cd82b0a5a", "e2de29049d62de925cf709024b92774cd82b0a5a"]},{"id": "bc8e59e4c7c2cbb6695ee5488aa569780449b212", "title": "Expert network: effective and efficient learning from human decisions in text categorization and retrieval", "authors": ["Yiming Yang"], "date": "SIGIR '94", "abstract": "Expert Network (ExpNet) is our new approach to automatic categorization and retrieval of natural language texts. We use a training set of texts with expert-assigned categories to construct a network which approximately reflects the conditional probabilities of categories given a text. The input nodes of the network are words in the training texts, the nodes on the intermediate level are the training texts, and the output nodes are categories. The links between nodes are computed based on… ", "references": ["f61812ea95500993fada9f12c23577d2ba670d33", "67977b5f51a3cd57364f2b1e9bbcee7d6f3d4486", "67977b5f51a3cd57364f2b1e9bbcee7d6f3d4486", "67977b5f51a3cd57364f2b1e9bbcee7d6f3d4486", "f61812ea95500993fada9f12c23577d2ba670d33", "f61812ea95500993fada9f12c23577d2ba670d33", "67977b5f51a3cd57364f2b1e9bbcee7d6f3d4486", "f61812ea95500993fada9f12c23577d2ba670d33", "f61812ea95500993fada9f12c23577d2ba670d33", "f61812ea95500993fada9f12c23577d2ba670d33"]},{"id": "890c16ca29a781a7b793c603822ffd57aee9f57f", "title": "An Evaluation of Statistical Approaches to Text Categorization", "authors": ["Yiming Yang"], "date": "1999", "abstract": "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were… ", "references": ["33536f0bf7925517ead9f44419617edfb24922d2", "d81d19d270106805389d22b8d54b1f755797d440", "6aae3851aca403fe4ea9953ec2b312ceb21bfb36", "6aae3851aca403fe4ea9953ec2b312ceb21bfb36", "bc8e59e4c7c2cbb6695ee5488aa569780449b212", "f926a0022e794485ec469124894aaaf29b087d70", "f926a0022e794485ec469124894aaaf29b087d70", "0c97e8fcd80d9a3779826f2930724c9d789faa05", "0c04909ed933469246defcf9aca2b71ae8e3f623", "6aae3851aca403fe4ea9953ec2b312ceb21bfb36"]},{"id": "8c4c181abcfd6ed7672a4782f7af3489b01490b4", "title": "Co-regularization Based Semi-supervised Domain Adaptation", "authors": ["Hal Daumé", "Abhishek Kumar", "Avishek Saha"], "date": "NIPS", "abstract": "This paper presents a co-regularization based approach to semi-supervised domain adaptation. Our proposed approach (EA++) builds on the notion of augmented space (introduced in EASYADAPT (EA) [1]) and harnesses unlabeled data in target domain to further assist the transfer of information from source to target. This semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner. Our theoretical analysis (in… ", "references": ["0ee9d391bb03b0d1eee16ccf5faed3df0a1b63f8", "5098221cf78ba60b5afd26c171da50baf2670996", "5098221cf78ba60b5afd26c171da50baf2670996", "06935a5c7b5a592e960f7bbac739750d8ebdcf01", "0ee9d391bb03b0d1eee16ccf5faed3df0a1b63f8", "081c5efc798a69ce80ae929278749541673ea8d4", "5098221cf78ba60b5afd26c171da50baf2670996", "96c6bc559b79d8fd518f431c707e8b44ce3bc4de", "5098221cf78ba60b5afd26c171da50baf2670996", "e219a61354d972a28954e655a7c53373508a08b6"]},{"id": "c608ec27a937361122d178b38b6b7387440b58eb", "title": "A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data", "authors": ["David J. Miller", "Hasan S. Uyar"], "date": "NIPS", "abstract": "We address statistical classifier design given a mixed training set consisting of a small labelled feature set and a (generally larger) set of unlabelled features.", "references": ["85cf3d1e397faab094238efb0094aef2d49d49c2"]},{"id": "f2671b151fad7e176176b35d425b2b6356ff4595", "title": "Improving Text Classification by Shrinkage in a Hierarchy of Classes", "authors": ["Andrew McCallum", "Ronald Rosenfeld", "Andrew Y. Ng"], "date": "ICML", "abstract": "When documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy. The U.S. patent database and Yahoo are two examples. This paper shows that the accuracy of a naive Bayes text classi er can be signi cantly improved by taking advantage of a hierarchy of classes. We adopt an established statistical technique called shrinkage that smoothes parameter estimates of a data-sparse child with its parent in order to obtain more robust parameter… ", "references": ["094fc15bc058b0d62a661a1460885a9490bdb1bd", "094fc15bc058b0d62a661a1460885a9490bdb1bd", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "15252afe259894bd3d0f306f29eca5e90ab05eac", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715", "4ec00abf9ff66d6f16378978faf907b047834cbb", "4ec00abf9ff66d6f16378978faf907b047834cbb", "01fa57bd91f731522c861404d29e4604ba6ac6d3", "700666f0c59a4fedc8b08294424c47cb99a8e2ff", "dd5a10e0fc5c5241b3ce3b824e5c48e02f0f1715"]},{"id": "5f78d6f79b3ef103cb2d8d170632eb74d9496412", "title": "Committee-Based Sampling For Training Probabilistic Classifiers", "authors": ["Ido Dagan", "Shlomo Argamon"], "date": "ICML", "abstract": "Abstract In many real-world learning tasks, it is expensive to acquire a sufficient number of labeled examples for training.", "references": ["4be257343bcdca8682c451139c817354bea8f984", "92ccb8c1aef61610248e311b8c84e24e86c7f1cc", "fe30dc915eefa40755b25a363813fcc575536661", "eb27eb32e8dbfcfce55900e12dff4abb5620ca3a", "92ccb8c1aef61610248e311b8c84e24e86c7f1cc", "92ccb8c1aef61610248e311b8c84e24e86c7f1cc", "92ccb8c1aef61610248e311b8c84e24e86c7f1cc"]},{"id": "f83f4c72cd43eb99bde196ba33d034380fec3789", "title": "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation", "authors": ["George F. Foster", "Cyril Goutte", "Roland Kuhn"], "date": "EMNLP", "abstract": "We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model… ", "references": ["022f600ff683d0b5158dac85832b8aecd00fe86f", "40cbaf4106ab60586e8ffa7624fc779172cfd490", "1f12451245667a85d0ee225a80880fc93c71cc8b", "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "40cbaf4106ab60586e8ffa7624fc779172cfd490", "0a0dba86c980d4ba5f881aea0239fa689084e6c5", "022f600ff683d0b5158dac85832b8aecd00fe86f", "5a179f4e3fac2bf66a07d9e0816d04465f7d33d6", "195df0de3c4c181d26391dd73746c7aefe709ab6"]},{"id": "855530bd8af356b5d12c1ac66ee056e03c81c124", "title": "A discriminative model based entity dictionary weighting approach for spoken language understanding", "authors": ["Xiaohu Liu", "Ruhi Sarikaya"], "date": "2014", "abstract": "Spoken language understanding (SLU) systems use various features to detect the domain, intent and semantic slots of a query. In addition to n-grams, features generated from entity dictionaries are often used in model training. Clean or properly weighted dictionaries are critical to improve model's coverage and accuracy for unseen entities during test time. However, clean dictionaries are hard to obtain for some applications since they are automatically generated and can potentially contain… ", "references": ["0c3f68edcca8542ff07397e378d3af084b26b2e9", "fdb813d8b927bdd21ae1858cafa6c34b66a36268", "d84b57362e2010f6f65357267df7e0157af30684", "59a00f44abf28284053901638e50b2c9c0b136c9", "884f6bc72d777de19079107588f782099519414e", "59a00f44abf28284053901638e50b2c9c0b136c9", "4ca91c58eb35395e4a5fb5ffaee925e7c4f1ae81", "59a00f44abf28284053901638e50b2c9c0b136c9", "d84b57362e2010f6f65357267df7e0157af30684", "99b1ca578b8d23edc9d9231cfab880954400ac99"]},{"id": "93a9694b6a4149e815c30a360347593b75860761", "title": "Variable-Length Word Encodings for Neural Translation Models", "authors": ["Rohan Chitnis", "John DeNero"], "date": "EMNLP", "abstract": "Recent work in neural machine translation has shown promising performance, but the most eective architectures do not scale naturally to large vocabulary sizes.", "references": ["2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "53ca064b9f1b92951c1997e90b776e95b0880e52", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "1956c239b3552e030db1b78951f64781101125ed", "53ca064b9f1b92951c1997e90b776e95b0880e52", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "53ca064b9f1b92951c1997e90b776e95b0880e52", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa"]},{"id": "6de6d6b35d1b7ac21c6c7252a1cfb445379e6e72", "title": "Multitask Learning for Spoken Language Understanding", "authors": ["Gökhan Tür"], "date": "ICASSP", "abstract": null, "references": []},{"id": "3f3d110cf78f759760c354bfde1b2ceb9883c544", "title": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "authors": ["Dmitrijs Milajevs", "Dimitri Kartsaklis", "Matthew Purver"], "date": "EMNLP", "abstract": "We provide a comparative study between neural word representations and traditional vector spaces based on cooccurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional… ", "references": ["f6fd84f9eb41bb8a4e4d2ae4fe1339258e271bc0", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "745d86adca56ec50761591733e157f84cfb19671", "f6fd84f9eb41bb8a4e4d2ae4fe1339258e271bc0", "500d570ce02abf42bc1bc535620741d4c5665e6a", "f6fd84f9eb41bb8a4e4d2ae4fe1339258e271bc0", "6b57643fb2c1b0a94409c2d30f76602c32583a80", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917", "27e38351e48fe4b7da2775bf94341738bc4da07e"]},{"id": "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "title": "Domain Adaptation with Structural Correspondence Learning", "authors": ["John Blitzer", "Ryan T. McDonald", "Fernando C Pereira"], "date": "EMNLP", "abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among… ", "references": ["897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "ad269ba941949a1d66b6649a71d752784c576dc3", "89a3d24b4d6e28706c75c9794e35502138fa29fb", "567dc4e26ece98e96c2e798ae8acafa5883945a9", "eb42a490cf4f186d3383c92963817d100afd81e2", "47f5682448cdc0b650b54e7f59d22d72f4976c2d", "89a3d24b4d6e28706c75c9794e35502138fa29fb", "b4299baa815ca5a815a70fba94a9f6f2b42fff19", "89a3d24b4d6e28706c75c9794e35502138fa29fb", "89a3d24b4d6e28706c75c9794e35502138fa29fb"]},{"id": "3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4", "title": "Semantic Parsing via Paraphrasing", "authors": ["Jonathan Berant", "Percy Liang"], "date": "ACL", "abstract": "A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this paper, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a… ", "references": ["0fd0e3854ee696148e978ec33d5c042554cd4d23", "74fe7ec751cd50295b15cfd46389a8fefb37c414", "eaf63218521a6678d46e77767aaf23f4ff12920c", "b2ac51e10a3510eadac5eac5e4fb828f086fab88", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "b2ac51e10a3510eadac5eac5e4fb828f086fab88", "7acfdc905f734abf966aed58abb983bc015ff7fe", "c6afe8a8aa13de8e3f2710ef07b22ce86a005419", "b2ac51e10a3510eadac5eac5e4fb828f086fab88"]},{"id": "b672ef69f60aea81220d658963445c41e60bb0e3", "title": "Instance Weighting for Domain Adaptation in NLP", "authors": ["Jing Jiang", "ChengXiang Zhai"], "date": "ACL", "abstract": "Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.", "references": ["235723a15c86c369c99a42e7b666dfe156ad2cba", "235723a15c86c369c99a42e7b666dfe156ad2cba"]},{"id": "fc303b3b0be5b476ae5f3d8414b685e91d0378c6", "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "authors": ["Stephan Gouws", "Yoshua Bengio", "Gregory S. Corrado"], "date": "2015", "abstract": "We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used… ", "references": ["dac72f2c509aee67524d3321f77e97e8eff51de6", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731", "9d3aaa919c78c06f24588d97ed1028d51860b321", "9d3aaa919c78c06f24588d97ed1028d51860b321", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731", "0157dcd6122c20b5afc359a799b2043453471f7f", "bc1022b031dc6c7019696492e8116598097a8c12", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "0d3233d858660aff451a6c2561a05378ed09725a", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731"]},{"id": "5079c42cb329f70a4f2b94fd429ea8d35a043f56", "title": "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches", "authors": ["Eneko Agirre", "Enrique Alfonseca", "Aitor Soroa"], "date": "HLT-NAACL", "abstract": "This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are… ", "references": ["0f10c6f57e7b640a2e89e94b5ca7d2b0d46d3925", "25f9b944f222989cff080c41f747522e35cf186d", "0a32b3d027064798fb31ce42894fec31e834f7db", "25f9b944f222989cff080c41f747522e35cf186d", "0a32b3d027064798fb31ce42894fec31e834f7db", "afb180e7615fedf3479707aea9db9ee156f9406e", "0f10c6f57e7b640a2e89e94b5ca7d2b0d46d3925", "fd1901f34cc3673072264104885d70555b1a4cdc", "6b64e068a8face2540fc436af40dbcd2b0912bbf", "534534e32f5e477eeb3301faa6265c23d221f684"]},{"id": "c75b30bc3f3282b378f6f48071c7d26c1ee88f37", "title": "Grammatical structures for word-level sentiment detection", "authors": ["Asad B. Sayeed", "Jordan Boyd-Graber", "Amy Weinberg"], "date": "HLT-NAACL", "abstract": "Existing work in fine-grained sentiment analysis focuses on sentences and phrases but ignores the contribution of individual words and their grammatical connections. This is because of a lack of both (1) annotated data at the word level and (2) algorithms that can leverage syntactic information in a principled way. We address the first need by annotating articles from the information technology business press via crowdsourcing to provide training and testing data. To address the second need, we… ", "references": ["4d135641931a6efce82bd9c1d69d86e08d3cd28d", "25202a549e76a50473b6608ba160b82e916c1c34", "9b4876f7313b111074e79a01f570e6e9e02c0dce", "d9b0190b06ac7270e9052895f8592beb4959ccfd", "157d40d3e7fba266544cbbfd5f84852ddd944f8b", "25202a549e76a50473b6608ba160b82e916c1c34", "157d40d3e7fba266544cbbfd5f84852ddd944f8b", "3f87718c0e50ea09c75b943f949f900b81b704c9", "25202a549e76a50473b6608ba160b82e916c1c34", "157d40d3e7fba266544cbbfd5f84852ddd944f8b"]},{"id": "71e90c4015e3dea597dbd583f3d3d08cdc0077fb", "title": "Opinion Mining with Deep Recurrent Neural Networks", "authors": ["Ozan Irsoy", "Claire Cardie"], "date": "EMNLP", "abstract": "Recurrent neural networks (RNNs) are connectionist models of sequential data that are naturally applicable to the analysis of natural language.", "references": ["c782f5a99254725517e5bd526dcc63fb59210589", "98e0522c84756c216f85bcf8b95bb867ed010415", "9819b600a828a57e1cde047bbe710d3446b30da5", "98e0522c84756c216f85bcf8b95bb867ed010415", "c782f5a99254725517e5bd526dcc63fb59210589", "9819b600a828a57e1cde047bbe710d3446b30da5", "9819b600a828a57e1cde047bbe710d3446b30da5", "533ee188324b833e059cb59b654e6160776d5812", "b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1", "b13813b49f160e1a2010c44bd4fb3d09a28446e3"]},{"id": "523f420cb55d8070f565c87a50099a9a5b0b9206", "title": "Modeling Semantic Containment and Exclusion in Natural Language Inference", "authors": ["Bill MacCartney", "Christopher D. Manning"], "date": "COLING", "abstract": "We propose an approach to natural language inference based on a model of natural logic, which identifies valid inferences by their lexical and syntactic features, without full semantic interpretation. We greatly extend past work in natural logic, which has focused solely on semantic containment and monotonicity, to incorporate both semantic exclusion and implicativity. Our system decomposes an inference problem into a sequence of atomic edits linking premise to hypothesis; predicts a lexical… ", "references": ["37c5bd3db1444d6ad331b876fbccb6e009eef44d", "757a192157d7587433db93a899d32c4c5e832489", "318e9eb50bb879f01efb7b4ed0482c8e458b6e8c", "51c85b779c5bb2efd2c736c85c14930a0a78db1d", "51c85b779c5bb2efd2c736c85c14930a0a78db1d", "37c5bd3db1444d6ad331b876fbccb6e009eef44d", "4639cb3718e4eb698a0285548ed2bf23ad9908a9", "3d0f57ebc7de2b83802f8633c21152100493c7c5", "ae83cc2a1425196215a06aec25032b305896d223", "318e9eb50bb879f01efb7b4ed0482c8e458b6e8c"]},{"id": "fae0dd3a8350fad208089a1b3b7ac3a366939d68", "title": "Probabilistic Modeling of Joint-context in Distributional Similarity", "authors": ["Oren Melamud", "Ido Dagan", "Deniz Yuret"], "date": "CoNLL", "abstract": "Most traditional distributional similarity models fail to capture syntagmatic patterns that group together multiple word features within the same joint context. In this work we introduce a novel generic distributional similarity scheme under which the power of probabilistic models can be leveraged to effectively model joint contexts. Based on this scheme, we implement a concrete model which utilizes probabilistic n-gram language models. Our evaluations suggest that this model is particularly… ", "references": ["9b96a0c9dff57484cc2b5b3a0a394ea20c9d669b", "0eb7f8ccf96f714998245a8cb1a35690a3adbafb", "917fbd64a435cb33e0e5b4cd73fe830db7b166db", "cfbff14902e93761a4fa5b9b68c7ffc40c5210f8", "186f9616e98218d29d08608de311a86d40bec6b5", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "cfbff14902e93761a4fa5b9b68c7ffc40c5210f8", "5974441a0bebfb45579491a9a28bca4fff6bc256", "186f9616e98218d29d08608de311a86d40bec6b5", "9b96a0c9dff57484cc2b5b3a0a394ea20c9d669b"]},{"id": "a8307715a3dd8cb4857b88e93d358438a9467762", "title": "Chinese Document Retrieval at TREC-6", "authors": ["Ross Wilkinson"], "date": "TREC", "abstract": "Recherche d'information a partir d'un corpus de 160000 documents en langue chinoise. Resultats obtenus par 12 groupes de travail utilisant des methodes differentes ", "references": []},{"id": "8334df0103443b78546a3cec9284f668106774fc", "title": "Voting Between Multiple Data Representations for Text Chunking", "authors": ["Hong Shen", "Anoop Sarkar"], "date": "Canadian Conference on AI", "abstract": "This paper considers the hypothesis that voting between multiple data representations can be more accurate than voting between multiple learning models This hypothesis has been considered before (cf [San00]) but the focus was on voting methods rather than the data representations In this paper, we focus on choosing specific data representations combined with simple majority voting On the community standard CoNLL-2000 data set, using no additional knowledge sources apart from the training data… ", "references": ["861138f881ec752592cd69320ef14cceaf2971c1", "444471fdeb54a87202a20101503ec52c2e16e512", "f5bb34e38e3403054d4396fc48882f02eae1ffcc", "48649e3cf38d711cbaea177519becdd696c12b4c", "f5bb34e38e3403054d4396fc48882f02eae1ffcc", "861138f881ec752592cd69320ef14cceaf2971c1", "7f1b336efaa17811800d8737b20f6f61d23f90e0", "48649e3cf38d711cbaea177519becdd696c12b4c", "444471fdeb54a87202a20101503ec52c2e16e512", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6"]},{"id": "3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118", "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "authors": ["Marco Baroni", "Georgiana Dinu", "Germán Kruszewski"], "date": "ACL", "abstract": "Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block.", "references": ["a5991db236c230b6e1dca0ddb8944cd478c20fa5", "83a768a0720d0a1f68792827a422395001291614", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "83a768a0720d0a1f68792827a422395001291614", "553fb89d5858826c02f26e94262e8958debc777e", "a5991db236c230b6e1dca0ddb8944cd478c20fa5", "1a8c33f9e51ba01e1cdade7029f96892c7c7087b", "a5991db236c230b6e1dca0ddb8944cd478c20fa5", "1a8c33f9e51ba01e1cdade7029f96892c7c7087b", "553fb89d5858826c02f26e94262e8958debc777e"]},{"id": "8dbb0b9ca61e2753c6759446c6909acda616095a", "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference", "authors": ["Biswajit Paria", "K. M. Annervaz", "Sanjay Podder"], "date": "2016", "abstract": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of… ", "references": ["596c882de006e4bb4a93f1fa08a5dd467bee060a", "13fe71da009484f240c46f14d9330e932f8de210", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "8314f8eef3b64054bfc00607507a92de92fb7c85", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "75ddc7ee15be14013a3462c01b38b0548486fbcb", "13fe71da009484f240c46f14d9330e932f8de210"]},{"id": "f5433dfc8ced247847e97c5f270fd3b91ae8c0fa", "title": "Segmentation Standard for Chinese Natural Language Processing", "authors": ["Chu-Ren Huang", "Keh-Jiann Chen", "Li-Li Chang"], "date": "1997", "abstract": "This paper proposes a segmentation standard for Chinese natural language processing. The standard is proposed to achieve linguistic felicity, computational feasibility, and data uniformity. Linguistic felicity is maintained by defining a segmentation unit to be equivalent to the theoretical definition of word, and by providing a set of segmentation principles that are equivalent to a functional definition of a word. Computational feasibility is ensured by the fact that the above functional… ", "references": ["c4f298f9b15a56a38bb009425b129380b286f3a8"]},{"id": "f4c018bcc8ea707b83247866bdc8ccb87cd9f5da", "title": "Neural Word Embedding as Implicit Matrix Factorization", "authors": ["Omer Levy", "Yoav Goldberg"], "date": "NIPS", "abstract": "We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a… ", "references": ["553fb89d5858826c02f26e94262e8958debc777e", "dac72f2c509aee67524d3321f77e97e8eff51de6", "dac72f2c509aee67524d3321f77e97e8eff51de6", "715115f21d206ac35c488d775fe30b14b262c327", "2012f32199adc88747d5a1b47c7b4ba1cb3cb995", "715115f21d206ac35c488d775fe30b14b262c327", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "553fb89d5858826c02f26e94262e8958debc777e", "553fb89d5858826c02f26e94262e8958debc777e", "500d570ce02abf42bc1bc535620741d4c5665e6a"]},{"id": "705dcc8eadba137834e4b0359e2d696d4b209f5b", "title": "Neural Tree Indexers for Text Understanding", "authors": ["Tsendsuren Munkhdalai", "Hong Yu"], "date": "2017", "abstract": "Recurrent neural networks (RNNs) process input text sequentially and model the conditional transition between word tokens. In contrast, the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language. However, the current recursive architecture is limited by its dependence on syntactic tree. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers (NTI) that… ", "references": ["36c097a225a95735271960e2b63a2cb9e98bff83", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "13fe71da009484f240c46f14d9330e932f8de210", "4b9af9b9ed1bfb6b0688019d9fa79875bbcd8f3f", "687bac2d3320083eb4530bf18bb8f8f721477600", "36c097a225a95735271960e2b63a2cb9e98bff83", "27e38351e48fe4b7da2775bf94341738bc4da07e", "687bac2d3320083eb4530bf18bb8f8f721477600", "452059171226626718eb677358836328f884298e"]},{"id": "c3b8367a80181e28c95630b9b63060d895de08ff", "title": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval", "authors": ["Xiaodong Liu", "Jianfeng Gao", "Ye-Yi Wang"], "date": "HLT-NAACL", "abstract": "Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross… ", "references": ["687bac2d3320083eb4530bf18bb8f8f721477600", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "af44f5db5b4396e1670cda07eff5ad84145ba843", "bc1022b031dc6c7019696492e8116598097a8c12", "dac72f2c509aee67524d3321f77e97e8eff51de6", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "7e8d5a108c28cdfb92f419ce919fbf7993dfebfc", "7e8d5a108c28cdfb92f419ce919fbf7993dfebfc", "3f3d110cf78f759760c354bfde1b2ceb9883c544", "bc1022b031dc6c7019696492e8116598097a8c12"]},{"id": "138b6767d572e84147da34dd38573b0eff5171b7", "title": "Semi-supervised learning : from Gaussian fields to Gaussian processes", "authors": ["Xiaojin Zhu", "John D. Lafferty", "Zoubin Ghahramani"], "date": "2003", "abstract": "Abstract: \"We show that the Gaussian random fields and harmonic energy minimizing function framework for semi-supervised learning can be viewed in terms of Gaussian processes, with covariance matrices derived from the graph Laplacian. We derive hyperparameter learning with evidence maximization, and give an empirical study of various ways to parameterize the graph weights.\" ", "references": ["0eedbab3ae55fd6a4e7bbc75fcc261293384f883", "f0ddbcb32e50514de5c89c8ceca58345c5a43948", "8b4a99762d33927e4db312082f9552cce1df9182", "fb3790be51620283eff2bcb1e30292a3dae29719", "3d3b01a9ce510c80c72a31595045bb40844e404a", "fb3790be51620283eff2bcb1e30292a3dae29719", "60de4b6068407defa3c88f5feeb8b74d8e55fe9c", "8e6779bb55f7fbed5684ded55df51747ea678a84", "3d3b01a9ce510c80c72a31595045bb40844e404a", "0eedbab3ae55fd6a4e7bbc75fcc261293384f883"]},{"id": "6bbc0c752570c46a772f2982728f9ad4191f25dd", "title": "Cluster Kernels for Semi-Supervised Learning", "authors": ["Olivier Chapelle", "Jason Weston", "Bernhard Schölkopf"], "date": "NIPS", "abstract": "We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach. ", "references": ["44a07a1fc4143cab722751197b12dde3dcc6032b", "8e6779bb55f7fbed5684ded55df51747ea678a84", "c12620cd3022ed03cb35a6265a81dc0f5571d14a", "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8", "63287d3220fe96d5cbf73067545abbb88cc180a6", "c12620cd3022ed03cb35a6265a81dc0f5571d14a", "e45c2420e6dc59ba6d357fb0c996ebf43c861560", "127a1612de2194245377f9cefb049c8c34cacf9f", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "44a07a1fc4143cab722751197b12dde3dcc6032b"]},{"id": "34dc22dcbdf1e09fb48691ee1fc6fe4bb8f834c3", "title": "Efficiently Inducing Features of Conditional Random Fields", "authors": ["Andrew McCallum"], "date": "2003", "abstract": "Conditional Random Fields (CRFs) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary, non-independent features of the input. Faced with this freedom, however, an important question remains: what features should be used? This paper presents an efficient feature induction method for CRFs. The method is founded on the principle of iteratively… ", "references": ["8ec2d52a2c7ac954adfdbe0f3a314379d89b3858", "bece46ed303f8eaef2affae2cba4e0aef51fe636", "8ec2d52a2c7ac954adfdbe0f3a314379d89b3858", "61a559a5ab77b449758795c86c6ff8a42b389987", "4df361d65a15ca9a7fc27c58c38b04d1f41e6f62", "bece46ed303f8eaef2affae2cba4e0aef51fe636", "61a559a5ab77b449758795c86c6ff8a42b389987", "bece46ed303f8eaef2affae2cba4e0aef51fe636", "61a559a5ab77b449758795c86c6ff8a42b389987", "10143673f3f1a49d346120928d6b9a56851d6cf0"]},{"id": "2826f9dccdcceb113b33ccf2841d488f1419bb30", "title": "Stanford Neural Machine Translation Systems for Spoken Language Domains", "authors": ["Minh-Thang Luong", "Christopher D. Manning"], "date": "2015", "abstract": "Neural Machine Translation (NMT), though recently developed, has shown promising results for various language pairs. Despite that, NMT has only been applied to mostly formal texts such as those in the WMT shared tasks. This work further explores the effectiveness of NMT in spoken language domains by participating in the MT track of the IWSLT 2015. We consider two scenarios: (a) how to adapt existing NMT systems to a new domain and (b) the generalization of NMT to low-resource language pairs… ", "references": ["1214a350d1f7509eb09a07498c9403baed964a74", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "1214a350d1f7509eb09a07498c9403baed964a74", "1214a350d1f7509eb09a07498c9403baed964a74", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1214a350d1f7509eb09a07498c9403baed964a74", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "1214a350d1f7509eb09a07498c9403baed964a74"]},{"id": "8e6779bb55f7fbed5684ded55df51747ea678a84", "title": "Partially labeled classification with Markov random walks", "authors": ["Martin Szummer", "Tommi S. Jaakkola"], "date": "NIPS", "abstract": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution… ", "references": ["e8b75090c82893fe4e17c8d5c9f4176845ec598c", "e07c3df9d8d53c3be8cb9e982da98a4471322d90", "8f9d6be4fee69c9a859c9055b09f2ee94864f5b7"]},{"id": "1150f9289c6151506e3f7cf0e6ebbcfd49f1dace", "title": "Active Learning with Statistical Models", "authors": ["David A. Cohn", "Zoubin Ghahramani", "Michael I. Jordan"], "date": "1994", "abstract": "For many types of learners one can compute the statistically \"optimal\" way to select data. We review how these techniques have been used with feedforward neural networks [MacKay, 1992; Cohn, 1994]. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are expensive and approximate, the techniques for mixtures of Gaussians and… ", "references": ["3e06680314e1cf32706686e6520107976fdb7064", "a34e35dbbc6911fa7b94894dffdc0076a261b6f0", "a7404527c3a6aa542ea183da9c821efda05a2afc", "3e06680314e1cf32706686e6520107976fdb7064", "a34e35dbbc6911fa7b94894dffdc0076a261b6f0", "a7404527c3a6aa542ea183da9c821efda05a2afc", "45f43abc49a8a60e6b43ddbda5af9fc6c88d663d", "a34e35dbbc6911fa7b94894dffdc0076a261b6f0", "a34e35dbbc6911fa7b94894dffdc0076a261b6f0", "a7404527c3a6aa542ea183da9c821efda05a2afc"]},{"id": "0eedbab3ae55fd6a4e7bbc75fcc261293384f883", "title": "Learning from Labeled and Unlabeled Data using Graph Mincuts", "authors": ["Avrim Blum", "Shuchi Chawla"], "date": "ICML", "abstract": "Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data. ", "references": ["250748b4494cec56abd55ae049bdd38f4d42e5c8", "53dcb8199cda481d67663efd29f0d80f6f29bf32", "d7a8e3850d7833777aa708e45ff4d0bc47ab1e05", "d0794c1a57cad9c35028427e6c084642346c720f", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "d7a8e3850d7833777aa708e45ff4d0bc47ab1e05", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "cfe5315080aa0b092dda19a127f9a7a2b61508bd", "afcd6da7637ddeef6715109aca248da7a24b1c65", "d7a8e3850d7833777aa708e45ff4d0bc47ab1e05"]},{"id": "50c56af1eb05cfb6ec81e84a6924fb46cb202747", "title": "Using unlabeled data to improve text classification", "authors": ["Kanal Paul Nigam", "Tom Michael Mitchell"], "date": "2001", "abstract": "One key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately. This dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers. By assuming that documents are created by a parametric generative model, Expectation-Maximization (EM) finds local maximum a posteriori models and classifiers from all the… ", "references": ["1c0ece611643cfb8f3a23e4802c754ea583ebe37", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "80ef14d2a1b8c7efbf45bedae9d001fe5446c7de", "5f78d6f79b3ef103cb2d8d170632eb74d9496412", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "ea4feb953b86f6a099d61ffa70d21c59be99f76a", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "5f78d6f79b3ef103cb2d8d170632eb74d9496412", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "f2671b151fad7e176176b35d425b2b6356ff4595"]},{"id": "01dfd7b78017ea4059f02081680a9fd4b2bb2a34", "title": "Selective Sampling Using the Query by Committee Algorithm", "authors": ["Yoav Freund", "H. Sebastian Seung", "Naftali Tishby"], "date": "1997", "abstract": "We analyze the “query by committee” algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons. ", "references": ["788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b", "10ddb646feddc12337b5a755c72e153e37088c02", "788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b", "e449143111fa4a270aa36d1515ca9bba9b172304", "b3c19172a59922fa7cbed8f09f30ed3a1ea74b4e", "a7404527c3a6aa542ea183da9c821efda05a2afc", "941ef255d31b5becbf0a3281bcf7ac0122e4c833", "107ed240fd5a94aa84d9f6297fd6e43a88008755", "e0b8fa3496283d4d808fba9ff62d5f024bcf23be", "10ddb646feddc12337b5a755c72e153e37088c02"]},{"id": "6f17768a9fe231a2fd38708be90f98db3890c986", "title": "Employing Em in Pool-based Active Learning for Text Classiication", "authors": [], "date": "1998", "abstract": "This paper shows how a text classiier's need for labeled training data can be reduced by a combination of active learning and Expectation Maximization (EM) on a pool of unlabeled data. Query-by-Committee is used to actively select documents for labeling, then EM with a naive Bayes model further improves classiication accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model. We also present a metric for better measuring… ", "references": ["80ef14d2a1b8c7efbf45bedae9d001fe5446c7de", "1150f9289c6151506e3f7cf0e6ebbcfd49f1dace", "5194b668c67aa83c037e71599a087f63c98eb713", "1150f9289c6151506e3f7cf0e6ebbcfd49f1dace", "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49", "5194b668c67aa83c037e71599a087f63c98eb713", "80ef14d2a1b8c7efbf45bedae9d001fe5446c7de", "63287d3220fe96d5cbf73067545abbb88cc180a6", "08627b368e6b7d142cfcece74d52d21d48cc64a6", "217a0ecf9795721f9f3661f5562a5b1afd4a3b59"]},{"id": "8f79332b1361e9eaa9da3327f83f57dcac5cd11d", "title": "Bayesian Experimental Design: A Review", "authors": ["Kathryn Chaloner", "Isabella Verdinelli"], "date": "1995", "abstract": "This paper reviews the literature on Bayesian experimental design, both for linear and nonlinear models. A uniied view of the topic is presented by putting experimental design in a decision theoretic framework. This framework justiies many optimality criteria, and opens new possibilities. Various design criteria become part of a single, coherent approach. ", "references": ["baf42023606f1f003accaf5ffa5d830dfb36026a", "5aa5459e20d7991200ae78960881b3dfa1934421", "5875da28d34bacec39e8656a9065bcfa3d8c0b16", "48ffb5b41db39abf9abc3ab410bddbee394a0930", "fa03cbad4356ece5e878466ed0595fbf007474cb", "1dfa44223418aa893d523dc8cb6d4389cad0f127", "baf42023606f1f003accaf5ffa5d830dfb36026a", "baf42023606f1f003accaf5ffa5d830dfb36026a", "baf42023606f1f003accaf5ffa5d830dfb36026a", "1dfa44223418aa893d523dc8cb6d4389cad0f127"]},{"id": "6e50f382197a590765e6d6ee9d6ec899434097e6", "title": "Information geometry and alternating minimization procedures", "authors": ["Imre Csiszár", "Gábor E. Tusnády"], "date": "1984", "abstract": null, "references": []},{"id": "90507f43e958c59ccea41aafd06ab2728749429c", "title": "Computation of channel capacity and rate-distortion functions", "authors": ["Richard E. Blahut"], "date": "1972", "abstract": "By defining mutual information as a maximum over an appropriate space, channel capacities can be defined as double maxima and rate-distortion functions as double minima. This approach yields valuable new insights regarding the computation of channel capacities and rate-distortion functions. In particular, it suggests a simple algorithm for computing channel capacity that consists of a mapping from the set of channel input probability vectors into itself such that the sequence of probability… ", "references": []},{"id": "a8797f1d253c75669d96e6fcceda2be3f8534e1d", "title": "Support Vector Machine Active Learning with Applications to Text Classification", "authors": ["Simon Tong", "Daphne Koller"], "date": "2001", "abstract": "Support vector machines have met with significant success in numerous real-world learning tasks.", "references": ["941ef255d31b5becbf0a3281bcf7ac0122e4c833", "01dfd7b78017ea4059f02081680a9fd4b2bb2a34", "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8", "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8", "5f78d6f79b3ef103cb2d8d170632eb74d9496412", "5194b668c67aa83c037e71599a087f63c98eb713", "64f72adbe8280f92bae8de867d96b02846b76e7f", "0df1aac45ff562089a3bdbcb34e2481a71478651", "0df1aac45ff562089a3bdbcb34e2481a71478651", "5f78d6f79b3ef103cb2d8d170632eb74d9496412"]},{"id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "title": "Latent Dirichlet Allocation", "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "date": "2003", "abstract": "We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable… ", "references": ["d2f8842663375d47ab8d57e726ee28fc3d88bf5d", "4ba18b2f35515f7f3ad3bc38100730c5808a52af", "4ba18b2f35515f7f3ad3bc38100730c5808a52af", "ac59a2ec67391bc7dcdd1618a7e9dd95f47b7511", "e2de29049d62de925cf709024b92774cd82b0a5a", "4ba18b2f35515f7f3ad3bc38100730c5808a52af", "4ba18b2f35515f7f3ad3bc38100730c5808a52af", "6120cc252bc74239012f11b8b075cb7cb16bee26", "6120cc252bc74239012f11b8b075cb7cb16bee26", "3d03b20d932128101f5ed212b867ce22c2d21529"]},{"id": "26dbfca1155d823b72db68663af3262b3da08da6", "title": "Agglomerative Information Bottleneck", "authors": ["Noam Slonim", "Naftali Tishby"], "date": "NIPS", "abstract": "We introduce a novel distributional clustering algorithm that maximizes the mutual information per cluster between data and given categories.", "references": ["3467dcf87e4c4af891a8a91679c35e3f24ed46e7", "e338f0f6f3c41441bd8d3fa99b86e12322722e4f", "7ad58ea541efaec494ea1ec6a5414ea01e236b04", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "7ad58ea541efaec494ea1ec6a5414ea01e236b04", "7dbdb4209626fd92d2436a058663206216036e68", "8b586837ac372141eedafd124c57335cbc893bed", "8b586837ac372141eedafd124c57335cbc893bed", "7ad58ea541efaec494ea1ec6a5414ea01e236b04", "3de5d40b60742e3dfa86b19e7f660962298492af"]},{"id": "0565c6903f86e2c649c326ee7607ecc5ccef3a0f", "title": "Measuring Semantic Similarity by Latent Relational Analysis", "authors": ["Peter D. Turney"], "date": "2005", "abstract": "This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similarity. LRA measures similarity in the semantic relations between two pairs of words. When two pairs have a high degree of relational similarity, they are analogous. For example, the pair cat:meow is analogous to the pair dog:bark. There is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning). In the Vector Space… ", "references": ["da236f03773145a3a1e3246c94c5e0e526fcbe4b", "bceed24a780d6e3e6f2cfac8ea9c4403442465e8", "da236f03773145a3a1e3246c94c5e0e526fcbe4b", "da236f03773145a3a1e3246c94c5e0e526fcbe4b", "fced0c675eff3e8a9b0c18c169aa87c0d30695e3", "fced0c675eff3e8a9b0c18c169aa87c0d30695e3", "bddf98047d69af505a0e33643565ecec280fd1c9", "fced0c675eff3e8a9b0c18c169aa87c0d30695e3", "fced0c675eff3e8a9b0c18c169aa87c0d30695e3", "bddf98047d69af505a0e33643565ecec280fd1c9"]},{"id": "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "title": "Unsupervised Models for Named Entity Classification", "authors": ["Michael Collins", "Yoram Singer"], "date": "EMNLP", "abstract": "This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \"seed\" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling… ", "references": ["41e936981f5a2d55bfec0143e9a15e23ad96436b", "4f739d92813866af8f2a9734912c419bebd940fe", "41e936981f5a2d55bfec0143e9a15e23ad96436b", "b3e9130ecab419f8267fccadf80c1ee2489be793", "41e936981f5a2d55bfec0143e9a15e23ad96436b", "b03f43c5620bfc8993ea25dee20ce52a203ebcf7", "3764baa7465201f054083d02b58fa75f883c4461", "4f739d92813866af8f2a9734912c419bebd940fe", "41e936981f5a2d55bfec0143e9a15e23ad96436b", "b03f43c5620bfc8993ea25dee20ce52a203ebcf7"]},{"id": "944cba683d10d8c1a902e05cd68e32a9f47b372e", "title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods", "authors": ["David Yarowsky"], "date": "ACL", "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%. ", "references": ["67df61766618f54e3b136c18aa28694395b5fd6d", "1d922631a6bf8361d7602e12cafb9e15d421c827", "1bcb0d197554534702640626e57b085051747fce", "1d922631a6bf8361d7602e12cafb9e15d421c827", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "1bcb0d197554534702640626e57b085051747fce", "2904db594c5817305e3a8d8b0248f1754036602a", "b53b6b7ffd1435c2c6a1b6684f9975b73648d131", "85b9eb556c211d954b31d9d58fed6891a07ab473", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214"]},{"id": "41e936981f5a2d55bfec0143e9a15e23ad96436b", "title": "Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping", "authors": ["Ellen Riloff", "Rosie Jones"], "date": "AAAI/IAAI", "abstract": "Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap… ", "references": ["8245f6099f547008522ebbe6fb813d8132085746", "bdc08721414c972ab451f8ef3ef39d63c741b324", "8446830f3c05b97c4d12a0751c022d1ae6a5115b", "6958e58fcaf3f45572bc4e7cf7d45798f0cad175", "e80b34a55aa56578f9a4f27ea207f8c42c93a378", "97a3d56d74575aae5d563ab2a0438a25ffbb69ae", "acec622ca4fb7e01a56116522d35ded149969d0a", "8245f6099f547008522ebbe6fb813d8132085746", "8446830f3c05b97c4d12a0751c022d1ae6a5115b", "8dadbf2dfebe794ad4fc5022f8bb65195c8f0d5a"]},{"id": "2ebfd4a8f0ac488a1bc6b86fb4e2e38071adf841", "title": "The Disambiguation of Nominalisations", "authors": ["Maria Lapata"], "date": "2002", "abstract": "This paper addresses the interpretation of nominalisations, a particular class of compound nouns whose head noun is derived from a verb and whose modifier is interpreted as an argument of this verb. Any attempt to automatically interpret nominalisations needs to take into account: (a) the selectional constraints imposed by the nominalised compound head, (b) the fact that the relation of the modifier and the head noun can be ambiguous, and (c) the fact that these constraints can be easily… ", "references": ["8011fe874ad8584ef778dbeb9de3cebfe8458ce5", "5914dadbc23cdb87b553912efe11664f9d93c32a", "5914dadbc23cdb87b553912efe11664f9d93c32a", "2112ba31752ab484faee7cf5978b379560318d75", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "2112ba31752ab484faee7cf5978b379560318d75", "5914dadbc23cdb87b553912efe11664f9d93c32a", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "2112ba31752ab484faee7cf5978b379560318d75", "4a7b86044535c64641224cd1b99e0e0af315569a"]},{"id": "be3f6b91b0eae23e37b3fb877b6cc7fc7dfcef5a", "title": "Lexical chains as representations of context for the detection and correction of malapropisms", "authors": ["Graeme Hirst", "David St-Onge"], "date": "1995", "abstract": "Natural language utterances are, in general, highlyambiguous, and a unique interpretationcan usuallybe determined only by taking into account the constraining inﬂuence of the context in which theutterance occurred. Much of the research in natural language understanding in the last twenty yearscan be thought of as attempts to characterize and represent context and then derive interpretationsthatﬁt best with that context. Typically, this research was heavy with AI, taking context to be nothing… ", "references": ["7dfb5cf11676d8f22427394eecfcea4dab768670", "d2044ca37a948fc34ea1f3f87e9090ec8bda4a33", "854b0acd17715fca3b5455d51217a023ca4a1883", "ead03592e343d3e82567237d7ec23cecc209a508", "854b0acd17715fca3b5455d51217a023ca4a1883", "ca40dc1300ab085406455894dd42fd02f9cc36f8"]},{"id": "e46c547dcbf65d26e85ff723bae34067cceb3477", "title": "WordNet Sits the S.A.T. - A Knowledge-Based Approach to Lexical Analogy", "authors": ["Tony Veale"], "date": "ECAI", "abstract": "One can measure the extent to which a knowledge-base enables intelligent or creative behavior by determining how useful such a knowledge-base is to the solution of standard psychometric or scholastic tests. In this paper we consider the utility of WordNet, a comprehensive lexical knowledge-base of English word meanings, to the solution of S.A.T. analogies. We propose that these analogies test a student's ability to recognize and estimate a measure of pairwise analogical similarity, and describe… ", "references": []},{"id": "212d2715aee9fbefe140685b088b789d6c8277b0", "title": "Similarity of Semantic Relations", "authors": ["Peter D. Turney"], "date": "2006", "abstract": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent… ", "references": ["1e41fe310351f5d2f6ff2f930f9c062ba43cbe0f", "bddf98047d69af505a0e33643565ecec280fd1c9", "94d2efafe5fd6f47b62e6165e361888ded570e0a", "1e41fe310351f5d2f6ff2f930f9c062ba43cbe0f", "bddf98047d69af505a0e33643565ecec280fd1c9", "1e41fe310351f5d2f6ff2f930f9c062ba43cbe0f", "dc74c850257ea516e3ac32ec30c72d56d80e53a6", "0565c6903f86e2c649c326ee7607ecc5ccef3a0f", "a67f083830790586ed41823d45a7b330d0a2fd95", "0565c6903f86e2c649c326ee7607ecc5ccef3a0f"]},{"id": "dbfd191afbbc8317577cbc44afe7156df546e143", "title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "authors": ["Marti A. Hearst"], "date": "COLING", "abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text.", "references": ["094c0495ebb34c7eb61bad86a96eeebab06dab08", "d06dd8fd5b90a88d56692d7afdcd64de2cc01c6b", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "094c0495ebb34c7eb61bad86a96eeebab06dab08", "1bcb0d197554534702640626e57b085051747fce", "1bcb0d197554534702640626e57b085051747fce", "c6a24aa2b33a2875e4410eb17b44e280cddf4402", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "a24b123b4adf115a0826f0dd3d7b0ef6a182a3c5", "094c0495ebb34c7eb61bad86a96eeebab06dab08"]},{"id": "f3844a65398d3ef22e3c6323e2ec94388e033bb4", "title": "Purest ever example-based machine translation: Detailed presentation and assessment", "authors": ["Yves Lepage", "Etienne Denoual"], "date": "2006", "abstract": "We have designed, implemented and assessed an EBMT system that can be dubbed the “purest ever built”: it strictly does not make any use of variables, templates or patterns, does not have any explicit transfer component, and does not require any preprocessing or training of the aligned examples. It uses only a specific operation, proportional analogy, that implicitly neutralizes divergences between languages and captures lexical and syntactic variations along the paradigmatic and syntagmatic… ", "references": ["767a26f86203b7f1efc41603225c9311ecc72dbe", "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "67aff52aeb121833398121cf2cd9670ed1a38b1d", "f09ed22d2689c89f29fbbeeff63a835c467e04ad", "f09ed22d2689c89f29fbbeeff63a835c467e04ad", "767a26f86203b7f1efc41603225c9311ecc72dbe", "67aff52aeb121833398121cf2cd9670ed1a38b1d", "dc9fc854d78d09a91b029a7c79eb284bea46c886", "b393a326e794a69856ec364db1265e9cf5c9a7d1", "8119849065d27a6a37c7b86efec479aa79c16a72"]},{"id": "e517e1645708e7b050787bb4734002ea194a1958", "title": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL", "authors": ["Peter D. Turney"], "date": "2001", "abstract": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of… ", "references": ["6b64e068a8face2540fc436af40dbcd2b0912bbf", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "5da1f22f51f761690d75c8bb25b0328b67c2b44a", "c2d7252950d51f67fb4613c6a364be48baa709b5", "fd1901f34cc3673072264104885d70555b1a4cdc", "591394cfab5879934a59f6c44f03b6c4009042f0", "546fdb984bd63214dac8f552ef8d8ae6fa7c7d1a", "fd43f1dd66646c8b7abd1053fbfc7b447d7cb21e", "6b64e068a8face2540fc436af40dbcd2b0912bbf", "c2d7252950d51f67fb4613c6a364be48baa709b5"]},{"id": "1755f2ac7eb8a091f3613be47da844803df86fc1", "title": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals", "authors": ["Roxana Girju", "Preslav Nakov", "Deniz Yuret"], "date": "SemEval@ACL", "abstract": "The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of relations between pairs of words in a text. We present an evaluation task designed to provide a framework for comparing different approaches to classifying semantic relations between nominals in a sentence. This is part of SemEval, the 4th edition of the semantic evaluation event previously known as SensEval. We define the task, describe the training/test data and their creation, list… ", "references": ["dd566a85c98064b36990693da21c760f76815f72", "dbfd191afbbc8317577cbc44afe7156df546e143", "dbfd191afbbc8317577cbc44afe7156df546e143", "0c739b915d633cc3c162e4ef1e57b796c2dc2217", "35471fa1234fb7f7dc9586df0c5b23371f806a04", "35471fa1234fb7f7dc9586df0c5b23371f806a04", "56fa2a97da50ddd77dfd5546a4f8807b3cbc15d3", "7e0d29ab6f59d35547bdb48c0139958dc75023a2", "4656da2393dc4dc5935989483a176a07beb59dc1", "4656da2393dc4dc5935989483a176a07beb59dc1"]},{"id": "06cb835bda3420186e2c6f6fa2dbc1613a9b2d75", "title": "Retrieval time from semantic memory", "authors": ["Allan J. Collins", "M. Ross Quillian"], "date": "1969", "abstract": "To ascertain the truth of a sentence such as “A canary can fly,” people utilize long-term memory. Consider two possible organizations of this memory. First, people might store with each kind of bird that flies (e.g., canary) the fact that it can fly. Then they could retrieve this fact directly to decide the sentence is true. An alternative organization would be to store only the generalization that birds can fly, and to infer that “A canary can fly” from the stored information that a canary is… ", "references": []},{"id": "74afa62f0f35a3153230c488ab223a5afb4e8b3e", "title": "A Comparison of Statistical Models for the Extraction of Lexical Information from Text Corpor", "authors": ["Simon Dennis"], "date": "2003", "abstract": "The Syntagmatic Paradigmatic model (SP; Dennis & Harrington 2001, Dennis submitted) and the Pooled Adjacent Context model (PAC; Redington, Chater & Finch 1998) are compared on their ability to extract syntactic, semantic and associative information from a corpus of text. On a measure of syntactic class (and subclass) information based on the WordNet lexical database (Miller 1990), the models performed similarly with a small advantage for the PAC model. On a measure of semantic structure based… ", "references": ["68bba47161b403b86a232ccc36c935030e5e4f70", "ca95bf87cf9f898390ef847d71949a99e2e5038e", "d87dea73ca64a5cfb7754a33f08adb3e2ff65903", "dd35b0855779b4854ae2d23280ff0674d6f77506", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "d87dea73ca64a5cfb7754a33f08adb3e2ff65903", "fcfaac57f007d4272234706117d8935adb9acaad", "fcfaac57f007d4272234706117d8935adb9acaad", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4"]},{"id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "title": "A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "authors": ["Thomas K. Landauer", "Susan T. Dumais"], "date": "1997", "abstract": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of… ", "references": ["7ecf4de1651f6ae28d6756ac81fe25f6f6773ac4", "89868d62919ed8ccf125ee3c4174e1f8d50139a7", "7ecf4de1651f6ae28d6756ac81fe25f6f6773ac4", "52b99d29c931d9aaf1b3d6f48b31577affef0208", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "289d3a9562f57d0182d1aae9376b0e3793d80272", "7ecf4de1651f6ae28d6756ac81fe25f6f6773ac4", "4336c33f0ab03e8722712792d25e46922eed73ed", "0745f57639358ed2a4dffa58bd2367694821224e", "289d3a9562f57d0182d1aae9376b0e3793d80272"]},{"id": "1d8d5b866e19b4072d5cd1a2c0659468db300b57", "title": "Modelling the effects of semantic ambiguity in word recognition", "authors": ["Jm Rodd", "M. Gareth Gaskell", "William D. Marslen-Wilson"], "date": "2004", "abstract": "Most words in English are ambiguous between different interpretations; words can mean different things in different contexts. We investigate the implications of different types of semantic ambiguity for connectionist models of word recognition. We present a model in which there is competition to activate distributed semantic representations. The model performs well on the task of retrieving the different meanings of ambiguous words, and is able to simulate data reported by Rodd, Gaskell, and… ", "references": ["762b76a6743b5f828a543b20be86bd45cc6acb66", "ba4c74682978f82ae89fd3cf302e0a2c7461ea31", "762b76a6743b5f828a543b20be86bd45cc6acb66", "3539d2becfefffdb6fa28f4e216c78b94e662d32", "e487cb69b0d684da254b7fa9a08315583b600eac", "3539d2becfefffdb6fa28f4e216c78b94e662d32", "e97cee4cb47e950c4194ba3fb625ee6f8d5c1db4", "45de42670a4fea8cd302cdba7abdb4bddd9b16aa", "45de42670a4fea8cd302cdba7abdb4bddd9b16aa", "45de42670a4fea8cd302cdba7abdb4bddd9b16aa"]},{"id": "54c2d1d5b8855a7e1b1e87995215c1cf4662f654", "title": "Metaphor as an Emergent Property of Machine-Readable Dictionaries", "authors": ["William B. Dolan"], "date": "1995", "abstract": "Previous computational attempts to handle nonliteral word usage have been restricted to \"toy\" systems that combine hand-coded lexicons with restricted sets of metaphor types that can be used to sanction specific classes of semantic subcategorization violations. These hand-coded efforts are unlikely to ever scale up to the rigors of real, free text. We describe an example-based approach to metaphor interpretation which exploits a large lexical knowledge base derived from a machine-readable… ", "references": []},{"id": "4966f2d75734b4abd4ad105b85eff675cb781b5d", "title": "Experiments on Linguistically-Based Term Associations", "authors": ["Gerda Ruge"], "date": "1991", "abstract": "Abstract A description of the hyperterm system REALIST (REtrieval Aids by LInguistics and STatistics) and in more detail a description of its semantic component is given. We call a hyperterm system a system that contains different kinds of term relations. The semantic component of REALIST generates semantic term relations such as synonyms. It takes as input a free-text database and generates as output term pairs that are semantically related with respect to their meanings in the database. This… ", "references": ["3a720f42ae2e68d1a6f5fe19f8170532e0caef73", "e8e8f5e40c4c40223cb7deffbfc9a22493c7c18e", "9c27e4605c18255f5c9839f7f30b4e3e47c5038b", "1452a5689b6c220fd286c079cc81efd7aedc7bb9", "1452a5689b6c220fd286c079cc81efd7aedc7bb9", "e8e8f5e40c4c40223cb7deffbfc9a22493c7c18e", "8b99ce7e6b73e5274b12e623f4c492cfe35be75b", "8b99ce7e6b73e5274b12e623f4c492cfe35be75b", "8b99ce7e6b73e5274b12e623f4c492cfe35be75b", "4751342adc0ee90cb92fc384eba82cd4a26a9c31"]},{"id": "fbd0e0ad4e06902b10b6a157b9db92df577720f1", "title": "Learning surface text patterns for a Question Answering System", "authors": ["Deepak Ravichandran", "Eduard H. Hovy"], "date": "ACL", "abstract": "In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern… ", "references": ["13d7795f4dc74070ddfe95187ce5e50d852702c5", "7cb56a8518eeb15604f908cdad18660f11b55417", "2f6c079bc68b93e7d0934a0b88c2105a2e39e040", "040ec82087483db3e2c584c1a3901f8fc392c0a3", "c447d259ef71ef567f96faf406683a5d86a37084", "2f6c079bc68b93e7d0934a0b88c2105a2e39e040", "7cb56a8518eeb15604f908cdad18660f11b55417", "13d7795f4dc74070ddfe95187ce5e50d852702c5", "040ec82087483db3e2c584c1a3901f8fc392c0a3", "03a1cf16053deadf60801ee7808d31d2ee0c45ea"]},{"id": "9e2caa39ac534744a180972a30a320ad0ae41ea3", "title": "Word Association Norms, Mutual Information and Lexicography", "authors": ["Kenneth Ward Church", "Patrick Hanks"], "date": "ACL", "abstract": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs… ", "references": ["0de98bd19065702bb44b8aa5ad43ef6923fdacb0", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "0de98bd19065702bb44b8aa5ad43ef6923fdacb0", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "bedfbe4395ead517cfa6e369a3564c8fb6876c04", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10"]},{"id": "f932caac89709a716a7d3e6632caf9f34d709518", "title": "A Probabilistic Account of Logical Metonymy", "authors": ["Mirella Lapata", "Alex Lascarides"], "date": "2003", "abstract": "In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to… ", "references": ["cae22af9bb9b7a2cebe2b4ee0a0364004ab73491", "fcacf8a7e46f8f84142c059031bad3e7f498a039", "0b8849902fb39a71f3f2061e06b4d2e4e7f5dc3a", "7c7babeb5c77591c6918d890fd7bd78a490420e5", "cae22af9bb9b7a2cebe2b4ee0a0364004ab73491", "f1c25c6d030605df497ac599deaf6c0693e6c80e", "77e83a97efd4b4e3812d67f5fda65396f46a9063", "0b8849902fb39a71f3f2061e06b4d2e4e7f5dc3a", "0b8849902fb39a71f3f2061e06b4d2e4e7f5dc3a", "f1c25c6d030605df497ac599deaf6c0693e6c80e"]},{"id": "c84f76770b820d69a6a1f3914a1c84e7c20a8271", "title": "Semantic Cognition: A Parallel Distributed Processing Approach", "authors": ["Timothy T. Rogers", "James L. McClelland"], "date": "2004", "abstract": "This groundbreaking monograph offers a mechanistic theory of the representation and use of semantic knowledge, integrating the strengths and overcoming many of the weaknesses of hierarchical, categorization-based approaches, similarity-based approaches, and the approach often called \"theory theory.\" Building on earlier models by Geoffrey Hinton in the 1980s and David Rumelhart in the early 1990s, the authors propose that performance in semantic tasks arises through the propagation of graded… ", "references": ["a29355274d8b081cedb330d7505eeda7d85140dc", "a29355274d8b081cedb330d7505eeda7d85140dc", "3fb4fe7fc8c201949535b61b5d6c7e1ccdc000f0", "61374d14a581b03af7e4fe0342a722ea94911490", "f4c34dd337b62966e853755ea068f1c0448d5c4f", "45977e4421ed85f0b39af51193d72cc7371ff6a9", "61374d14a581b03af7e4fe0342a722ea94911490", "3fb4fe7fc8c201949535b61b5d6c7e1ccdc000f0", "ddce9772879d92ef32c6e016e5716a034a5406aa", "1bfe2ca67c84724c12f6c94b4896721b9fdf0b70"]},{"id": "4656da2393dc4dc5935989483a176a07beb59dc1", "title": "Classifying the Semantic Relations in Noun Compounds via a Domain-Specific Lexical Hierarchy", "authors": ["Barbara Rosario", "Marti A. Hearst"], "date": "EMNLP", "abstract": "We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes… ", "references": ["b22b62cad8f8b2a7c26d89e6b98f1fc0d5393dac", "b22b62cad8f8b2a7c26d89e6b98f1fc0d5393dac", "d1b49582e60231515e9f23d30ba74838a2333def", "b22b62cad8f8b2a7c26d89e6b98f1fc0d5393dac", "b22b62cad8f8b2a7c26d89e6b98f1fc0d5393dac", "2eae0f08186952643c3a7ead2eba2d41fda58cec", "a66910eaf52ad83b3c8eabe91bc6ebefa64f56cc", "709fca8ad83a64e2d55eb5f57b2a2e3371bb32c1", "52b605b16c9cd474401ce846330360c6b44cbd19", "2eae0f08186952643c3a7ead2eba2d41fda58cec"]},{"id": "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb", "title": "Verb Semantics and Lexical Selection", "authors": ["Zhibiao Wu", "Martha Palmer"], "date": "ACL", "abstract": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT.", "references": ["6b28b05451c69a4f2ec883cef5cfb6999ccab971", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "cae22af9bb9b7a2cebe2b4ee0a0364004ab73491", "cae22af9bb9b7a2cebe2b4ee0a0364004ab73491", "ca798cca16e5624f621a3934854eaa6a8511ff6c", "f10edd0e00c9ab10ca089c1ec9f23e81e48fb1af", "6b28b05451c69a4f2ec883cef5cfb6999ccab971", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "d9f0f36365cfb8b203ed15879188109195c64e03", "ca798cca16e5624f621a3934854eaa6a8511ff6c"]},{"id": "b791d488eef45ef79da812f7569fc2cc83196aa5", "title": "EuroWordNet: A multilingual database with lexical semantic networks", "authors": ["Piek Vossen"], "date": "Springer Netherlands", "abstract": "Introduction to EuroWordNet P. Vossen. The Linguistic Design of the EuroWordNet Database A. Alonge, et al. Compatibility in Interpretation of Relations in EuroWordNet P. Vossen, et al. A Semantic Network of English: The Mother of All WordNets C. Fellbaum. Cross-Linguistic Alignment of Wordnets with an Inter-Lingual-Index W. Peters, et al. ", "references": []},{"id": "46d946344e5ed8871b113af3e498c4640fb31d9b", "title": "Syntactic Contexts for Finding Semantically Related Words", "authors": ["Lonneke van der Plas", "Gosse Bouma"], "date": "2004", "abstract": "Finding semantically related words is a first step in the direction of automatic ontology building. Guided by the view that similar words occur in similar contexts, we looked at the syntactic context of words to measure their semantic similarity. Words that occur in a direct object relation with the verb drink, for instance, have something in common (liquidity, ...). Co-occurrence data for common nouns and proper names, for several syntactic relations, was collected from an automatically parsed… ", "references": ["9e2caa39ac534744a180972a30a320ad0ae41ea3", "084c55d6432265785e3ff86a2e900a49d501c00a", "a9c9605885e7cb52fe8ea89fd1a7e93d7f36899b", "ea419a6e8583d12f6b91be48df41edb7a8b8d6b6", "5da1f22f51f761690d75c8bb25b0328b67c2b44a", "a9c9605885e7cb52fe8ea89fd1a7e93d7f36899b", "a9c9605885e7cb52fe8ea89fd1a7e93d7f36899b", "5da1f22f51f761690d75c8bb25b0328b67c2b44a", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "fd1901f34cc3673072264104885d70555b1a4cdc"]},{"id": "3751d4d5e9332f6e98824a6fd814e2ce8f497daf", "title": "Word Sense Disambiguation using Conceptual Density", "authors": ["Eneko Agirre", "German Rigau"], "date": "1996", "abstract": "This paper present a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been… ", "references": ["6e55023e67ee4681736b0ca5ef516b8abaca0ca0", "59407446503d49a8cf5f5643b17502835b62f139", "4017c6c606bc2c04bcda87891e32bc03280c3198", "1d922631a6bf8361d7602e12cafb9e15d421c827", "2eae0f08186952643c3a7ead2eba2d41fda58cec", "6e55023e67ee4681736b0ca5ef516b8abaca0ca0", "6e55023e67ee4681736b0ca5ef516b8abaca0ca0", "63936c6ce3ffedd4e4bc653a7ce3c3d295adb8c0", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214"]},{"id": "384720361a8cae1f2bbb87e05215cc58fef74ac1", "title": "Extracting Semantic Representations from Large Text Corpora", "authors": ["Malti Patel", "John A. Bullinaria", "Joseph P. Levy"], "date": "NCPW", "abstract": "Many connectionist language processing models have now reached a level of detail at which more realistic representations of semantics are required. In this paper we discuss the extraction of semantic representations from the word co-occurrence statistics of large text corpora and present a preliminary investigation into the validation and optimisation of such representations. We find that there is significantly more variation across the extraction procedures and evaluation criteria than is… ", "references": ["a7e62ca1603444c9589be66a08825058abf44845", "a8a3ff991718aaeceb03948e1a694070675763c1", "dbd1994e7e63e8aef509604d3c38652cbf30f9de", "d859f0d4c78bc772b350854cae30e72e5610ce71", "d30f3840a1a3c1e3be3580e2922d4eecc54a0b53", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "a7e62ca1603444c9589be66a08825058abf44845", "d30f3840a1a3c1e3be3580e2922d4eecc54a0b53", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "dbd1994e7e63e8aef509604d3c38652cbf30f9de"]},{"id": "a4a1b6b2580609e5a64fd62ea4f1ff6bcc2ba8bd", "title": "Using Subcategorization to Resolve Verb Class Ambiguity", "authors": ["Mirella Lapata", "Chris Brew"], "date": "EMNLP", "abstract": "Levin's (1993) taxonomy of verbs and their classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give exhibit no class ambiguity. But other verbs, such as write, can inhabit more than one class. In some of these ambiguous cases the appropriate class for a particular token of a verb is immediately obvious from inspection of the surrounding context. In others it is not, and an application which wants to recover this information will be forced to rely on… ", "references": []},{"id": "e8066c5522ebfa8f0f08589dcbe5f315bfec90c1", "title": "Scaling Context Space", "authors": ["James R. Curran", "Marc Moens"], "date": "ACL", "abstract": "Context is used in many NLP systems as an indicator of a term's syntactic and semantic function. The accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term. However, the quantity variable is no longer fixed by limited corpus resources. Given fixed training time and computational resources, it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus. However, with an effectively… ", "references": ["ea419a6e8583d12f6b91be48df41edb7a8b8d6b6", "f6e1db64a6e7e724bfc088ed0f3c2fcf3ede06d5", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "99328d4b34d1ac02252258a9437b8b2c1acdb92c", "f6e1db64a6e7e724bfc088ed0f3c2fcf3ede06d5", "3de5d40b60742e3dfa86b19e7f660962298492af", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "4580518cf3ff4bd6c6bcd4a2d5b8f0d146f2e24a", "f6e1db64a6e7e724bfc088ed0f3c2fcf3ede06d5", "5eb328cf7e94995199e4c82a1f4d0696430a80b5"]},{"id": "d61b4f56253d65b586a5553768c40168e897dff7", "title": "Detecting the Organization of Semantic Subclasses of Japanese Verbs", "authors": ["Akira Oishi", "Yuji Matsumoto"], "date": "1997", "abstract": "This paper describes an approach to detect the organization of semantic subclasses of Japanese verbs. First, we classify verbs along two dimensions: thematic and aspectual. In the thematic dimension, we exploit the pattern of case marking particles which are attached to arguments of verbs. In the aspectual dimension, we exploit the classification of adverbs which modify verbs in a corpus. By combining the results of two classifications, we obtain an elaborate classification of verbs. We can… ", "references": ["ad174dce3323869d32b06eb3a7779fb255ce2839", "ad174dce3323869d32b06eb3a7779fb255ce2839", "ad174dce3323869d32b06eb3a7779fb255ce2839", "ad174dce3323869d32b06eb3a7779fb255ce2839", "79a4b8a2ea7fce2ea6ab1aa1f22019a1e3b8d1ae"]},{"id": "8b6ce9178000cf7304694e56bbe804e2674d4a7f", "title": "Establishing the Upper Bound and Inter-judge Agreement of a Verb Classification Task", "authors": ["Paola Merlo", "Suzanne Stevenson"], "date": "LREC", "abstract": "Detailed knowledge about verbs is critical in many NLP and IR tasks, yet manual determination of such knowledge for large numbers of verbs is difficult, time-consuming and resource intensive. Recent responses to this problem have attempted to classify verbs automatically, as a first step to automatically build lexical resources. In order to estimate the upper bound of a verb classification task, which appears to be difficult and subject to variability among experts, we investigated the… ", "references": ["84f4940d8f8169dc2bdfbd31cffa21be08e0ba48", "6934c934a7b80b06e00e168a5d8b07aada4ae197", "cfcce52943ac3c16ae0bff6c4bbc260b067a759b", "19093faac77eb132b2c8ce21a20f66718e60f2b6", "cfcce52943ac3c16ae0bff6c4bbc260b067a759b", "613b6c9a85ae338cd3b405dc019c8edb1c15717c", "bcd7cc92f6162f3fcc29c1c109acca8bef27e1e9", "6934c934a7b80b06e00e168a5d8b07aada4ae197", "feb5f9a821a90b30020872275a0c1d0b0ad72106", "6934c934a7b80b06e00e168a5d8b07aada4ae197"]},{"id": "1521ddb27860cc8834f8a82e62665bf983c8ad2c", "title": "The Word-Space Model : Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces", "authors": ["Magnus Sahlgren"], "date": "2006", "abstract": "The word-space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in ter ... ", "references": ["fbec1c3e45237b0cd7e6e1f7daf928fecef2d2a3", "527eb9c939801f1edcedace66eff7bbc02f74e80", "fbec1c3e45237b0cd7e6e1f7daf928fecef2d2a3", "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "e569d99f3a0fcfa038631dda2b44c73a6e8e97b8", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "5d24afe3a62331ebfad400c3fec77c836d2b99db", "fbec1c3e45237b0cd7e6e1f7daf928fecef2d2a3"]},{"id": "6b64e068a8face2540fc436af40dbcd2b0912bbf", "title": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy", "authors": ["Jay J. Jiang", "David W. Conrath"], "date": "ROCLING", "abstract": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts.", "references": ["36c4c51917b1f53ee85c459f2597e115df53eb05", "36c4c51917b1f53ee85c459f2597e115df53eb05", "f9a25e0dc776857fc24ebc7115c980312f2719b1", "f9a25e0dc776857fc24ebc7115c980312f2719b1", "f9a25e0dc776857fc24ebc7115c980312f2719b1", "36c4c51917b1f53ee85c459f2597e115df53eb05", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "ca40dc1300ab085406455894dd42fd02f9cc36f8", "ca40dc1300ab085406455894dd42fd02f9cc36f8", "ca40dc1300ab085406455894dd42fd02f9cc36f8"]},{"id": "6e986f302a0b0b0fd700c89b94ec54585c5e45a7", "title": "Automatic Acquisition of the Lexical Semantics of Verbs from Sentence Frames", "authors": ["Mort Webster", "Mitchell P. Marcus"], "date": "ACL", "abstract": "This paper presents a computational model of verb acquisition which uses what we will call the principle of structured overcommitment to eliminate the need for negative evidence. The learner escapes from the need to be told that certain possibilities cannot occur (i.e., are \"ungrammatical\") by one simple expedient: It assumes that all properties it has observed are either obligatory or forbidden until it sees otherwise, at which point it decides that what it thought was either obligatory or… ", "references": ["fb0641ea5b2c74b0e1fc5163f87d14a886c50e9e", "058ae758447f426da57f2cec7e43992b8be4a9dd", "5ef314815d5e5be9e7aa0a050604f00e56f2d325", "fb0641ea5b2c74b0e1fc5163f87d14a886c50e9e"]},{"id": "19093faac77eb132b2c8ce21a20f66718e60f2b6", "title": "Supervised Learning of Lexical Semantic Verb Classes Using Frequency Distributions", "authors": ["Suzanne Stevenson", "Paola Merlo", "Natalia Kariaeva Rutgers"], "date": "SIGLEX Workshop On…", "abstract": "Vve zeport a number of computatmnal experiments m supervised learning whose goal Is to automatmally classify a set of verbs into lexmal semanUc classes, based on frequency dls tnbutmn approxlmatmns of grammatical features extracted from a very large annotated corpus DlstnbuUons of five syntactic features that approximate tranmUvlty alternatmns and thematic role assignments are sufficient to reduce error rate by 56% over chance We conclude that corpus da ta is a usable repository of verb class… ", "references": ["1510a8316786a5144f31012f5049136002a33ba7", "4c02e5b0411d76f74d78c55a9cf9588d78fa51a6", "1b4f35e5298fc199eeb54b64dd5ee71565eb4d0d"]},{"id": "42053bb5bc617a3c75c56f61cc6be1e8e53f4e29", "title": "Role of Word Sense Disambiguation in Lexical Acquisition: Predicting Semantics from Syntactic Cues", "authors": ["Bonnie J. Dorr", "Douglas A. Jones"], "date": "COLING", "abstract": "This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e… ", "references": []},{"id": "45de5bfc4895b1cdb8177cf312327c60ca513099", "title": "Thematic proto-roles and argument selection", "authors": ["David R. Dowty"], "date": "1991", "abstract": "As a novel attack on the perennially vexing questions of the theoretical status of thematic roles and the inventory of possible roles, this paper defends a strategy of basing accounts of roles on more unified domains of linguistic data than have been used in the past to motivate roles, addressing in particular the problem of ARGUMENT SELECTION (principles determining which roles are associated with which grammatical relations). It is concluded that the best theory for describing this domain is… ", "references": []},{"id": "b2c04cc369b8f08f399d5fb95ddc884d52cfebd2", "title": "Semantic feature production norms for a large set of living and nonliving things", "authors": ["Ken McRae", "George S. Cree", "Chris McNorgan"], "date": "2005", "abstract": "Semantic features have provided insight into numerous behavioral phenomena concerning concepts, categorization, and semantic memory in adults, children, and neuropsychological populations. Numerous theories and models in these areas are based on representations and computations involving semantic features. Consequently, empirically derived semantic feature production norms have played, and continue to play, a highly useful role in these domains. This article describes a set of feature norms… ", "references": ["7c9a4334c1ca95e5d495c5d4e0e0a0edd4acc203", "7c9a4334c1ca95e5d495c5d4e0e0a0edd4acc203", "2d4d118626f234c68daaa7212cb9e42cf32ffdec", "c2cde741f943a5739130dcc9c25691d940a7da94", "f0ae61f7240293e056f5299dac6dc9d65669b247", "7ecf4de1651f6ae28d6756ac81fe25f6f6773ac4", "53a16e9155788c24bf5acadb39bceeca6732c2be", "c0311652af4aeb551857d3ac254370568b7ce411", "7c9a4334c1ca95e5d495c5d4e0e0a0edd4acc203", "7ecf4de1651f6ae28d6756ac81fe25f6f6773ac4"]},{"id": "3107651f0f2ec7a6e6b713d820f5e083b6446c8d", "title": "Distributed Representations of Ambiguous Words and Their Resolution in a Connectionist Network", "authors": ["Alan H. Kawamoto"], "date": "1988", "abstract": "Publisher Summary This chapter shows how a distributed representation of ambiguous words captures the effects of frequency and contextual biases quite naturally. Moreover, the flexibility of this type of representation allows both polysemy and ambiguity to be treated identically, and suggests how neologisms can be handled. Although the use of distributed representations in psychology is not very widespread, these ideas are continually explored because of the demonstrations of their utility in a… ", "references": []},{"id": "94a461dd1bd2c70c562fc0791a1cb7d933c83e01", "title": "Representing Properties Locally", "authors": ["Karen Olseth Solomon", "Lawrence W. Barsalou"], "date": "2001", "abstract": "Theories of knowledge such as feature lists, semantic networks, and localist neural nets typically use a single global symbol to represent a property that occurs in multiple concepts. Thus, a global symbol represents mane across HORSE, PONY, and LION. Alternatively, perceptual theories of knowledge, as well as distributed representational systems, assume that properties take different local forms in different concepts. Thus, different local forms of mane exist for HORSE, PONY, and LION, each… ", "references": ["2b5bbd672b70306838a90e26be964761023e4398", "715b6ff25b06761f2dd386e2cb5adeb6ce905d09", "f35bf7814242e8eb302f680eae6d02e294eeb4d5", "cec8c321d9c6f7d8965af2e076d93c7fea791b1f", "eca25d88a3121fda671f56a64560f11b4f1b1c21", "cec8c321d9c6f7d8965af2e076d93c7fea791b1f", "484fad4405c88774b9ae0fd24b4aaf588daff763", "715b6ff25b06761f2dd386e2cb5adeb6ce905d09", "f35bf7814242e8eb302f680eae6d02e294eeb4d5", "3106e66537a0c8f53278e553bcb38f0b0992ec0e"]},{"id": "40f616fe271de6908b87f04a4d39f75aba41c3bc", "title": "Word Sense Discovery Based on Sense Descriptor Dissimilarity", "authors": ["Reinhard Rapp"], "date": "2003", "abstract": "In machine translation, information on word ambiguities is usually provided by the lexicographers who construct the lexicon. In this paper we propose an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word. The approach is based on the statistics of the distributional similarity between the words in a corpus. Our algorithm works as follows: The 20 strongest first-order associations to the ambiguous word are considered as sense… ", "references": ["3317f2788b2b07d9ba4cb4335e29316fcf8a971a", "944cba683d10d8c1a902e05cd68e32a9f47b372e", "a6c75fd9e25ef1ec1dc8eaa41a3cbcfe92df1cfa", "d5bd9ad9a3b49ce936679a3c2bf5ec7277bcb5f5", "a6c75fd9e25ef1ec1dc8eaa41a3cbcfe92df1cfa", "49af3e80343eb80c61e727ae0c27541628c7c5e2", "3317f2788b2b07d9ba4cb4335e29316fcf8a971a", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "77377e0322a98f50fd93d2da2cc0abf35e608948", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617"]},{"id": "88e190feb57cf046abca09a0569bab50d824d9d5", "title": "Representation and structure in connectionist models", "authors": ["Jeffrey L. Elman"], "date": "1991", "abstract": "Abstract : This paper focuses on the nature of representations in connectionist models. It addresses two issues: Can connectionist models develop representations which possess internal structure and which provide the basis for productive and systematic behavior; and Can representations which are fundamentally context-sensitive support grammatical behavior which appears to be abstract and general? Results from two simulations are reported.. The simulations address problems in the distinction… ", "references": []},{"id": "1e554939c3788b70d0e7b316aa7f051d889dd493", "title": "Rules and Maps in Connectionist Symbol Processing", "authors": ["David S. Touretzky"], "date": "1989", "abstract": "Abstract : This report contains two papers to be presented at the Eleventh Annual Conference of the Cognitive Science Society. The first describes a simulation of chunking in a connectionist network. The network applies context-sensitive rewrite rules to strings of symbols as they flow through its input buffer. Chunking is implemented as a form of self-supervised learning using back-propagation. Over time, the network improves its efficiency by replacing simple rule sequences with more complex… ", "references": []},{"id": "a801005c1eb6e293f0ea571c9080b131db9e6992", "title": "Sentence Comprehension: A Parallel Distributed Processing Approach", "authors": ["James L. McClelland", "Mark F. St. John", "Roman Taraban"], "date": "1989", "abstract": "Abstract In this paper, we review basic aspects of conventional approaches to sentence comprehension and point out some of the difficulties faced by models that take these approaches. We then describe an alternative approach, based on the principles of parallel distributed processing, and show how it offers different answers to basic questions about the nature of the language processing mechanism. We describe an illustrative simulation model that captures the key characteristics of the approach… ", "references": ["a044c62299ddb831c53c5a1a30a40ba043ae4b03", "f616b489a1065188891b0b62f235708fea7b4189", "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7", "a044c62299ddb831c53c5a1a30a40ba043ae4b03", "a2641de9a59d4f176a6e088d79846576e5ff9513", "4e91cf1e102ff02077ade48ac80806afb7a0e96a", "eb08c068252dd92c3bd670d21ae26547ebcaf00f", "88e190feb57cf046abca09a0569bab50d824d9d5", "4e91cf1e102ff02077ade48ac80806afb7a0e96a", "9928cac725ebe6db7b974bbd65738d33dc95332d"]},{"id": "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "title": "Towards History-Based Grammars: Using Richer Models for Probabilistic Parsing", "authors": ["Ezra Black", "Frederick Jelinek", "Salim Roukos"], "date": "1992", "abstract": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a… ", "references": ["8cf661487d8708a3e9a74e9cc83ce290aa5355b8", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "bdaf232c561f1f50e88b1d24097e214890b37e8b", "943362e27fdfff2628765f001777bf05fb0b8f95", "943362e27fdfff2628765f001777bf05fb0b8f95", "098de23f08e080bed8a224bf1ad2e504688d3db3", "2d30aa623fd96da99a16c0c3bde73f50c92a5c42", "943362e27fdfff2628765f001777bf05fb0b8f95", "860dfdaa8187bd22809f00396b30c66a2fc1ef24", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10"]},{"id": "4fa569625b5ab35e955a8d5be11a4aa9f59ca424", "title": "On learning the past-tenses of English verbs: implicit rules or parallel distributed processing", "authors": ["David E. Rumelhart", "James L. McClelland"], "date": "1986", "abstract": "Abstract : This paper presents an alternative to the standard rule based account of a child's acquisition of the past tense in English.", "references": []},{"id": "a4c0e02d99de82149efd719260e5a5549a13854a", "title": "An Empirical Comparison of Probability Models for Dependency Grammar", "authors": ["Jason Eisner"], "date": "1996", "abstract": "This technical report is an appendix to Eisner (1996): it gives superior experimental results that were reported only in the talk version of that paper, with details of how the results were obtained. Eisner (1996) trained three probability models on a small set of about 4,000 conjunction-free, dependencygrammar parses derived from the Wall Street Journal section of the Penn Treebank, and then evaluated the models on a held-out test set, using a novel O(n 3 ) parsing algorithm. The present paper… ", "references": ["5752b8dcec5856b7ad6289bbe1177acce535fba4", "f853daccfcb2350f9adcd75331d148b04c21e5ef", "a2b5cb0285d593dde7c5a1b844ab0361aebfc85d", "33be02735525a3eb6111ee790ea2e15775019d21", "5752b8dcec5856b7ad6289bbe1177acce535fba4", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "adfef97814b292a09520d8c78a141e7a4baf8726", "f853daccfcb2350f9adcd75331d148b04c21e5ef"]},{"id": "436772d9a916f0382800cf18581cfdfd4f83c457", "title": "Immediate-Head Parsing for Language Models", "authors": ["Eugene Charniak"], "date": "ACL", "abstract": "We present two language models based upon an \"immediate-head\" parser --- our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models… ", "references": ["3fc44ff7f37ec5585310666c183c65e0a0bb2446", "673992da19d9209434615b12d55bdd36be706e9e", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "0ffa423a5283396c88ff3d4033d541796bd039cc", "79fbfc1dc8846379074aaf4deb7fb0a96722eeed", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "3fc44ff7f37ec5585310666c183c65e0a0bb2446"]},{"id": "e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3", "title": "Probabilistic Top-Down Parsing and Language Modeling", "authors": ["Brian Roark"], "date": "2001", "abstract": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the… ", "references": ["2652c6a08ebcf75d4448cecef6faa058021aeba1", "f2d00cf1be1f129c88d0471263b90a3f3f06e942", "2652c6a08ebcf75d4448cecef6faa058021aeba1", "8ad8e98574a275930bf04a477ce3532fd13c503c", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "2a5e619f2c5f4220438b1357e596db5b1578398d", "de92006681796ca5a0b5ed044cff47488e98be92", "54c846ee00c6132d70429cc279e8577f63ed05e4", "673992da19d9209434615b12d55bdd36be706e9e", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58"]},{"id": "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "title": "Statistical Decision-Tree Models for Parsing", "authors": ["David M. Magerman"], "date": "1995", "abstract": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any… ", "references": ["3de5d40b60742e3dfa86b19e7f660962298492af"]},{"id": "a5eb96540ef53b49eac2246d6b13635fe6e54451", "title": "A general framework for adaptive processing of data structures", "authors": ["Paolo Frasconi", "Marco Gori", "Alessandro Sperduti"], "date": "1998", "abstract": "A structured organization of information is typically required by symbolic processing. On the other hand, most connectionist models assume that data are organized according to relatively poor structures, like arrays or sequences. The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information. In particular, relations between data variables are expressed by directed acyclic graphs… ", "references": ["fa7a32d9ce76cd016cf21d4f956e19d90e87b0dc", "e6dfc50eeb234d3458b13c055a57b0119c5c9435", "e6dfc50eeb234d3458b13c055a57b0119c5c9435", "fa7a32d9ce76cd016cf21d4f956e19d90e87b0dc", "f8e3da72acbe4fb13031f503b9151ea44d80c2ea", "6a835df43fdc2f79126319f6fa033bb42147c6f6", "3e33eca03933caaec671e20692e79d1acc9527e1", "e6dfc50eeb234d3458b13c055a57b0119c5c9435", "25bdc473f8377c1200adbd691fbb3cc77fa7bf70", "3e33eca03933caaec671e20692e79d1acc9527e1"]},{"id": "e6c7adc28e20d361d5c35aa9808094b10f6a34d1", "title": "Convolution Kernels for Natural Language", "authors": ["Michael Collins", "Nigel Duffy"], "date": "NIPS", "abstract": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give… ", "references": ["7dd9743183f07b7653cc0335fcc1042aa71032c6", "29fdbbd3bb0b3c798a57e10576d318281d37dd2a", "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac", "5e7cc4189113dd4aa26bf0f517cf406ef874c154", "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac", "f330f1f472f860212b980bb9be81eff884f7f0e1", "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac", "67de7786e49c285bf85cd8f9404aea4bd42e2da9", "7dd9743183f07b7653cc0335fcc1042aa71032c6", "29fdbbd3bb0b3c798a57e10576d318281d37dd2a"]},{"id": "844db702be4bc149b06b822b47247e15f5894cc3", "title": "Discriminative Reranking for Natural Language Parsing", "authors": ["Michael Collins", "Terry K Koo"], "date": "2005", "abstract": "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how… ", "references": ["3fc44ff7f37ec5585310666c183c65e0a0bb2446", "2ae8397c07bd3c76f84c7cdac7897e8b7dec9029", "fe638b5610475d4524684fb2c2b7b08c119c8700", "a46152d8ad27ae47086334c33c8376185b40340d", "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1", "ee6892b9c7f1a491e0925b913b66281c48408f74", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "fe638b5610475d4524684fb2c2b7b08c119c8700", "2a5e619f2c5f4220438b1357e596db5b1578398d", "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1"]},{"id": "fe638b5610475d4524684fb2c2b7b08c119c8700", "title": "New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron", "authors": ["Michael Collins", "Nigel Duffy"], "date": "ACL", "abstract": "This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the \"all subtrees\" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction… ", "references": ["584dfd7f167221bb68908b6228a208d6cf47a1b1", "155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa", "584dfd7f167221bb68908b6228a208d6cf47a1b1", "584dfd7f167221bb68908b6228a208d6cf47a1b1", "d73a70359f568ab32943a74f7891a27257847b3e", "d73a70359f568ab32943a74f7891a27257847b3e", "155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa", "e6c7adc28e20d361d5c35aa9808094b10f6a34d1", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "5e7cc4189113dd4aa26bf0f517cf406ef874c154"]},{"id": "8ac9089b40346c3b6c962798f929789595a21e4f", "title": "A Neural Network Parser that Handles Sparse Data", "authors": ["James Henderson"], "date": "IWPT", "abstract": "Simple Synchrony Networks (SSNs) have previously been shown to be a viable alternative method for syntactic parsing. Here we use an SSN to estimate the parameters of a probabilistic parsing model, and compare this parser's performance against a standard statistical parsing method, a Probabilistic Context Free Grammar. We focus these experiments on demonstrating one of the main advantages of SSNs, handling sparse data. We use smaller datasets than are typically used with statistical methods… ", "references": ["82ea4bc77ada4542231718bcdf5b1705e485c654", "0c6c23ed6b52df3048c6b25d5663e36c3aa975cb", "e6140a793a4554806eb39d15c018d8f782d2ac1e", "82ea4bc77ada4542231718bcdf5b1705e485c654", "29fdbbd3bb0b3c798a57e10576d318281d37dd2a", "2aa1013574ad2928353a6f79364e9c617f26072f", "e6140a793a4554806eb39d15c018d8f782d2ac1e", "82ea4bc77ada4542231718bcdf5b1705e485c654", "2aa1013574ad2928353a6f79364e9c617f26072f", "2aa1013574ad2928353a6f79364e9c617f26072f"]},{"id": "f330f1f472f860212b980bb9be81eff884f7f0e1", "title": "Text Classification using String Kernels", "authors": ["Huma Lodhi", "Craig Saunders", "Chris Watkins"], "date": "J. Mach. Learn. Res.", "abstract": "We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a… ", "references": ["ceb251324c5bcc44a881fc68e003771ecf4275f9", "67d5cd322a6c6d91a26d96518f839325e7ab9b92", "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac", "d5f169880e30e1f76827d72f862555d00b01bed9", "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49", "ceb251324c5bcc44a881fc68e003771ecf4275f9", "36aa0d0936b2cf128c646c36a1981807b5a27aaf", "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5", "7dd9743183f07b7653cc0335fcc1042aa71032c6", "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5"]},{"id": "d4ab5a707aeab5d52f162136bab86f3d2b1ff8ad", "title": "Learning first-pass structural attachment preferences with dynamic grammars and recursive neural networks", "authors": ["Patrick Sturt", "Fabrizio Costa", "Paolo Frasconi"], "date": "2003", "abstract": "One of the central problems in the study of human language processing is ambiguity resolution: how do people resolve the extremely pervasive ambiguity of the language they encounter? One possible answer to this question is suggested by experience-based models, which claim that people typically resolve ambiguities in a way which has been successful in the past. In order to determine the course of action that has been \"successful in the past\" when faced with some ambiguity, it is necessary to… ", "references": ["0af7d0b414827af2865c03ab52b2bee8b94fe646", "0af7d0b414827af2865c03ab52b2bee8b94fe646", "e50e9b1350fb387075fbce321adaa0b27f4fb4d6", "e12699afd17766df7df17edf8319cc3bddc029fe", "e14e94b9579308a8a9b685ccc63ca2c7894311e1", "e14e94b9579308a8a9b685ccc63ca2c7894311e1", "f2cb26dd36db893ff284ff7d8767c02729821cfe", "e14e94b9579308a8a9b685ccc63ca2c7894311e1", "4e91cf1e102ff02077ade48ac80806afb7a0e96a", "0af7d0b414827af2865c03ab52b2bee8b94fe646"]},{"id": "d73a70359f568ab32943a74f7891a27257847b3e", "title": "What is the Minimal Set of Fragments that Achieves Maximal Parse Accuracy?", "authors": ["Rens Bod"], "date": "ACL", "abstract": "We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precision of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy. ", "references": ["76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "5371f05bec8a597ca1a8cdf57fd358600dc698db", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "3764baa7465201f054083d02b58fa75f883c4461", "aa4e5b36158b21995ec6d65ede6a40494e285a0a", "54f2fd5606562691cdea3062fc6b5463f4f86750", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "54f2fd5606562691cdea3062fc6b5463f4f86750", "351752e0080d7635aef227b5d0bd6461cf0b14bd", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"]},{"id": "81926eab7d1eb37b0c72c8aeb04420617568e965", "title": "Probabilistic Parsing Using Left Corner Language Models", "authors": ["Christopher D. Manning", "Bob Carpenter"], "date": "1997", "abstract": "We introduce a novel probabilistic grammar model based on a probabilistic version of a left-corner parser. The left-corner strategy is attractive because rule probabilities can be conditioned on both top-down goals and bottom-up derivations. We develop the underlying theory and explain how a grammar can be induced from analyzed data. We show that the left-corner approach provides an advantage over simple top-down probabilistic context-free grammars in parsing the Wall Street Journal using a… ", "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "c17db5eb1f20ab61fd99a15a4312378861238b71", "7689778171dc100bb636fc0e4e2ce4063967d3c9", "8ad8e98574a275930bf04a477ce3532fd13c503c", "bdaf232c561f1f50e88b1d24097e214890b37e8b", "8ad8e98574a275930bf04a477ce3532fd13c503c", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "d11ac5f5b46f8936cd12854295b5bb95d2bfc7e4", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"]},{"id": "cecef246b1cb60c0c03b5ba229701987f3c3088b", "title": "Wide Coverage Incremental Parsing by Learning Attachment Preferences", "authors": ["Fabrizio Costa", "Vincenzo Lombardo", "Giovanni Soda"], "date": "AI*IA", "abstract": "This paper presents a novel method for wide coverage parsing using an incremental strategy, which is psycholinguistically motivated. A recursive neural network is trained on treebank data to learn first pass attachments, and is employed as a heuristic for guidingpa rsingde cision. The parser is lexically blind and uses beam search to explore the space of plausible partial parses and returns the full analysis havinghi ghest probability. Results are based on preliminary tests on the WSJ section… ", "references": ["16568e028cb044cc0963ecd8bf9902a45ba167a8", "8f8e36b40ea49eddaa7e45193ec9758e185fd686", "e50e9b1350fb387075fbce321adaa0b27f4fb4d6", "e50e9b1350fb387075fbce321adaa0b27f4fb4d6", "2a5e619f2c5f4220438b1357e596db5b1578398d", "e50e9b1350fb387075fbce321adaa0b27f4fb4d6", "54c846ee00c6132d70429cc279e8577f63ed05e4", "c5415f40c3c7eebf9832dba880cf4c0ebf1af732", "c5415f40c3c7eebf9832dba880cf4c0ebf1af732", "54c846ee00c6132d70429cc279e8577f63ed05e4"]},{"id": "bd5a55b27310ad9ccb8b37fa59c028e2149a8ccd", "title": "Ambiguity resolution analysis in incremental parsing of natural language", "authors": ["Fabrizio Costa", "Paolo Frasconi", "Giovanni Soda"], "date": "2005", "abstract": "Incremental parsing gains its importance in natural language processing and psycholinguistics because of its cognitive plausibility. Modeling the associated cognitive data structures, and their dynamics, can lead to a better understanding of the human parser. In earlier work, we have introduced a recursive neural network (RNN) capable of performing syntactic ambiguity resolution in incremental parsing. In this paper, we report a systematic analysis of the behavior of the network that allows us… ", "references": ["f8efefaa4cf1f573dfe8ac1cf31293bc59369139", "4b2ca8f2146ac82ca87d9f314e18573415e381f3", "d4ab5a707aeab5d52f162136bab86f3d2b1ff8ad", "e50e9b1350fb387075fbce321adaa0b27f4fb4d6", "81a6899d1ecc7458a9d5b915ffa94c43b576d857", "e50e9b1350fb387075fbce321adaa0b27f4fb4d6", "4e91cf1e102ff02077ade48ac80806afb7a0e96a", "e14e94b9579308a8a9b685ccc63ca2c7894311e1", "f8efefaa4cf1f573dfe8ac1cf31293bc59369139", "81a6899d1ecc7458a9d5b915ffa94c43b576d857"]},{"id": "6c9f553e723a40a6713453b734b552c1928bf52b", "title": "PCFG Models of Linguistic Tree Representations", "authors": ["Mark Johnson"], "date": "1998", "abstract": "The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average… ", "references": ["cdfefdebd4686a878e6572cb8ba2da9d8efbe552", "3764baa7465201f054083d02b58fa75f883c4461", "b84276fe751ca4f1389549281383b151a746107b", "b84276fe751ca4f1389549281383b151a746107b", "938ca87821c6ec6fcb263179aaa71824aefa9b5a", "2a5e619f2c5f4220438b1357e596db5b1578398d", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "cdfefdebd4686a878e6572cb8ba2da9d8efbe552", "da838db79e7593018894ada44db35eee670941d6", "cdfefdebd4686a878e6572cb8ba2da9d8efbe552"]},{"id": "5c8d05e27e36ebd64ee43fe1670262cdcc2123ba", "title": "Measuring praise and criticism: Inference of semantic orientation from association", "authors": ["Peter D. Turney", "Michael L. Littman"], "date": "2003", "abstract": "The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., \"honest\", \"intrepid\") and negative semantic orientation indicates criticism (e.g., \"disturbing\", \"superfluous\"). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions… ", "references": ["6db4e86e6377cd703aaaf3a3b471b62e033757ae", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "c8eafcd758663642141cbdb1cab4f79c90d3cc0c", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "e217ab0cbe2d9db16c37756e84341fda7999520d", "6db4e86e6377cd703aaaf3a3b471b62e033757ae", "6db4e86e6377cd703aaaf3a3b471b62e033757ae", "6db4e86e6377cd703aaaf3a3b471b62e033757ae"]},{"id": "8ec5c6abf7b8c64d5939285bfe97b56d9bd2c6f4", "title": "Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis", "authors": ["Hiroshi Kanayama", "Tetsuya Nasukawa"], "date": "EMNLP", "abstract": "This paper proposes an unsupervised lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a specific domain. The lexical entries to be acquired are called polar atoms, the minimum human-understandable syntactic structures that specify the polarity of clauses. As a clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts. Using the overall density and precision of… ", "references": ["bf522ebd27a823e265dd7a98dfb079f41d37fb18", "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "9e7c7853a16a378cc24a082153b282257a9675b7", "ababc1999b5f31409c78c39d1842219821e37a6d", "9b4876f7313b111074e79a01f570e6e9e02c0dce", "9e7c7853a16a378cc24a082153b282257a9675b7", "167e1359943b96b9e92ee73db1df69a1f65d731d", "ab3eec0a211e16751effa729281dd448001203db", "ff75055d4e47737702d3b550879d6128cec13233", "ab3eec0a211e16751effa729281dd448001203db"]},{"id": "9de3d523623899badd57cc6996fde336aed545e7", "title": "Validating the Coverage of Lexical Resources for Affect Analysis and Automatically Classifying New Words along Semantic Axes", "authors": ["Gregory Grefenstette", "Yan Qu", "James G. Shanahan"], "date": "Computing Attitude and Affect…", "abstract": "In addition to factual content, many texts contain an emotional dimension. This emotive, or affect, dimension has not received a great amount of attention in computational linguistics until recently. However, now that messages (including spam) have become more prevalent than edited texts (such as newswire), recognizing this emotive dimension of written text is becoming more important. One resource needed for identifying affect in text is a lexicon of words with emotion-conveying potential… ", "references": ["4bf9fb7b8aaf072497ce0cfbb046cd7451473e62", "e517e1645708e7b050787bb4734002ea194a1958", "65518ed8d80e858f92ef65d3591d6e8bbf0b5023", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "8a26ded61b67cce241352ba8742cd8fa2541d605", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "f3f3dcfcaa960ec201e0381f4d026e57e64bea76", "8c7aee4d436ff52a85a8c9982d28c6ebda3f2b8c", "65518ed8d80e858f92ef65d3591d6e8bbf0b5023"]},{"id": "d2d8b52f59945b4a3ef9d20ab44e108319eead6f", "title": "Mining WordNet for a Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses", "authors": ["Alina Andreevskaia", "Sabine Bergler"], "date": "EACL", "abstract": "Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then collapsed into a… ", "references": ["c95055d5f16fcba89bad2e182a6e5a10ddb64367", "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "3609a2128980a61e760c09b26c74d467289253d3", "0048eb28f6cfb297ebd30dc17347baf171724039", "a88a61933c3c8807021b29196a9f6db8cd05a7a2", "c95055d5f16fcba89bad2e182a6e5a10ddb64367", "9de3d523623899badd57cc6996fde336aed545e7", "0048eb28f6cfb297ebd30dc17347baf171724039", "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "0048eb28f6cfb297ebd30dc17347baf171724039"]},{"id": "c95055d5f16fcba89bad2e182a6e5a10ddb64367", "title": "Using WordNet to Measure Semantic Orientations of Adjectives", "authors": ["Jaap Kamps", "maarten marx", "Maarten de Rijke"], "date": "LREC", "abstract": "Current WordNet-based measures of distance or similarity focus almost exclusively on WordNet’s taxonomic relations.", "references": ["9e7c7853a16a378cc24a082153b282257a9675b7", "e517e1645708e7b050787bb4734002ea194a1958", "3a8d4fd2c30e5031a574bc25363c8639912b3bbd", "7bc73f7a102ba29780eae9648c2e099e9f002fa2", "3a8d4fd2c30e5031a574bc25363c8639912b3bbd", "3a8d4fd2c30e5031a574bc25363c8639912b3bbd", "7bc73f7a102ba29780eae9648c2e099e9f002fa2", "6db4e86e6377cd703aaaf3a3b471b62e033757ae", "9e7c7853a16a378cc24a082153b282257a9675b7", "265be00bf112c6cb2fa3e8176bff8394a114dbde"]},{"id": "157d40d3e7fba266544cbbfd5f84852ddd944f8b", "title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "authors": ["Tejashri Inadarchand Jain", "Dipak Nemade"], "date": "2010", "abstract": "There has been a recent swell of interest in the automatic identification and extraction of opinions, emotions, and sentiments in text. Motivation for this task comes from the desire to provide tools for information analysts in government, commercial, and political domains, who want to automatically track attitudes and feelings in the news and on-line forums. How do people feel about recent events in the Middle East? Is the rhetoric from a particular opposition group intensifying? What is the… ", "references": ["1cff7cc15555c38607016aaba24059e76b160adb", "ababc1999b5f31409c78c39d1842219821e37a6d", "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "1cff7cc15555c38607016aaba24059e76b160adb", "a8ddcabf410b5691cd73880df2ee4b83a9799a5d", "310b72fbc3d384ca88ca994b33476b8a2be2e27f", "ababc1999b5f31409c78c39d1842219821e37a6d", "38825a919dab5acbd48b3333a26689aea9428f7a", "ababc1999b5f31409c78c39d1842219821e37a6d"]},{"id": "de00b6d04d7d4c7d554b404f54b4362c9ba3df92", "title": "A model of textual affect sensing using real-world knowledge", "authors": ["Hugo Liu", "Henry Lieberman", "Ted Selker"], "date": "IUI '03", "abstract": "This paper presents a novel way for assessing the affective qualities of natural language and a scenario for its use. Previous approaches to textual affect sensing have employed keyword spotting, lexical affinity, statistical methods, and hand-crafted models. This paper demonstrates a new approach, using large-scale real-world knowledge about the inherent affective nature of everyday situations (such as \"getting into a car accident\") to classify sentences into \"basic\" emotion categories. This… ", "references": []},{"id": "9e7c7853a16a378cc24a082153b282257a9675b7", "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews", "authors": ["Peter D. Turney"], "date": "2002", "abstract": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \"subtle nuances\") and a negative semantic orientation when it has bad associations (e.g., \"very cavalier\"). In this paper, the… ", "references": ["9e2caa39ac534744a180972a30a320ad0ae41ea3", "733234e097dceb9011baa8914930861996eb0b5e", "e517e1645708e7b050787bb4734002ea194a1958", "733234e097dceb9011baa8914930861996eb0b5e", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "a145854ede2f62098bf4e92de1584ab270b676c9", "e517e1645708e7b050787bb4734002ea194a1958", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "5581992944c66522dd1b11f8a6150aeef2d95b7a"]},{"id": "96761dc3d8adaeff44eb9c07501eb3109802ee4b", "title": "SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining", "authors": ["Andrea Esuli", "Fabrizio Sebastiani"], "date": "LREC", "abstract": "Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses.", "references": ["af5c4034493461af13a7f5480e081becf0218511", "3570ad7102c8484824b383455a3c017455b7efaa", "167e1359943b96b9e92ee73db1df69a1f65d731d", "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "af5c4034493461af13a7f5480e081becf0218511", "97bdc5522ae46b281389654199d656eb67728a32", "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "167e1359943b96b9e92ee73db1df69a1f65d731d", "3570ad7102c8484824b383455a3c017455b7efaa"]},{"id": "8a26ded61b67cce241352ba8742cd8fa2541d605", "title": "Affect analysis of text using fuzzy semantic typing", "authors": ["Pero Subasic", "Alison Huettner"], "date": "2000", "abstract": "We propose a convenient fusion of natural-language processing and fuzzy logic techniques for analyzing affect content in free text; our main goals are fast analysis and visualization of affect content for decision-making. The primary linguistic resource for fuzzy semantic typing is the fuzzy affect lexicon, from which other important resources are generated, notably the fuzzy thesaurus and affect category groups. Free text is tagged with affect categories from the lexicon, and the affect… ", "references": ["402627e4eb8c95e4aae3026fd921aa08cd792006", "402627e4eb8c95e4aae3026fd921aa08cd792006"]},{"id": "1cff7cc15555c38607016aaba24059e76b160adb", "title": "Annotating Expressions of Opinions and Emotions in Language", "authors": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie"], "date": "2005", "abstract": "This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented. ", "references": ["310b72fbc3d384ca88ca994b33476b8a2be2e27f", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "7c89cbf5d860819c9b5e5217d079dc8aafcba336", "b419fb257870fe8024fb3f07dddd66328ae644cb", "ce20678cfbffded477f43df156f6ab37f6edc6a0", "7c89cbf5d860819c9b5e5217d079dc8aafcba336", "7c89cbf5d860819c9b5e5217d079dc8aafcba336", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "bc8af2c7e4049c473f662a0ed9a985b3f399c70d", "5581992944c66522dd1b11f8a6150aeef2d95b7a"]},{"id": "836f5bc6b13404790d01c337e71511c1eed96b53", "title": "A Corpus-based Approach to Finding Happiness", "authors": ["Rada Mihalcea", "Hugo Liu"], "date": "AAAI Spring Symposium…", "abstract": "What are the sources of happiness and sadness in everyday life.", "references": ["146ab1794dc148f0ffd15c5e27225d7cd1c3b299", "146ab1794dc148f0ffd15c5e27225d7cd1c3b299", "2d8f71d92f8dfc26e39f4e5aa68d2a45f9b6d0c4", "1c30f9e4feb185ed04e4f4af31a943ad8c4c180f", "1c30f9e4feb185ed04e4f4af31a943ad8c4c180f", "2d8f71d92f8dfc26e39f4e5aa68d2a45f9b6d0c4", "1c30f9e4feb185ed04e4f4af31a943ad8c4c180f", "146ab1794dc148f0ffd15c5e27225d7cd1c3b299", "f488dd6f2920b1d9780080dde1fcc0caf3be6d5b", "1c30f9e4feb185ed04e4f4af31a943ad8c4c180f"]},{"id": "a3b3aad58ecc6aed599c7567d4fe07ad3480a866", "title": "Approximation algorithms for classification problems with pairwise relationships: metric labeling and Markov random fields", "authors": ["Jon M. Kleinberg", "Éva Tardos"], "date": "2002", "abstract": "In a traditional classification problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified; this issue is one of the principal motivations for the framework of Markov random fields, and it arises in areas such as image processing… ", "references": ["b543d70d3e4fe6673a2e39832f005fa7ebc79cec", "0fd469a2954f8a3dad2de4438199443e54a1849c", "3261bb81085f59efae1e1c72453c47daaee777ac", "0fd469a2954f8a3dad2de4438199443e54a1849c", "a57520b68e73b5e1fc3668b443daf74ebe957cc7", "94ca2c3f6ad7051e0b424002e815ead666a2a7ed", "47865b56fee61d9c9ff477f7c79f090cc6663d3a", "4aeea86e589383e2ec2d4214e919ebda9277c452", "cc5a04a4f4919cffc3cf8530974bc8de4c90c753", "0fd469a2954f8a3dad2de4438199443e54a1849c"]},{"id": "9b4876f7313b111074e79a01f570e6e9e02c0dce", "title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "authors": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann"], "date": "HLT/EMNLP", "abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline. ", "references": ["3f46b61a7216e5763ffab5d33e06b88c9d490c85", "a8ddcabf410b5691cd73880df2ee4b83a9799a5d", "97bdc5522ae46b281389654199d656eb67728a32", "1cff7cc15555c38607016aaba24059e76b160adb", "45472c24919b1a7a008f84131dcb7b9f729a72ed", "a8ddcabf410b5691cd73880df2ee4b83a9799a5d", "45472c24919b1a7a008f84131dcb7b9f729a72ed", "45472c24919b1a7a008f84131dcb7b9f729a72ed", "45472c24919b1a7a008f84131dcb7b9f729a72ed", "45472c24919b1a7a008f84131dcb7b9f729a72ed"]},{"id": "28633b18c2b3bad80b1edb552146cc200b90f0e7", "title": "Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches", "authors": ["Pimwadee Chaovalit", "Lina Zhou"], "date": "2005", "abstract": "Web content mining is intended to help people discover valuable information from large amount of unstructured data on the web.", "references": ["dd52c776306c304b223a30c9f7beee38b45d59b8", "dd52c776306c304b223a30c9f7beee38b45d59b8", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "5c8d05e27e36ebd64ee43fe1670262cdcc2123ba", "4f0d67a3a8a61d3a7ef0f940274ecff5f0d640ea", "9e7c7853a16a378cc24a082153b282257a9675b7", "dd52c776306c304b223a30c9f7beee38b45d59b8", "fc93f3182bb0fda36764de93978dbd71b8128e6f", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc"]},{"id": "ec7c68427a26f812532b1c913c68fcf84b7de58e", "title": "Beyond the point cloud: from transductive to semi-supervised learning", "authors": ["Vikas Sindhwani", "Partha Niyogi", "Mikhail Belkin"], "date": "ICML '05", "abstract": "Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make… ", "references": ["49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f", "49b8dff62cccc26023c876460234bf29084a382f"]},{"id": "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "title": "Mining and summarizing customer reviews", "authors": ["Minqing Hu", "Bing Liu"], "date": "KDD '04", "abstract": "Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services.", "references": ["4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37", "4bd970a37c59c97804ff93cbb2c108e081de3a37"]},{"id": "26affdaceca32cd4a5fde0db61ffef02a59baa13", "title": "Learning Recursive Distributed Representations for Holistic Computation", "authors": ["Lonnie Chrisman"], "date": "1991", "abstract": "A number of connectionist models capable of representing data with compositional structure have recently appeared. These new models suggest the intriguing possibility of performing holistic structure-sensitive computations with distributed representations. Two possible forms of holistic inference, transformational inference and confluent inference, are identified and compared. Transformational inference was successfully demonstrated by Chalmers; however, the pure transformational approach does… ", "references": []},{"id": "14a9a814a54dbab99388fafbd96a1c5fe249e376", "title": "Learning Distributed Representations for the Classification of Terms", "authors": ["Alessandro Sperduti", "Antonina Starita", "Christoph Goller"], "date": "IJCAI", "abstract": "This paper is a study on LRAAM-based (Labeling Recursive Auto-Associative Memory)classification of symbolic recursive structures encoding terms. The results reported here have been obtained by combining an LRAAM network with an analog perceptron. The approach used was to interleave the development of representations (unsupervised learning of the LRAAM) with the learning of the classification task. In this way, the representations are optimized with respect to the classification task. The… ", "references": ["668087f0ae7ce1de6e0bd0965dbb480c08103260"]},{"id": "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7", "title": "Efficient Non-Parametric Function Induction in Semi-Supervised Learning", "authors": ["Olivier Delalleau", "Yoshua Bengio", "Nicolas Le Roux"], "date": "AISTATS", "abstract": "There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones. This paper follows up on proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples. It extends them to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of… ", "references": ["38a49f2d906b48a36ab4baca448298666a9ec259", "125842668eab7decac136db8a59d392dc5e4e395", "138b6767d572e84147da34dd38573b0eff5171b7", "8e6779bb55f7fbed5684ded55df51747ea678a84", "8e6779bb55f7fbed5684ded55df51747ea678a84", "69fe08fb1aa15bbab4ca26c31cc9302e325870b1", "ceb251324c5bcc44a881fc68e003771ecf4275f9", "8e6779bb55f7fbed5684ded55df51747ea678a84", "1a8bed1f13ff4b7b3ae4eedee25a17f7ad2583eb", "1a8bed1f13ff4b7b3ae4eedee25a17f7ad2583eb"]},{"id": "bd210021bf4a559c1849351d90c20ce7fcd34dea", "title": "Distributed representations for terms in hybrid reasoning systems", "authors": ["Alessandro Sperduti", "Antonina Starita", "Christoph Goller"], "date": "1997", "abstract": "This paper is a study on LRAAM-based (Labeling Recursive Auto-Associative Memory) clas-siication of symbolic recursive structures encoding terms. The results reported here have been obtained by combining an LRAAM network with an analog perceptron. The approach used was to interleave the development of representations (unsupervised learning of the LRAAM) with the learning of the classiica-tion task. In this way, the representations are optimized with respect to the classiica-tion task. The… ", "references": ["ef634444b659fca4a0783d01cf46088bb0cd4695", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "512a1f65f9b2a7e7a6e8a1dc91a43f6509f83d02", "512a1f65f9b2a7e7a6e8a1dc91a43f6509f83d02", "f03db7ef9cf309561eb02eb317b875deb8817c01", "f03db7ef9cf309561eb02eb317b875deb8817c01", "ef634444b659fca4a0783d01cf46088bb0cd4695", "b7aeec4ca9416c80b77cf72ed43ca9698bb5f2f4"]},{"id": "f48d6233238c9da9cf36cbcdc9e5d00cf6e1b4b0", "title": "A Connectionist Parser with Recursive Sentence Structure and Lexical Disambiguation", "authors": ["George Berg"], "date": "AAAI", "abstract": "In order to be taken seriously, connectionist natural language processing systems must be able to parse syntactically complex sentences. Current connectionist parsers either ignore structure or impose prior restrictions on the structural complexity of the sentences they can process -- either number of phrases or the \"depth\" of the sentence structure. XERIC networks, presented here, are distributed representation connectionist parsers which can analyze and represent syntactically varied… ", "references": ["bd46c1b5948abe04e565a8bae6454da63a1b021e", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "1bb1057fb35b583bb1ea283e6059cf4a81090fe9", "6a835df43fdc2f79126319f6fa033bb42147c6f6", "9a2f22a9afb0f1a97e3c636055a2307776f259b3", "9b0bf4ef287a2794332d8df22b6820d5f0f49a35", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7", "9b0bf4ef287a2794332d8df22b6820d5f0f49a35", "111fd833a4ae576cfdbb27d87d2f8fc0640af355"]},{"id": "6a835df43fdc2f79126319f6fa033bb42147c6f6", "title": "Recursive Distributed Representations", "authors": ["Jordan B. Pollack"], "date": "1990", "abstract": "Abstract A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of… ", "references": ["4fa569625b5ab35e955a8d5be11a4aa9f59ca424", "70623eb30f076d6de599c4dff13b04bd62e29e0c", "c96fe25817c5fca96719cfa56cdaeeb2d17c93d7", "70623eb30f076d6de599c4dff13b04bd62e29e0c", "9928cac725ebe6db7b974bbd65738d33dc95332d", "70623eb30f076d6de599c4dff13b04bd62e29e0c", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "114b2df7ba63669c2f901d4e3c298d9360c4ae7d", "4fa569625b5ab35e955a8d5be11a4aa9f59ca424", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657"]},{"id": "4add0dd0041403f585cf85b455aae487e37dfd3a", "title": "Structure Sensitivity in Connectionist Models", "authors": ["Lars Niklasson"], "date": "1993", "abstract": "Annotation: Published in The Proceedings of the 1993 Connectionist Models Summer School, (Eds) Mozer et al., Lawrence Erlbaum, 1993. ", "references": []},{"id": "f03db7ef9cf309561eb02eb317b875deb8817c01", "title": "Encoding Labeled Graphs by Labeling RAAM", "authors": ["Alessandro Sperduti"], "date": "NIPS", "abstract": "In this paper we propose an extension to the RAAM by Pollack. This extension, the Labeling RAAM (LRAAM), can encode labeled graphs with cycles by representing pointers explicitly. Data encoded in an LRAAM can be accessed by pointer as well as by content. Direct access by content can be achieved by transforming the encoder network of the LRAAM into an analog Hopfield network with hidden units. Different access procedures can be defined depending on the access key. Sufficient conditions on the… ", "references": ["71dd4d477ca17b4db3b270d25225822ff3a41fac", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "564427596799f7967c91934966cd3c6bd31cb06d", "cb204afe3e9237f50ef8ea3b8dbf751cc096e2a0", "9c2e776fb40667fde3ece360cb829f717ce5daf9", "fb31c68da5bb2e4d069858c2f9531dd236f8e046", "4d20a039f2fec9a6439a3326c8e3b435a0d11e99", "9c2e776fb40667fde3ece360cb829f717ce5daf9", "9552ac39a57daacf3d75865a268935b5a0df9bbb", "fb31c68da5bb2e4d069858c2f9531dd236f8e046"]},{"id": "ffd77cd376a8ba679e01300f0eb7518892701f62", "title": "Overview of Multilingual Opinion Analysis Task at NTCIR-7", "authors": ["Yohei Seki", "David Kirk Evans", "Noriko Kando"], "date": "NTCIR", "abstract": "This paper describes an overview of the Multilingual Opinion Analysis Task from 2007 to 2008 at the Seventh NTCIR Workshop. We created test collections of 22, 17, 17, 16 topics (7,163, 4,711, 6,174, and 5,301 sentences) in Japanese, English, Traditional Chinese, and Simplified Chinese. Using this test collection, we conducted five subtasks: (1) mandatory opinionated sentence judgment, and optional subtasks of (2) relevant sentence judgment, (3) polarity judgment, (4) opinion holder extraction… ", "references": ["9b4876f7313b111074e79a01f570e6e9e02c0dce", "ea55d078ba065a4cff3787f5d4bd55d98939f682"]},{"id": "b8f896caa1226713ed2731101cb8de21195dbf0b", "title": "Recovering Latent Information in Treebanks", "authors": ["David Chiang", "Daniel M. Bikel"], "date": "COLING", "abstract": "Many recent statistical parsers rely on a preprocessing step which uses hand-written, corpus-specific rules to augment the training data with extra information. For example, head-finding rules are used to augment node labels with lexical heads. In this paper, we provide machinery to reduce the amount of human effort needed to adapt existing models to new corpora: first, we propose a flexible notation for specifying these rules that would allow them to be shared by different models; second, we… ", "references": ["76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "e1b4d6ba9970fb69545d20d6e7882e4d478a812f", "f0c90a5bc53027d76b24854209a4cdb1bd75dd2f", "4535a8309c1a84ad885865200de13eaae3ebe754", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "4535a8309c1a84ad885865200de13eaae3ebe754", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "3fc44ff7f37ec5585310666c183c65e0a0bb2446"]},{"id": "23c93e0bb33ed2cf4432003d2ceedb2dbc658cb7", "title": "When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging", "authors": ["Alina Andreevskaia", "Sabine Bergler"], "date": "ACL", "abstract": "This study presents a novel approach to the problem of system portability across different domains: a sentiment annotation system that integrates a corpus-based classifier trained on a small set of annotated in-domain data and a lexicon-based system trained on WordNet. The paper explores the challenges of system portability across domains and text genres (movie reviews, news, blogs, and product reviews), highlights the factors affecting system performance on out-of-domain and smallset in-domain… ", "references": ["e14609a3a6c6f8ef3269d3e0728f88da57826698", "58ff3f13e06bb47a343414b6aa2d173dd63f7319", "1b7c02e4f07477a5ae2d1aa52db8ac6ddb55f53c", "c76ac8a61ad046c0394db869db8f781c6aa62ead", "167e1359943b96b9e92ee73db1df69a1f65d731d", "d895647b4a80861703851ef55930a2627fe19492", "167e1359943b96b9e92ee73db1df69a1f65d731d", "c76ac8a61ad046c0394db869db8f781c6aa62ead", "d2d8b52f59945b4a3ef9d20ab44e108319eead6f", "c0c29293fcc01e41dfa00350f1527ca4371ec045"]},{"id": "d9b0190b06ac7270e9052895f8592beb4959ccfd", "title": "Sentiment Composition", "authors": ["Karo Moilanen", "Stephen Pulman"], "date": "2007", "abstract": "Sentiment classification of grammatical constituents can be explained in a quasicompositional way. The classification of a complex constituent is derived via the classification of its component constituents and operations on these that resemble the usual methods of compositional semantic analysis. This claim is illustrated with a description of sentiment propagation, polarity reversal, and polarity conflict resolution within various linguistic constituent types at various grammatical levels. We… ", "references": ["3f46b61a7216e5763ffab5d33e06b88c9d490c85"]},{"id": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques", "authors": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "date": "EMNLP", "abstract": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative.", "references": ["03dc0ddcacb60cc09b884dcfb3b8cd78bc2cf1a3", "c40775a9ca3e493eaf91ed2e6e526c775776c47a", "44e915a220ce74badf755aae870fa0b69ee2b82a", "656859af2ed88cfa23f2bd063c1816a8fc04c47e", "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49", "f177c6022e4798b156bfc53de3a976d0c6c357cb", "03dc0ddcacb60cc09b884dcfb3b8cd78bc2cf1a3", "dd52c776306c304b223a30c9f7beee38b45d59b8", "44e915a220ce74badf755aae870fa0b69ee2b82a", "04ce064505b1635583fa0d9cc07cac7e9ea993cc"]},{"id": "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "title": "Probabilistic CFG with Latent Annotations", "authors": ["Takuya Matsuzaki", "Yusuke Miyao", "Jun'ichi Tsujii"], "date": "ACL", "abstract": "This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model… ", "references": ["b9eea85e590f6e522e3681b8e45012684c60b0fd", "b8f896caa1226713ed2731101cb8de21195dbf0b", "b9eea85e590f6e522e3681b8e45012684c60b0fd", "2ae8397c07bd3c76f84c7cdac7897e8b7dec9029", "155a2680a1a7fe495d0ab47224fcdbbfb3d1caaa", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "2588593c42126e059fb8aad7673fa1736755f1e1", "2588593c42126e059fb8aad7673fa1736755f1e1", "0c0eab87d4855c42ae6395bf2e27eefe55003b4a"]},{"id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "title": "Accurate Unlexicalized Parsing", "authors": ["Dan Klein", "Christopher D. Manning"], "date": "ACL", "abstract": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible… ", "references": ["f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "1007e2bfb377757a75f51a0edaf745edeabf3757", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "1007e2bfb377757a75f51a0edaf745edeabf3757", "436772d9a916f0382800cf18581cfdfd4f83c457", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "3764baa7465201f054083d02b58fa75f883c4461", "3fc44ff7f37ec5585310666c183c65e0a0bb2446"]},{"id": "0bf69a49c2baed67fa9a044daa24b9e199e73093", "title": "Inducing Probabilistic Grammars by Bayesian Model Merging", "authors": ["Andreas Stolcke", "Stephen M. Omohundro"], "date": "ICGI", "abstract": "We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data… ", "references": ["204f6148bc6aba37eb5a7c5686d80547a99425b1", "ff10417468d6a4ebbfbfb123e366c0caf3fcd66c", "6272baf82e2e442edab4fb613ef2b7186bf5f1fb", "3de5d40b60742e3dfa86b19e7f660962298492af", "6272baf82e2e442edab4fb613ef2b7186bf5f1fb", "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b", "9d076613d7c36dbda4a6ff42fbdd076604b96630", "6272baf82e2e442edab4fb613ef2b7186bf5f1fb", "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b", "6c79a9bb8f885050cad70b4c69e016b186ffa538"]},{"id": "93231398214275e4316aa19ced49a508ace56ffa", "title": "Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-Bank Grammar for Parsing", "authors": ["Detlef Prescher"], "date": "ECML", "abstract": "Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree-bank. While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition. Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible to automatically induce an accurate parser from a… ", "references": ["0606291dae96446e812ea8f09d9fbdc6acc3ec37", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "e53c204088216e2930e01e90b58bac83ff702d39", "aad8c5ae265e8f645101245afb9d9c9cdf40b4ca", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "b8f896caa1226713ed2731101cb8de21195dbf0b", "0606291dae96446e812ea8f09d9fbdc6acc3ec37", "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "0606291dae96446e812ea8f09d9fbdc6acc3ec37", "fed64bc406adaef9d993f34363f6f1818bbb118e"]},{"id": "338a891907dce447da9a0fa2f27221bd35164163", "title": "Mining the peanut gallery: opinion extraction and semantic classification of product reviews", "authors": ["Kushal Dave", "Steve Lawrence", "David M. Pennock"], "date": "WWW '03", "abstract": "The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on… ", "references": ["a88a61933c3c8807021b29196a9f6db8cd05a7a2", "a88a61933c3c8807021b29196a9f6db8cd05a7a2", "8a26ded61b67cce241352ba8742cd8fa2541d605", "fc93f3182bb0fda36764de93978dbd71b8128e6f", "7e7d692f9f50281ad8600be1fa8f158157cf4be2", "8a26ded61b67cce241352ba8742cd8fa2541d605", "8a26ded61b67cce241352ba8742cd8fa2541d605", "7e7d692f9f50281ad8600be1fa8f158157cf4be2", "7e7d692f9f50281ad8600be1fa8f158157cf4be2", "7e7d692f9f50281ad8600be1fa8f158157cf4be2"]},{"id": "45472c24919b1a7a008f84131dcb7b9f729a72ed", "title": "The Sentimental Factor: Improving Review Classification Via Human-Provided Information", "authors": ["Philip Beineke", "Trevor J. Hastie", "Shivakumar Vaithyanathan"], "date": "ACL", "abstract": "Sentiment classification is the task of labeling a review document according to the polarity of its prevailing opinion (favorable or unfavorable). In approaching this problem, a model builder often has three sources of information available: a small collection of labeled documents, a large collection of unlabeled documents, and human understanding of language. Ideally, a learning method will utilize all three sources. To accomplish this goal, we generalize an existing procedure that uses the… ", "references": ["a88a61933c3c8807021b29196a9f6db8cd05a7a2", "4f0d67a3a8a61d3a7ef0f940274ecff5f0d640ea", "9e7c7853a16a378cc24a082153b282257a9675b7", "4f0d67a3a8a61d3a7ef0f940274ecff5f0d640ea", "4f0d67a3a8a61d3a7ef0f940274ecff5f0d640ea", "a88a61933c3c8807021b29196a9f6db8cd05a7a2", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "38ad328db3d9239fec56c0d329366ef5e60031e8", "38ad328db3d9239fec56c0d329366ef5e60031e8", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc"]},{"id": "0650df86ad901fb9aadd9033a83c328a6f595666", "title": "A Language Modeling Approach to Predicting Reading Difficulty", "authors": ["Kevyn Collins-Thompson", "James P. Callan"], "date": "HLT-NAACL", "abstract": "We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform… ", "references": ["1b030f63fb0a6d59ca786f0053b32f5649e431a9", "f7d26cc701ef9d860712a6f0b4c7f0145b8c4d1e", "cddeb5149f4de157d4daeb609a8b1432a8126e7b"]},{"id": "3bc4736f9b8512043ed47357a81f26b93a1204b6", "title": "Semi-supervised learning with graphs", "authors": ["Xiaojin Zhu", "John D. Lafferty", "Ronald Rosenfeld"], "date": "2005", "abstract": "In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build… ", "references": ["c5c6ea2f23fe8d3e986c4c99e83a90c204538619", "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7", "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7", "e2de29049d62de925cf709024b92774cd82b0a5a", "c5c6ea2f23fe8d3e986c4c99e83a90c204538619", "c5c6ea2f23fe8d3e986c4c99e83a90c204538619", "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7", "0eedbab3ae55fd6a4e7bbc75fcc261293384f883", "0eedbab3ae55fd6a4e7bbc75fcc261293384f883", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4"]},{"id": "d8f07fe5d91e267978bb2f14e019115990eb227b", "title": "Sentiment Extraction from Unstructured Text using Tabu Search-Enhanced Markov Blanket", "authors": ["Xue Bai", "Rema Padman", "Edoardo M. Airoldi"], "date": "2004", "abstract": "Extracting sentiments from unstructured text has emerged as an important problem in many disciplines. An accurate method would enable us, for example, to mine on-line opinions from the Internet and learn customers’ preferences for economic or marketing research, or for leveraging a strategic advantage. In this paper, we propose a two-stage Bayesian algorithm that is able to capture the dependencies among words, and, at the same time, finds a vocabulary that is efficient for the purpose of… ", "references": ["e2de29049d62de925cf709024b92774cd82b0a5a", "de00b6d04d7d4c7d554b404f54b4362c9ba3df92", "9e7c7853a16a378cc24a082153b282257a9675b7", "a2d705c1b95e5fda7a016089650761ba954adda9", "b62b80384f402d38c3425db6a99899d1cb9c50c6", "a88a61933c3c8807021b29196a9f6db8cd05a7a2", "07e6b38dc70fc45e50ee780f36e767bed15544fb", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "9e7c7853a16a378cc24a082153b282257a9675b7"]},{"id": "ce20678cfbffded477f43df156f6ab37f6edc6a0", "title": "Learning Subjective Language", "authors": ["Janyce Wiebe", "Theresa Wilson", "Melanie Martin"], "date": "2004", "abstract": "Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using… ", "references": ["a3ff7801bcf72fea30117c88d397403a570c5c68", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "3a8d4fd2c30e5031a574bc25363c8639912b3bbd", "3a8d4fd2c30e5031a574bc25363c8639912b3bbd", "79ba34117323c9f8a9e653db81f1598748137293", "cf1db9d4b45752edb368855224e0572d41c6a169", "713ddf3bfd2727b054c3818930cc85cec2229d97", "3cb09327e68400bf05e6f373e046a3a08e82510e", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "9e2caa39ac534744a180972a30a320ad0ae41ea3"]},{"id": "da0cc33fac4d926eaa61f86566572a4653b3e990", "title": "Learning Composition Models for Phrase Embeddings", "authors": ["Mo Yu", "Mark Dredze"], "date": "2015", "abstract": "Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate… ", "references": ["2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "62da3df1c7ffd0b3ec4282acba36f827bab82c58", "81b3b3fe994a9eda6d3f9d2149aa4492d1933975", "62da3df1c7ffd0b3ec4282acba36f827bab82c58", "c8dfdb6bc17094fc1c35757a0020dea8d813b7b6", "dac72f2c509aee67524d3321f77e97e8eff51de6", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "62da3df1c7ffd0b3ec4282acba36f827bab82c58", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "dac72f2c509aee67524d3321f77e97e8eff51de6"]},{"id": "a88a61933c3c8807021b29196a9f6db8cd05a7a2", "title": "Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus", "authors": ["Peter D. Turney", "Michael L. Littman"], "date": "2002", "abstract": "The evaluative character of a word is called its semantic orientation.", "references": ["c8eafcd758663642141cbdb1cab4f79c90d3cc0c", "fc93f3182bb0fda36764de93978dbd71b8128e6f", "fc93f3182bb0fda36764de93978dbd71b8128e6f", "c13d3506f716fb9fb0c417b5132144db42f80dd4", "9567b8782f7bcf3f94c7543e36062f77bf584f5b", "05a70e13d2d9ae9a61a83ad4c9aee50844603590", "05a70e13d2d9ae9a61a83ad4c9aee50844603590", "c13d3506f716fb9fb0c417b5132144db42f80dd4", "e517e1645708e7b050787bb4734002ea194a1958", "e517e1645708e7b050787bb4734002ea194a1958"]},{"id": "3f46b61a7216e5763ffab5d33e06b88c9d490c85", "title": "Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences", "authors": ["Hong Yu", "Vasileios Hatzivassiloglou"], "date": "EMNLP", "abstract": "Opinion question answering is a challenging task for natural language processing.", "references": ["76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "9e7c7853a16a378cc24a082153b282257a9675b7", "c4a974906a9f6c7380edc9e2281931bde78828b1", "3a8d4fd2c30e5031a574bc25363c8639912b3bbd", "c4a974906a9f6c7380edc9e2281931bde78828b1", "7c89cbf5d860819c9b5e5217d079dc8aafcba336", "9e7c7853a16a378cc24a082153b282257a9675b7", "0af651764f9b3af12596a55d036498da13d0937e", "7c89cbf5d860819c9b5e5217d079dc8aafcba336"]},{"id": "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "title": "Early results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons", "authors": ["Andrew McCallum", "Wei Li"], "date": "CoNLL", "abstract": "Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character n-grams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle… ", "references": ["1c0ece611643cfb8f3a23e4802c754ea583ebe37", "a574e320d899e7e82e341eb64baef7dfe8a24642", "0e494ad9265fcb4e87ff585c65db4c795940cf9b", "4df361d65a15ca9a7fc27c58c38b04d1f41e6f62", "a574e320d899e7e82e341eb64baef7dfe8a24642", "a574e320d899e7e82e341eb64baef7dfe8a24642", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "34dc22dcbdf1e09fb48691ee1fc6fe4bb8f834c3", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "50df0dd6c8f9811e99f6c73b2f88330aeda2eb98"]},{"id": "b52fe0b796e4c899624ed3e9d9ea566453156844", "title": "Recognizing Named Entities in Tweets", "authors": ["Xiaohua Liu", "Shaodian Zhang", "Ming Zhou"], "date": "ACL", "abstract": "The challenges of Named Entities Recognition (NER) for tweets lie in the insufficient information in a tweet and the unavailability of training data. We propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to tackle these challenges. The KNN based classifier conducts pre-labeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine… ", "references": ["8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "51bdd0b839fe8ab1daafa05d1ed03edff601f4c7", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "51bdd0b839fe8ab1daafa05d1ed03edff601f4c7", "aa9efc8b2737eac0675ba5abb5feab8305482c12", "4a554da55fd9ff76c99e25d2ce937b225dc1100c", "4a554da55fd9ff76c99e25d2ce937b225dc1100c", "51bdd0b839fe8ab1daafa05d1ed03edff601f4c7", "aa9efc8b2737eac0675ba5abb5feab8305482c12", "f2537e72219933ee97a57a5b35e74e59bf3a25e7"]},{"id": "5920903e1c2c7ea165ad84a8fe6dbafdd586cbe7", "title": "Nerit: Named Entity Recognition for Informal Text", "authors": ["David Etter", "Francis Ferraro", "Benjamin Van Durme"], "date": "2013", "abstract": "We describe a multilingual named entity recognition system using language independent feature templates, designed for processing short, informal media arising from Twitter and other microblogging services. We crowdsource the annotation of tens of thousands of English and Spanish tweets and present classification results on this resource. ", "references": ["10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398", "aa9efc8b2737eac0675ba5abb5feab8305482c12", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "52b1f688ddb8f943fa65cc4b875181f6497ae14c", "a07f56a919687e432e92c403d371a9e2d564ef3f", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "14935c3ffb1cafd53a23d84bec66388a77422435"]},{"id": "14935c3ffb1cafd53a23d84bec66388a77422435", "title": "Named Entity Recognition in Tweets: An Experimental Study", "authors": ["Alan Ritter", "Sam Clark", "Oren Etzioni"], "date": "EMNLP", "abstract": "People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner.", "references": ["d54f4215dbdf272820f080b8fc2cbba99bd634e7", "b52fe0b796e4c899624ed3e9d9ea566453156844", "b52fe0b796e4c899624ed3e9d9ea566453156844", "33522bd6edc05fa1ae1d6d668b694c3416faa4ba", "dac72f2c509aee67524d3321f77e97e8eff51de6", "d84b57362e2010f6f65357267df7e0157af30684", "d54f4215dbdf272820f080b8fc2cbba99bd634e7", "d54f4215dbdf272820f080b8fc2cbba99bd634e7", "dac72f2c509aee67524d3321f77e97e8eff51de6", "33522bd6edc05fa1ae1d6d668b694c3416faa4ba"]},{"id": "12510c9659c71ce821fe671de5ed7033ba0af31c", "title": "Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields", "authors": ["Xinnian Mao", "Yuan Dong", "Haila Wang"], "date": "IJCNLP", "abstract": "Chinese word segmentation (CWS), named entity recognition (NER) and part-ofspeech tagging is the lexical processing in Chinese language. This paper describes the work on these tasks done by France Telecom Team (Beijing) at the fourth International Chinese Language Processing Bakeoff. In particular, we employ Conditional Random Fields with different features for these tasks. In order to improve NER relatively low recall; we exploit non-local features and alleviate class imbalanced distribution… ", "references": ["f6f9cfb31ab8178456dbc897c812b2d7ad2986f7", "4f410ab5c8b12b34b38421241366ee456bbebab9", "4c64663accd02bb3221fa460663a016f38f9ed8f", "35640547b3ba7989b5abbb9d269055e736d9dff3", "35640547b3ba7989b5abbb9d269055e736d9dff3", "6870e2694c4dd33e6d48bac4e8a063d39977fb9b"]},{"id": "5ee9bed324a4f0716554409dc367df3beeb27b44", "title": "Deep Learning for Chinese Word Segmentation and POS Tagging", "authors": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu"], "date": "EMNLP", "abstract": "This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart… ", "references": ["ce0675bb7de5dcad3301fb232e090f7b9d821705", "d4414eb1d95dda4ba347f7b6e8335ff513c91e1f", "83e3bba16f45e2a792e6cc69514529c9c08fe0e7", "fad6b4acdcb1bacd043982be6c785c14964b2563", "929f9332970683c9bcccde8fe49fabcd2ae687a6", "ce0675bb7de5dcad3301fb232e090f7b9d821705", "c7607b791d5df3b45dc49a1dab48d8fae07ebd4b", "83e3bba16f45e2a792e6cc69514529c9c08fe0e7", "3594af2ebf510609651bf282dfea65c8e837b1a7", "d4414eb1d95dda4ba347f7b6e8335ff513c91e1f"]},{"id": "a07f56a919687e432e92c403d371a9e2d564ef3f", "title": "Joint Inference of Named Entity Recognition and Normalization for Tweets", "authors": ["Xiaohua Liu", "Ming Zhou", "Furu Wei"], "date": "ACL", "abstract": "Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random… ", "references": ["f2537e72219933ee97a57a5b35e74e59bf3a25e7", "b52fe0b796e4c899624ed3e9d9ea566453156844", "d54f4215dbdf272820f080b8fc2cbba99bd634e7", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "14935c3ffb1cafd53a23d84bec66388a77422435", "f2537e72219933ee97a57a5b35e74e59bf3a25e7", "9347976b487048208002a5b24366157ed3274c2c", "f1f007c029d13dee5b21a90a174dbb21e7770706", "1cf53c62906a48b9d607f94468bf777855199a28", "1cf53c62906a48b9d607f94468bf777855199a28"]},{"id": "00a2a05f44a790b7886cd25192f4b91d3b81dde2", "title": "The Unreasonable Effectiveness of Word Representations for Twitter Named Entity Recognition", "authors": ["Colin Cherry", "Hongyu Guo"], "date": "HLT-NAACL", "abstract": "Named entity recognition (NER) systems trained on newswire perform very badly when tested on Twitter. Signals that were reliable in copy-edited text disappear almost entirely in Twitter’s informal chatter, requiring the construction of specialized models. Using wellunderstood techniques, we set out to improve Twitter NER performance when given a small set of annotated training tweets. To leverage unlabeled tweets, we build Brown clusters and word vectors, enabling generalizations across… ", "references": ["c5262730d8854f88106bdc204860ccf236b3345f", "f2537e72219933ee97a57a5b35e74e59bf3a25e7", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "09c2640b09b1eb0068afaece6bf9556dac2f5d14", "09c2640b09b1eb0068afaece6bf9556dac2f5d14", "b52fe0b796e4c899624ed3e9d9ea566453156844", "f2537e72219933ee97a57a5b35e74e59bf3a25e7", "dac72f2c509aee67524d3321f77e97e8eff51de6", "dac72f2c509aee67524d3321f77e97e8eff51de6", "f2537e72219933ee97a57a5b35e74e59bf3a25e7"]},{"id": "b3e9130ecab419f8267fccadf80c1ee2489be793", "title": "A Corpus-Based Approach for Building Semantic Lexicons", "authors": ["Ellen Riloff", "Jessica Shepherd"], "date": "1997", "abstract": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a… ", "references": ["1d922631a6bf8361d7602e12cafb9e15d421c827", "1cf6f1209d29c151b693861e083850f1b385c595", "d756c70a3cb6df2fdf52f0027942f10712ebde56", "f3f09d77332979d8315c775c1e6654323ff661cd", "5213e864b82c4dde46ce9f6b82c403729426d3a2", "d756c70a3cb6df2fdf52f0027942f10712ebde56", "1cf6f1209d29c151b693861e083850f1b385c595", "6a94da952fb8ffc77881028081e90efb494f1c5d", "d756c70a3cb6df2fdf52f0027942f10712ebde56", "0dbef0679ae6cb5725ae6e1b5b071f36e9514469"]},{"id": "9c72719fa829a3ab5686bdd5c6f4f2538a1a92f5", "title": "Unsupervised Multilingual Learning for POS Tagging", "authors": ["Benjamin Snyder", "Tahira Naseem", "Regina Barzilay"], "date": "EMNLP", "abstract": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. We formulate a hierarchical Bayesian model for jointly predicting bilingual streams of part-of-speech tags. The model learns language-specific features while capturing cross-lingual patterns in tag distribution for aligned words. Once the parameters of our… ", "references": ["444dc7fe40b5702e79e908834ca2fcfdbc422cd2", "36ffcc1cc218ca36de384a107fb48e5abe2e6359", "6d3577027ef1fd485d982761181226f2e00d3fc3", "327c88dd06722a967be9c6b1176fbd79554967e7", "444dc7fe40b5702e79e908834ca2fcfdbc422cd2", "3a2c3cf32c8c214cccebc45ebc2a2a1dd7d1e658", "327c88dd06722a967be9c6b1176fbd79554967e7", "36ffcc1cc218ca36de384a107fb48e5abe2e6359", "2acda726310bd9e848ea4fca57f1fce35a2b0ad3", "0ce9c261a7f69668da2066da0ad736e6eccdcd36"]},{"id": "e4b66a159672c613f9d4c7cdb8e6c6f85d871bf5", "title": "Refining Automatically-Discovered Lexical Relations: Combining Weak Techniques for Stronger Results", "authors": ["Marti A. Hearst", "Gregory Grefenstette"], "date": "1992", "abstract": "Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the difficulties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suffice for a particular task; for this reason we are interested in finding ways to combine various techniques and improve their results. Accordingly, we conducted experiments to refine the results of an automatic lexical… ", "references": ["ba6f89c980e3b01eaf1b63d8a04e5ffb49f16e2e", "2510562fc1f7ff1eba53731961d3b4c4bc5a5b09", "2510562fc1f7ff1eba53731961d3b4c4bc5a5b09", "dff419bcae89bad0718d4ceaa48dc55e99a993c7", "dff419bcae89bad0718d4ceaa48dc55e99a993c7", "dff419bcae89bad0718d4ceaa48dc55e99a993c7", "094c0495ebb34c7eb61bad86a96eeebab06dab08", "4f739d92813866af8f2a9734912c419bebd940fe", "094c0495ebb34c7eb61bad86a96eeebab06dab08", "dff419bcae89bad0718d4ceaa48dc55e99a993c7"]},{"id": "0285f18f1642c3684e6abb7d5162348278c41abf", "title": "Translating Collocations for Bilingual Lexicons: A Statistical Approach", "authors": ["Frank Smadja", "Kathleen McKeown", "Vasileios Hatzivassiloglou"], "date": "1996", "abstract": "Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis. We describe a program named Champollion which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations. Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains. The… ", "references": ["1f55d2bca810edbf9870934a41d956d06ae2d9cf", "67df61766618f54e3b136c18aa28694395b5fd6d", "67df61766618f54e3b136c18aa28694395b5fd6d", "076d615fdf50f0ffc94876547fcbd2a40e19b90c", "a3ff7801bcf72fea30117c88d397403a570c5c68", "7a62850d3518fad477a911123c43f9abc9920bed", "67df61766618f54e3b136c18aa28694395b5fd6d", "7a62850d3518fad477a911123c43f9abc9920bed", "a3ff7801bcf72fea30117c88d397403a570c5c68", "4fe2a45babab10c1bfae05d2464363f4e52bbaf9"]},{"id": "c2d95e890ee904f70701fa27326d31980424d5dd", "title": "Lexical Semantic Relatedness and Its Application in Natural Language Processing", "authors": ["Alexander Budanitsky"], "date": "1999", "abstract": "Lexical Semantic Relatedness and Its Application in Natural Language Processing Alexander Budanitsky Department of Computer Science University of Toronto August 1999 A great variety of Natural Language Processing tasks, from word sense disambiguation to text summarization to speech recognition, rely heavily on the ability to measure semantic relatedness or distance between words of a natural language. This report is a comprehensive study of recent computational methods of measuring lexical… ", "references": ["2b0d62c4a7748b980d80cf1deb2e1e3c7ef6d4e0", "229d7370abb8bc4d905d786bd745b772df7bd625", "463ac10906087d8185ab52aebb26616aacb9cabc", "8d94832e245906775b428e949ac1f635bfb28ad3", "2b0d62c4a7748b980d80cf1deb2e1e3c7ef6d4e0", "43a5df330daa9e177ac275d02f2677e9d2fa62d5", "667f788be30854a7e29ca52dbe4357360ff76a06", "229d7370abb8bc4d905d786bd745b772df7bd625", "2b0d62c4a7748b980d80cf1deb2e1e3c7ef6d4e0", "2b0d62c4a7748b980d80cf1deb2e1e3c7ef6d4e0"]},{"id": "5c2cf68e49df90bff81f2a7c457ce38c73f9cb98", "title": "Machine Learning Algorithms for Portuguese Named Entity Recognition", "authors": ["Ruy Luiz Milidiú", "Julio C. Duarte", "Roberto Cavalcante"], "date": "2007", "abstract": "Named Entity Recognition (NER) is an important task in Natu- ral Language Processing. It provides key features that help on more ela- borated document management and information extraction tasks. In this paper, we propose seven machine learning approaches that use HMM, TBL and SVM to solve Portuguese NER. The performance of each model- ing approach is empirically evaluated. The SVM-based extractor shows a 88.11%F-score, which is our best observed value, slightly better than TBL. This is very… ", "references": ["eb5143cd5292ed8137475dff4454918b16442c55", "5158565c46424c6daea3f05f197760821a763a2a", "bf4cd4812a351c3803b79e7504df1db4ce8a5f2d", "930953db6dfcc03587a7eb9481301a8ed0074227", "5158565c46424c6daea3f05f197760821a763a2a", "eb5143cd5292ed8137475dff4454918b16442c55", "51bdd0b839fe8ab1daafa05d1ed03edff601f4c7", "930953db6dfcc03587a7eb9481301a8ed0074227", "091a509ce952fffb314af6d73b8059791e780b09", "8628a174b1bf2d11788916bf63c015287699da37"]},{"id": "0617dd6924df7a3491c299772b70e90507b195dc", "title": "The Automatic Content Extraction (ACE) Program - Tasks, Data, and Evaluation", "authors": ["George R. Doddington", "Alexis Mitchell", "Ralph M. Weischedel"], "date": "LREC", "abstract": "The objective of the ACE program is to develop technology to automatically infer from human language data the entities being mentioned, the relations among these entities that are directly expressed, and the events in which these entities participate. Data sources include audio and image data in addition to pure text, and Arabic and Chinese in addition to English. The effort involves defining the research tasks in detail, collecting and annotating data needed for training, development, and… ", "references": []},{"id": "afacd9d2048902a8faf82c4f79584d6a05170ba6", "title": "Using Deep Belief Nets for Chinese Named Entity Categorization", "authors": ["You Ouyang", "Tiejun Zhao"], "date": "NEWS@ACL", "abstract": "Identifying named entities is essential in understanding plain texts. Moreover, the categories of the named entities are indicative of their roles in the texts. In this paper, we propose a novel approach, Deep Belief Nets (DBN), for the Chinese entity mention categorization problem. DBN has very strong representation power and it is able to elaborately self-train for discovering complicated feature combinations. The experiments conducted on the Automatic Context Extraction (ACE) 2004 data set… ", "references": ["8978cf7574ceb35f4c3096be768c7547b28a35d0", "a51be478b6c11f1c61192e563b6f6ae2ef48b9fa", "f919eec778b9cd4106c1c1a468e3f2f341fe49a1", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "fc9b4a83045c9eca29d5309b2e4ebab81f3ba0d6", "f919eec778b9cd4106c1c1a468e3f2f341fe49a1", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657", "fc9b4a83045c9eca29d5309b2e4ebab81f3ba0d6", "f919eec778b9cd4106c1c1a468e3f2f341fe49a1"]},{"id": "7d37dff2d8e65764e7293750051d519359d8835d", "title": "Similarity-Based Models of Word Cooccurrence Probabilities", "authors": ["Ido Dagan", "Lillian Lee", "Fernando C Pereira"], "date": "1999", "abstract": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations “eat a peach” and ”eat a beach” is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given… ", "references": ["297e478f92cef1cd090706fc59fde5ea0836ce80", "3cb09327e68400bf05e6f373e046a3a08e82510e", "1cf6f1209d29c151b693861e083850f1b385c595", "3cb09327e68400bf05e6f373e046a3a08e82510e", "36c4c51917b1f53ee85c459f2597e115df53eb05", "1d922631a6bf8361d7602e12cafb9e15d421c827", "1d922631a6bf8361d7602e12cafb9e15d421c827", "2eae0f08186952643c3a7ead2eba2d41fda58cec", "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0", "2eae0f08186952643c3a7ead2eba2d41fda58cec"]},{"id": "1ad9b8e92f303cebfd4c9a97ca99ac09ace82fcc", "title": "Vectorial representations of meaning for a computational model of language comprehension", "authors": ["William Schuler", "Stephen T. Wu"], "date": "2010", "abstract": "This thesis aims to define and extend a line of computational models for text comprehension that are humanly plausible. Since natural language is human by nature, computational models of human language will always be just that — models. To the degree that they miss out on information that humans would tap into, they may be improved by considering the human process of language processing in a linguistic, psychological, and cognitive light. \nApproaches to constructing vectorial semantic spaces… ", "references": []},{"id": "b5e12de7d3f2a66bfcf10a510f9ac80afb3541c1", "title": "Phrase Clustering for Discriminative Learning", "authors": ["Dekang Lin", "Xiaoyun Wu"], "date": "ACL/IJCNLP", "abstract": "We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers. To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification. Our results show that phrase clusters offer significant improvements over word clusters. Our NER system achieves the best current result on the widely used CoNLL benchmark… ", "references": ["47bb53c0ee0b7f2a56422ee3f493dc155af4a066", "7e958d809a45ba3ae9eef8c5381e8cad8f11de10", "31462c9d3848d593e57eff18ce6d6d384a717f92", "00ae51ba9340abc30d36804f9b51ab83b81cec23", "00ae51ba9340abc30d36804f9b51ab83b81cec23", "d879eba4996654832237da054f78f78fcda77687", "d879eba4996654832237da054f78f78fcda77687", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "31462c9d3848d593e57eff18ce6d6d384a717f92", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb"]},{"id": "523f255f90be3f1ae0f151c60cac50467e965ed0", "title": "Evaluating Word Representation Features in Biomedical Named Entity Recognition Tasks", "authors": ["Buzhou Tang", "Hongxin Cao", "Hua Xu"], "date": "2014", "abstract": "Biomedical Named Entity Recognition (BNER), which extracts important entities such as genes and proteins, is a crucial step of natural language processing in the biomedical domain. Various machine learning-based approaches have been applied to BNER tasks and showed good performance. In this paper, we systematically investigated three different types of word representation (WR) features for BNER, including clustering-based representation, distributional representation, and word embeddings. We… ", "references": ["d499bd7d135713220c499323ff1437e4419a8353", "1bbe6b9e2310fbae3560dcb5ef2961272684d5aa", "68314ee783105924c9a89c10e031ffb6342849af", "a30d65349295fe1e3c3fa3a74ae288c054d261c9", "1bbe6b9e2310fbae3560dcb5ef2961272684d5aa", "68314ee783105924c9a89c10e031ffb6342849af", "04ee51e94e6599127ca857c9b70ab6e760c30fde", "d499bd7d135713220c499323ff1437e4419a8353", "cedc75ade5a0209a6510e4dbe4ee9b5bfbf72836", "d17a9ff1cf2d2d1dd0a5d741d8b44cdf040ddb63"]},{"id": "7eff3ef2c978dcbc8b5a4ab99fa4f0a187afa5ed", "title": "A Maximum Entropy Approach to FrameNet Tagging", "authors": ["Michael Fleischman", "Eduard H. Hovy"], "date": "HLT-NAACL", "abstract": "The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using Viterbi search to find the highest probability tag sequence for a given sentence. Further we examine the use of syntactic pattern based re-ranking to further increase performance. We analyze our strategy using both extracted and human generated syntactic features… ", "references": ["37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "c07c690601169dc1155b2dcf90941b32f606a9d4", "957ddc4ca4c8f7be5efba176716a4ff6b23d80d5", "2be4a6cfe8228b6f4f648ff10dbf1e62fc4562f7", "37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "957ddc4ca4c8f7be5efba176716a4ff6b23d80d5"]},{"id": "6d8018bd8b288baca0c55522877efd1b49258747", "title": "Environmental Determinants of Lexical Processing Effort", "authors": ["Scott A. McDonald"], "date": "2000", "abstract": "A central concern of psycholinguistic research is explaining the relative ease or difficulty involved in processing words. In this thesis, we explore the connection between lexical processing effort and measurable properties of the linguistic environment. Distributional information (information about a word’s contexts of use) is easily extracted from large language corpora in the form of co-occurrence statistics. We claim that such simple distributional statistics can form the basis of a… ", "references": ["6c9d2ffc90d1ae5b0593e922f2035f1f767ad5ba", "6c9d2ffc90d1ae5b0593e922f2035f1f767ad5ba", "4e91cf1e102ff02077ade48ac80806afb7a0e96a", "5bbc3d8ca0f62ce85e4b1fbb6de7a8624ebaeda7", "5bbc3d8ca0f62ce85e4b1fbb6de7a8624ebaeda7", "70a2f872625ce84e491cbc99683e9647ffd897de", "68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "857e9b3037e4ec7780bc5d2773074a25ff1ddadd", "6c9d2ffc90d1ae5b0593e922f2035f1f767ad5ba", "857e9b3037e4ec7780bc5d2773074a25ff1ddadd"]},{"id": "6ffea7929f0e4bbee9e98755eb3d8fc09e89cf4e", "title": "Chunking with Support Vector Machines", "authors": ["Taku Kudo", "Yuji Matsumoto"], "date": "NAACL", "abstract": "We apply Support Vector Machines (SVMs) to identify English base phrases (chunks.", "references": ["9e85832b04cc3700c2c26d6ba93fdeae39cac04a", "c026ffbd40e4beb386c18946e9079fac5da4afc0", "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49", "8682725f2968836a5cd933eed4fdce80b0833bbc", "8682725f2968836a5cd933eed4fdce80b0833bbc", "72748bb4e8c1f0818a70dda783445e7e775142f5", "923db0aeb26a6dc1cb42069c9db04e5dd2d2200a", "717fddf184c49107dbc374f7c63252143f425d0e", "923db0aeb26a6dc1cb42069c9db04e5dd2d2200a", "008a2291a257072f22764196a3acf0a394bf203a"]},{"id": "923db0aeb26a6dc1cb42069c9db04e5dd2d2200a", "title": "Use of Support Vector Learning for Chunk Identification", "authors": ["Taku Kudoh", "Yuji Matsumoto"], "date": "CoNLL/LLL", "abstract": "In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling. ", "references": []},{"id": "9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce", "title": "Natural Language Parsing as Statistical Pattern Recognition", "authors": ["David M. Magerman"], "date": "1994", "abstract": "Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. \nIn this work, I propose an automatic method for acquiring a statistical parser from a set of parsed… ", "references": ["8cf9b7c08655dadad0cad00771f3c9670181004e", "bb20f121c979b535bbeade5ac06676d627d4ad7d", "da838db79e7593018894ada44db35eee670941d6", "860dfdaa8187bd22809f00396b30c66a2fc1ef24", "09550accec47459a61fe1710a0a32c2ec22449bd", "09550accec47459a61fe1710a0a32c2ec22449bd", "783e2c620b596c2305606594bab0d2fb1f7fe003", "17beaab6b27faef07f988325503b7c30a6377753", "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a", "860dfdaa8187bd22809f00396b30c66a2fc1ef24"]},{"id": "205b9f4891a2ead886604f161a44b3aed483609a", "title": "Conditional Structure versus Conditional Estimation in NLP Models", "authors": ["Dan Klein", "Christopher D. Manning"], "date": "EMNLP", "abstract": "This paper separates conditional parameter estimation, which consistently raises test set accuracy on statistical NLP tasks, from conditional model structures, such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy. Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other ways in which the independence assumptions of the conditional… ", "references": ["04ce064505b1635583fa0d9cc07cac7e9ea993cc", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "f0b5342b00268bd5125afc9670622e19b015d21b", "bc3ffe0f4522444f6b16476b08df1534570134f6", "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1", "d560a8d279075a529e9cadb0d664b27957aac5a2", "050e804fe71b65d8d334d1655ba4dbba35b51fb5", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1", "f0b5342b00268bd5125afc9670622e19b015d21b"]},{"id": "3da63687558e077ab2ef0b4a24985c2614602c25", "title": "Identifying Semantic Roles Using Combinatory Categorial Grammar", "authors": ["Daniel Gildea", "Julia Hockenmaier"], "date": "EMNLP", "abstract": "We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles. ", "references": ["fc090a68e45e0e6337136777d21c87b76a90ae72", "15b6080c3dfdfba4869020d03d089bc443ce022b", "3101bd0e1d44b216db5a85a2fc4354b6cf1d80ba", "33be02735525a3eb6111ee790ea2e15775019d21", "976c95f69e8ee160868b1d54d477f56212ee794b", "c37a1e4e9d453fe6960e20bb3e0c89f16e1f0766", "fc090a68e45e0e6337136777d21c87b76a90ae72", "33be02735525a3eb6111ee790ea2e15775019d21", "976c95f69e8ee160868b1d54d477f56212ee794b", "3fc44ff7f37ec5585310666c183c65e0a0bb2446"]},{"id": "c07c690601169dc1155b2dcf90941b32f606a9d4", "title": "Assigning Function Tags to Parsed Text", "authors": ["Don Blaheta", "Eugene Charniak"], "date": "ANLP", "abstract": "It is generally recognized that the common nonterminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree. For example, the Penn Tree-bank gives each constituent zero or more 'function tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already… ", "references": ["ac3947d7f4b33a773d1cd2e88c8937eab3205af5"]},{"id": "8e824aaf67f4f4f068455c6dbb7a6ed877794bd6", "title": "Classifier Combination for Improved Lexical Disambiguation", "authors": ["Eric Brill", "Jingjing Wu"], "date": "COLING-ACL", "abstract": "One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementatry behavior can be used to our advantage. By using contextual cues to guide tagger combination, we… ", "references": ["6082156a2270b6567ebdc85f6570ffacc3d903c2", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "6082156a2270b6567ebdc85f6570ffacc3d903c2", "3c1dfe0b52c0638ff69a187ae183e8cdeebff303", "ebfa1ac159607569053c087041023259e9d27541", "5bbc136957247b3aa864cb475a6591cf2e40576f", "80f092b92c383bc55982a48f650ba445c169422f", "3c1dfe0b52c0638ff69a187ae183e8cdeebff303", "6a94da952fb8ffc77881028081e90efb494f1c5d", "5bbc136957247b3aa864cb475a6591cf2e40576f"]},{"id": "ba8ae7c8261a77ec93727f741d50001b9353f65d", "title": "Part-of-Speech Tagging Based on Hidden Markov Model Assuming Joint Independence", "authors": ["Sang-Zoo Lee", "Jun'ichi Tsujii", "Hae-Chang Rim"], "date": "ACL", "abstract": "In this paper we present part-of-speech taggers based on hidden Markov models, which adopt a less strict Markov assumption to consider rich contexts. In models whose parameters are very specific like lexicalized ones, sparse-data problem is very serious and also conditional probabilities tend to be estimated unreliably. To overcome data-sparseness, a simplified version of the well-known back-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint… ", "references": []},{"id": "a574e320d899e7e82e341eb64baef7dfe8a24642", "title": "A Maximum Entropy Model for Part-Of-Speech Tagging", "authors": ["Adwait Ratnaparkhi"], "date": "EMNLP", "abstract": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these… ", "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "733234e097dceb9011baa8914930861996eb0b5e", "3de5d40b60742e3dfa86b19e7f660962298492af", "00f20179b9087fbf24b6656008a9380c590d9ec9"]},{"id": "a5aa95289383a3fd91dd68a314b1031d3e165c3b", "title": "Equations for Part-of-Speech Tagging", "authors": ["Eugene Charniak", "Curtis Hendrickson", "Mike Perkowitz"], "date": "AAAI", "abstract": "We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the… ", "references": ["9463e3eca9f3b053fca7ca64abb157aaeac35f4f", "f853daccfcb2350f9adcd75331d148b04c21e5ef", "d811628d5f8230720c13ee9bb844badc3c3b6ae2", "729316fbded86763104f3412cadf98f00a9a3993", "307c05b07c845a815c577d6bb43aea151efbce80", "307c05b07c845a815c577d6bb43aea151efbce80", "729316fbded86763104f3412cadf98f00a9a3993", "f853daccfcb2350f9adcd75331d148b04c21e5ef", "d811628d5f8230720c13ee9bb844badc3c3b6ae2", "8c23a242622abc0bf7e7b93f3822b1fc4d9d1f6a"]},{"id": "03380e7083807d3264472871dc0582036cf79479", "title": "Introduction to Montague semantics", "authors": ["David R. Dowty", "Robert E. Wall", "Stanley Peters"], "date": "1980", "abstract": "1. Introduction.- 2. The Syntax and Semantics of Two Simple Languages.- I. The Language L0.- 1. Syntax of L0.- 2. Semantics of L0.- II. The Language L0E.- 1. Syntax of L0E.- 2. Semantics of L0E.- 3. Alternative Formulations of L0E and L0.- III. A Synopsis of Truth-Conditional Semantics.- IV. The Notion of Truth Relative to a Model.- V. Validity and Entailment Defined in Terms of Possible Models.- VI. Model Theory and Deductive Systems.- Exercises.- Note.- 3. First-Order Predicate Logic.- I. The… ", "references": []},{"id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "title": "A stochastic parts program and noun phrase parser for unrestricted text", "authors": ["Kenneth Ward Church"], "date": "1989", "abstract": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is… ", "references": ["bd459cc59b09e612eeec5327d0690d1508ffe362", "e3a81d9b7464c0097e391c31fb4d84467533091c", "a1d58af78b248154ec5c3d7ce96a97d85d3b68bd", "aca56c819acc4561f008efa21a32ae2ed6d3c820"]},{"id": "1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b", "title": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger", "authors": ["Kristina Toutanvoa", "Christopher D. Manning"], "date": "EMNLP", "abstract": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the… ", "references": ["d560a8d279075a529e9cadb0d664b27957aac5a2", "3d9db1146acd2da5ad7b85d81c737f9260576c37", "733234e097dceb9011baa8914930861996eb0b5e", "231f6de83cfa4d641da1681e97a11b689a48e3aa", "9dad7b5ba8309259994908baa28660ad41845dc4", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "733234e097dceb9011baa8914930861996eb0b5e", "d560a8d279075a529e9cadb0d664b27957aac5a2", "084c55d6432265785e3ff86a2e900a49d501c00a", "a574e320d899e7e82e341eb64baef7dfe8a24642"]},{"id": "dea301ddc616ed7da7568b91a6627699fcbd07cd", "title": "Structured Models for Fine-to-Coarse Sentiment Analysis", "authors": ["Ryan T. McDonald", "Kerry Hannan", "Jeffrey C. Reynar"], "date": "ACL", "abstract": "In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions. The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another. Experiments show that this method can significantly reduce classification error relative to… ", "references": ["d3b27746f7a53f2dc5d9b8c2f3d343313622ec36", "ff75055d4e47737702d3b550879d6128cec13233", "1e19a94d547ee023837c14c361139185e2353fc0", "167e1359943b96b9e92ee73db1df69a1f65d731d", "4bb8b75308b5e9021c02285584e1851479089d91", "10d21ca7728cb3dd15731accedda9ea711d8a0f4", "5aa70188f70d349580aed96c10a68f57dace2d33", "4bb8b75308b5e9021c02285584e1851479089d91", "3c9d9f3c6f7508f4e29730924529dc993c27cddc", "ff75055d4e47737702d3b550879d6128cec13233"]},{"id": "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac", "title": "Convolution kernels on discrete structures", "authors": ["David Haussler"], "date": "1999", "abstract": "We introduce a new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs. The method can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set. The family of kernels generated generalizes the family of radial basis kernels. It can also be used to deene kernels in the form of joint Gibbs probability distributions. Kernels can be built from hidden Markov random elds, generalized regular… ", "references": ["3d3b01a9ce510c80c72a31595045bb40844e404a", "c35cc80fe8c6cdea742d4fa1af1f2e698d41aba7", "f5ed356c72ef8b45114c330d5d5704fa09aa9bf2", "e45c2420e6dc59ba6d357fb0c996ebf43c861560", "3d3b01a9ce510c80c72a31595045bb40844e404a", "d27c7569fdbcbb57ff511f5293e32b547acca7b3", "c35cc80fe8c6cdea742d4fa1af1f2e698d41aba7", "5f2be15cdf6f5b461b9c61495eb496351d7fc91a", "f5ed356c72ef8b45114c330d5d5704fa09aa9bf2", "5f2be15cdf6f5b461b9c61495eb496351d7fc91a"]},{"id": "40fd86ed795b17e6abf00e81b4b13d955813a4c6", "title": "Classification Using Word Subsequences and Dependency Subtrees", "authors": ["Shotaro Matsumoto", "Hiroya Takamura", "Manabu Okumura"], "date": "2005", "abstract": "Document sentiment classification is a task to classify a document according to the positive or negative polarity of its opinion (favorable or unfavorable). We propose using syntactic relations between words in sentences for document sentiment classification. Specifically, we use text mining techniques to extract frequent word sub-sequences and dependency sub-trees from sentences in a document dataset and use them as features of support vector machines. In experiments on movie review datasets… ", "references": ["de20b12ebf9fe5c4c74a7bb75239e0b45c587b1f", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "de20b12ebf9fe5c4c74a7bb75239e0b45c587b1f", "de20b12ebf9fe5c4c74a7bb75239e0b45c587b1f", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "3a923a81a8f5069d50f827c22abe4546385603b4", "167e1359943b96b9e92ee73db1df69a1f65d731d", "338a891907dce447da9a0fa2f27221bd35164163", "3a923a81a8f5069d50f827c22abe4546385603b4", "bc7308a97ec2d3f7985d48671abe7a8942a5b9f8"]},{"id": "0f26d6ce9e7dae5276ce1d2b2a578151a45887b5", "title": "Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters", "authors": ["Alistair Kennedy", "Diana Inkpen"], "date": "2005", "abstract": "We present a method for determining the sentiment expressed by a customer review. The semantic orientation of a review can be positive, negative, or neutral. Our method counts positive and negative terms, but also takes into account contextual valence shifters, such as negations and intensifiers. Tests are done taking both negations and intensifiers into account, and also using only negations without intensifiers. Negations are used to reverse the semantic polarity of a particular term, while… ", "references": ["a8ddcabf410b5691cd73880df2ee4b83a9799a5d", "1cbade0a871e63cab3d5b84cbc84b88a2057596b", "5c8d05e27e36ebd64ee43fe1670262cdcc2123ba", "9e7c7853a16a378cc24a082153b282257a9675b7", "27b502f5b01d237c6f0c850c5eb7f103b2810e70", "a88a61933c3c8807021b29196a9f6db8cd05a7a2", "9e7c7853a16a378cc24a082153b282257a9675b7", "1cbade0a871e63cab3d5b84cbc84b88a2057596b", "ce20678cfbffded477f43df156f6ab37f6edc6a0", "a88a61933c3c8807021b29196a9f6db8cd05a7a2"]},{"id": "97bdc5522ae46b281389654199d656eb67728a32", "title": "Determining the Sentiment of Opinions", "authors": ["Soo-Min Kim", "Eduard H. Hovy"], "date": "COLING", "abstract": "Identifying sentiments (the affective parts of opinions) is a challenging problem.", "references": ["96f9e8ee890b4e142dd34dcf93fd52678eecd2b5", "9076fde21e10803f581c2b39ab47250fa1d4a0be"]},{"id": "f0456ee92718b04e8ea51f47c9486455fafcf7d3", "title": "A SYSTEM FOR AFFECTIVE RATING OF TEXTS", "authors": ["Stephen D. Durbin"], "date": "2003", "abstract": "In pursuit of automated text understanding, two broad types of approach can be distinguished: analytic methods (e.g. named entity extraction) that provide specific items of information, and synthetic methods (e.g. topic identification) that provide a global characterization. Recent interest in identifying overall affect or sentiment in text falls into the second category. Judging from the limited results reported so far, it appears to be a more challenging problem than topic identification… ", "references": []},{"id": "64b8553bd62997d332910078bcd5ee74a43f9350", "title": "Mining Opinion Features in Customer Reviews", "authors": ["Minqing Hu", "Bing Liu"], "date": "AAAI", "abstract": "It is a common practice that merchants selling products on the Web ask their customers to review the products and associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds. This makes it difficult for a potential customer to read them in order to make a decision on whether to buy the product. In this project, we aim to summarize all the customer… ", "references": ["7cb63b055d43aadfa0912e69106d96f6016e962d", "97c0af623d9eac54abb076da8c2282e9c8b0bbb0", "7cb63b055d43aadfa0912e69106d96f6016e962d", "a08f461f66993884089050f1018da2f55f3da0aa", "e17283db2dadc7467802648ac2a94945dc7a5242", "d80a6a85b0c263d638877fff66ddc12963e3c34f", "4f3cb3bb5e8d77d457d418c4ff1f271566c87506", "d80a6a85b0c263d638877fff66ddc12963e3c34f", "7cb63b055d43aadfa0912e69106d96f6016e962d", "7cb63b055d43aadfa0912e69106d96f6016e962d"]},{"id": "5ccef5423f3cb428121eb6fef2224d803c136806", "title": "Exploring Sentiment Summarization", "authors": ["Philip Beineke", "Trevor J. Hastie", "Shivakumar Vaithyanathan"], "date": "2004", "abstract": "We introduce the idea of a sentiment summary, a single passage from a document that captures a key aspect of the author’s opinion about his or her subject. Using supervised data from the Rotten Tomatoes website, we examine features that appear to be helpful in locating a good summary sentence. These features are used to fit Naive Bayes and regularized logistic regression models for summary extraction. ", "references": []},{"id": "bc7308a97ec2d3f7985d48671abe7a8942a5b9f8", "title": "Sentiment Analysis using Support Vector Machines with Diverse Information Sources", "authors": ["Tony Mullen", "Nigel Collier"], "date": "EMNLP", "abstract": "This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text.", "references": ["ad4598b1f1d0f40bc9a29939dd7287fd33b5696f", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "9e7c7853a16a378cc24a082153b282257a9675b7", "5c8d05e27e36ebd64ee43fe1670262cdcc2123ba", "9e7c7853a16a378cc24a082153b282257a9675b7", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "d27f2fce889b3fd4da8fef2b40d47f9651e84419", "9e2caa39ac534744a180972a30a320ad0ae41ea3", "d27f2fce889b3fd4da8fef2b40d47f9651e84419"]},{"id": "ab3eec0a211e16751effa729281dd448001203db", "title": "Learning Extraction Patterns for Subjective Expressions", "authors": ["Ellen Riloff", "Janyce Wiebe"], "date": "EMNLP", "abstract": "This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision. ", "references": ["f177c6022e4798b156bfc53de3a976d0c6c357cb", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "e80b34a55aa56578f9a4f27ea207f8c42c93a378", "f177c6022e4798b156bfc53de3a976d0c6c357cb", "22fb3b3b2bdf768dd435eedfc5ef5155d3e56b1a", "8dadbf2dfebe794ad4fc5022f8bb65195c8f0d5a", "f177c6022e4798b156bfc53de3a976d0c6c357cb", "79398502f4dcc812cefcb944fc748b32998aec5c", "f177c6022e4798b156bfc53de3a976d0c6c357cb", "41e936981f5a2d55bfec0143e9a15e23ad96436b"]},{"id": "25be54c49920d7d3520135c341817b1172eb079f", "title": "Affect analysis of text using fuzzy semantic typing", "authors": ["Pero Subasic", "Alison Huettner"], "date": "2001", "abstract": "We propose a novel, convenient fusion of natural language processing and fuzzy logic techniques for analyzing the affect content in free text. Our main goals are fast analysis and visualization of affect content for decision making. The main linguistic resource for fuzzy semantic typing is the fuzzy-affect lexicon, from which other important resources, the fuzzy thesaurus and affect category groups, are generated. Free text is tagged with affect categories from the lexicon and the affect… ", "references": ["5b1e32465a4fb1369b18f62ba6dd1c52809cc818", "8a26ded61b67cce241352ba8742cd8fa2541d605", "82b03ff061d8180a27ce3744860c82aabd01e93a", "5b1e32465a4fb1369b18f62ba6dd1c52809cc818", "82b03ff061d8180a27ce3744860c82aabd01e93a", "93f67c2c67c99583d8f3f396f00b9ab156beb193", "5b1e32465a4fb1369b18f62ba6dd1c52809cc818", "5b1e32465a4fb1369b18f62ba6dd1c52809cc818", "f26cbe40db22c9b99fe95d368c3aff94beaef488", "8a26ded61b67cce241352ba8742cd8fa2541d605"]},{"id": "310b72fbc3d384ca88ca994b33476b8a2be2e27f", "title": "Sentiment analyzer: extracting sentiments about a given topic using natural language processing techniques", "authors": ["Jeonghee Yi", "Tetsuya Nasukawa", "Wayne Niblack"], "date": "2003", "abstract": "We present sentiment analyzer (SA) that extracts sentiment (or opinion) about a subject from online text documents. Instead of classifying the sentiment of an entire document about a subject, SA detects all references to the given subject, and determines sentiment in each of the references using natural language processing (NLP) techniques. Our sentiment analysis consists of 1) a topic specific feature term extraction, 2) sentiment extraction, and 3) (subject, sentiment) association by… ", "references": ["5581992944c66522dd1b11f8a6150aeef2d95b7a", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "5581992944c66522dd1b11f8a6150aeef2d95b7a", "338a891907dce447da9a0fa2f27221bd35164163", "4f0d67a3a8a61d3a7ef0f940274ecff5f0d640ea", "8a26ded61b67cce241352ba8742cd8fa2541d605", "3c4e3462e3e9f0dd7b379f3e300ff47eefa803e5", "8a26ded61b67cce241352ba8742cd8fa2541d605", "3c4e3462e3e9f0dd7b379f3e300ff47eefa803e5", "df8568c6e19d427aae989887a47c3a88f8124dda"]},{"id": "e99f196cf21e0781ef1e119d14e6db45cd71bf3b", "title": "Finding scientific topics", "authors": ["Thomas R L Griffiths", "Mark Steyvers"], "date": "2004", "abstract": "A first step in identifying the content of a document is determining which topics that document addresses.", "references": []},{"id": "97a5bdf0ed22a5688abef32b282e922da362e7b5", "title": "Topics over time: a non-Markov continuous-time model of topical trends", "authors": ["Xuerui Wang", "Andrew McCallum"], "date": "KDD '06", "abstract": "This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data, but also how the structure changes over time.", "references": ["6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567", "6fe0df37aae7246f485d4dc50a8cb7447487c567"]},{"id": "c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e", "title": "Supervised Topic Models", "authors": ["David M. Blei", "Jon D. McAuliffe"], "date": "NIPS", "abstract": "We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from… ", "references": ["7a2252ccce2b65abc3759149b5c06587cc318e2f", "7a2252ccce2b65abc3759149b5c06587cc318e2f", "e5554c9d5fa92af69992d72ed1fdfbe953b03fb4", "e5554c9d5fa92af69992d72ed1fdfbe953b03fb4", "d98d0d1900b13b87aa4ffd6b69c046beb63f0434", "7a2252ccce2b65abc3759149b5c06587cc318e2f", "473f4b7f8ae2b03dda2593f54b316ff7d55db26b", "992c958fe0f4bd148b6f4304e1b5e458b8575cb1", "6436dce0e39f15a1ca9269e6ca813dfebb0af3a2", "473f4b7f8ae2b03dda2593f54b316ff7d55db26b"]},{"id": "107da5e5023b6caf546790ede6d6a370bb4c8e68", "title": "Exemplar-Based Models for Word Meaning in Context", "authors": ["Katrin Erk", "Sebastian Padó"], "date": "ACL", "abstract": "This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vector-per-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. ", "references": ["06ec4411dcc43946e5fd14da5b8b6c2ef895b52e", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "31264fe5b6be8b2842a4599e622e9fb2125f1f95", "6b2c2801ad74ec48a4854c92d2f0d854a2d90073", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "80a1d3943881952dbc584bf1ab8914eec7b07324", "6b2c2801ad74ec48a4854c92d2f0d854a2d90073"]},{"id": "38c5a728c57baa59907ee0890137b75a5269f4a7", "title": "Context-theoretic Semantics for Natural Language: an Overview", "authors": ["Daoud Clarke"], "date": "Proceedings of the Workshop…", "abstract": "We present the context-theoretic framework, which provides a set of rules for the nature of composition of meaning based on the philosophy of meaning as context. Principally, in the framework the composition of the meaning of words can be represented as multiplication of their representative vectors, where multiplication is distributive with respect to the vector space. \n \nWe discuss the applicability of the framework to a range of techniques in natural language processing, including… ", "references": ["ec4bc784723e0a470ef49b7d29604433d6a69d95", "265be00bf112c6cb2fa3e8176bff8394a114dbde", "265be00bf112c6cb2fa3e8176bff8394a114dbde", "c878708039d14ef1f4d1574d51da69d56c7d1aa0", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "265be00bf112c6cb2fa3e8176bff8394a114dbde", "2cf440cc108311062a1914351400a75147a7d0c8", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "527eb9c939801f1edcedace66eff7bbc02f74e80", "7c69a90236f1c57348de858918c554a9420f1521"]},{"id": "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a", "title": "Semantic hashing", "authors": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "date": "2009", "abstract": "We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs ''semantic hashing'': Documents are mapped to memory addresses in such a way that semantically similar documents… ", "references": ["2184fb6d32bc46f252b940035029273563c4fc82", "0f0fb40889d16cb8b99bb7bcb4f3c2d39beb0be3", "355b86dafd852e4df905f6ad9402c7d03831d618", "0f0fb40889d16cb8b99bb7bcb4f3c2d39beb0be3", "0f0fb40889d16cb8b99bb7bcb4f3c2d39beb0be3", "e17529924798975856310a75cb3df3066ac7ccfa", "969c47fe20e41b3331c8aec3b2b964396d914b2c", "355b86dafd852e4df905f6ad9402c7d03831d618", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "cfaae9b6857b834043606df3342d8dc97524aa9d"]},{"id": "0f0fb40889d16cb8b99bb7bcb4f3c2d39beb0be3", "title": "The rate adapting poisson model for information retrieval and object recognition", "authors": ["Peter V. Gehler", "Alex Holub", "Max Welling"], "date": "ICML '06", "abstract": "Probabilistic modelling of text data in the bag-of-words representation has been dominated by directed graphical models such as pLSI, LDA, NMF, and discrete PCA. Recently, state of the art performance on visual object recognition has also been reported using variants of these models. We introduce an alternative undirected graphical model suitable for modelling count data. This \"Rate Adapting Poisson\" (RAP) model is shown to generate superior dimensionally reduced representations for subsequent… ", "references": ["355b86dafd852e4df905f6ad9402c7d03831d618", "f9cf9b6291aded2a82652002511aea36b6c5057c", "355b86dafd852e4df905f6ad9402c7d03831d618", "355b86dafd852e4df905f6ad9402c7d03831d618", "f9cf9b6291aded2a82652002511aea36b6c5057c", "f9cf9b6291aded2a82652002511aea36b6c5057c", "f9cf9b6291aded2a82652002511aea36b6c5057c", "f9cf9b6291aded2a82652002511aea36b6c5057c", "f9cf9b6291aded2a82652002511aea36b6c5057c", "355b86dafd852e4df905f6ad9402c7d03831d618"]},{"id": "693a614718a96d61968ec573b2932a3301092c9a", "title": "Neural Networks: A Comprehensive Foundation", "authors": ["Simon Haykin"], "date": "1998", "abstract": "From the Publisher: \nThis book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and… ", "references": []},{"id": "4c891b14e91e89797dac565e788b7e3e23813ec1", "title": "Contextualizing Semantic Representations Using Syntactically Enriched Vector Models", "authors": ["Stefan Thater", "Hagen Fürstenau", "Manfred Pinkal"], "date": "ACL", "abstract": "We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised… ", "references": ["cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "6b2c2801ad74ec48a4854c92d2f0d854a2d90073", "1ea92d57e3b7d79d5a11d347a23a9f9330d9c838", "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93", "b5d67d1dc671bce42a9daac0c3605adb3fcfc697", "bd3ce4274a1a3d14e938104f25225e4b85f6a451", "bd3ce4274a1a3d14e938104f25225e4b85f6a451", "1ea92d57e3b7d79d5a11d347a23a9f9330d9c838", "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "9875ca912adcd4ba97e0eefeeb064c87073a69a5"]},{"id": "8dabfccd40dad42c0740f592f34ba048cb1730e3", "title": "Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection", "authors": ["Linlin Li", "Benjamin Roth", "Caroline Sporleder"], "date": "ACL", "abstract": "This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks… ", "references": ["78492cbe9227090898a476dac38ec06a2bf89c7b", "78492cbe9227090898a476dac38ec06a2bf89c7b", "e7c40bbd0da20defc831feb0d97cdb2a9735fdde", "7139b94b39d225946e02c32eda05af4c7e29741c", "a91760aca33559a6c7703c0fccf3289e1c4dd729", "626517aded5ce3f90c51023f255546aaaf678b89", "472250e02d764999f77b25230e43794e5fc92c5e", "626517aded5ce3f90c51023f255546aaaf678b89", "603336f4b24ade85cba363b4815916ac0611fb20", "78492cbe9227090898a476dac38ec06a2bf89c7b"]},{"id": "ae29b936d437a93ad259ee008ba56fe82ab4db61", "title": "Semi-supervised Semantic Role Labeling Using the Latent Words Language Model", "authors": ["Koen Deschacht", "Marie-Francine Moens"], "date": "EMNLP", "abstract": "Semantic Role Labeling (SRL) has proved to be a valuable tool for performing automatic analysis of natural language texts. Currently however, most systems rely on a large training set, which is manually annotated, an effort that needs to be repeated whenever different languages or a different set of semantic roles is used in a certain application. A possible solution for this problem is semi-supervised learning, where a small set of training examples is automatically expanded using unlabeled… ", "references": ["eca1f7388ca481533859248456052fe700097135", "eca1f7388ca481533859248456052fe700097135", "eca1f7388ca481533859248456052fe700097135", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "3de5d40b60742e3dfa86b19e7f660962298492af", "91b219faf899de810c22c26bdf59b68f9e3de444", "91b219faf899de810c22c26bdf59b68f9e3de444", "dc4a6ee4276fa077014f8a1755def3c559764b61", "3de5d40b60742e3dfa86b19e7f660962298492af"]},{"id": "c1e48526eddd68b5bf98739a578ab69a009f570d", "title": "Word sense disambiguation: A survey", "authors": ["Roberto Navigli"], "date": "2009", "abstract": "Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner.", "references": ["e5019ace68169b17e3c83a0cbe92b2217c30523a", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "c9e0893440197a0f4d888a6151c7c7d6423be29d", "100d860cfe7daecc84a9344a514d1fc79457ae2a", "c9e0893440197a0f4d888a6151c7c7d6423be29d", "acc2b86c4a4298441426bfed2d431d3b2dd991d9", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "e5019ace68169b17e3c83a0cbe92b2217c30523a", "3137a2997991b1360f51b7a60b606d629eda3c54", "57dd5669aa2f35094d0168721920c1e5e54af595"]},{"id": "9474fe3f566863ab7410e74b66ac848eac7cb4e2", "title": "Improving generative statistical parsing with semi-supervised word clustering", "authors": ["Marie Candito", "Benoît Crabbé"], "date": "IWPT", "abstract": "We present a semi-supervised method to improve statistical parsing performance. We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing. We use a combination of lexicon-aided morphological clustering that preserves tagging ambiguity, and unsupervised word clustering, trained on a large unannotated corpus. We apply these clusterings to the French Treebank, and we train a parser with the PCFG-LA unlexicalized algorithm of (Petrov… ", "references": ["790ecefeaf2b471b439743a772ccce026131bef5", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "fd1901f34cc3673072264104885d70555b1a4cdc", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "f52de7242e574b70410ca6fb70b79c811919fc00", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "f120ea4eca8a7d96ddb4e5073198e67f4a50d854", "fd1901f34cc3673072264104885d70555b1a4cdc", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "790ecefeaf2b471b439743a772ccce026131bef5"]},{"id": "cbbf322dd1d97b70ffdd61e5afbad8d45b951efc", "title": "Concept Discovery from Text", "authors": ["Dekang Lin", "Patrick Pantel"], "date": "COLING", "abstract": "Broad-coverage lexical resources such as WordNet are extremely useful.", "references": ["9378a3797d5f815babe7b392a199ea9d8d4f1dcf", "050e804fe71b65d8d334d1655ba4dbba35b51fb5", "788bc1801215c8eb9b466ad4db00fec5832534de", "fdb7ed73a09c7ff1fab63a8d8116aa4a5f699811", "fd1901f34cc3673072264104885d70555b1a4cdc", "fd1901f34cc3673072264104885d70555b1a4cdc", "8ff7b7cf3849640b7cfb8f08e2946fd151fed34c", "49af3e80343eb80c61e727ae0c27541628c7c5e2", "788bc1801215c8eb9b466ad4db00fec5832534de", "8ff7b7cf3849640b7cfb8f08e2946fd151fed34c"]},{"id": "3317f2788b2b07d9ba4cb4335e29316fcf8a971a", "title": "Discovering word senses from text", "authors": ["Patrick Pantel", "Dekang Lin"], "date": "KDD '02", "abstract": "Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the… ", "references": ["9378a3797d5f815babe7b392a199ea9d8d4f1dcf"]},{"id": "df3fd99704ab829157062bb44fb4929f9cba9217", "title": "Word sense disambiguation and information retrieval", "authors": ["Mark Sanderson"], "date": "SIGIR '94", "abstract": "It has often been thought that word sense ambiguity is a cause of poor performance in Information Retrieval (IR) systems. The belief is that if ambiguous words can be correctly disambiguated, IR performance will increase. However, recent research into the application of a word sense disambiguator to an IR system failed to show any performance increase. From these results it has become clear that more basic research is needed to investigate the relationship between sense ambiguity… ", "references": ["84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af", "84422ba04d0113a24033f49be772d586359200af"]},{"id": "0a32b3d027064798fb31ce42894fec31e834f7db", "title": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness", "authors": ["Alexander Budanitsky", "Graeme Hirst"], "date": "2006", "abstract": "The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why… ", "references": ["6b64e068a8face2540fc436af40dbcd2b0912bbf", "fd1901f34cc3673072264104885d70555b1a4cdc", "eaa82498a122bb846f13c6419d5752f1e2109320", "cfd5ddd4bac01a078d4b47fcc4d6fac085f6e583", "6db4e86e6377cd703aaaf3a3b471b62e033757ae", "cfd5ddd4bac01a078d4b47fcc4d6fac085f6e583", "eaa82498a122bb846f13c6419d5752f1e2109320", "495f3405da229b903797472c64d09d83659fdb34", "6db4e86e6377cd703aaaf3a3b471b62e033757ae", "495f3405da229b903797472c64d09d83659fdb34"]},{"id": "477804ac9dbf871b4fe5e5ac80467413dd619a63", "title": "Investigations on Word Senses and Word Usages", "authors": ["Katrin Erk", "Diana McCarthy", "Nicholas Gaylord"], "date": "ACL/IJCNLP", "abstract": "The vast majority of work on word senses has relied on predefined sense inventories and an annotation schema where each word instance is tagged with the best fitting sense. This paper examines the case for a graded notion of word meaning in two experiments, one which uses WordNet senses in a graded fashion, contrasted with the \"winner takes all\" annotation, and one which asks annotators to judge the similarity of two usages. We find that the graded responses correlate with annotations from… ", "references": ["cb9cc883bdd08d58feee5c7da01acff6fdb4ad78", "6557bd2f2b6a2b95a381bd073561c75786406118", "237f937a56f255c4c77aac7c614860e3f017c52d", "00162f43964fd457a9158408c1ac0e8990489782", "fd31b1eb3ec20a818142639a8a7ce7ed19074f54", "f51bc6a833a98070c0068593595aec9c4157d1a9", "7ef3ac14cdb484aaa2b039850093febd5cf73a21", "00162f43964fd457a9158408c1ac0e8990489782", "f51bc6a833a98070c0068593595aec9c4157d1a9", "00162f43964fd457a9158408c1ac0e8990489782"]},{"id": "46be284f1e1ece64465af6fe3a69ce544e0c7e33", "title": "The TREC-8 Question Answering Track Evaluation", "authors": ["Ellen M. Voorhees", "Dawn M. Tice"], "date": "TREC", "abstract": null, "references": ["2b4d358cda26b03484091d88c1871c5edf38b373", "691b598b993ff019e426ac29a7817bb01d16fdfa", "6e357a9470e1345b7885fb3842598a01fb8210c6", "691b598b993ff019e426ac29a7817bb01d16fdfa", "691b598b993ff019e426ac29a7817bb01d16fdfa", "691b598b993ff019e426ac29a7817bb01d16fdfa", "6e357a9470e1345b7885fb3842598a01fb8210c6", "2b4d358cda26b03484091d88c1871c5edf38b373", "6e357a9470e1345b7885fb3842598a01fb8210c6", "691b598b993ff019e426ac29a7817bb01d16fdfa"]},{"id": "a9fee459ed211f53bfadef22e3ab774d0e927358", "title": "Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis", "authors": ["Evgeniy Gabrilovich", "Shaul Markovitch"], "date": "IJCAI", "abstract": "Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge.", "references": ["6b64e068a8face2540fc436af40dbcd2b0912bbf", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "e0c01df98a6b633b25c96c1a99b713ac96f1c5be", "b132192076c65ee9c16c851728827634991d6868", "7d37dff2d8e65764e7293750051d519359d8835d", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "e0c01df98a6b633b25c96c1a99b713ac96f1c5be", "79f35f583ce9910d7dc5a0127d257574c60a41cb", "e5305866d701a2c102c5f81fbbf48bf6ac29f252", "bddf98047d69af505a0e33643565ecec280fd1c9"]},{"id": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "title": "Learning Question Classifiers", "authors": ["Dan Roth"], "date": "COLING", "abstract": "In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We… ", "references": ["5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "5afbc0e1cb4cfb6f22bb1b11de3498a610a99ec7", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "f24e2d8df650e8c8b0bf61af8ce5aed83e476be4", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "7d8b12abbaf0283f9f7612f48d1b46e981a0539d", "46e765ac3b3d163fdf60040f8742d199c0508675", "dd53ce7b4e9e1f4f236ae741b95d0553314f0cc4"]},{"id": "f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "title": "Question Answering Using Enhanced Lexical Semantic Models", "authors": ["Wen-tau Yih", "Ming-Wei Chang", "Andrzej Pastusiak"], "date": "ACL", "abstract": "In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR… ", "references": ["ceb3de7d1cef5236fe99be671a5cd137e9df255c", "0dad0da221dea30c3a0e90c45a0699aeb850af49", "a09bf86e8b764507cb47c182433765eba0b3d53c", "d2161251488dbba08616a9cdd4223a0ac1190cef", "0a32b3d027064798fb31ce42894fec31e834f7db", "d680b29d34bd1c08d874d9c2eab43931b7602669", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "0dad0da221dea30c3a0e90c45a0699aeb850af49", "cf42f1c29cd54269f9e38cd281d510bae11fa111", "a09bf86e8b764507cb47c182433765eba0b3d53c"]},{"id": "7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e", "title": "Automatic Feature Engineering for Answer Selection and Extraction", "authors": ["Aliaksei Severyn", "Alessandro Moschitti"], "date": "EMNLP", "abstract": "This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features… ", "references": ["cc97ed569340d3d9ff0bbe8fbc90dd538a1f9b31", "a81267ee7f7fd605243e6303d674ef5a1b17bfc4", "b92309d04f439ca3625feb3bca38db57596f6948", "b92309d04f439ca3625feb3bca38db57596f6948", "8d2654447614fb05d50401b29bbf940fd18f9c22", "228920ddc0d376c376ae534ceed589005f51867a", "839169f88ee73fcf904b2857d9d9a16925756cfc", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "b92309d04f439ca3625feb3bca38db57596f6948"]},{"id": "8bd6e85e7f6a9880f81a54fe2d049cacc82fc427", "title": "A Practical Approach for Representing Context and for Performing Word Sense Disambiguation Using Neural Networks", "authors": ["Stephen I. Gallant"], "date": "1991", "abstract": "Representing and manipulating context information is one of the hardest problems in natural language processing. This paper proposes a method for representing some context information so that the correct meaning for a word in a sentence can be selected. The approach is primarily based on work by Waltz and Pollack (1985, 1984), who emphasized neutrally plausible systems. By contrast this paper focuses on computationally feasible methods applicable to full-scale natural language processing… ", "references": ["3604cea44612cee7677b8ad488d6daad05238267", "76cb5a926a789e1f28035fcc7fa01d5900df6d0e", "becb7696a14e2058c98aac348ed67e295013863b", "b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b", "3106e66537a0c8f53278e553bcb38f0b0992ec0e", "becb7696a14e2058c98aac348ed67e295013863b", "76cb5a926a789e1f28035fcc7fa01d5900df6d0e", "00c6914dab0fb75c0fe5c8d8ad57d726223b7d9b", "4a25ce040332ffad2cc79c899e05bd5021a9b734", "4a25ce040332ffad2cc79c899e05bd5021a9b734"]},{"id": "0fd0e3854ee696148e978ec33d5c042554cd4d23", "title": "Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions", "authors": ["Michael Heilman", "Noah A. Smith"], "date": "HLT-NAACL", "abstract": "We describe tree edit models for representing sequences of tree transformations involving complex reordering phenomena and demonstrate that they offer a simple, intuitive, and effective method for modeling pairs of semantically related sentences. To efficiently extract sequences of edits, we employ a tree kernel as a heuristic in a greedy search routine. We describe a logistic regression model that uses 33 syntactic features of edit sequences to classify the sentence pairs. The approach leads… ", "references": ["ca3d3551e5e10ab4254e9adad94eb428e8e75586", "ca3d3551e5e10ab4254e9adad94eb428e8e75586", "70a2fcfc4e78e8d6db23bf2922f18dd73162b644", "d680b29d34bd1c08d874d9c2eab43931b7602669", "523f420cb55d8070f565c87a50099a9a5b0b9206", "523f420cb55d8070f565c87a50099a9a5b0b9206", "c6afe8a8aa13de8e3f2710ef07b22ce86a005419", "70a2fcfc4e78e8d6db23bf2922f18dd73162b644", "70a2fcfc4e78e8d6db23bf2922f18dd73162b644", "ae83cc2a1425196215a06aec25032b305896d223"]},{"id": "46b8cbcdff87b842c2c1d4a003c831f845096ba7", "title": "Order-Embeddings of Images and Language", "authors": ["Ivan Vendrov", "Ryan Kiros", "Raquel Urtasun"], "date": "2015", "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current… ", "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "7afd833f484c8032e7fdc5f53188d2ebb0fb9934", "44040913380206991b1991daf1192942e038fe31", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "44040913380206991b1991daf1192942e038fe31", "0612745dbd292fc0a548a16d39cd73e127faedde", "233bc1bbdf5c4c08da204f545b1eaf15876ea786", "0612745dbd292fc0a548a16d39cd73e127faedde", "0612745dbd292fc0a548a16d39cd73e127faedde", "7afd833f484c8032e7fdc5f53188d2ebb0fb9934"]},{"id": "228920ddc0d376c376ae534ceed589005f51867a", "title": "Answer Extraction as Sequence Tagging with Tree Edit Distance", "authors": ["Xuchen Yao", "Benjamin Van Durme", "Peter Clark"], "date": "HLT-NAACL", "abstract": "Our goal is to extract answers from preretrieved sentences for Question Answering (QA). We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. This casts answer extraction as an answer sequence tagging problem for the first time, where knowledge of shared structure between question and source sentence is incorporated through features based on Tree Edit Distance (TED). Our… ", "references": ["0fd0e3854ee696148e978ec33d5c042554cd4d23", "d680b29d34bd1c08d874d9c2eab43931b7602669", "eaf63218521a6678d46e77767aaf23f4ff12920c", "8d151cb226fd4235b04468fc620419cd0f48fbcb", "d680b29d34bd1c08d874d9c2eab43931b7602669", "0fd0e3854ee696148e978ec33d5c042554cd4d23", "e1163694d71b7372fdb5676f156806dfa6f38a58", "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "4c4a95645bb719cb53d668cca3b104e529746377", "0fd0e3854ee696148e978ec33d5c042554cd4d23"]},{"id": "d6520da982493225779ef2cac3411d10de50f5a7", "title": "Towards Building Contextual Representations of Word Senses Using Statistical Models", "authors": ["Claudia Leacock", "Geoffrey G. Towell", "Ellen M. Voorhees"], "date": "Workshop On The Acquisition…", "abstract": "A b s t r a c t Automatic corpus-based sense resolution, or sense dlsambiguation, techniques tend to focus either on very local context or on topical context.", "references": ["bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "84422ba04d0113a24033f49be772d586359200af", "d5f169880e30e1f76827d72f862555d00b01bed9", "84422ba04d0113a24033f49be772d586359200af", "b53b6b7ffd1435c2c6a1b6684f9975b73648d131", "402627e4eb8c95e4aae3026fd921aa08cd792006", "785737feb1e55d46b021d261ad5ecc705a79103e", "402627e4eb8c95e4aae3026fd921aa08cd792006", "bf9bb11568744402f3d569a83093c58b9b58349b"]},{"id": "04305cc88e1b55365d9bcb5039d26ba5d4595cfd", "title": "Information Retrieval Based on Word Senses", "authors": ["Jan O. Pedersen"], "date": "1995", "abstract": "This paper proposes an algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co-occurrence. It diiers from standard approaches by allowing for as ne grained distinctions as is warranted by the information at hand, rather than supposing a xed number of senses per word, and by allowing for more than one sense to be assigned to a given word occurrence. The algorithm is applied to the standard vector-space information retrieval model and an… ", "references": ["df3fd99704ab829157062bb44fb4929f9cba9217", "85b9eb556c211d954b31d9d58fed6891a07ab473", "1d922631a6bf8361d7602e12cafb9e15d421c827", "85b9eb556c211d954b31d9d58fed6891a07ab473", "59407446503d49a8cf5f5643b17502835b62f139", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "364fc5142fcad8ed0f9aaee6044276eb269fb017", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "b53b6b7ffd1435c2c6a1b6684f9975b73648d131", "244ac8d99def8e6238f318e5a4cdcec8023970e1"]},{"id": "b53b6b7ffd1435c2c6a1b6684f9975b73648d131", "title": "Corpus-Based Statistical Sense Resolution", "authors": ["Claudia Leacock", "Geoffrey G. Towell", "Ellen M. Voorhees"], "date": "HLT", "abstract": "The three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polysemous word by using knowledge about patterns of word cooccurrences. The techniques were based on Bayesian decision theory, neural, networks, and content vectors as used in information retrieval. To understand these methods better, we posed a very specific problem: given a set of contexts, each containing the noun line in a known sense, construct a classifier that selects the… ", "references": ["d6fb495cbb103c36608147a1eafa95988b75d2d2", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "1d922631a6bf8361d7602e12cafb9e15d421c827", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "06ebafcd79f9d3cc113da7eb986c872dec7c071e", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "539a5738cbb215ec2b61c37a88631728faf21a6c", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "d5f169880e30e1f76827d72f862555d00b01bed9"]},{"id": "1d922631a6bf8361d7602e12cafb9e15d421c827", "title": "Word-Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora", "authors": ["David Yarowsky"], "date": "COLING", "abstract": "This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories.", "references": []},{"id": "59407446503d49a8cf5f5643b17502835b62f139", "title": "Using WordNet to disambiguate word senses for text retrieval", "authors": ["Ellen M. Voorhees"], "date": "SIGIR", "abstract": "This paper describes an automatic indexing procedure that uses the “IS-A” relations contained within WordNet and the set of nouns contained in a text to select a sense for each plysemous noun in the text. The result of the indexing procedure is a vector in which some of the terms represent word senses instead of word stems. Retrieval experiments comparing the effectivenss of these sense-based vectors vs. stem-based vectors show the stem-based vectors to be superior overall, although the sense… ", "references": []},{"id": "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0", "title": "Contextual word similarity and estimation from sparse data", "authors": ["Ido Dagan", "Shaul Marcus", "Shaul Markovitch"], "date": "1995", "abstract": "Abstract In recent years there is much interest in word co-occurrence relations, such as n-grams, verb–object combinations, or co-occurrence within a limited context. This paper discusses how to estimate the likelihood of co-occurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved co-occurrence and other co-occurrences that contain similar words. These analogies are based on the assumption that similar word co… ", "references": ["9fd464768c016843a928e05a9f6983805be2fff1", "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "3de5d40b60742e3dfa86b19e7f660962298492af", "ef9190e7669ea5523c3ef61180b35385b0ea345f", "3de5d40b60742e3dfa86b19e7f660962298492af", "3cb09327e68400bf05e6f373e046a3a08e82510e", "c80bcd31f077f9a632ce959278d1c2fc095131a8", "5eb328cf7e94995199e4c82a1f4d0696430a80b5", "17b304e82a536f215fa250091edd5dc9c2421862"]},{"id": "fbe358ce706371b93c10c4395cab9a78ad3aef67", "title": "Multi-instance Multi-label Learning for Relation Extraction", "authors": ["Mihai Surdeanu", "Julie Tibshirani", "Christopher D. Manning"], "date": "EMNLP-CoNLL", "abstract": "Distant supervision for relation extraction (RE) -- gathering training data by aligning a database of facts with text -- is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised… ", "references": ["677455e832f1f07d060188238c4164e2450c3cd1", "ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f", "677455e832f1f07d060188238c4164e2450c3cd1", "677455e832f1f07d060188238c4164e2450c3cd1", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "9c7f4412b8f0310a91334aed79b8553b2ad70908", "ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f", "677455e832f1f07d060188238c4164e2450c3cd1", "ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f", "9c7f4412b8f0310a91334aed79b8553b2ad70908"]},{"id": "9ce464683fa0653245ac4c28e295d35758b955d1", "title": "Non-Expert Correction of Automatically Generated Relation Annotations", "authors": ["Matthew R. Gormley", "Adam Gerber", "Mark Dredze"], "date": "Mturk@HLT-NAACL", "abstract": "We explore a new way to collect human annotated relations in text using Amazon Mechanical Turk. Given a knowledge base of relations and a corpus, we identify sentences which mention both an entity and an attribute that have some relation in the knowledge base. Each noisy sentence/relation pair is presented to multiple turkers, who are asked whether the sentence expresses the relation. We describe a design which encourages user efficiency and aids discovery of cheating. We also present results… ", "references": ["b7fb11ef06b0dcdc89ef0a5507c6c9ccea4206d8"]},{"id": "60bc22ff917ff9ca92f17e0a2d0973a066be9096", "title": "To Re(label), or Not To Re(label)", "authors": ["Christopher H. Lin", "Mausam", "Daniel S. Weld"], "date": "HCOMP", "abstract": "One of the most popular uses of crowdsourcing is to provide training data for supervised machine learning algorithms. Since human annotators often make errors, requesters commonly ask multiple workers to label each example.  But is this strategy always the most cost effective use of crowdsourced workers? We argue \"No\" --- often classifiers can achieve higher accuracies when trained with noisy \"unilabeled\" data. However, in some cases relabeling is extremely important.  We discuss three factors… ", "references": ["a3f853572e12b51c4c227590168c95b7cd0ca666", "6c3c36fbc2cf24baf2301e80da57ed68cab97cd6", "faec62dae11e5d53c74bb96cc0843d52d4e0a861", "dc88d84afc20fcc9d3f627eec537c903871f880e", "faec62dae11e5d53c74bb96cc0843d52d4e0a861", "a3f853572e12b51c4c227590168c95b7cd0ca666", "80704426db81ba784edf62ac0fba9a8e7e9725c4", "222883709a49136bee93fb55c9d13d2d9be452b4", "faec62dae11e5d53c74bb96cc0843d52d4e0a861", "6c3c36fbc2cf24baf2301e80da57ed68cab97cd6"]},{"id": "498ca0a1f8c980586408addf7ab2919ecdb7dd3d", "title": "Factorizing YAGO: scalable machine learning for linked data", "authors": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "date": "WWW", "abstract": "Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly.", "references": ["9f54a0057d0694bc7d1dcf69d186e313ca92775c"]},{"id": "3fbc710e6584187e143582c5be20ebcdb4ff363a", "title": "Distant Supervision for Relation Extraction with Matrix Completion", "authors": ["Miao Fan", "Deli Zhao", "Edward Y. Chang"], "date": "ACL", "abstract": "The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training… ", "references": ["d48edf9e81653f4c3da716b037b0b50d54c5b034", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "5a4450ae8a92d7d34131620531ba5e5cb853148b", "81bed85b8533c6efb07757ca825fa05adad38bde", "81bed85b8533c6efb07757ca825fa05adad38bde", "9b2a46afb792ad6a5cbc44765eedb573d6300503", "9b2a46afb792ad6a5cbc44765eedb573d6300503", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "81bed85b8533c6efb07757ca825fa05adad38bde", "a5bf4826f769cfdd835a509d23c69ecf60023108"]},{"id": "311eb232e4bd3ed53b1ef3381d75b65615d4e29c", "title": "Typed Tensor Decomposition of Knowledge Bases for Relation Extraction", "authors": ["Wen-tau Yih", "Christopher Meek"], "date": "EMNLP", "abstract": "While relation extraction has traditionally been viewed as a task relying solely on textual data, recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data, the performance of relation extraction can be improved significantly. Following this new paradigm, we propose a tensor decomposition approach for knowledge base embedding that is highly scalable, and is especially suitable for relation extraction. By… ", "references": ["5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d", "834cb8e1e738b8d2c6d24e652ac966d6e7089a46", "d84b57362e2010f6f65357267df7e0157af30684", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82", "5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d", "50d53cc562225549457cbc782546bfbe1ac6f0cf", "5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "50d53cc562225549457cbc782546bfbe1ac6f0cf"]},{"id": "8dff21517f7ac744089a260dbc3e2f48649e3119", "title": "Learning to Extract Relations from the Web using Minimal Supervision", "authors": ["Razvan C. Bunescu", "Raymond J. Mooney"], "date": "ACL", "abstract": "We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents. ", "references": ["1c7d38f68fe1150895a186e30b60c02dd89a676a", "dbfd191afbbc8317577cbc44afe7156df546e143", "1c7d38f68fe1150895a186e30b60c02dd89a676a"]},{"id": "d48edf9e81653f4c3da716b037b0b50d54c5b034", "title": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations", "authors": ["Raphael Hoffmann", "Congle Zhang", "Daniel S. Weld"], "date": "ACL", "abstract": "Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text.", "references": ["d84b57362e2010f6f65357267df7e0157af30684", "bd298c1bcefcd7feb108111cd72758c265d16ee6", "8dff21517f7ac744089a260dbc3e2f48649e3119", "a3fa819575c78be3cbcc8aa394fd21a182dce669", "8dff21517f7ac744089a260dbc3e2f48649e3119", "6c8898cda9a1f13607e24306f6f64f20e0ff2ae7", "0c236e611a90018e84d9de23d1cff241354079be", "8dff21517f7ac744089a260dbc3e2f48649e3119", "a3fa819575c78be3cbcc8aa394fd21a182dce669", "0c236e611a90018e84d9de23d1cff241354079be"]},{"id": "05369d319bc1811fbfc57abfcef00273b325128e", "title": "Big Data versus the Crowd: Looking for Relationships in All the Right Places", "authors": ["Ce Zhang", "Feng Niu", "Jude W. Shavlik"], "date": "ACL", "abstract": "Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study… ", "references": ["ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f", "bce3b04e80c2fc9c0fdce337c005cb93aa1b7cbb", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "bce3b04e80c2fc9c0fdce337c005cb93aa1b7cbb", "a3fa819575c78be3cbcc8aa394fd21a182dce669", "9ce464683fa0653245ac4c28e295d35758b955d1", "a3fa819575c78be3cbcc8aa394fd21a182dce669", "a3f853572e12b51c4c227590168c95b7cd0ca666", "ec49316a5b2ab3da7a493ae276d02bd0e4a0b50f", "a3fa819575c78be3cbcc8aa394fd21a182dce669"]},{"id": "ea0bc259e6e3abdc72c2fc58887e71bbab4e9c2b", "title": "LCT-MALTA's Submission to RepEval 2017 Shared Task", "authors": ["Hoa Vu"], "date": "RepEval@EMNLP", "abstract": "We present in this paper our team LCTMALTA’s submission to the RepEval 2017 Shared Task on natural language inference. Our system is a simple system based on a standard BiLSTM architecture, using as input GloVe word embeddings augmented with further linguistic information. We use max pooling on the BiLSTM outputs to obtain embeddings for sentences. On both the matched and the mismatched test sets, our system clearly beats the shared task’s BiLSTM baseline model. ", "references": ["f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6", "83e7654d545fbbaaf2328df365a781fb67b841b4", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "83e7654d545fbbaaf2328df365a781fb67b841b4", "f04df4e20a18358ea2f689b4c129781628ef7fc1"]},{"id": "4521f5e30024dee07de088288aa5607bdeb38ad5", "title": "Combining Distant and Partial Supervision for Relation Extraction", "authors": ["Gabor Angeli", "Julie Tibshirani", "Christopher D. Manning"], "date": "EMNLP", "abstract": "Broad-coverage relation extraction either requires expensive supervised training data, or suffers from drawbacks inherent to distant supervision.", "references": ["d48edf9e81653f4c3da716b037b0b50d54c5b034", "ffad89234de4222b958bcb70afb59ed1cfc2982d", "d48edf9e81653f4c3da716b037b0b50d54c5b034", "fbe358ce706371b93c10c4395cab9a78ad3aef67", "d84b57362e2010f6f65357267df7e0157af30684", "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "05369d319bc1811fbfc57abfcef00273b325128e", "7d7eee7135ceef77683c406a996e3f387e6c941c", "ffad89234de4222b958bcb70afb59ed1cfc2982d", "fbe358ce706371b93c10c4395cab9a78ad3aef67"]},{"id": "1d9a6ff3e8b345ca39236dc8fa8236b1902b9265", "title": "Character-level Intra Attention Network for Natural Language Inference", "authors": ["Han Yang", "Marta R. Costa-jussà", "José A. R. Fonollosa"], "date": "2017", "abstract": "Natural language inference (NLI) is a central problem in language understanding. End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently. \nIn this paper, we propose Character-level Intra Attention Network (CIAN) for the NLI task. In our model, we use the character-level convolutional network to replace the standard word embedding layer, and we use the intra attention to capture the intra-sentence semantics. The proposed CIAN model provides improved… ", "references": ["4d070993cb75407b285e14cb8aac0077624ef4d9", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "13d9323a8716131911bfda048a40e2cde1a76a46", "5082a1a13daea5c7026706738f8528391a1e6d59", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "13fe71da009484f240c46f14d9330e932f8de210", "5082a1a13daea5c7026706738f8528391a1e6d59", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "13fe71da009484f240c46f14d9330e932f8de210"]},{"id": "17ad4237ed84911c421bf00bea6313faf66e103a", "title": "Integrating Linguistic Resources: The American National Corpus Model", "authors": ["Nancy Ide", "Keith Suderman"], "date": "LREC", "abstract": "This paper describes the architecture of the American National Corpus and the design decisions we have made in order to make the corpus easy to use with a variety of existing tools with varying functionality, and to allow for layering multiple annotations over the data. The overall goal of the ANC project is to provide an “open linguistic infrastructure” for American English, consisting of as many self-generated or contributed annotations of the data as possible together with derived. The… ", "references": ["b0682d939444ca5852645b20336b37f46c62dc1d", "f3ccb87b7c10d3feb7478da2b1768efd7f016498", "524af7e697a2a059520c771a7313d2fc943cd694", "524af7e697a2a059520c771a7313d2fc943cd694", "b0682d939444ca5852645b20336b37f46c62dc1d", "ae00bc40e5ed4ffa289e81b75ffc44655c1590ac", "b0682d939444ca5852645b20336b37f46c62dc1d", "b0682d939444ca5852645b20336b37f46c62dc1d", "8e824aaf67f4f4f068455c6dbb7a6ed877794bd6", "0b5d9636668336ef14b3ace5390b5cabd478f88a"]},{"id": "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "title": "Modeling Relations and Their Mentions without Labeled Text", "authors": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum"], "date": "ECML/PKDD", "abstract": "Several recent works on relation extraction have been applying the distant supervision paradigm: instead of relying on annotated text to learn how to predict relations, they employ existing knowledge bases (KBs) as source of supervision.", "references": ["9c7f4412b8f0310a91334aed79b8553b2ad70908", "ffac08c2467904858ba23b3fc425fe3c290defbc", "d84b57362e2010f6f65357267df7e0157af30684", "eb1ee935c42237201f29a931b0b0620ed3e7edbc", "9452e711ce2e7e0d4e35aaeb5ab8731de62a5809", "ffac08c2467904858ba23b3fc425fe3c290defbc", "d84b57362e2010f6f65357267df7e0157af30684", "2e9c0ad377b295e679167b6317615f0302746277", "4f410ab5c8b12b34b38421241366ee456bbebab9", "b23bb0ecc39669d783a693ae4b1dc9e8198c421d"]},{"id": "5670f4e460ccb7e6021b15d50d879a98a7a7b01c", "title": "Re-Active Learning: Active Learning with Relabeling", "authors": ["Christopher H. Lin", "Mausam", "Daniel S. Weld"], "date": "AAAI", "abstract": "Active learning seeks to train the best classifier at the lowest annotation cost by intelligently picking the best examples to label.", "references": ["6c3c36fbc2cf24baf2301e80da57ed68cab97cd6", "56ba05f2d72ed3bf1388a5294ce23d7c3e80128d", "60bc22ff917ff9ca92f17e0a2d0973a066be9096", "6953420c593842697dd09bc2cf7ffbbaf67a6e8e", "199dcbb1e5287eedb458c867b171cc83c06b0d2a", "7e7c378a1e1384f6d7f3be10a146627ec2ad63c2", "60bc22ff917ff9ca92f17e0a2d0973a066be9096", "547e0f6948e69115d3c7df39243eea660ba0dbc4", "0a20a309deda54fe14580007759c9c7623c58694", "56ba05f2d72ed3bf1388a5294ce23d7c3e80128d"]},{"id": "3adff57fd09965224506a1bacc0579d9d3c8c11e", "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "authors": ["Matthew Dunn", "Levent Sagun", "Kyunghyun Cho"], "date": "2017", "abstract": "We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering.", "references": ["d1505c6123c102e53eb19dff312cb25cea840b72", "d1505c6123c102e53eb19dff312cb25cea840b72", "9653d5c2c7844347343d073bbedd96e05d52f69b", "d1505c6123c102e53eb19dff312cb25cea840b72", "f2e50e2ee4021f199877c8920f1f984481c723aa", "f2e50e2ee4021f199877c8920f1f984481c723aa", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "9653d5c2c7844347343d073bbedd96e05d52f69b", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "6e565308c8081e807709cb4a917443b737e6cdb4"]},{"id": "34365ffe1ecf19bc642e4d33fd7ba57f16ef4b8a", "title": "A Proposal for Evaluating Answer Distillation from Web Data", "authors": ["Bhaskar Mitra", "Grady Simon", "Li Deng"], "date": "2016", "abstract": "Information retrieval systems can attempt to answer the user’s query directly, by extracting an appropriate passage of text from a corpus and presenting it on the results page. However, sometimes the passage of text contains extraneous information, or multiple passages are needed to form an answer. In cases like these, some sort of answer distillation system could be useful, taking as input the query and the answer-containing passage, and producing a succinct answer for presentation to the user… ", "references": ["46be284f1e1ece64465af6fe3a69ce544e0c7e33", "be3a65ef15f79ebb8296e6a0e8d1a9cb5c0f3638", "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "be3a65ef15f79ebb8296e6a0e8d1a9cb5c0f3638", "564257469fa44cdb57e4272f85253efb9acfd69d", "46be284f1e1ece64465af6fe3a69ce544e0c7e33", "333f98412ff246cd646551b4ca6f4b059dc1ea81", "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "d1505c6123c102e53eb19dff312cb25cea840b72"]},{"id": "995b7affd684b910d5a1c520c3af00fd20cc39b0", "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications", "authors": ["Wei He", "Haifeng Wang"], "date": "2018", "abstract": "This paper introduces DuReader, a new large-scale, open-domain Chinese ma- chine reading comprehension (MRC) dataset, designed to address real-world MRC.", "references": ["3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "05dd7254b632376973f3a1b4d39485da17814df5", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "f92272e33b11a0d2f47b5b65446c0f1a913cfd17", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "b1e20420982a4f923c08652941666b189b11b7fe", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "05dd7254b632376973f3a1b4d39485da17814df5"]},{"id": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge", "authors": ["Peter Clark", "Isaac Cowhey", "Oyvind Tafjord"], "date": "2018", "abstract": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering.", "references": ["cf8c493079702ec420ab4fc9c0fabb56b2a16c84", "564257469fa44cdb57e4272f85253efb9acfd69d", "74f5ea3952cef12b13675b4232a28b8e61ffe4da", "a5ae605457fd8c1c5cc2675417d44f8f59fc7c33", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "a5ae605457fd8c1c5cc2675417d44f8f59fc7c33", "478b4a5123bd5fda98bb35e6317d7f3555fec97d", "74f5ea3952cef12b13675b4232a28b8e61ffe4da", "478b4a5123bd5fda98bb35e6317d7f3555fec97d", "05dd7254b632376973f3a1b4d39485da17814df5"]},{"id": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "authors": ["Guokun Lai", "Qizhe Xie", "Eduard H. Hovy"], "date": "EMNLP", "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of… ", "references": ["a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "3eda43078ae1f4741f09be08c4ecab6229046a5c", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "b1e20420982a4f923c08652941666b189b11b7fe", "6eec608f266de95eb817e9a6086641abc3c91e5f", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "6eec608f266de95eb817e9a6086641abc3c91e5f", "b1e20420982a4f923c08652941666b189b11b7fe", "35b91b365ceb016fb3e022577cec96fb9b445dc5"]},{"id": "8314f8eef3b64054bfc00607507a92de92fb7c85", "title": "Natural language inference", "authors": ["Christopher D. Manning", "Bill MacCartney"], "date": "2009", "abstract": "Inference has been a central topic in artificial intelligence from the start, but while automatic methods for formal deduction have advanced tremendously, comparatively little progress has been made on the problem of natural language inference (NLI), that is, determining whether a natural language hypothesis h can justifiably be inferred from a natural language premise p. The challenges of NLI are quite different from those encountered in formal deduction: the emphasis is on informal reasoning… ", "references": ["523f420cb55d8070f565c87a50099a9a5b0b9206", "523f420cb55d8070f565c87a50099a9a5b0b9206", "02e5cae2cb6f7cd290de44ad144a55fcde12c2ec", "da83b8903383f419e26732b5e0cc2b2ef8b0205d", "5feb2c61b04532869e44d1ca4e48c7108aee5fd3", "9bbbc7d26536acd67ea7efaaa90e698790ae78c4", "ca3d3551e5e10ab4254e9adad94eb428e8e75586", "bc3db492047da12beae9cc3ced5feea6b16df10b", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "523f420cb55d8070f565c87a50099a9a5b0b9206"]},{"id": "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "title": "A Maximum Entropy Approach to Natural Language Processing", "authors": ["Adam L. Berger", "Stephen Della Pietra", "Vincent J. Della Pietra"], "date": "1996", "abstract": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this… ", "references": ["8a9b6828c5e4339025bb78af6b025d21b4830800", "ab7b5917515c460b90451e67852171a531671ab8", "8a9b6828c5e4339025bb78af6b025d21b4830800", "ab7b5917515c460b90451e67852171a531671ab8", "af386a4e0f2615ed929fdc64a86df8e383bd6121", "b951b9f78b98a186ba259027996a48e4189d37e5", "ab7b5917515c460b90451e67852171a531671ab8", "358fccd9e2633679038ae8cf8e747adefb4dc214", "851bce6405b781079359498bfd6237b95d3acc6c", "12666047cb4588405c0c111396c34ceaa0d0f3e0"]},{"id": "837fcdfe8fdcc9c7f2f8a8c58b2afd7e64b43ee0", "title": "Hidden Markov Model} Induction by Bayesian Model Merging", "authors": ["Andreas Stolcke", "Stephen M. Omohundro"], "date": "NIPS", "abstract": "This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce… ", "references": ["da757ead286e8c41ab55a70f7a403e17d177fa45", "d7a3e0ae653cb533726319a9c02fa93ffb2e76eb", "310c5457dbb1133908a9e51cda843be5fcc1b9a8", "204f6148bc6aba37eb5a7c5686d80547a99425b1", "ae7beb7920485aca9c252ce3ecc3972c52eb3c37", "ae7beb7920485aca9c252ce3ecc3972c52eb3c37", "d7a3e0ae653cb533726319a9c02fa93ffb2e76eb", "ae7beb7920485aca9c252ce3ecc3972c52eb3c37", "ae7beb7920485aca9c252ce3ecc3972c52eb3c37", "9d076613d7c36dbda4a6ff42fbdd076604b96630"]},{"id": "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "title": "Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling", "authors": ["Sanjeev Khudanpur", "Jingjing Wu"], "date": "2000", "abstract": "A new statistical language model is presented which combines collocational dependencies with two important sources of long-range statistical dependence: the syntactic structure and the topic of a sentence. These dependencies or constraints are integrated using the maximum entropy technique. Substantial improvements are demonstrated over a trigram model in both perplexity and speech recognition accuracy on the Switchboard task. A detailed analysis of the performance of this language model is… ", "references": ["0b26fa1b848ed808a0511db34bce2426888f0b68", "e4efb4b4c07d02b4031111b1cb97a7a13b5c928a", "6612de2c77458ca746325272a99e22c156263383", "5209d1b3f57ee9fc6c08b1022cab5cb360eecc1f", "5209d1b3f57ee9fc6c08b1022cab5cb360eecc1f", "bccfe1f56cf0568e3c37f945d39faaabe3c1c44b", "6612de2c77458ca746325272a99e22c156263383", "d3e5c17d9a45b00d1923d256f3fd607553ce0231", "bccfe1f56cf0568e3c37f945d39faaabe3c1c44b", "5209d1b3f57ee9fc6c08b1022cab5cb360eecc1f"]},{"id": "1506c6fb9f4fed076ab4bfaaffa210e1ffa16c23", "title": "Acoustic Modeling for the SRI Hub4 Partitioned Evaluation Continuous Speech Recognition System", "authors": ["Ananth Sankar", "Larry Heck", "Andreas Stolcke"], "date": "1997", "abstract": "We describe the developmentof the SRI systemevaluated in the 1996 DARPA continuous speech recogn ition (CSR) Hub4 partitioned evaluation (PE). The task for the Hub4 evaluation was to recognize speech from broadcast television and radio shows. Recognizingsuch speech by machines poses many challenges. First, the segments to be recognized could be very long. This introduces a problem in training and recognition b ecauseof the consequentincreased system memory requirement. A simple segmentation… ", "references": ["778015de7b81dfde54367dd57fb76c86faa72be4", "131085d733735d5fefc969c8e0274f92e5a269b1"]},{"id": "f2bf2377db2a93472edb916cddd19523c2bb907e", "title": "An Empirical Analysis of Multiple-Turn Reasoning Strategies in Reading Comprehension Tasks", "authors": ["Yelong Shen", "Xiaodong Liu", "Jianfeng Gao"], "date": "2017", "abstract": "Reading comprehension (RC) is a challenging task that requires synthesis of information across sentences and multiple turns of reasoning. Using a state-of-the-art RC model, we empirically investigate the performance of single-turn and multiple-turn reasoning on the SQuAD and MS MARCO datasets. The RC model is an end-to-end neural network with iterative attention, and uses reinforcement learning to dynamically control the number of turns. We find that multiple-turn reasoning outperforms single… ", "references": ["e94697b98b707f557436e025bdc8498fa261d3bc", "c636a2dd242908fe2e598a1077c0c57bfdea8633", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "0f2ea810c16275dc74e880296e20dbd83b1bae1c", "564257469fa44cdb57e4272f85253efb9acfd69d", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "05dd7254b632376973f3a1b4d39485da17814df5", "564257469fa44cdb57e4272f85253efb9acfd69d", "564257469fa44cdb57e4272f85253efb9acfd69d", "ff1861b71eaedba46cb679bbe2c585dbe18f9b19"]},{"id": "104715e1097b7ebee436058bfd9f45540f269845", "title": "Reading Wikipedia to Answer Open-Domain Questions", "authors": ["Danqi Chen", "Adam Fisch", "Antoine Bordes"], "date": "ACL", "abstract": "This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article.", "references": ["97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "832fc9327695f7425d8759c6aaeec0fa2d7b0a90", "05dd7254b632376973f3a1b4d39485da17814df5", "af44f5db5b4396e1670cda07eff5ad84145ba843", "832fc9327695f7425d8759c6aaeec0fa2d7b0a90", "e978d832a4d86571e1b52aa1685dc32ccb250f50", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "6e565308c8081e807709cb4a917443b737e6cdb4", "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "1f54c4d6cf211f6eeb532283ca924e06bf3f7f1e"]},{"id": "0b89397e6dd9eb617a8b860fc091c87b356a255f", "title": "Enhanced word classing for model M", "authors": ["Stanley F. Chen", "Stephen M. Chu"], "date": "INTERSPEECH", "abstract": "Model M is a superior class-based n-gram model that has shown improvements on a variety of tasks and domains. In previous work with Model M, bigram mutual information clustering has been used to derive word classes. In this paper, we introduce a new word classing method designed to closely match with Model M. The proposed classing technique achieves gains in speech recognition word-error rate of up to 1.1% absolute over the baseline clustering, and a total gain of up to 3.0% absolute over a… ", "references": ["2e6dd008f2837dea054cd968ae6a50325809833d", "2e6dd008f2837dea054cd968ae6a50325809833d", "ed23c461535afc492e80c63ee8d1ed55b8a176e1", "ed23c461535afc492e80c63ee8d1ed55b8a176e1"]},{"id": "cb826a3899752b796f14df1c50378c64954a6b0a", "title": "Statistical Significance Tests for Machine Translation Evaluation", "authors": ["Philipp Koehn"], "date": "EMNLP", "abstract": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality?", "references": ["f013d1eaf0b5e089281c64c8a75d2b7ec390891d", "e4d3bf856ce5259360a8033d50abcdd22873bcd6", "1f12451245667a85d0ee225a80880fc93c71cc8b", "f013d1eaf0b5e089281c64c8a75d2b7ec390891d", "a6ed57eabb350b220836a910045c45d069817e98", "e2a68774f92d1e894cbbbef2c819e4592990eb4b", "ab7b5917515c460b90451e67852171a531671ab8", "1f12451245667a85d0ee225a80880fc93c71cc8b", "e4d3bf856ce5259360a8033d50abcdd22873bcd6", "a6ed57eabb350b220836a910045c45d069817e98"]},{"id": "7424d48786b23ffdfe6d938133b89830420afc9b", "title": "Dynamic programming search for continuous speech recognition", "authors": ["Hermann Ney", "Stefan Ortmanns"], "date": "1999", "abstract": "The authors gives a unifying view of the dynamic programming approach to the search problem. They review the search problem from the statistical point-of-view and show how the search space results from the acoustic and language models required by the statistical approach. Starting from the baseline one-pass algorithm using a linear organization of the pronunciation lexicon, they have extended the baseline algorithm toward various dimensions. To handle a large vocabulary, they have shown how the… ", "references": ["b48aa079a3909609b9136b34d0b9156e81632515", "71ca8232d14a6922ad208a0b2979341c2c0431b0", "a4d369e29183a6bc78c2b425ba39fdf358a46aa4", "b48aa079a3909609b9136b34d0b9156e81632515", "97c012543a803d6c7de1163076623f6c2cac5769", "a4d369e29183a6bc78c2b425ba39fdf358a46aa4", "b48aa079a3909609b9136b34d0b9156e81632515", "71ca8232d14a6922ad208a0b2979341c2c0431b0", "f020294ed44c50088c8d51d377242729c07184b0", "71ca8232d14a6922ad208a0b2979341c2c0431b0"]},{"id": "591080c335daba2494f18cc04c9f7071501af0eb", "title": "Large-Scale Distributed Language Modeling", "authors": ["Ahmad Emami", "Kishore Papineni", "Jeffrey Scott Sorensen"], "date": "2007", "abstract": "A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The… ", "references": ["0afe29a9a8871faab23cbf90cf499c735a3b0c51", "0afe29a9a8871faab23cbf90cf499c735a3b0c51", "0afe29a9a8871faab23cbf90cf499c735a3b0c51", "6a923c9f89ed53b6e835b3807c0c1bd8d532687b", "4993485330a0a847e6a500dfef47060c5cebe2ac", "4993485330a0a847e6a500dfef47060c5cebe2ac", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "0afe29a9a8871faab23cbf90cf499c735a3b0c51", "6a923c9f89ed53b6e835b3807c0c1bd8d532687b", "29053eab305c2b585bcfbb713243b05646e7d62d"]},{"id": "4bf7edee5a4c4cfdbdd43a607c402420129fa277", "title": "Query-Reduction Networks for Question Answering", "authors": ["Minjoon Seo", "Sewon Min", "Hannaneh Hajishirzi"], "date": "2016", "abstract": "In this paper, we study the problem of question answering when reasoning over multiple facts is required.", "references": ["452059171226626718eb677358836328f884298e", "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "05dd7254b632376973f3a1b4d39485da17814df5", "564257469fa44cdb57e4272f85253efb9acfd69d", "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "564257469fa44cdb57e4272f85253efb9acfd69d", "4d05ab884a6c1645b80ce5d02b09c7e5ff499790", "452059171226626718eb677358836328f884298e", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e"]},{"id": "7ed7a41c275f2870b840a5e6c3eaec8888c9480c", "title": "A Global Joint Model for Semantic Role Labeling", "authors": ["Kristina Toutanova", "Aria Haghighi", "Christopher D. Manning"], "date": "2008", "abstract": "We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases. The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments. We evaluate the… ", "references": ["844db702be4bc149b06b822b47247e15f5894cc3", "f11e1b8c08898a34f29e6bef0bf5b29dc93ebe11", "971849feaae420079b6843bf8b74db849bc8291d", "f11e1b8c08898a34f29e6bef0bf5b29dc93ebe11", "31274eabb84407e3bc2c1d14b804cb7bcc068111", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "502a0987e09450129a4ab22492e69448a08bedc9", "844db702be4bc149b06b822b47247e15f5894cc3", "1ae5c1646ea445a670fe6cc8bf72b589dd9f6e5c", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d"]},{"id": "0afe29a9a8871faab23cbf90cf499c735a3b0c51", "title": "Distributed Language Modeling for N-best List Re-ranking", "authors": ["Ying Zhang", "Almut Silja Hildebrand", "Stephan E. Vogel"], "date": "EMNLP", "abstract": "In this paper we describe a novel distributed language model for N-best list re-ranking. The model is based on the client/server paradigm where each server hosts a portion of the data and provides information to the client. This model allows for using an arbitrarily large corpus in a very efficient way. It also provides a natural platform for relevance weighting and selection. We applied this model on a 2.97 billion-word corpus and re-ranked the N-best list from Hiero, a state-of-the-art phrase… ", "references": ["f2f323efc200be1bda41dca52063d97432706a18", "eddc57c88fcef195538035eb205355db656cba98", "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "f2f323efc200be1bda41dca52063d97432706a18", "09c76da2361d46689825c4efc37ad862347ca577", "eddc57c88fcef195538035eb205355db656cba98", "9df3e898f36c8cd7afc82f3cd080dc94fd779aca", "ad3d2f463916784d0c14a19936c1544309a0a440", "ad3d2f463916784d0c14a19936c1544309a0a440", "f2f323efc200be1bda41dca52063d97432706a18"]},{"id": "c18df1edc0a45891806d44896a8f666944e93d01", "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "authors": ["Abhishek Das", "Satwik Kottur", "Dhruv Batra"], "date": "2017", "abstract": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative ‘image guessing’ game between two agents – Q-BOT and A-BOT– who communicate in natural language dialog so that Q-BOT can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end – from pixels to multi-agent multi-round dialog to game reward.,,We demonstrate two experimental results.,,First… ", "references": ["97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "0772905d40b9afa3dc087a88184f09f3b3e1464f", "f91d1db0fa2ac12e0262a8f5e6a371da29a3c100", "2caa021d85d4878d3369000e0068f617576d6cca", "2231f44be9a8472a46d8e8a628b4e52b9a8f44e0", "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60", "abf39d0158cdd392335dbdeaded021b84737ad0e", "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "1298dae5751fb06184f6b067d1503bde8037bdb7"]},{"id": "70d3d2e0a8f34d6c3cb7890e249e2ed6a574ce50", "title": "Neural Semantic Role Labeling with Dependency Path Embeddings", "authors": ["Michael Roth", "Mirella Lapata"], "date": "2016", "abstract": "This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques.", "references": ["62da3df1c7ffd0b3ec4282acba36f827bab82c58", "73183e53ce4843471d655a3bb6eee0384ca86c70", "a605b33fa7684c3d710f47d283e96e64746f3c43", "62da3df1c7ffd0b3ec4282acba36f827bab82c58", "62da3df1c7ffd0b3ec4282acba36f827bab82c58", "c6b8c1728e6d2572b16ca2bfa5c3c82bb0fd8be6", "a605b33fa7684c3d710f47d283e96e64746f3c43", "73183e53ce4843471d655a3bb6eee0384ca86c70", "3ba1eee0cf2517f40c54c20bf36cbb7f882f72a6", "a605b33fa7684c3d710f47d283e96e64746f3c43"]},{"id": "8495259ca47c938fbfc6a0a71633b27e907d998b", "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling", "authors": ["Diego Marcheggiani", "Anton Frolov", "Ivan Titov"], "date": "CoNLL", "abstract": "We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves competitive performance on English, even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the… ", "references": ["eec3a236ecd185712ce65fb336141f8656eea13d", "14c146d457bbd201f3a117ee9c848300d341e5d0", "dee93d4481ac590f6debcd2816f1f8fd27b627d9", "423967e24c338e042a36af4dc39e6a34e965b6ec", "14c146d457bbd201f3a117ee9c848300d341e5d0", "14c146d457bbd201f3a117ee9c848300d341e5d0", "3aa52436575cf6768a0a1a476601825f6a62e58f", "3aa52436575cf6768a0a1a476601825f6a62e58f", "34added00bcdab4b0ea8539bd64d279350732764", "73183e53ce4843471d655a3bb6eee0384ca86c70"]},{"id": "c6b8c1728e6d2572b16ca2bfa5c3c82bb0fd8be6", "title": "Semantic Role Labeling with Neural Network Factors", "authors": ["Nicholas FitzGerald", "Oscar Täckström", "Dipanjan Das"], "date": "EMNLP", "abstract": "We present a new method for semantic role labeling in which arguments and semantic roles are jointly embedded in a shared vector space for a given predicate.", "references": ["8be124576df1804806b585a3d521496563fe2f1d", "b5c6f0d18fd783536b4e6c2205d75b7c4477c6d2", "b5c6f0d18fd783536b4e6c2205d75b7c4477c6d2", "62da3df1c7ffd0b3ec4282acba36f827bab82c58", "dee93d4481ac590f6debcd2816f1f8fd27b627d9", "a5196dfc949cf73ddaa5d66a43b8a212f4476377", "8be124576df1804806b585a3d521496563fe2f1d", "af9b9235a68307c782d14d4bf12cb80d662d247f", "8be124576df1804806b585a3d521496563fe2f1d", "62da3df1c7ffd0b3ec4282acba36f827bab82c58"]},{"id": "c4100faa2cc35bc72e61dcbb173f1fee5e8e8840", "title": "Semantic Role Labeling", "authors": ["Cícero Nogueira dos Santos", "Ruy Luiz Milidiú"], "date": "2012", "abstract": "This chapter presents the application of the ETL approach to semantic role labeling (SRL). The SRL task consists in detecting basic event structures in a given text. Some of these event structures include who did what to whom, when and where. We evaluate the performance of ETL over two English language corpora: CoNLL-2004 and CoNLL-2005. ETL system achieves regular results for the two corpora. However, for the CoNLL-2004 Corpus, our ETL system outperforms the TBL system proposed by Higgins [4… ", "references": []},{"id": "b29cdc426784534ad5c83ccf646fde67c4695607", "title": "Recognizing Implied Predicate-Argument Relationships in Textual Inference", "authors": ["Asher Stern", "Ido Dagan"], "date": "ACL", "abstract": "We investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure. While prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios. Such scenarios provide prior information, which substantially eases the task. We provide a large and freely available evaluation dataset for our task setting, and propose methods to cope with it, while… ", "references": ["acbac8a75b25384bcc10953b2becc8278b9240c9", "acbac8a75b25384bcc10953b2becc8278b9240c9", "db8885a0037fe47d973ade79d696586453710233", "eb42a490cf4f186d3383c92963817d100afd81e2", "abba83b5747e98d10ab982df9ae94cc799668f16", "8c91cf51fd57533c763f8d9d0c5fc22e5be0c843", "eb42a490cf4f186d3383c92963817d100afd81e2", "56f327daa0bf8465bc3528a752d916fef1ce10b3", "8c91cf51fd57533c763f8d9d0c5fc22e5be0c843", "db8885a0037fe47d973ade79d696586453710233"]},{"id": "b1d7f596fc34fd6cc6bbfc22a083bca8d2d38f14", "title": "Semantic Role Features for Machine Translation", "authors": ["Ding Liu", "Daniel Gildea"], "date": "COLING", "abstract": "We propose semantic role features for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation. ", "references": ["d01737b617acc555153f4660417908bf3971b1a5", "03a52216777990f504d95e0d30cc53573055d07c", "7d0c052eabed016faeb1fba49dcd8ef6c551a79c", "35199edf885516904f0fc0f8e2f35e14040568ab", "7d0c052eabed016faeb1fba49dcd8ef6c551a79c", "9d8612724225d85f7e0e218358ee3df52581f4a6", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "03a52216777990f504d95e0d30cc53573055d07c"]},{"id": "1de2f2d45e505eab0bc520ee554353c6d509f32a", "title": "A Bayesian Approach to Unsupervised Semantic Role Induction", "authors": ["Ivan Titov", "Alexandre Klementiev"], "date": "EACL", "abstract": "We introduce two Bayesian models for unsupervised semantic role labeling (SRL) task. The models treat SRL as clustering of syntactic signatures of arguments with clusters corresponding to semantic roles. The first model induces these clusterings independently for each predicate, exploiting the Chinese Restaurant Process (CRP) as a prior. In a more refined hierarchical model, we inject the intuition that the clusterings are similar across different predicates, even though they are not… ", "references": ["ae29b936d437a93ad259ee008ba56fe82ab4db61", "36fcfac4cfda949fd8b633771384f9820ff4f37f", "3a89b351ed688aa8463222eee524bdc4f48f086c", "d2b78ad97078a8bfef6c2492b478532fb14b14de", "60f1cf5b596183dec1825088ae1925cb1e8d0d94", "3a89b351ed688aa8463222eee524bdc4f48f086c", "60f1cf5b596183dec1825088ae1925cb1e8d0d94", "ae29b936d437a93ad259ee008ba56fe82ab4db61", "d2b78ad97078a8bfef6c2492b478532fb14b14de", "ed73c8b844f9bc19170fd904c30c402fe215b9b1"]},{"id": "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "title": "The Proposition Bank: An Annotated Corpus of Semantic Roles", "authors": ["Martha Palmer", "Paul Kingsbury", "Daniel Gildea"], "date": "2005", "abstract": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We… ", "references": ["79fd56e2d7da241852359ed558e4e6ca174e419f", "45de5bfc4895b1cdb8177cf312327c60ca513099", "79fd56e2d7da241852359ed558e4e6ca174e419f", "bcd7cc92f6162f3fcc29c1c109acca8bef27e1e9", "79fd56e2d7da241852359ed558e4e6ca174e419f", "12f03b33b4a916431f32fd3c8bdd885caba1e3ca", "b0e5ab189f770b7e106db429f2980510065ef125", "6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "e33cf703e6f101b892f559cfa7941531d84af4a3"]},{"id": "c7d3f610b528226f1c862c4f9cd6b37623f7390f", "title": "The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages", "authors": ["Jan Hajic", "Massimiliano Ciaramita", "Yi Zhang"], "date": "CoNLL Shared Task", "abstract": "For the 11th straight year, the Conference on Computational Natural Language Learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2009, the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages. This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task. In… ", "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "cab88777bbde2e884726cf72ff7b3f9126efc8ff", "cab88777bbde2e884726cf72ff7b3f9126efc8ff", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "f60b10c45c8ca2be15e7cad3750f80689f2b9197", "d8daa0c8464695c5fd1b1981ad590cd8cb9071a8", "14c146d457bbd201f3a117ee9c848300d341e5d0", "44acd22bfb89f6e3d5529b2fa92b18b035e6bab5", "44acd22bfb89f6e3d5529b2fa92b18b035e6bab5", "44acd22bfb89f6e3d5529b2fa92b18b035e6bab5"]},{"id": "b5c6f0d18fd783536b4e6c2205d75b7c4477c6d2", "title": "The Importance of Syntactic Parsing and Inference in Semantic Role Labeling", "authors": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih"], "date": "2008", "abstract": "We present a general framework for semantic role labeling. The framework combines a machine-learning technique with an integer linear programming-based inference procedure, which incorporates linguistic and structural constraints into a global decision process. Within this framework, we study the role of syntactic parsing information in semantic role labeling. We show that full syntactic parsing information is, by far, most relevant in identifying the argument, especially, in the very first… ", "references": ["a584e4b607e972783cea22daaaf1114ea94a8035", "531c793838c806b39cb63a9109382b27155672ab", "480519b1488d3457a714342d4ffe7a3e363daf70", "ccea966a744af5ea34c0584ca777c32ebbb60a89", "ccea966a744af5ea34c0584ca777c32ebbb60a89", "15fad23ab1f03b39aeeee0fed24803e98c47c28d", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "15fad23ab1f03b39aeeee0fed24803e98c47c28d", "15fad23ab1f03b39aeeee0fed24803e98c47c28d"]},{"id": "9462eee3e5eff15df5e97c38e24072c65e581cee", "title": "Representation of Linguistic Form and Function in Recurrent Neural Networks", "authors": ["Ákos Kádár", "Grzegorz Chrupała", "Afra Alishahi"], "date": "2017", "abstract": "We present novel methods for analyzing the activation patterns of recurrent neural networks from a linguistic point of view and explore the types of linguistic structure they learn.", "references": ["605d738a39df3c5e596613ab0ca6925f0eecdf35", "fafa541419b3756968fe5b3156c6f0257cb29c23", "605d738a39df3c5e596613ab0ca6925f0eecdf35", "fafa541419b3756968fe5b3156c6f0257cb29c23", "3aa52436575cf6768a0a1a476601825f6a62e58f", "36da622533db1b55e6e918b81d47b96975d729d7", "cea967b59209c6be22829699f05b8b1ac4dc092d", "fafa541419b3756968fe5b3156c6f0257cb29c23", "cea967b59209c6be22829699f05b8b1ac4dc092d", "32de44f01a96d4473d21099d15e25bc2b9f08e2f"]},{"id": "1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "title": "Developing a large semantically annotated corpus", "authors": ["Valerio Basile", "Johan Bos", "Noortje Venhuizen"], "date": "LREC", "abstract": "What would be a good method to provide a large collection of semantically annotated texts with formal, deep semantics rather than shallow? We argue that a bootstrapping approach comprising state-of-the-art NLP tools for parsing and semantic interpretation, in combination with a wiki-like interface for collaborative annotation of experts, and a game with a purpose for crowdsourcing, are the starting ingredients for fulfilling this enterprise. The result is a semantic resource that anyone can… ", "references": ["cbc2a7afcfd45c2fbad42a97e2609ead37ef7b74", "99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "4f74a7a6ccd4197865a9e9ff91e360c36e89ba53", "1446d3f9b912a46ee1211e88408843745c751a4b", "eb395d2292610700c55b14748f9ac3ce6b910fed", "351b175603e9a5a4c80372c54460d0c589bb5c47", "1446d3f9b912a46ee1211e88408843745c751a4b", "cbc2a7afcfd45c2fbad42a97e2609ead37ef7b74", "1bf6428b55c5e793c4b7d1377a4eb9a12197af1e", "351b175603e9a5a4c80372c54460d0c589bb5c47"]},{"id": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering", "authors": ["Aishwarya Agrawal", "Jiasen Lu", "Dhruv Batra"], "date": "2015", "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs… ", "references": ["abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "62a956d7600b10ca455076cd56e604dfd106072a", "564257469fa44cdb57e4272f85253efb9acfd69d", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "62a956d7600b10ca455076cd56e604dfd106072a", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "5fa973b8d284145bf0ced9acf2913a74674260f6"]},{"id": "d8259bcbe9cb0cf5bad6ea25645f4407fc544a1c", "title": "Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates", "authors": ["Matthew Gerber", "Joyce Yue Chai"], "date": "ACL", "abstract": "Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We… ", "references": ["6579b5ef0873d695f36028157be02ad36b3982c8", "912a14e09b1f2715f2de4bddd652bdb8851a3ea8", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "14c146d457bbd201f3a117ee9c848300d341e5d0", "2dd074cf4321525671911d26890e2adb1fe4f608", "621566b223a73c0a7d8cf918297ac02c5e58af38", "bb6898d6041e97c4946661b3a3df0f82286a43b5", "f00dec2031ac595d367733f7a22ec97efc70e25b", "6579b5ef0873d695f36028157be02ad36b3982c8", "bb6898d6041e97c4946661b3a3df0f82286a43b5"]},{"id": "7bc7d7e9f16c9a6e669224f3bdd6e95401493c45", "title": "CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles", "authors": ["Chenguang Wang", "Alan Akbik", "Anbang Xu"], "date": "EMNLP", "abstract": "Crowdsourcing has proven to be an effective method for generating labeled data for a range of NLP tasks. However, multiple recent attempts of using crowdsourcing to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced. In this paper, we postulate that while producing SRL annotation does require expert involvement in general, a large subset of SRL labeling tasks is in… ", "references": ["cdf05ffcf2273fc632e8046e62974647f4623cc6", "5a1f9b2b2b15b399f0d2dde04a9595ed0744cde0", "8bbff2ad5414eb70509ff7a6c6b8be834a95d19c", "0165568bcc1a819c18564567f2ec15d859be2519", "8bbff2ad5414eb70509ff7a6c6b8be834a95d19c", "5a1f9b2b2b15b399f0d2dde04a9595ed0744cde0", "1e58d7e5277288176456c66f6b1433c41ca77415", "7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "cdf05ffcf2273fc632e8046e62974647f4623cc6", "a690b1643db8f97fb2b3d66db6f808a9c1181e12"]},{"id": "fda558136b2d2b812a608b21fe22959d48db1078", "title": "Human-in-the-Loop Parsing", "authors": ["Luheng He", "Julian Michael", "Luke Zettlemoyer"], "date": "EMNLP", "abstract": "This paper demonstrates that it is possible for a parser to improve its performance with a human in the loop, by posing simple questions to non-experts. For example, given the first sentence of this abstract, if the parser is uncertain about the subject of the verb “pose,” it could generate the question What would pose something? with candidate answers this paper and a parser. Any fluent speaker can answer this question, and the correct answer resolves the original uncertainty. We apply the… ", "references": ["2253a140c49855b6d09fef6655760ef045b092b5", "2253a140c49855b6d09fef6655760ef045b092b5", "8ea6f636007c0423f425bf6c3430b6def078c834", "8ea6f636007c0423f425bf6c3430b6def078c834", "4c4dcf6655204130f330002a9fb45c4fd436d5ea", "0fac3a87cf6f214a10f4e39d73e119ea81b52e46", "4deedbe82a9de796e111260dd194bd2d84220b39", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "4c4dcf6655204130f330002a9fb45c4fd436d5ea", "440dc6e5a06f35890d04b36768fe7b6e6d320684"]},{"id": "72d7c465ef199a9670b3da7a318b0227f5cc3229", "title": "Visual Referring Expression Recognition: What Do Systems Actually Learn?", "authors": ["Volkan Cirik", "Louis-Philippe Morency", "Taylor Berg-Kirkpatrick"], "date": "2018", "abstract": "We present an empirical analysis of the state-of-the-art systems for referring expression recognition -- the task of identifying the object in an image referred to by a natural language expression -- with the goal of gaining insight into how these systems reason about language and vision. Surprisingly, we find strong evidence that even sophisticated and linguistically-motivated models for this task may ignore the linguistic structure, instead relying on shallow correlations introduced by… ", "references": ["ce264a4e1490e959d84ddd60edbb0edcbfb3af38", "8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5", "3c1bbd2672c11a796f1e6e6aa787257498ec8bec", "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "4c16428a0bae507d2a1785860f07168a807d8e59", "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "b1e20420982a4f923c08652941666b189b11b7fe", "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "2997b26ffb8c291ce478bd8a6e47979d5a55c466", "b1e20420982a4f923c08652941666b189b11b7fe"]},{"id": "1778e32c18bd611169e64c1805a51abff341ca53", "title": "Natural Language Inference over Interaction Space", "authors": ["Yichen Gong", "Heng Luo"], "date": "2018", "abstract": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis.", "references": ["83b83ee4f27388445bdebb199cd75e5bf546dd85", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "cff79255a94b9b05a4ce893eb403a522e0923f04", "83b83ee4f27388445bdebb199cd75e5bf546dd85", "83b83ee4f27388445bdebb199cd75e5bf546dd85", "cff79255a94b9b05a4ce893eb403a522e0923f04", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "26fe009b958e8728382d9d764bd7153632f0b869", "ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "83b83ee4f27388445bdebb199cd75e5bf546dd85"]},{"id": "3c092128a2c98e5e3be5f8872cf05c635430cd60", "title": "Evaluating Compositionality in Sentence Embeddings", "authors": ["Ishita Dasgupta", "Demi Guo", "Noah D. Goodman"], "date": "2018", "abstract": "An important frontier in the quest for human-like AI is compositional semantics: how do we design systems that understand an infinite number of expressions built from a finite vocabulary? Recent research has attempted to solve this problem by using deep neural networks to learn vector space embeddings of sentences, which then serve as input to supervised learning problems like paraphrase detection and sentiment analysis. Here we focus on 'natural language inference' (NLI) as a critical test of… ", "references": ["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "c333778104f648c385b4631f7b4a859787e9d3d3", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "8314f8eef3b64054bfc00607507a92de92fb7c85"]},{"id": "c2d3746a1f755928b5011932285d686eb5a9127b", "title": "Fast Discriminative Visual Codebooks using Randomized Clustering Forests", "authors": ["Frank Moosmann", "Bill Triggs", "Frédéric Jurie"], "date": "NIPS", "abstract": "Some of the most effective recent methods for content-based image classification work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting \"visual word\" codes over the image, and classifying these with a conventional classifier such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce… ", "references": ["b3e7d3e37e67af7f4546b46051063bea1b62dbae", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "8d32093cd04d6beffb6d757f58b5ac950543ff7d", "6ae643b467ce873de1ce7962a7fa24dda1a28e68", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "6ae643b467ce873de1ce7962a7fa24dda1a28e68", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "d0faf378ca4d6227b47bb36254a57f7ceeb0c566", "6ae643b467ce873de1ce7962a7fa24dda1a28e68"]},{"id": "b6371f8c70c2684faefd99fffcc556c3a75dd7f4", "title": "Lost in quantization: Improving particular object retrieval in large scale image databases", "authors": ["James Philbin", "Ondřej Chum", "Andrew Zisserman"], "date": "2008", "abstract": "The state of the art in visual object retrieval from large databases is achieved by systems that are inspired by text retrieval.", "references": ["4cab9c4b571761203ed4c3a4c5a07dd615f57a91", "bef32204602dcac233751688fc45c7cecb4439b6", "4d524cbc9368435dcae2c7020b6b404f9883fe6f", "bef32204602dcac233751688fc45c7cecb4439b6", "9ce8626db4aa1229bf728536c139161c0d9d4641", "28e4b8ebbdb0e80f03b6f0578deeb38694af081e", "3d4f1cd7d49f414ab80e11dfd1f2fa9a22d1b8cf", "3d4f1cd7d49f414ab80e11dfd1f2fa9a22d1b8cf", "b3e7d3e37e67af7f4546b46051063bea1b62dbae", "8b440596b28dc6683caa2b5f6fbca70963e5909e"]},{"id": "dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "title": "Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study", "authors": ["Jianguo Zhang", "Marcin Marszalek", "Cordelia Schmid"], "date": "2006", "abstract": "Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover’s Distance and the χ2 distance. We first evaluate the… ", "references": ["76dfa58f92d79f004297588e33f192c619085d39", "76dfa58f92d79f004297588e33f192c619085d39", "4a0eb83e1c2b7afbf6d20c7775492764bd039141", "edfc506e67681bf7baa4362ca60cda39744383f5", "35e855b0c1af4cc7bf62a8eb459c949776fbe7ee", "397f22d68805551c500077f4a9b4dbea868d1fb3", "c393b31ca71e8c4dd7c8c5a11653b18447c90466", "0c91808994a250d7be332400a534a9291ca3b60e", "35e855b0c1af4cc7bf62a8eb459c949776fbe7ee", "c393b31ca71e8c4dd7c8c5a11653b18447c90466"]},{"id": "c65be1f97642510843667d36e399de58837d3419", "title": "Supervised translation-invariant sparse coding", "authors": ["Jianchao Yang", "Kai Yu", "Thomas S. Huang"], "date": "2010", "abstract": "In this paper, we propose a novel supervised hierarchical sparse coding model based on local image descriptors for classification tasks. The supervised dictionary training is performed via back-projection, by minimizing the training error of classifying the image level features, which are extracted by max pooling over the sparse codes within a spatial pyramid. Such a max pooling procedure across multiple spatial scales offer the model translation invariant properties, similar to the… ", "references": ["16802aa97dce7e60fd32d85b31f69e0d18e14726", "1e80f755bcbf10479afd2338cec05211fdbd325c", "9067db319ecb338312070a92b081341e6f03a6c6", "a5e23ef59eaf3fd897c460c28d23a982c72e8f65", "54a9c2553138932426faebcaa67a63a84a56b55d", "16802aa97dce7e60fd32d85b31f69e0d18e14726", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "92281d5002178003bd7060fc66677a3471cdaa4b", "16802aa97dce7e60fd32d85b31f69e0d18e14726", "1e80f755bcbf10479afd2338cec05211fdbd325c"]},{"id": "38594b15593dce53ae1888b072b417013f1830ba", "title": "Efficient learning of sparse, distributed, convolutional feature representations for object recognition", "authors": ["Kihyuk Sohn", "Dae Yon Jung", "Alfred O. Hero"], "date": "2011", "abstract": "Informative image representations are important in achieving state-of-the-art performance in object recognition tasks. Among feature learning algorithms that are used to develop image representations, restricted Boltzmann machines (RBMs) have good expressive power and build effective representations. However, the difficulty of training RBMs has been a barrier to their wide use. To address this difficulty, we show the connections between mixture models and RBMs and present an efficient training… ", "references": ["202cbbf671743aefd380d2f23987bd46b9caaf97", "8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "498efaa51f5eda731dc6199c3547b9465717fa68", "498efaa51f5eda731dc6199c3547b9465717fa68", "202cbbf671743aefd380d2f23987bd46b9caaf97", "be9a17321537d9289875fe475b71f4821457b435", "498efaa51f5eda731dc6199c3547b9465717fa68", "be9a17321537d9289875fe475b71f4821457b435", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2"]},{"id": "2f7713dcc35e7c05becf3be5522f36c9546b0364", "title": "Locality-constrained Linear Coding for image classification", "authors": ["Jinjun Wang", "Jianchao Yang", "Yihong Gong"], "date": "2010", "abstract": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance.", "references": ["35a73b1b25f98baeb87e10c86710c3597a4dac0b", "7ae4d5ec354f2a54b5cd70395cb12283390d0638", "f2e186f2ebdd4ea5d4d324711f14c20b33b647ab", "f2e186f2ebdd4ea5d4d324711f14c20b33b647ab", "fc3098cff5469c55c3e81dc127563afe6dbadf22", "f2e186f2ebdd4ea5d4d324711f14c20b33b647ab", "35a73b1b25f98baeb87e10c86710c3597a4dac0b", "f2e186f2ebdd4ea5d4d324711f14c20b33b647ab", "f2e186f2ebdd4ea5d4d324711f14c20b33b647ab", "8d32093cd04d6beffb6d757f58b5ac950543ff7d"]},{"id": "dc22b98797333881542ccacea3179ba1cec522ae", "title": "Constructing Category Hierarchies for Visual Recognition", "authors": ["Marcin Marszalek", "Cordelia Schmid"], "date": "ECCV", "abstract": "Class hierarchies are commonly used to reduce the complexity of the classification problem. This is crucial when dealing with a large number of categories. In this work, we evaluate class hierarchies currently constructed for visual recognition. We show that top-down as well as bottom-up approaches, which are commonly used to automatically construct hierarchies, incorporate assumptions about the separability of classes. Those assumptions do not hold for visual recognition of a large number of… ", "references": ["b3e7d3e37e67af7f4546b46051063bea1b62dbae", "ee3391a1c0288ccf150b7d4c883918cfedb655bc", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "530b99c3819bd4c4f1884fa89c9a0d6e024156bd", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "b02f474196fb9bd61fa3d418a7ba8ac500e8d422", "352c53e56c52a49d33dcdbec5690c2ba604b07d0", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "352c53e56c52a49d33dcdbec5690c2ba604b07d0", "812355cec91fa30bb50e9e992a3549af39e4f6eb"]},{"id": "804d14a9033b4ff532ba39c2f7d0aeac8a5548f7", "title": "Towards Scalable Dataset Construction: An Active Learning Approach", "authors": ["Brendan Collins", "Jia Deng", "Fei-Fei Li"], "date": "ECCV", "abstract": "As computer vision research considers more object categories and greater variation within object categories, it is clear that larger and more exhaustive datasets are necessary.", "references": ["6c88cc985c0e8acdeacbc584de689a837db2f0fd", "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "7c6f0c1917bb0f7e23c4c35b553045fa39663211", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "7c6f0c1917bb0f7e23c4c35b553045fa39663211", "6c88cc985c0e8acdeacbc584de689a837db2f0fd", "075bfb99ce2dbaa2005500dff90f893b7caa68c2", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "075bfb99ce2dbaa2005500dff90f893b7caa68c2", "7c6f0c1917bb0f7e23c4c35b553045fa39663211"]},{"id": "4e65c9f0a64b6a4333b12e2adc3861ad75aca83b", "title": "Image Classification Using Super-Vector Coding of Local Image Descriptors", "authors": ["Xi Zhou", "Kai Yu", "Thomas S. Huang"], "date": "ECCV", "abstract": "This paper introduces a new framework for image classification using local visual descriptors. The pipeline first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model. For all the three steps we suggest novel solutions which make our approach appealing in theory, more scalable in computation, and transparent in classification. Our experiments demonstrate that the proposed… ", "references": ["ccd52aff02b0f902f4ce7247c4fee7273014c41c", "23694b6d61668e62bb11f17c1d75dde3b4951948", "b98c68f01d84ac07dc7fc51af782018070da748f", "23694b6d61668e62bb11f17c1d75dde3b4951948", "9d65ba8bb20ae6dd001b9833c525c279dfe18916", "dc98fcae6a44735d38600500b789bd47bc986d8c", "b98c68f01d84ac07dc7fc51af782018070da748f", "b98c68f01d84ac07dc7fc51af782018070da748f", "23694b6d61668e62bb11f17c1d75dde3b4951948", "040c23e5a409fbdedd5032263dfcb1a4d7dfd200"]},{"id": "b3e7d3e37e67af7f4546b46051063bea1b62dbae", "title": "Scalable Recognition with a Vocabulary Tree", "authors": ["David Nistér", "Henrik Stewénius"], "date": "2006", "abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD’s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and… ", "references": ["12c7fc38debaf3589e712973642246bd54fe63b3", "625bce34ec80d29242340400d916e799d2975430", "d0faf378ca4d6227b47bb36254a57f7ceeb0c566", "d0faf378ca4d6227b47bb36254a57f7ceeb0c566", "d0faf378ca4d6227b47bb36254a57f7ceeb0c566", "514b8c50a5b427e2aae75f877454ec9ab3cb4e99", "642e328cae81c5adb30069b680cf60ba6b475153", "625bce34ec80d29242340400d916e799d2975430", "67f693427d956c0dbc822e7f3452aee8ca36204b", "d0faf378ca4d6227b47bb36254a57f7ceeb0c566"]},{"id": "7c6f0c1917bb0f7e23c4c35b553045fa39663211", "title": "Learning object categories from Google's image search", "authors": ["Rob Fergus", "Fei-Fei Li", "Andrew Zisserman"], "date": "2005", "abstract": "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision.", "references": ["642e328cae81c5adb30069b680cf60ba6b475153", "f354310098e09c1e1dc88758fca36767fd9d084d", "1cf1527807ebb16020b04d4166e7ba8d27652302", "d044d7d92dd1fb80275d04d035aed71bcd3374e5", "97c2c77cd76ed176683fac79c115729ad45482bc", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "1cf1527807ebb16020b04d4166e7ba8d27652302", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5"]},{"id": "24ca5231df7cbd31e11a154c0c63ad48295f0398", "title": "Integrating time alignment and neural networks for high performance continuous speech recognition", "authors": ["Patrick Haffner", "Michael A. Franzini", "Alex H. Waibel"], "date": "1991", "abstract": "The authors describe two systems in which neural network classifiers are merged with dynamic programming (DP) time alignment methods to produce high-performance continuous speech recognizers. One system uses the connectionist Viterbi-training (CVT) procedure, in which a neural network with frame-level outputs is trained using guidance from a time alignment procedure. The other system uses multi-state time-delay neural networks (MS-TDNNs), in which embedded DP time alignment allows network… ", "references": []},{"id": "530b99c3819bd4c4f1884fa89c9a0d6e024156bd", "title": "Semantic Hierarchies for Visual Object Recognition", "authors": ["Marcin Marszalek", "Cordelia Schmid"], "date": "2007", "abstract": "In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection. We evaluate how our approach influences the classification accuracy and speed on the Pascal VOC challenge 2006… ", "references": ["dee20a7ce7745fc367c8bc7ede4f7b8c22efa52d", "f4fb18a9f0d3a849d314daef1053d369998c5144", "c059e46ec0c5df9769c391ad1d84395bf6eae5a4", "f4fb18a9f0d3a849d314daef1053d369998c5144", "f4fb18a9f0d3a849d314daef1053d369998c5144", "b02f474196fb9bd61fa3d418a7ba8ac500e8d422", "b02f474196fb9bd61fa3d418a7ba8ac500e8d422", "b02f474196fb9bd61fa3d418a7ba8ac500e8d422", "c059e46ec0c5df9769c391ad1d84395bf6eae5a4", "b02f474196fb9bd61fa3d418a7ba8ac500e8d422"]},{"id": "f866ac085771f5676800db2d9b102975b2a1b2d7", "title": "Connectionist Viterbi training: a new hybrid method for continuous speech recognition", "authors": ["M. Franzini", "K.F. Lee", "Alexander Waibel"], "date": "1990", "abstract": "A hybrid method for continuous-speech recognition which combines hidden Markov models (HMMs) and a connectionist technique called connectionist Viterbi training (CVT) is presented. CVT can be run iteratively and can be applied to large-vocabulary recognition tasks. Successful completion of training the connectionist component of the system, despite the large network size and volume of training data, depends largely on several measures taken to reduce learning time. The system is trained and… ", "references": ["1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "c25ba45504a3345c94a5c1ac7952b5e835e6c97b", "e2656c7c34cd19d131710aad96ea7d1b96440985", "e2656c7c34cd19d131710aad96ea7d1b96440985", "c25ba45504a3345c94a5c1ac7952b5e835e6c97b", "a33cb603f95ac7c65a445c9022d1d60f7821a5a1", "a33cb603f95ac7c65a445c9022d1d60f7821a5a1", "834b3738673dacc767563c2714239852a8a6d4b4", "ff07698f776f27878fa904f7f51220e1cdf0b744", "4a42b2104ca8ff891ae77c40a915d4c94c8f8428"]},{"id": "092c275005ae49dc1303214f6d02d134457c7053", "title": "LabelMe: A Database and Web-Based Tool for Image Annotation", "authors": ["Bryan C. Russell", "Antonio Torralba", "William T. Freeman"], "date": "2005", "abstract": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We… ", "references": ["472ff211d18aa2ca2b65b69d86499430ad287499", "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e", "38101fac622a70b78f13625fc6502000b8756d3a", "1ca9da67e2c427e59a5a54c9b31157e6b8b4843e", "472ff211d18aa2ca2b65b69d86499430ad287499", "38101fac622a70b78f13625fc6502000b8756d3a", "b82d251ed367593366680acebc81fdb070b04a18", "7c6f0c1917bb0f7e23c4c35b553045fa39663211", "7c6f0c1917bb0f7e23c4c35b553045fa39663211", "1ca9da67e2c427e59a5a54c9b31157e6b8b4843e"]},{"id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "title": "A Tutorial on Hidden Markov Models and Selected Applications", "authors": ["Lawrence R. Rabiner"], "date": "1989", "abstract": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting. ", "references": ["664eb4fb59f2ce8f2e019a77653f9ed2cc5df591", "69fc8c03d21e22e30d6642824c37158b314f36c3", "664eb4fb59f2ce8f2e019a77653f9ed2cc5df591", "664eb4fb59f2ce8f2e019a77653f9ed2cc5df591", "0ad9f79c744bec1f1325b621ff590172db9a511d", "1c18985b2e70d671f07d784ac9120fdd090569a9", "f352b1770357f931807c6232ac879f2845980413", "242b209f24703f8c97494cb1499312f30d15655f", "f352b1770357f931807c6232ac879f2845980413", "242b209f24703f8c97494cb1499312f30d15655f"]},{"id": "a4bfe622ab32e6c645eb4be2079734355b22d304", "title": "Handwritten zip code recognition with multilayer networks", "authors": ["Yann LeCun", "Ofer Matan", "Henry S. Baird"], "date": "1990", "abstract": "An application of back-propagation networks to handwritten zip code recognition is presented. Minimal preprocessing of the data is required, but the architecture of the network is highly constrained and specifically designed for the task. The input of the network consists of size-normalized images of isolated digits. The performance on zip code digits provided by the US Postal Service is 92% recognition, 1% substitution, and 7% rejects. Structured neural networks can be viewed as statistical… ", "references": ["3aa4c691289f56f9af6cf543633cfb3917274281", "3aa4c691289f56f9af6cf543633cfb3917274281", "9aec973227713cd45f156090d82a3056cca8060f", "01b6affe3ea4eae1978aec54e87087feb76d9215", "01b6affe3ea4eae1978aec54e87087feb76d9215", "9aec973227713cd45f156090d82a3056cca8060f", "3aa4c691289f56f9af6cf543633cfb3917274281", "01b6affe3ea4eae1978aec54e87087feb76d9215", "3aa4c691289f56f9af6cf543633cfb3917274281", "3aa4c691289f56f9af6cf543633cfb3917274281"]},{"id": "52a3dca70cc21b540ad5d7b7b6e23744c20095f7", "title": "Reading handwritten digits: a ZIP code recognition system", "authors": ["Ofer Matan", "Henry S. Baird", "Timothy J. Thompson"], "date": "1992", "abstract": "A neural network algorithm-based system that reads handwritten ZIP codes appearing on real US mail is described. The system uses a recognition-based segmenter, that is a hybrid of connected-components analysis (CCA), vertical cuts, and a neural network recognizer. Connected components that are single digits are handled by CCA. CCs that are combined or dissected digits are handled by the vertical-cut segmenter. The four main stages of processing are preprocessing, in which noise is removed and… ", "references": []},{"id": "d5eec41043d91964879c4c745c7165f823967f29", "title": "Robust Face Recognition via Sparse Representation", "authors": ["John Wright", "Allen Y. Yang", "Yuliang Ma"], "date": "2009", "abstract": "We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object… ", "references": ["9b21fb5271f9ad98021494f09980b0036fa6f315", "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "6642e9c6cf7432e2d11b7edf7cd47f1285acd54e", "34860ead8af5cdb2550f2767eaacf71d4a5d0c80", "9b21fb5271f9ad98021494f09980b0036fa6f315", "63140301f88a0c5223f92afbf2acfec9e537f6be", "34860ead8af5cdb2550f2767eaacf71d4a5d0c80", "34860ead8af5cdb2550f2767eaacf71d4a5d0c80", "e94543dafa59f4c61e6fa2d46568946eb2058588", "ed2ad7dfdb82039f63908b20dd736a92b6fdf3d5"]},{"id": "bb4362bd6f0bc5bb467fc8f169723243caa97d1d", "title": "Joint learning of visual attributes, object classes and visual saliency", "authors": ["Gang Wang", "David A. Forsyth"], "date": "2009", "abstract": "We present a method to learn visual attributes (eg.“red”, “metal”, “spotted”) and object classes (eg. “car”, “dress”, “umbrella”) together. We assume images are labeled with category, but not location, of an instance. We estimate models with an iterative procedure: the current model is used to produce a saliency score, which, together with a homogeneity cue, identifies likely locations for the object (resp. attribute); then those locations are used to produce better models with multiple… ", "references": ["461d2c494d0353834c54f13e74cc80cd56dbe365", "90d6e7f2202f754d8588f9536e3f5b4a24701f24", "461d2c494d0353834c54f13e74cc80cd56dbe365", "461d2c494d0353834c54f13e74cc80cd56dbe365", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "b98c68f01d84ac07dc7fc51af782018070da748f", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "461d2c494d0353834c54f13e74cc80cd56dbe365", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "8e523721feebeaee18e487607b7d0920ac6cd3b4"]},{"id": "d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "title": "Attribute and simile classifiers for face verification", "authors": ["Neeraj Kumar", "Alexander C. Berg", "Shree K. Nayar"], "date": "2009", "abstract": "We present two novel methods for face verification. Our first method - “attribute” classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - “simile” classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle… ", "references": ["4d15254f6f31356963cc70319ce416d28d8924a3", "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3", "b14d3375cb9c92e04a8bd9176bbaa4f536f29461", "f28dfadba11bd3489d008827d9b1a539b34b50df", "b14d3375cb9c92e04a8bd9176bbaa4f536f29461", "4d15254f6f31356963cc70319ce416d28d8924a3", "bcd2e7d42b4c8ad3538205692a5899f3d040fd1b", "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3", "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3", "6642e9c6cf7432e2d11b7edf7cd47f1285acd54e"]},{"id": "0566bf06a0368b518b8b474166f7b1dfef3f9283", "title": "Learning to detect unseen object classes by between-class attribute transfer", "authors": ["Christoph H. Lampert", "Hannes Nickisch", "Stefan Harmeling"], "date": "2009", "abstract": "We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing… ", "references": ["dcd27094c0a91a33455a318a242aa6d6eea6248b", "5f2b11d835b72b1f6385ede0631d49b10c6f2914", "5f2b11d835b72b1f6385ede0631d49b10c6f2914", "dcd27094c0a91a33455a318a242aa6d6eea6248b", "461d2c494d0353834c54f13e74cc80cd56dbe365", "dd626564bd47e9fc67a5b276301282ba2fe3d833", "2aa330150205f30ce23a6fce88c1de6776744574", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "dcd27094c0a91a33455a318a242aa6d6eea6248b", "dd626564bd47e9fc67a5b276301282ba2fe3d833"]},{"id": "e264e1e55433f158bf8aa8b260bf430d76d5fa28", "title": "Semantic Modeling of Natural Scenes for Content-Based Image Retrieval", "authors": ["Julia Vogel", "Bernt Schiele"], "date": "2006", "abstract": "In this paper, we present a novel image representation that renders it possible to access natural scenes by local semantic description. Our work is motivated by the continuing effort in content-based image retrieval to extract and to model the semantic content of images. The basic idea of the semantic modeling is to classify local image regions into semantic concept classes such as water, rocks, or foliage. Images are represented through the frequency of occurrence of these local concepts… ", "references": ["6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e", "a8cf3f0ea76961eca50bf26ab31e677037cab622", "142f056a365dccd029c0897fcfa7833aecf2212f", "c4d165edae9e431a59993641d3b85c9bb640513a", "142f056a365dccd029c0897fcfa7833aecf2212f", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "a8cf3f0ea76961eca50bf26ab31e677037cab622", "c4d165edae9e431a59993641d3b85c9bb640513a", "142f056a365dccd029c0897fcfa7833aecf2212f", "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e"]},{"id": "0e19e69403501be0c4e9cb19bd5a11632721ba58", "title": "Recognition using regions", "authors": ["Chunhui Gu", "Joseph J. Lim", "Jagannath Malik"], "date": "2009", "abstract": "This paper presents a unified framework for object detection, segmentation, and classification using regions. Region features are appealing in this context because: (1) they encode shape and scale information of objects naturally; (2) they are only mildly affected by background clutter. Regions have not been popular as features due to their sensitivity to segmentation errors. In this paper, we start by producing a robust bag of overlaid regions for each image using Arbeldez et al., CVPR 2009… ", "references": ["56766bab76cdcd541bf791730944a5e453006239", "38101fac622a70b78f13625fc6502000b8756d3a", "38101fac622a70b78f13625fc6502000b8756d3a", "4bb0f3a570936826401b4d1d322725bec3267dce", "54b224478a63e33441c651175c522f3702062fc4", "339c13bfe3371a71ab486381721dbb689ff415ab", "339c13bfe3371a71ab486381721dbb689ff415ab", "88482475e5dffab106149c7b358732e6c973e611", "4bb0f3a570936826401b4d1d322725bec3267dce", "88482475e5dffab106149c7b358732e6c973e611"]},{"id": "1395f0561db13cad21a519e18be111cbe1e6d818", "title": "Semantic segmentation using regions and parts", "authors": ["Pablo Arbeláez", "Bharath Hariharan", "Jitendra Malik"], "date": "2012", "abstract": "We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals.", "references": ["b5410a46dd09267b5d90eab26db897e9ab7a9e70", "88482475e5dffab106149c7b358732e6c973e611", "88482475e5dffab106149c7b358732e6c973e611", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "82fae97673a353271b1d4c001afda1af6ef6dc23", "b5410a46dd09267b5d90eab26db897e9ab7a9e70", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "37e41557932cc0035eab23fd767bde68f6475c3a", "d8e2a74b265788a9f029a97ce0d1aa4a11ca1618"]},{"id": "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "title": "Visual categorization with bags of keypoints", "authors": ["Gabriela Csurka"], "date": "eccv", "abstract": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class.", "references": ["fb5d3ef312a0619ade531937159a8b84fe049345", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "f9f836d28f52ad260213d32224a6d227f8e8849a", "f284e1186cf6ae92f3b7a15678f59e61badede59", "2c82d70ca7bedf750707b92781d0a48b991f7156", "fb5d3ef312a0619ade531937159a8b84fe049345", "2c82d70ca7bedf750707b92781d0a48b991f7156", "ac7f973658b55563f4d56e5b763c9049dd1034e0", "fb5d3ef312a0619ade531937159a8b84fe049345", "ac7f973658b55563f4d56e5b763c9049dd1034e0"]},{"id": "d65b8a4c07146e617a0029f98fcf5317a0a68a39", "title": "Recognizing indoor scenes", "authors": ["Ariadna Quattoni", "Antonio Torralba"], "date": "CVPR", "abstract": "Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global… ", "references": []},{"id": "cec734d7097ab6b1e60d95228ffd64248eb89d66", "title": "Histograms of oriented gradients for human detection", "authors": ["Navneet Dalal", "Bill Triggs"], "date": "2005", "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively… ", "references": ["f284e1186cf6ae92f3b7a15678f59e61badede59", "bb490d879512b3d43b267e3ac8931c099a5a2fd3", "bb490d879512b3d43b267e3ac8931c099a5a2fd3", "36cd88ed2c17a596001e9c7d89533ac46c28dec0", "36cd88ed2c17a596001e9c7d89533ac46c28dec0", "36cd88ed2c17a596001e9c7d89533ac46c28dec0", "bb490d879512b3d43b267e3ac8931c099a5a2fd3", "ac7f973658b55563f4d56e5b763c9049dd1034e0", "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a", "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a"]},{"id": "cb2115a6765e8484830865b8ad5e6cc5dd29b48d", "title": "CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts", "authors": ["João Carreira", "Cristian Sminchisescu"], "date": "2012", "abstract": "We present a novel framework to generate and rank plausible hypotheses for the spatial extent of objects in images using bottom-up computational processes and mid-level selection cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge of the properties of individual object classes, by solving a sequence of Constrained Parametric Min-Cut problems (CPMC) on a regular image grid. In a subsequent step, we learn to rank the… ", "references": ["ae89e41f77a7c3ac84be98d83fd925b539639b57", "54569a7e3726f7ccfedec667c352b8cdd5052138", "54569a7e3726f7ccfedec667c352b8cdd5052138", "7ca434a6eaf5cf4d6e86aca524e753fadb607979", "140d2acd4cdbc30b102dac34f4c68f279ace6a26", "1c41c1f86b92a8c011e0324d90624d539a849b8b", "1c41c1f86b92a8c011e0324d90624d539a849b8b", "8ec1f8cbe8c9da709d519f99fc670604c268742f", "7ca434a6eaf5cf4d6e86aca524e753fadb607979", "5b0b675416d0603008d5650bbb259f8ab230e8cb"]},{"id": "2eb6caace8296fd4dfd4947efa4fe911c8e133b2", "title": "Measuring the Objectness of Image Windows", "authors": ["Bogdan Alexe", "Thomas Deselaers", "Vittorio Ferrari"], "date": "2012", "abstract": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class.", "references": ["f4f03f0c435f8a2891b048d19d7a0b8e3e5263b4", "f4f03f0c435f8a2891b048d19d7a0b8e3e5263b4", "bb4362bd6f0bc5bb467fc8f169723243caa97d1d", "f4f03f0c435f8a2891b048d19d7a0b8e3e5263b4", "2dd55b3bcaf50c1228569d0efe5620a910c1cd07", "f4f03f0c435f8a2891b048d19d7a0b8e3e5263b4", "0cb898218e3c0db2fff8a6dcfadfa6d4d3517336", "f4f03f0c435f8a2891b048d19d7a0b8e3e5263b4", "4131a2862d9f926c6727da6dc75c8fda25f4a9e5", "bb4362bd6f0bc5bb467fc8f169723243caa97d1d"]},{"id": "06b2739f4daba0382870f7e34ab653fa444993c0", "title": "Bottom-Up Segmentation for Top-Down Detection", "authors": ["Sanja Fidler", "Roozbeh Mottaghi", "Raquel Urtasun"], "date": "2013", "abstract": "In this paper we are interested in how semantic segmentation can help object detection. Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional HOG filters as well as a set of novel… ", "references": ["18ccd8bd64b50c1b6a83a71792fd808da7076bc9", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "140d2acd4cdbc30b102dac34f4c68f279ace6a26", "b5410a46dd09267b5d90eab26db897e9ab7a9e70", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "18ccd8bd64b50c1b6a83a71792fd808da7076bc9", "d8be51d4a941b9db67b13278a2d033e28517bfaa", "d2efe575c931cf923e47ec5c7f444d53aae549cd", "3d5cbe65c12f2e2201fc0f038978374045eed8ad", "f7611fc5ee6c5d678cc4cd1a438e07f4914e5f48"]},{"id": "f99408de2ae6c5c036e1825bdadf7b193c3ba734", "title": "Regionlets for Generic Object Detection", "authors": ["Xiaoyu Wang", "Ming Yang", "Yuanqing Lin"], "date": "2013", "abstract": "Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as region lets. A region let is a base feature extraction region… ", "references": ["6741e2d0cf378e96541286e399f3ac16dbb06045", "6741e2d0cf378e96541286e399f3ac16dbb06045", "636cea485f77373ae18e32ac8e1c4e37555db5bd", "2dd55b3bcaf50c1228569d0efe5620a910c1cd07", "2eb6caace8296fd4dfd4947efa4fe911c8e133b2", "54b224478a63e33441c651175c522f3702062fc4", "2eb6caace8296fd4dfd4947efa4fe911c8e133b2", "7a6f9485a4ba58e80bbe7b1a55d0ff523207f7a1", "a9ce496186120df8f9ed3367e76a4947419e992e", "a9ce496186120df8f9ed3367e76a4947419e992e"]},{"id": "fd1da46066aa0c13646c453a22d1da055254874d", "title": "Objects as Attributes for Scene Classification", "authors": ["Li-Jia Li", "Hao Su", "Fei-Fei Li"], "date": "ECCV Workshops", "abstract": "Robust low-level image features have proven to be effective representations for a variety of high-level visual recognition tasks, such as object recognition and scene classification. But as the visual recognition tasks become more challenging, the semantic gap between low-level feature representation and the meaning of the scenes increases. In this paper, we propose to use objects as attributes of scenes for scene classification. We represent images by collecting their responses to a large… ", "references": ["a5331349557fababfac48d47e49b44583e3bd5f6", "cec734d7097ab6b1e60d95228ffd64248eb89d66", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "461d2c494d0353834c54f13e74cc80cd56dbe365", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "15d2aa6511bd0a8de5cb690bf406d90eef902ff1", "6dbaff29d3898cf60f63f5a34cb9610ebb75220c", "a5331349557fababfac48d47e49b44583e3bd5f6", "a5331349557fababfac48d47e49b44583e3bd5f6", "461d2c494d0353834c54f13e74cc80cd56dbe365"]},{"id": "f6908334853988faf987be40024ba88480170441", "title": "Efficient Object Category Recognition Using Classemes", "authors": ["Lorenzo Torresani", "Martin Szummer", "Andrew W. Fitzgibbon"], "date": "ECCV", "abstract": "We introduce a new descriptor for images which allows the construction of efficient and compact classifiers with good accuracy on object category recognition.", "references": ["ba0548583a5ab3dca551f60e30f85ea42b2a4873", "d136d77dcdfb34381d8f581f3866d10293a519fd", "b3e7d3e37e67af7f4546b46051063bea1b62dbae", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "88482475e5dffab106149c7b358732e6c973e611", "aa1fa18231b8c6b35a21796af446899fc681a107", "88482475e5dffab106149c7b358732e6c973e611", "88482475e5dffab106149c7b358732e6c973e611", "b3e7d3e37e67af7f4546b46051063bea1b62dbae"]},{"id": "759d9a6c9206c366a8d94a06f4eb05659c2bb7f2", "title": "Toward Open Set Recognition", "authors": ["Walter J. Scheirer", "Anderson Rocha", "Terrance E. Boult"], "date": "2013", "abstract": "To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of “closed set” recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is “open set” recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and… ", "references": ["a6c024f0b3561237a0d976512576cf09932859e3", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "a6c024f0b3561237a0d976512576cf09932859e3", "a6c024f0b3561237a0d976512576cf09932859e3", "f2c806d3080d9df131ce4f85a1d7969ac7da15ce", "0302bb2d5476540cfb21467473f5eca843caf90b", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "6eb778507efff8b42f86415e6028bc062d3ce7ef", "6eb778507efff8b42f86415e6028bc062d3ce7ef"]},{"id": "a1dd806b8f4f418d01960e22fb950fe7a56c18f1", "title": "Interactively building a discriminative vocabulary of nameable attributes", "authors": ["Devi Parikh", "Kristen Grauman"], "date": "2011", "abstract": "Human-name able visual attributes offer many advantages when used as mid-level features for object recognition, but existing techniques to gather relevant attributes can be inefficient (costing substantial effort or expertise) and/or insufficient (descriptive properties need not be discriminative). We introduce an approach to define a vocabulary of attributes that is both human understandable and discriminative. The system takes object/scene-labeled images as input, and returns as output a set… ", "references": ["0566bf06a0368b518b8b474166f7b1dfef3f9283", "461d2c494d0353834c54f13e74cc80cd56dbe365", "a251dac6589a83e0bbcf9bef9a80c21222aeecbb", "461d2c494d0353834c54f13e74cc80cd56dbe365", "5d33a10752af9ea30993139ac6e3a323992a5831", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "bb4362bd6f0bc5bb467fc8f169723243caa97d1d", "bb4362bd6f0bc5bb467fc8f169723243caa97d1d", "461d2c494d0353834c54f13e74cc80cd56dbe365", "71466ac288ae41752df5820989a71b403accbb20"]},{"id": "3958db5769c927cfc2a9e4d1ee33ecfba86fe054", "title": "Describable Visual Attributes for Face Verification and Image Search", "authors": ["Neeraj Kumar", "Alexander C. Berg", "Shree K. Nayar"], "date": "2011", "abstract": "We introduce the use of describable visual attributes for face verification and image search. Describable visual attributes are labels that can be given to an image to describe its appearance. This paper focuses on images of faces and the attributes used to describe them, although the concepts also apply to other domains. Examples of face attributes include gender, age, jaw shape, nose size, etc. The advantages of an attribute-based representation for vision tasks are manifold: They can be… ", "references": ["d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3", "d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "82f3b7cacc15e026fd3a7639091d54162f6ae064", "d363d3343cb70ad757d472d80622e1a7c5982fd9", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "bcd2e7d42b4c8ad3538205692a5899f3d040fd1b", "82f3b7cacc15e026fd3a7639091d54162f6ae064", "6642e9c6cf7432e2d11b7edf7cd47f1285acd54e", "0566bf06a0368b518b8b474166f7b1dfef3f9283"]},{"id": "23e568fcf0192e4ff5e6bed7507ee5b9e6c43598", "title": "Relative attributes", "authors": ["Devi Parikh", "Kristen Grauman"], "date": "2011", "abstract": "Human-nameable visual “attributes” can benefit various recognition tasks. However, existing techniques restrict these properties to categorical labels (for example, a person is ‘smiling’ or not, a scene is ‘dry’ or not), and thus fail to capture more general semantic relationships. We propose to model relative attributes. Given training data stating how object/scene categories relate according to different attributes, we learn a ranking function per attribute. The learned ranking functions… ", "references": ["a251dac6589a83e0bbcf9bef9a80c21222aeecbb", "461d2c494d0353834c54f13e74cc80cd56dbe365", "bb4362bd6f0bc5bb467fc8f169723243caa97d1d", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "a251dac6589a83e0bbcf9bef9a80c21222aeecbb", "a1dd806b8f4f418d01960e22fb950fe7a56c18f1", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "a1dd806b8f4f418d01960e22fb950fe7a56c18f1", "461d2c494d0353834c54f13e74cc80cd56dbe365", "c6a8aef1bf134294482d8088f982d5643347d2ff"]},{"id": "e8f811399746c059bf4d4c3d43334045e0222209", "title": "Learning Fast Approximations of Sparse Coding", "authors": ["Karol Gregor", "Yann LeCun"], "date": "ICML", "abstract": "In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors.", "references": ["a5e23ef59eaf3fd897c460c28d23a982c72e8f65", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "92281d5002178003bd7060fc66677a3471cdaa4b", "20b003b91679fd88b63e9bfab6f05f1e7ecabe2c", "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "9af121fbed84c3484ab86df8f17f1f198ed790a0", "cf80cc34528273d8fbe17783efe802a6509e1562", "e64a9960734215e2b1866ea3cb723ffa5585ac14", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8"]},{"id": "86cf4c859d34c84fefdd8d36e5ea8ab691948512", "title": "Parallel Training for Deep Stacking Networks", "authors": ["Li Deng", "Brian Hutchinson", "Dong Yu"], "date": "INTERSPEECH", "abstract": "The Deep Stacking Network (DSN) is a special type of deep architecture developed to enable and benefit from parallel learning of its model parameters on large CPU clusters. As a prospective key component of future speech recognizers, the architectural design of the DSN and its parallel training endow the DSN with scalability over a vast amount of training data. In this paper, we present our first parallel implementation of the DSN training algorithm. Particularly, we show the tradeoff between… ", "references": ["b3275d20929462b051cc99a47383af9c7ca0ac0e", "b3275d20929462b051cc99a47383af9c7ca0ac0e", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "e3c1bf806c325f306e5084c3bd332b83d2077e2a", "e0c6c4d884e3977203e798d3e46469663423762c", "b3275d20929462b051cc99a47383af9c7ca0ac0e", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "e60ff004dde5c13ec53087872cfcdd12e85beb57"]},{"id": "dd626564bd47e9fc67a5b276301282ba2fe3d833", "title": "Sharing Visual Features for Multiclass and Multiview Object Detection", "authors": ["Antonio Torralba", "Kevin P. Murphy", "William T. Freeman"], "date": "2004", "abstract": "We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity… ", "references": ["67c63f6b3eeb0b619d6306fcf1c4b144710f79b0", "a5331349557fababfac48d47e49b44583e3bd5f6", "3f0f92d501450088ab9104cefc99a2001ff8c385", "3f0f92d501450088ab9104cefc99a2001ff8c385", "a5331349557fababfac48d47e49b44583e3bd5f6", "a5331349557fababfac48d47e49b44583e3bd5f6", "4f948352d3dd8e53a83315359cf5797084a480d2", "b82d251ed367593366680acebc81fdb070b04a18", "a5331349557fababfac48d47e49b44583e3bd5f6", "61b933b8ef5b10ae4f6491a89f89972322534cf0"]},{"id": "375d7b8a70277d5d7b5e0cc999b03ba395c42901", "title": "Auto-encoder bottleneck features using deep belief networks", "authors": ["Tara N. Sainath", "Brian Kingsbury", "Bhuvana Ramabhadran"], "date": "2012", "abstract": "Neural network (NN) bottleneck (BN) features are typically created by training a NN with a middle bottleneck layer.", "references": ["2443dc59cf3d6cc1deba6d3220d61664b1a7eada", "74d00ca7790a8f1c788f9db4e5cac6d9a97cc6cf", "74d00ca7790a8f1c788f9db4e5cac6d9a97cc6cf", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "74d00ca7790a8f1c788f9db4e5cac6d9a97cc6cf", "d7174b0cf599408fb723e6702504e27dc9d6c203", "d7174b0cf599408fb723e6702504e27dc9d6c203", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "008e9e2d3908c964d5b1c408c478215709dbea10", "5e9082caea65c76bfd23b8763872804473ee7872"]},{"id": "b0383b56a5275819c95cef5af11fdba72c5afacb", "title": "Improved pre-training of Deep Belief Networks using Sparse Encoding Symmetric Machines", "authors": ["Christian Plahl", "Tara N. Sainath", "David Nahamoo"], "date": "2012", "abstract": "Restricted Boltzmann Machines (RBM) continue to be a popular methodology to pre-train weights of Deep Belief Networks (DBNs). However, the RBM objective function cannot be maximized directly. Therefore, it is not clear what function to monitor when deciding to stop the training, leading to a challenge in managing the computational costs. The Sparse Encoding Symmetric Machine (SESM) has been suggested as an alternative method for pre-training. By placing a sparseness term on the NN output… ", "references": []},{"id": "21e2b9539a680af2a3d02c94c5315fa5ded4e3ad", "title": "Three things everyone should know to improve object retrieval", "authors": ["Relja Arandjelovic", "Andrew Zisserman"], "date": "2012", "abstract": "The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time in the manner of Video Google [28].", "references": ["2891033bc775ad5ca3737749f9de684d59335a64", "b6371f8c70c2684faefd99fffcc556c3a75dd7f4", "b6371f8c70c2684faefd99fffcc556c3a75dd7f4", "bd6023341be8105298568655a5446e225da07e03", "2891033bc775ad5ca3737749f9de684d59335a64", "cbe0eb0db59f55d0686b9310b265527e1ce860ad", "f2859c9bddec9cc6af4792b0fdbaa9ee5f44f6bd", "bd6023341be8105298568655a5446e225da07e03", "b368f2df712c94f77b913a67927cb88b84b10ba2", "cbe0eb0db59f55d0686b9310b265527e1ce860ad"]},{"id": "09a35fbc5d0a002102a00dad3cf16b67f7d6d694", "title": "Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search", "authors": ["Hervé Jégou", "Matthijs Douze", "Cordelia Schmid"], "date": "ECCV", "abstract": "This paper improves recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We, first, analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the… ", "references": ["28e4b8ebbdb0e80f03b6f0578deeb38694af081e", "4d524cbc9368435dcae2c7020b6b404f9883fe6f", "1b482585d7891627c3e14a07aa132a25923edc00", "b3e7d3e37e67af7f4546b46051063bea1b62dbae", "3f1e54ed3bd801766e1897d53a9fc962524dd3c2", "4d524cbc9368435dcae2c7020b6b404f9883fe6f", "4d524cbc9368435dcae2c7020b6b404f9883fe6f", "28e4b8ebbdb0e80f03b6f0578deeb38694af081e", "28e4b8ebbdb0e80f03b6f0578deeb38694af081e", "8b440596b28dc6683caa2b5f6fbca70963e5909e"]},{"id": "cbe0eb0db59f55d0686b9310b265527e1ce860ad", "title": "Efficient representation of local geometry for large scale object retrieval", "authors": ["Michal Perdoch", "Ondřej Chum", "Jiri Matas"], "date": "CVPR", "abstract": "State of the art methods for image and object retrieval exploit both appearance (via visual words) and local geometry (spatial extent, relative pose). In large scale problems, memory becomes a limiting factor - local geometry is stored for each feature detected in each image and requires storage larger than the inverted file and term frequency and inverted document frequency weights together. We propose a novel method for learning discretized local geometry representation based on minimization… ", "references": []},{"id": "72ed7aa31abf66fdd8175046f92bdde461fb2b49", "title": "A Practical Transfer Learning Algorithm for Face Verification", "authors": ["Xudong Cao", "David P. Wipf"], "date": "2013", "abstract": "Face verification involves determining whether a pair of facial images belongs to the same or different subjects. This problem can prove to be quite challenging in many important applications where labeled training data is scarce, e.g., family album photo organization software. Herein we propose a principled transfer learning approach for merging plentiful source-domain data with limited samples from some target domain of interest to create a classifier that ideally performs nearly as well as… ", "references": ["4118b4fc7d61068b9b448fd499876d139baeec81", "23965bd9b557b04b2c81a35ee5c16951c0e420f3", "013896c2ae9533be88f285a031ce839c672078b5", "013896c2ae9533be88f285a031ce839c672078b5", "63303c61889dae39895a08b8d910e4511cd2a545", "d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "23965bd9b557b04b2c81a35ee5c16951c0e420f3", "013896c2ae9533be88f285a031ce839c672078b5", "64cf1cda80a23ed6fc1c8e66065614ef7bdeadf3", "d3046251ec5d6e7f90ef5ef2b0ac885c01138555"]},{"id": "57ebeff9273dea933e2a75c306849baf43081a8c", "title": "Deep Convolutional Network Cascade for Facial Point Detection", "authors": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "date": "2013", "abstract": "We propose a new approach for estimation of the positions of facial key points with three-level carefully designed convolutional networks.", "references": ["bb12b81196df90cad4a964bb14edfdb113aeb4ce", "b9c681f15b43e13329c8c0e1a6c7283dae7c9099", "922838dd98d599d1d229cc73896d55e7a769aa7c", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "df412c52c929d6761ad4899b15f73478a9c96645", "7e6d4876d4b25b0aa31af2c3437fb3f640e16694", "48adff169c044c674e7cbcc033c81d77c7ac9b43", "b9c681f15b43e13329c8c0e1a6c7283dae7c9099", "58970f1f51432a094faaeb3f4f70aa1778d61a42", "58970f1f51432a094faaeb3f4f70aa1778d61a42"]},{"id": "5f14a9595b0796ce6e5338f157b763326c1f632f", "title": "Tom-vs-Pete Classifiers and Identity-Preserving Alignment for Face Verification", "authors": ["Thomas Berg", "Peter N. Belhumeur"], "date": "BMVC", "abstract": "We propose a method of face verification that takes advantage of a reference set of faces, disjoint by identity from the test faces, labeled with identity and face part locations. The reference set is used in two ways. First, we use it to perform an “identity-preserving” alignment, warping the faces in a way that reduces differences due to pose and expression but preserves differences that indicate identity. Second, using the aligned faces, we learn a large set of identity classifiers, each… ", "references": ["c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3", "d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "23965bd9b557b04b2c81a35ee5c16951c0e420f3", "23965bd9b557b04b2c81a35ee5c16951c0e420f3", "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3", "d3046251ec5d6e7f90ef5ef2b0ac885c01138555", "6e1cf77a796443e6b9fa703c33b069c523980871", "3958db5769c927cfc2a9e4d1ee33ecfba86fe054", "6cddc7e24c0581c50adef92d01bb3c73d8b80b41", "82f3b7cacc15e026fd3a7639091d54162f6ae064"]},{"id": "cfaae9b6857b834043606df3342d8dc97524aa9d", "title": "Learning a similarity metric discriminatively, with application to face verification", "authors": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "date": "2005", "abstract": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face… ", "references": ["fae82787fc4268f579823696bf8f54b22e253711", "fae82787fc4268f579823696bf8f54b22e253711", "22caccb8f64c258fcd051e9ab68179faa0219bb3", "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "f42b865e20e61a954239f421b42007236e671f19", "f42b865e20e61a954239f421b42007236e671f19", "22caccb8f64c258fcd051e9ab68179faa0219bb3", "2d7ef8c1bdf91b0988e369ba409a1a54c68e46fa", "2d7ef8c1bdf91b0988e369ba409a1a54c68e46fa"]},{"id": "aa1c888d43f1d254e9fece485c3d6fd2454b894f", "title": "Structured Prediction for Object Detection in Deep Neural Networks", "authors": ["Hannes Schulz", "Sven Behnke"], "date": "ICANN", "abstract": "Deep convolutional neural networks are currently applied to computer vision tasks, especially object detection. Due to the large dimensionality of the output space, four dimensions per bounding box of an object, classification techniques do not apply easily. We propose to adapt a structured loss function for neural network training which directly maximizes overlap of the prediction with ground truth bounding boxes. We show how this structured loss can be implemented efficiently, and demonstrate… ", "references": ["82635fb63640ae95f90ee9bdc07832eb461ca881", "82635fb63640ae95f90ee9bdc07832eb461ca881", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1109b663453e78a59e4f66446d71720ac58cec25", "67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6", "1109b663453e78a59e4f66446d71720ac58cec25", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "224d0eee53c2aa5d426d2c9b7fa5d843a47cf1db", "title": "Probabilistic Elastic Matching for Pose Variant Face Verification", "authors": ["Haoxiang Li", "Gang Hua", "Jianchao Yang"], "date": "2013", "abstract": "Pose variation remains to be a major challenge for real-world face recognition.", "references": ["6642e9c6cf7432e2d11b7edf7cd47f1285acd54e", "d59185710df9a2f8cdbdb665ec5a5b4a0d2286af", "133f01aec1534604d184d56de866a4bd531dac87", "5f14a9595b0796ce6e5338f157b763326c1f632f", "b1781da9de2bf4cfc780eba603ba4cb5e3e4b50c", "be86da00efdd8c2a7fdeb2334605796c24b370f0", "be86da00efdd8c2a7fdeb2334605796c24b370f0", "133f01aec1534604d184d56de866a4bd531dac87", "d59185710df9a2f8cdbdb665ec5a5b4a0d2286af", "133f01aec1534604d184d56de866a4bd531dac87"]},{"id": "316d51aaa37891d730ffded7b9d42946abea837f", "title": "Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild", "authors": ["Zhen Cui", "Wen Li", "Xilin Chen"], "date": "2013", "abstract": "In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp… ", "references": ["6e1cf77a796443e6b9fa703c33b069c523980871", "5b50fff498f98896a3acde32654936e13b9b840f", "0f573ea21cbf7e1192c009ecb07c5576c8220d41", "5b50fff498f98896a3acde32654936e13b9b840f", "23965bd9b557b04b2c81a35ee5c16951c0e420f3", "0f573ea21cbf7e1192c009ecb07c5576c8220d41", "f61a7a7cd13e2702f0fbacc05e13b355c1e297e2", "0f573ea21cbf7e1192c009ecb07c5576c8220d41", "6e1cf77a796443e6b9fa703c33b069c523980871", "5a93f9084e59cb9730a498ff602a8c8703e5d8a5"]},{"id": "f61a7a7cd13e2702f0fbacc05e13b355c1e297e2", "title": "Face recognition in unconstrained videos with matched background similarity", "authors": ["Lior Wolf", "Tal Hassner", "Itay Maoz"], "date": "2011", "abstract": "Recognizing faces in unconstrained videos is a task of mounting importance.", "references": ["06a1382a0fc63fb173fe570e7b6a84158d4e06a5", "d5eec41043d91964879c4c745c7165f823967f29", "809953cb31d3cd4f742bac97eb3ff1b6810809d2", "f28dfadba11bd3489d008827d9b1a539b34b50df", "809953cb31d3cd4f742bac97eb3ff1b6810809d2", "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca", "d5eec41043d91964879c4c745c7165f823967f29", "31cc777d48b61d1186bae221f1f7a3321441a741", "31cc777d48b61d1186bae221f1f7a3321441a741", "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca"]},{"id": "531e412155cde56382906aeee2a1b28ec61259c5", "title": "Learning to localize detected objects", "authors": ["Qieyun Dai", "Derek Hoiem"], "date": "2012", "abstract": "In this paper, we propose an approach to accurately localize detected objects. The goal is to predict which features pertain to the object and define the object extent with segmentation or bounding box. Our initial detector is a slight modification of the DPM detector by Felzenszwalb et al., which often reduces confusion with background and other objects but does not cover the full object. We then describe and evaluate several color models and edge cues for local predictions, and we propose two… ", "references": ["1854005a7178b2df6afaacdcf91bc35d90616075", "be305b0684f1a6ec8407c107187d28502b48f993", "03cf107beae8394c685cd3997c8c4740e2468a7c", "3044cb9dc9a364c6ec8d85fdc6dc4f9d14efcc20", "03cf107beae8394c685cd3997c8c4740e2468a7c", "82fae97673a353271b1d4c001afda1af6ef6dc23", "e79272fe3d65197100eae8be9fec6469107969ae", "82fae97673a353271b1d4c001afda1af6ef6dc23", "3044cb9dc9a364c6ec8d85fdc6dc4f9d14efcc20", "be305b0684f1a6ec8407c107187d28502b48f993"]},{"id": "4417f78b31546227784941bbd6f6532a177e60b8", "title": "Deep Learning using Linear Support Vector Machines", "authors": ["Yichuan Tang"], "date": "2013", "abstract": "Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these \"deep learning\" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a… ", "references": ["57a7260208da54b0abc9b9435bc40091553ca561", "82b9099ddf092463f497bd48bb112c46ca52c4d1", "57a7260208da54b0abc9b9435bc40091553ca561", "1e80f755bcbf10479afd2338cec05211fdbd325c", "cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb", "82b9099ddf092463f497bd48bb112c46ca52c4d1", "b7a7e37b5cfae27dcd9d99765a36ff6fd30e730b", "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14", "57a7260208da54b0abc9b9435bc40091553ca561", "57a7260208da54b0abc9b9435bc40091553ca561"]},{"id": "e2d894584986b44710f634b696db371f8aff92e0", "title": "Understanding Deep Architectures using a Recursive Convolutional Network", "authors": ["David Eigen", "Jason Tyler Rolfe", "Yann LeCun"], "date": "2014", "abstract": "A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature… ", "references": ["5d90f06bb70a0a3dced62413346235c02b1aa086", "38f35dd624cd1cf827416e31ac5e0e0454028eca", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "38f35dd624cd1cf827416e31ac5e0e0454028eca", "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "38f35dd624cd1cf827416e31ac5e0e0454028eca", "5a47ba057a858f8c024d2518cc3731fc7eb40de1", "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "5a47ba057a858f8c024d2518cc3731fc7eb40de1", "1e80f755bcbf10479afd2338cec05211fdbd325c"]},{"id": "f70ae50828e3b6166628f5e8edb239b1cca6b471", "title": "Multi-dimensional Recurrent Neural Networks", "authors": ["Alex Graves", "Santiago Fernández", "Jürgen Schmidhuber"], "date": "2007", "abstract": "Recurrent neural networks (RNNs) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. Some of the properties that make RNNs suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multi-dimensional domains. However, there has so far been no direct way of applying RNNs to data with more than one spatio-temporal dimension. This paper introduces multi… ", "references": ["2f83f6e1afadf0963153974968af6b8342775d82", "2f83f6e1afadf0963153974968af6b8342775d82", "047655e733a9eed9a500afd916efa566915b9110", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "5562a56da3a96dae82add7de705e2bd841eb00fc", "2f83f6e1afadf0963153974968af6b8342775d82", "047655e733a9eed9a500afd916efa566915b9110", "43fa5235e49fa6f16d047c999234d1b93df360b0", "30bc0ca0b965a7e01f1c4cf20684fd654f975e4a"]},{"id": "cd3d49f089f25c1dbb884378361b378a901032f8", "title": "Modified high-order neural network for invariant pattern recognition", "authors": ["Evgeny Artyomov", "Orly Yadid-Pecht"], "date": "2005", "abstract": "A modification for high-order neural networks (HONN) is described. The proposed modified HONN takes into account prior knowledge of the binary patterns that must be learned. This significantly reduces hence computation time as well as memory requirements for network configuration and weight storage. An \"approximately equal triangles\" scheme for weight sharing is also proposed. These modifications enable the efficient computation of HONNs for image fields of greater that 100×100 pixels without… ", "references": ["3da218e1f6603436dbd866a4c15a6c46031ce8ca", "222b7d981bcbe3b09cd92d49126e4640eaaa2dac", "6d913d999f2265928068fa83c808395f6e36c920", "6d913d999f2265928068fa83c808395f6e36c920", "ff3ae2b72efb62b1ea4f2111b7610641f6b1230b", "9b0f3af720aaec25b733f2b44c87f1f1d78386e1", "ff3ae2b72efb62b1ea4f2111b7610641f6b1230b", "577a675e619c0889af9acf8b7b5678955f13984c", "6e29a9f9625ac62f61de21fef26f014e1e9a6bb1", "222b7d981bcbe3b09cd92d49126e4640eaaa2dac"]},{"id": "8547b76e77995286c41148bf3a9626f75dc323a6", "title": "A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images", "authors": ["Marc'Aurelio Ranzato", "Yann LeCun"], "date": "2007", "abstract": "We describe an unsupervised learning algorithm for extracting sparse and locally shift-invariant features. We also devise a principled procedure for learning hierarchies of invariant features. Each feature detector is composed of a set of trainable convolutional filters followed by a max-pooling layer over non-overlapping windows, and a point-wise sigmoid non-linearity. A second stage of more invariant features is fed with patches provided by the first stage feature extractor, and is trained in… ", "references": ["f9e65fcb0e04174577f211d702d3f837e3624c5b", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "f9e65fcb0e04174577f211d702d3f837e3624c5b", "f9e65fcb0e04174577f211d702d3f837e3624c5b", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "5562a56da3a96dae82add7de705e2bd841eb00fc", "34103850fbd71d43b19f8335f378f421ab9a1aa8", "43c8a545f7166659e9e21c88fe234e0323855216", "cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb"]},{"id": "f2df0c1026ffa474f603a535e48e5c115d3d8629", "title": "Extreme learning machine: Theory and applications", "authors": ["Guang-Bin Huang", "Qin-Yu Zhu", "Chee Kheong Siew"], "date": "2006", "abstract": "Abstract It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades.", "references": ["d9a2cfd0e7991de8bd19749d78be26c44ccae94c", "064f1e85984b207c1eb3c53ac8b68037089b7a0b", "015999a72c70a960e59c51078b09c8f672af0d2c", "015999a72c70a960e59c51078b09c8f672af0d2c", "15fad834c1937363daf7060e1c493ba512a6e13e", "7cddbe4cfc10a32c91881beb9995b6120b59167b", "015999a72c70a960e59c51078b09c8f672af0d2c", "d9a2cfd0e7991de8bd19749d78be26c44ccae94c", "064f1e85984b207c1eb3c53ac8b68037089b7a0b", "015999a72c70a960e59c51078b09c8f672af0d2c"]},{"id": "8d32768d7cb1c5f363d2cace7343da5d28882edc", "title": "Deformable Part Models with CNN Features", "authors": ["Pierre-André Savalle", "Stavros Tsogkas", "Iasonas Kokkinos"], "date": "ECCV", "abstract": "In this work we report on progress in integrating deep convo-lutional features with Deformable Part Models (DPMs). We substitute the Histogram-of-Gradient features of DPMs with Convolutional Neural Network (CNN) features, obtained from the top-most, fifth, convolutional layer of Krizhevsky's network [8]. We demonstrate that we thereby obtain a substantial boost in performance (+14.5 mAP) when compared to the baseline HOG-based models. This only partially bridges the gap between DPMs and the… ", "references": ["1109b663453e78a59e4f66446d71720ac58cec25", "e79272fe3d65197100eae8be9fec6469107969ae", "cbb19236820a96038d000dc629225d36e0b6294a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5fc662287842e5cb2d23b5fa917354e957c573bf", "5fc662287842e5cb2d23b5fa917354e957c573bf", "1109b663453e78a59e4f66446d71720ac58cec25", "e79272fe3d65197100eae8be9fec6469107969ae", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},{"id": "ee30014d4915ffa88578e475a445ae3597b88346", "title": "A multiple circular path convolution neural network system for detection of mammographic masses", "authors": ["Shih-Chung Ben Lo", "Huai Li", "Matthew T. Freedman"], "date": "2002", "abstract": "A multiple circular path convolution neural network (MCPCNN) architecture specifically designed for the analysis of tumor and tumor-like structures has been constructed. We first divided each suspected tumor area into sectors and computed the defined mass features for each sector independently. These sector features were used on the input layer and were coordinated by convolution kernels of different sizes that propagated signals to the second layer in the neural network system. The convolution… ", "references": ["c94443c8f95fc03460e5675c0e708455718d0bc3", "43404e9d55367bc1de3bd8832af9c2ce1157587d", "ba54dd47c1b27766ed5347d29d90a39753b7c13a", "c94443c8f95fc03460e5675c0e708455718d0bc3", "43404e9d55367bc1de3bd8832af9c2ce1157587d", "43404e9d55367bc1de3bd8832af9c2ce1157587d", "c94443c8f95fc03460e5675c0e708455718d0bc3", "c94443c8f95fc03460e5675c0e708455718d0bc3", "7bda4a289f585f8a31d819b963006f6c6918478c", "ba54dd47c1b27766ed5347d29d90a39753b7c13a"]},{"id": "a1306ce652f556fbb9e794d91084a29294298e6d", "title": "Pedestrian Detection with Unsupervised Multi-stage Feature Learning", "authors": ["Pierre Sermanet", "Koray Kavukcuoglu", "Yann LeCun"], "date": "2013", "abstract": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional… ", "references": ["5b4b43f10c5779d67ccee15d8d0be10ed036971b", "7fe1a8ca95b63f5c5d60f929c5822bfa7d5ac8e5", "7fe1a8ca95b63f5c5d60f929c5822bfa7d5ac8e5", "5b4b43f10c5779d67ccee15d8d0be10ed036971b", "e79272fe3d65197100eae8be9fec6469107969ae", "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "7fe1a8ca95b63f5c5d60f929c5822bfa7d5ac8e5", "df5f7765e4bba55306ea9b4ba119772e8912b22f", "5b4b43f10c5779d67ccee15d8d0be10ed036971b"]},{"id": "8930f62a4b5eb1cbabf224cf84aa009ea798cfee", "title": "Modeling Visual Attention via Selective Tuning", "authors": ["John K. Tsotsos", "Sean M. Culhane", "Fernando Nuflo"], "date": "1995", "abstract": "A model for aspects of visual attention based on the concept of selective tuning is presented. It provides for a solution to the problems of selection in an image, information routing through the visual processing hierarchy and task-specific attentional bias. The central thesis is that attention acts to optimize the search procedure inherent in a solution to vision. It does so by selectively tuning the visual processing network which is accomplished by a top-down hierarchy of winner-take-all… ", "references": ["86876c68d05af50330d07ee10790ddcf6578c7d9", "3b1b12e4510e5dab24c270502b7dad25295ce61e", "230e93bf17eac4a0894e1a82afd7819bf80e66f2", "230e93bf17eac4a0894e1a82afd7819bf80e66f2", "1b4fc2407e8bd5598bde7e4da48c3818829bda7a", "002f856353651501d6971843162f2e9403cc20df", "699b3b27084f9af0993589a862f056fdcee81a8f", "699b3b27084f9af0993589a862f056fdcee81a8f", "230e93bf17eac4a0894e1a82afd7819bf80e66f2", "b04575fae27be5e719fd4f22e175a1780a9350d0"]},{"id": "3565c5a65842f26091578b9d71d496cc1561239d", "title": "A statistical method for 3D object detection applied to faces and cars", "authors": ["Henry Schneiderman", "Takeo Kanade"], "date": "2000", "abstract": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out… ", "references": ["a2531a801a1df4e65f53794bb56b52718b6dc472", "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d", "bbe488bb190d75f4b665d43e306bcab1ab228890", "bbe488bb190d75f4b665d43e306bcab1ab228890", "ccf5208521cb8c35f50ee8873df89294b8ed7292", "23234a0f211a44d9706b2570d474427b8f899ec1", "23234a0f211a44d9706b2570d474427b8f899ec1", "a2531a801a1df4e65f53794bb56b52718b6dc472", "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d", "a2531a801a1df4e65f53794bb56b52718b6dc472"]},{"id": "89d5b41b7fb0a122f811be270e6d5f72fc59d680", "title": "PANDA: Pose Aligned Networks for Deep Attribute Modeling", "authors": ["Ning Zhang", "Manohar Paluri", "Lubomir D. Bourdev"], "date": "2014", "abstract": "We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the… ", "references": ["8b44fad91b3e6e77715b62d4f8f106f9093aa39d", "19c9ac899d5c1a008eaee887556bc1b61ff8132e", "51c3e8f78df424dcabed728f7bc20b8b309ce1af", "aa577652ce4dad3ca3dde44f881972ae6e1acce7", "23e568fcf0192e4ff5e6bed7507ee5b9e6c43598", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "aa577652ce4dad3ca3dde44f881972ae6e1acce7", "55b29a2505149d06d8c1d616cd30edca40cb029c", "8b44fad91b3e6e77715b62d4f8f106f9093aa39d", "b8de958fead0d8a9619b55c7299df3257c624a96"]},{"id": "f0dc6279760b3cc9b8d103d1922fb5595317feea", "title": "End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression", "authors": ["Li Wan", "David Eigen", "Rob Fergus"], "date": "2015", "abstract": "Deformable Parts Models and Convolutional Networks each have achieved notable performance in object detection. Yet these two approaches find their strengths in complementary areas: DPMs are well-versed in object composition, modeling fine-grained spatial relationships between parts; likewise, ConvNets are adept at producing powerful image features, having been discriminatively trained directly on the pixels. In this paper, we propose a new model that combines these two approaches, obtaining the… ", "references": ["f99408de2ae6c5c036e1825bdadf7b193c3ba734", "713f73ce5c3013d9fb796c21b981dc6629af0bd5", "244bae85fda807361a51d4b26a14ecb2b2f8776b", "f99408de2ae6c5c036e1825bdadf7b193c3ba734", "1109b663453e78a59e4f66446d71720ac58cec25", "78662a293888d7e982061d16f6a71d0223420fad", "1109b663453e78a59e4f66446d71720ac58cec25", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "713f73ce5c3013d9fb796c21b981dc6629af0bd5"]},{"id": "9b535f4edc4cbf8d4fb6182ec6b5c54db3c1cccb", "title": "Coarse-to-Fine Face Detection", "authors": ["François Fleuret", "Donald Geman"], "date": "2001", "abstract": "We study visual selection: Detect and roughly localize all instances of a generic object class, such as a face, in a greyscale scene, measuring performance in terms of computation and false alarms. Our approach is sequential testing which is coarse-to-fine in both in the exploration of poses and the representation of objects. All the tests are binary and indicate the presence or absence of loose spatial arrangements of oriented edge fragments. Starting from training examples, we recursively… ", "references": ["5ab9d1a4ac312267c4ef2f552471e138bacf68df", "d728a28d3d2199f5bc34317e829965ad662def6a", "d728a28d3d2199f5bc34317e829965ad662def6a", "4db205eb15a72c177438275746eebeaabf7e6b1a", "5ab9d1a4ac312267c4ef2f552471e138bacf68df", "de5e95325e139fd0a46df1dd28aabecd0273b772", "84eb3581684121874119cbe7ad9e88c97ec08699", "5ab9d1a4ac312267c4ef2f552471e138bacf68df", "d728a28d3d2199f5bc34317e829965ad662def6a", "84eb3581684121874119cbe7ad9e88c97ec08699"]},{"id": "ccf5208521cb8c35f50ee8873df89294b8ed7292", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "authors": ["Yoav Freund", "Robert E. Schapire"], "date": "EuroCOLT", "abstract": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weightupdate Littlestone Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more… ", "references": ["8facf54fbb5cc49d3df9674b3facca5d92acbdc0", "8facf54fbb5cc49d3df9674b3facca5d92acbdc0", "888c09de60ce427669fe5a264fa3e787803eb9d2", "888c09de60ce427669fe5a264fa3e787803eb9d2", "8facf54fbb5cc49d3df9674b3facca5d92acbdc0", "888c09de60ce427669fe5a264fa3e787803eb9d2", "14658bf6b693af9f30920c70fad14563f6b0cc10", "ff027adc26fdbe6faa929e500f5fe3136077ffa7", "888c09de60ce427669fe5a264fa3e787803eb9d2", "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8"]},{"id": "76f560991d56ad689ec32f9e9d13291e0193f4cf", "title": "A general framework for object detection", "authors": ["Constantine Papageorgiou", "Michael Oren", "Tomaso A. Poggio"], "date": "1998", "abstract": "This paper presents a general trainable framework for object detection in static images of cluttered scenes.", "references": ["e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6", "68c4749d9d3f1724aa01778d69a3774c732ca44c", "e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6"]},{"id": "e787b59c7c4ff69c107eefa6870ad5c95022a739", "title": "A fast and robust face detection based on module switching network", "authors": ["Jung-Bae Kim", "Young Hoon Sung", "Seok-Cheol Kee"], "date": "2004", "abstract": "We suggest a face detection method for video surveillance system to store a user's images fast and robustly. Our system consists of face and motion detectors. The face detector adopts a gentle AdaBoost algorithm to detect a face. Employing a module switching network, we extend the detectable facial pose range without loss of time. The motion detector, using temporal edges and temporal variance, decides whether a user exists when the face detector fails to detect a face. The proposed method… ", "references": ["9008cdacbdcff8a218a6928e94fe7c6dfc237b24", "5ab9d1a4ac312267c4ef2f552471e138bacf68df", "f8d6914063458ce98125259d8b5887dea0315b35", "ebb34b75982f628f9ce5995821fff81fd967dc2d", "3c4fdffc12589f9f312a44802b8e2fe8311aa13e", "3c4fdffc12589f9f312a44802b8e2fe8311aa13e", "76f560991d56ad689ec32f9e9d13291e0193f4cf", "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8", "9fb7b636edeaf344394fdf37481d7b83eec75358", "9008cdacbdcff8a218a6928e94fe7c6dfc237b24"]},{"id": "ba86d894dd909634243a28d8c52711c8237ddd6e", "title": "Cascade AdaBoost Classifiers with Stage Features Optimization for Cellular Phone Embedded Face Detection System", "authors": ["Xusheng Tang", "Zongying Ou", "Pengfei Zhao"], "date": "ICNC", "abstract": "In this paper, we propose a novel feature optimization method to build a cascade Adaboost face detector for real-time applications on cellular phone, such as teleconferencing, user interfaces, and security access control.", "references": ["8ccc5ef87eab96a4cae226750eba8322b30606ea", "f3317b98195fe0be4acf7b450f015c1abca13ab9", "f3317b98195fe0be4acf7b450f015c1abca13ab9", "67c63f6b3eeb0b619d6306fcf1c4b144710f79b0", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63", "f3317b98195fe0be4acf7b450f015c1abca13ab9", "51963aac6c3a0b5087864c5bef3131dc59ef41eb", "8ccc5ef87eab96a4cae226750eba8322b30606ea", "f3317b98195fe0be4acf7b450f015c1abca13ab9", "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63"]},{"id": "887567782cb859ecd339693589056903b0071353", "title": "Face Detection: A Survey", "authors": ["Erik Hjelmås", "Boon Kee Low"], "date": "2001", "abstract": "In this paper we present a comprehensive and critical survey of face detection algorithms.", "references": ["7f1d51899314af6d668581b05d715017f05f52f0", "207a44830bffe0825790a431d4fded63dbe82d56", "d9ff511abd115edcc7cdf2fe5bc338522a9faa85", "e66bc8179717e0412371279ea2ea3647d859232b", "207a44830bffe0825790a431d4fded63dbe82d56", "a1d194fd28c58075efed3425483fe8cfc6df0ae6", "c13818310b9019bb10e58177abc8dc0554cd7836", "7f1d51899314af6d668581b05d715017f05f52f0", "30e2f29d26f31846d6e0294cfa3733adfc618bbb", "207a44830bffe0825790a431d4fded63dbe82d56"]},{"id": "06719154ab53d3a57041b2099167e3619f1677bc", "title": "An Embedded Robust Facial Feature Detector", "authors": ["Stéphane Le Roux", "Franck Mamalet", "Stefan Duffner"], "date": "2007", "abstract": "In this paper, we present a robust and optimized facial feature detector algorithm which meets the constraints of embedded processors allowing facial feature based services on mobile terminals, such as teleconferencing, advanced user interface, image indexing and security access control. The studied facial feature detector is based on convolutional neural networks, a feature extraction and classification technique which consists of a pipeline of convolution and sub-sampling operations, followed… ", "references": ["f50ee8921f3d11afda87823fe1066db20e220f9b", "944d85e4098f3699d7b7f2c92f3a11ecaacdaa69", "ebb34b75982f628f9ce5995821fff81fd967dc2d", "ebb34b75982f628f9ce5995821fff81fd967dc2d", "fa4b9138e4cc2efb7dd311f06d2b636bb0d7c2d7", "39524eeeeed96be8a2970caf0fa2673c9b4314b9", "68a859142ef42196e6a56305b8c6ac4cb2c9326e", "f50ee8921f3d11afda87823fe1066db20e220f9b", "f50ee8921f3d11afda87823fe1066db20e220f9b", "2450c618cca4cbd9b8cdbdb05bb57d67e63069b1"]},{"id": "ec2a4f726a45c7ff8a53ed35ef5ebe719d168646", "title": "Face detection and facial feature extraction using color, shape and symmetry-based cost functions", "authors": ["Eli Saber", "A. Murat Tekalp"], "date": "1996", "abstract": "This paper describes an algorithm for detecting human faces and subsequently localizing the eyes, nose, and mouth. First, we locate the face based on color and shape information. To this effect, a supervised pixel-based color classifier is used to mark all pixels which are within a prespecified distance of \"skin color\". This color-classification map is then subject to smoothing employing either morphological operations or filtering using a Gibbs random field model. The eigenvalues and… ", "references": []},{"id": "57930a675de539c59bc33f56d9894c999d264f72", "title": "Text detection and segmentation in complex color images", "authors": ["C. Garcia", "X. Apostolidis"], "date": "2000", "abstract": "Text is a very powerful index in content-based image and video indexing. We propose a new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background. Our goal is to minimize the number of false alarms and to binarize efficiently the detected text areas so that they can be processed by standard OCR software. First, potential areas of text are detected by enhancement and clustering processes, considering most of constraints… ", "references": []},{"id": "f8f5c282dc11937d29183b955dc3e4fbb677571b", "title": "Automatic text detection and tracking in digital video", "authors": ["Huiping Li", "David S. Doermann", "Omid E. Kia"], "date": "2000", "abstract": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based… ", "references": ["bc8b866cc58e82e6413367c8d770ef681e5abe66", "f565f502ad1acb81c5659b051c04683a34ed138f", "ca6dd9d18b70eb9b69cab1a13a9b4e72022c5f31", "778a307aa0cf8b2ed273b9089cb9aa8210f49f24", "f455a0f0e6d402648da33930fb39ef5587aab0e3", "f455a0f0e6d402648da33930fb39ef5587aab0e3", "d68b95534860e2bddd17d17ef7f362d16c550bde", "0bad076c5f89ac43026a2b5fca648d488f54f45e", "bc8b866cc58e82e6413367c8d770ef681e5abe66", "778a307aa0cf8b2ed273b9089cb9aa8210f49f24"]},{"id": "42a8ff86566538103c6116f9047a4c3128e1542c", "title": "Text detection, recognition in images and video frames", "authors": ["Datong Chen", "Jean-Marc Odobez", "Hervé Bourlard"], "date": "2004", "abstract": "Text embedded in images and videos represents a rich source of information for content-based indexing and retrieval applications.", "references": ["57930a675de539c59bc33f56d9894c999d264f72", "778a307aa0cf8b2ed273b9089cb9aa8210f49f24", "d68b95534860e2bddd17d17ef7f362d16c550bde", "e8cb23672f5d94a75a7ed9cc7c870be398bc0259", "3420ab835c1af02071364b1f4e0f69abf733d88c", "778a307aa0cf8b2ed273b9089cb9aa8210f49f24", "71d684a6ddbdc3f816e678e4f2ca9ec0a58f3387", "117dea69032c9bf51ee10ff3bbad9b7b4d9e638b", "e8cb23672f5d94a75a7ed9cc7c870be398bc0259", "bd6fbc383e603c83d7364f49a368d0f87371132c"]},{"id": "8426b3379a20ec8ba028ecc796f0b1c97a54673e", "title": "Robust Face Alignment Using Convolutional Neural Networks", "authors": ["Stefan Duffner", "Christophe Garcia"], "date": "VISAPP", "abstract": "Face recognition in real-world images mostly relies on three successive steps: face detection, alignment and identification. The second step of face alignment is crucial as the bounding boxes produced by robust face detection algorithms are still too imprecise for most face recognition techniques, i.e. they show slight variations in position, orientation and scale. We present a novel technique based on a specific neural architecture which, without localizing any facial feature points, precisely… ", "references": ["ed2ad7dfdb82039f63908b20dd736a92b6fdf3d5", "bd8eeaee7b622e0e2a6b7218df5236407c45091f", "b886f69662afd14f415188af824435f901790477", "b886f69662afd14f415188af824435f901790477", "b886f69662afd14f415188af824435f901790477", "f1b7ff9b23ce22fa448f6cdbc700e95582460be0", "0d1612c6b124835c8265a29dc2b0204400438f52", "bd8eeaee7b622e0e2a6b7218df5236407c45091f", "ed2ad7dfdb82039f63908b20dd736a92b6fdf3d5", "bd8eeaee7b622e0e2a6b7218df5236407c45091f"]},{"id": "cf7edca25496335f180851111f446879d0b534d1", "title": "Video-text extraction and recognition", "authors": ["Teo Boon Chen", "Devarun Ghosh", "S. Ranganath"], "date": "2004", "abstract": "The detection and recognition of text from video is an important issue in automated content-based indexing of visual information in video archives. In this paper, we present a comprehensive system for extracting and recognizing artificial text from unconstrained, general-purpose videos. Exploiting the temporal feature of videos, an edge-detection-based text segmentation method is applied only on selective frames for extracting text from a video scene. Subsequently, a combination of techniques… ", "references": ["0dd1def5778f24c2c5a5f1c114846326e8f86123", "8a1fec088fdd4f275c57ad1a1c8571acb5cd1109", "35dfde2eb6e601b3adcf620aa5679f9166b4cf93", "35dfde2eb6e601b3adcf620aa5679f9166b4cf93", "f8f5c282dc11937d29183b955dc3e4fbb677571b", "0dd1def5778f24c2c5a5f1c114846326e8f86123", "800754a30a6ac556d5276a093d3290786dab3ded", "35dfde2eb6e601b3adcf620aa5679f9166b4cf93", "14ce174bddee5b6b2750cf14574773b537ac4d42", "8a1fec088fdd4f275c57ad1a1c8571acb5cd1109"]},{"id": "93c8a2963be6088dd55959bfe8cd92916891fb66", "title": "Neural network-based text location in color images", "authors": ["Keechul Jung"], "date": "2001", "abstract": "Abstract This paper proposes neural network-based text locations in complex color images. Texture information extracted on several color bands using neural networks is combined and corresponding text location algorithms are then developed. Text extraction filters can be automatically constructed using neural networks. Comparisons with other text location methods are presented; indicating that the proposed system has a better accuracy. ", "references": ["43d19671151bf581c161dfaaf06be407a35d596c", "df619a202c5c359e1eeb3c6028ee53c4f8971ad3", "1fb7d1455a474cd255820e56c6c0d2b7f6007afa", "f565f502ad1acb81c5659b051c04683a34ed138f", "ce1e7aa9fd5ba2f54b342b7cac2625835771daf2", "43d19671151bf581c161dfaaf06be407a35d596c", "f565f502ad1acb81c5659b051c04683a34ed138f", "43d19671151bf581c161dfaaf06be407a35d596c", "d68b95534860e2bddd17d17ef7f362d16c550bde", "de034f320270091bcf7e80400ad5bf5340b3a46a"]},{"id": "7f3e0bd21ead365af8dd302ab7d5b1ab53e7d437", "title": "Affine-Invariant Recognition of Gray-Scale Characters Using Global Affine Transformation Correlation", "authors": ["Toru Wakahara", "Yoshimasa Kimura", "Akira Tomono"], "date": "2001", "abstract": "Describes a technique of gray-scale character recognition that offers both noise tolerance and affine-invariance. The key ideas are twofold. First is the use of normalized cross-correlation as a matching measure to realize noise tolerance. Second is the application of global affine transformation (GAT) to the input image so as to achieve affine-invariant correlation with the target image. In particular, optimal GAT is efficiently determined by the successive iteration method using topographic… ", "references": ["615aff1af91c888437d030725a7a9311922a2c9b", "fb76d9181214a31b9e658a2647eafd4d2cdb3099", "b07ce649d6f6eb636872527104b0209d3edc8188", "e10f473947a7afd55aa2ff13cf031e0f8a1accdd"]},{"id": "1d4816c612e38dac86f2149af667a5581686cdef", "title": "A Threshold Selection Method from Gray-Level Histograms", "authors": ["Nobuyuki Otsu"], "date": "1979", "abstract": "A nonparametric and unsupervised method ofautomatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zerothand the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also… ", "references": ["c70d1a2656ecc1335fd0c345e5014756541cae47"]},{"id": "bd568cc328d34c43af90e10f6b133c03994bee09", "title": "Segmentation and recognition of characters in scene images using selective binarization in color space and GAT correlation", "authors": ["Minoru Yokobayashi", "Toru Wakahara"], "date": "2005", "abstract": "This paper proposes a new technique of segmentation and recognition of characters with a wide variety of image degradations and complex backgrounds in natural scenes. The key ideas are twofold. One is segmentation of character and background by local/adaptive binarization of one of Cyan/Magenta/Yellow (CMY) color planes with the maximum breadth of histogram. The other is affine-invariant grayscale character recognition using global affine transformation (GAT) correlation. In experiments, we use… ", "references": ["6b4f762d9a5acd964411d8c737073c24ce16a3c8", "d736b49b949b4da969072b3282a0a161e6113147", "6b4f762d9a5acd964411d8c737073c24ce16a3c8", "d736b49b949b4da969072b3282a0a161e6113147", "8314dda1ec43ce57ff877f8f02ed89acb68ca035", "6e885c6c1d87fa111b742e5654fb46bd805c8922", "aba614c691bce892ae461f38aee2cc46eea8ef78", "c3a74fe4e79add9de4803a825b6eae013215dfe7", "d736b49b949b4da969072b3282a0a161e6113147", "c3a74fe4e79add9de4803a825b6eae013215dfe7"]},{"id": "0b8e87e9d7f1edebc8cb6992ed4eac17b4a8682c", "title": "Binarization and Recognition of Degraded Characters Using a Maximum Separability Axis in Color Space and GAT Correlation", "authors": ["Minoru Yokobayashi", "Toru Wakahara"], "date": "2006", "abstract": "This paper proposes a new technique of binarization and recognition of characters in color with a wide variety of image degradations and complex backgrounds. The key ideas are twofold. One is to automatically select one axis in the RGB color space that maximizes the between-class separability by a suitably chosen threshold for segmentation of character and background or binarization. The other is affine-invariant or distortion-tolerant grayscale character recognition using global affine… ", "references": ["aba614c691bce892ae461f38aee2cc46eea8ef78", "aba614c691bce892ae461f38aee2cc46eea8ef78", "609274b4d2f79b8a85ff1653a15d9b490747c10a", "7f3e0bd21ead365af8dd302ab7d5b1ab53e7d437", "1d4816c612e38dac86f2149af667a5581686cdef", "6b4f762d9a5acd964411d8c737073c24ce16a3c8", "1d4816c612e38dac86f2149af667a5581686cdef", "609274b4d2f79b8a85ff1653a15d9b490747c10a", "aba614c691bce892ae461f38aee2cc46eea8ef78", "c3a74fe4e79add9de4803a825b6eae013215dfe7"]},{"id": "16849f9ef7a750ce85675592d283f1c0330ab2e4", "title": "Multipath Sparse Coding Using Hierarchical Matching Pursuit", "authors": ["Liefeng Bo", "Xiaofeng Ren", "Dieter Fox"], "date": "2013", "abstract": "Complex real-world signals, such as images, contain discriminative structures that differ in many aspects including scale, invariance, and data channel.", "references": ["38594b15593dce53ae1888b072b417013f1830ba", "0c9633aedafe4ee8cf238fa06c40b84f47e17362", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "2f7713dcc35e7c05becf3be5522f36c9546b0364", "9067db319ecb338312070a92b081341e6f03a6c6", "6f568d757d2c1ab42f2006faa25690b74c3d2d44", "38594b15593dce53ae1888b072b417013f1830ba", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "0c9633aedafe4ee8cf238fa06c40b84f47e17362"]},{"id": "0a072cbdee54b83c8df43a431065f009d2cd2e70", "title": "Discriminative learned dictionaries for local image analysis", "authors": ["Julien Mairal", "Francis R. Bach", "Andrew Zisserman"], "date": "2008", "abstract": "Sparse signal models have been the focus of much recent research, leading to (or improving upon) state-of-the-art results in signal, image, and video restoration.", "references": ["e07416eabd4ba6c69fa473756bb04ae7161177be", "d0304e82a3aeb318da27d6d3725fb61fe80ebc60", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "ccd52aff02b0f902f4ce7247c4fee7273014c41c", "2077d0f30507d51a0d3bbec4957d55e817d66a59", "92281d5002178003bd7060fc66677a3471cdaa4b", "7219a488d67e739518b1b202528c5923451567be", "f2d5d3428070fb83e6e342d5c087ba60ef8263fd", "2077d0f30507d51a0d3bbec4957d55e817d66a59", "92281d5002178003bd7060fc66677a3471cdaa4b"]},{"id": "8c8c9c0ff996da9a2e8f8387788903e21a4c3668", "title": "Rapid automated three-dimensional tracing of neurons from confocal image stacks", "authors": ["Khalid A. Al-Kofahi", "Sharie Lasek", "Badrinath Roysam"], "date": "2002", "abstract": "Algorithms are presented for fully automatic three-dimensional (3D) tracing of neurons that are imaged by fluorescence confocal microscopy. Unlike previous voxel-based skeletonization methods, the present approach works by recursively following the neuronal topology, using a set of 4 /spl times/ N/sup 2/ directional kernels (e.g., N = 32), guided by a generalized 3D cylinder model. This method extends our prior work on exploratory tracing of retinal vasculature to 3D space. Since the… ", "references": ["8f25b129be3514bd126eda0674b489d1923954ac", "ff633414f6cd75d184983eeeea19c78bbf360c11", "00264b243285db9d05efc643fe4fd4f07b09ceb6", "8f25b129be3514bd126eda0674b489d1923954ac", "4dcbf23a36b52d24d311f41304209443257ae697", "89c17bcda69929aff0868f8b3000050895a952d7", "1c0991e43957f5d3e10c0a659b67f622bc64c379", "8f25b129be3514bd126eda0674b489d1923954ac", "f6eddd60f0368b459baf70f1f242f7aac94a4dc2", "ff633414f6cd75d184983eeeea19c78bbf360c11"]},{"id": "74a10cbbde0ad2a3d6b84f48811bb3fea3d7676f", "title": "Probabilistic modeling based vessel enhancement in thoracic CT scans", "authors": ["Gady Agam", "Changhua Wu"], "date": "2005", "abstract": "Vessel enhancement in volumetric data is a necessary prerequisite in various medical imaging applications with particular importance for automated nodule detection. Ideally, vessel enhancement filters should enhance vessels and vessel junctions while suppressing nodules and other non-vessel elements. A distinction between vessels and nodules is normally obtained through eigenvalue analysis of the curvature tensor which is a second order differential quantity and so is sensitive to noise… ", "references": ["cbf130a89881ec8046fe2b20bda7940b38877153", "9dadf602ba5849e5db34301b36b7081c99dcc7e7", "52faef9f54f7c741b4ac574a0f1a596ea9c22ee4", "cbf130a89881ec8046fe2b20bda7940b38877153", "f196cef7e67c9443c049fd175092316119d37ffe", "7a8115243c57cae484961da102767ebdc7ef849e", "cbf130a89881ec8046fe2b20bda7940b38877153", "2c4570a6e2ff603b0c8c78c7c4de5617639e2b01", "f196cef7e67c9443c049fd175092316119d37ffe", "4aa6f029a4d63da6f4d9b5a5cb87f8207aab88b0"]},{"id": "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec", "title": "A Computational Approach to Edge Detection", "authors": ["John F. Canny"], "date": "1986", "abstract": "This paper describes a computational approach to edge detection.", "references": ["75d5b6239679642070a29859ec009874f3be9a4a", "8d2ae155fcc16ea62a9bc35ba3fa9f0adc920d83", "8d2ae155fcc16ea62a9bc35ba3fa9f0adc920d83", "46279543a3e21be3180cadd277ff25ff863877d0", "8d2ae155fcc16ea62a9bc35ba3fa9f0adc920d83", "ae16e7b286c2ec2a9a931acfa0b38baf9c526749", "8d2ae155fcc16ea62a9bc35ba3fa9f0adc920d83", "aa4b60b5847999c2f778e3e67ca1f2201e396abb", "9cbd5a66c1198d5a14f13e03483b69321024444b", "c2d2fefc1c61298059f9a160f190e6957587b74e"]},{"id": "74227090d23c958f601ad05369fad587e3b546f1", "title": "Sparse Representation for Computer Vision and Pattern Recognition", "authors": ["John Wright", "Yi Ma", "Shuicheng Yan"], "date": "2010", "abstract": "Techniques from sparse signal representation are beginning to see significant impact in computer vision, often on nontraditional applications where the goal is not just to obtain a compact high-fidelity representation of the observed signal, but also to extract semantic information. The choice of dictionary plays a key role in bridging this gap: unconventional dictionaries consisting of, or learned from, the training samples themselves provide the key to obtaining state-of-the-art results and… ", "references": ["9067db319ecb338312070a92b081341e6f03a6c6", "d5eec41043d91964879c4c745c7165f823967f29", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "9067db319ecb338312070a92b081341e6f03a6c6", "9d65ba8bb20ae6dd001b9833c525c279dfe18916", "1a79cdb6a4e3c9723ef0d7a77579d7611b72fbe8", "0a072cbdee54b83c8df43a431065f009d2cd2e70", "ffd2b18ddbb8e08b7e21a91e723d07589ea60b6f", "ffd2b18ddbb8e08b7e21a91e723d07589ea60b6f"]},{"id": "4b3d5bb0f9597fff321d3d48e0d22b2cae7e648a", "title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "authors": ["Ingrid Daubechies", "Michel Defrise", "C. De Mol"], "date": "2003", "abstract": "We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted p-penalties on the coefficients of such expansions, with 1 ≤ p ≤ 2, still regularizes the problem. Use of such p-penalized problems with p < 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under… ", "references": ["7f9ffc3a186204076ad3b7a9086d7104c79e5e06", "df8d148efe659fb56e1deafa78ac94d3415f97ba", "7f9ffc3a186204076ad3b7a9086d7104c79e5e06", "19da2b7f74493b5a1f8876abcc3db2332a51f256", "48d9d162588aa024be03bdea1a98823c389de6d2", "df8d148efe659fb56e1deafa78ac94d3415f97ba", "2ced7c0cf116b241ddc8908918161ff995d0238a", "2ced7c0cf116b241ddc8908918161ff995d0238a", "48d9d162588aa024be03bdea1a98823c389de6d2", "601ba02449e03c7b36e882b80de24c42b887aa4d"]},{"id": "32b8f58a038df83138435b12a499c8bf0de13811", "title": "End-to-end scene text recognition", "authors": ["Kai Wang", "Boris Babenko", "Serge J. Belongie"], "date": "2011", "abstract": "This paper focuses on the problem of word detection and recognition in natural images.", "references": ["4b2a523d48cee04c09c327e14fb8928c5feff03c", "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1", "4b2a523d48cee04c09c327e14fb8928c5feff03c", "d175a196816e44c08928ad05e30fd774468d69aa", "d136d77dcdfb34381d8f581f3866d10293a519fd", "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1", "da8154af82fd62944399fc7fad65e44d82ee9ee2", "dbbd5fdc09349bbfdee7aa7365a9d37716852b32", "76548769a142f858acf9d32e9bc4a2c5445fc9de", "39c4ae83b5c92e0fa55de1ec7e5cf12589c408db"]},{"id": "12244deb997152492d96c6246ec21b2b9804800d", "title": "Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning", "authors": ["Adam Coates", "Blake Carpenter", "Andrew Y. Ng"], "date": "2011", "abstract": "Reading text from photographs is a challenging problem that has received a significant amount of attention. Two key components of most systems are (i) text detection from images and (ii) character recognition, and many recent methods have been proposed to design better feature representations and models for both. In this paper, we apply methods recently developed in machine learning -- specifically, large-scale algorithms for learning the features automatically from unlabeled data -- and show… ", "references": ["6286a82f72f632672c1890f3dd6bbb15b8e5168b", "76548769a142f858acf9d32e9bc4a2c5445fc9de", "76548769a142f858acf9d32e9bc4a2c5445fc9de", "4b2a523d48cee04c09c327e14fb8928c5feff03c", "76548769a142f858acf9d32e9bc4a2c5445fc9de", "4b2a523d48cee04c09c327e14fb8928c5feff03c", "4b2a523d48cee04c09c327e14fb8928c5feff03c", "82bac0ae7d60f5bef57deb837de404b4472ee0a0", "6286a82f72f632672c1890f3dd6bbb15b8e5168b", "76548769a142f858acf9d32e9bc4a2c5445fc9de"]},{"id": "19de9e4850a208800db50615afec2b08b25d4f99", "title": "Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary", "authors": ["Michal Aharon", "Michael Elad"], "date": "2008", "abstract": "Modeling signals by sparse and redundant representations has been drawing considerable attention in recent years. Coupled with the ability to train the dictionary using signal examples, these techniques have been shown to lead to state-of-the-art results in a series of recent applications. In this paper we propose a novel structure of such a model for representing image content. The new dictionary is itself a small image, such that every patch in it (in varying location and size) is a possible… ", "references": ["306de9c553695822ae9e6de044b6856baf0cce7d", "e07416eabd4ba6c69fa473756bb04ae7161177be", "08253ca281c3603e0eaf3a4955fa468d42d165b6", "1827aff2dc345680de29a937f002da41accb2fe0", "08253ca281c3603e0eaf3a4955fa468d42d165b6", "51d267b782e7caf2b6bc7240b1a5f48044ffe115", "92281d5002178003bd7060fc66677a3471cdaa4b", "92281d5002178003bd7060fc66677a3471cdaa4b", "e07416eabd4ba6c69fa473756bb04ae7161177be", "325b9d465ccc4c262c6009dcf2e3b02a09f8f7b3"]},{"id": "2f88b7d26f08c25ee336168fba0e37772c06ca6e", "title": "Analysis Operator Learning and its Application to Image Reconstruction", "authors": ["Simon Hawe", "Martin Kleinsteuber", "Klaus Diepold"], "date": "2013", "abstract": "Exploiting a priori known structural information lies at the core of many image reconstruction methods that can be stated as inverse problems. The synthesis model, which assumes that images can be decomposed into a linear combination of very few atoms of some dictionary, is now a well established tool for the design of image reconstruction algorithms. An interesting alternative is the analysis model, where the signal is multiplied by an analysis operator and the outcome is assumed to be sparse… ", "references": ["539346f7f35f83875d9e86851010ee35d1a47e69", "51d267b782e7caf2b6bc7240b1a5f48044ffe115", "f3c4964354c0af2a5e5b69eb43a2cec24d9e9e04", "f3c4964354c0af2a5e5b69eb43a2cec24d9e9e04", "51d267b782e7caf2b6bc7240b1a5f48044ffe115", "dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1", "54205667c1f65a320f667d73c354ed8e86f1b9d9", "20451ae408a80e16079b3fb84c86310693adde4b", "20451ae408a80e16079b3fb84c86310693adde4b", "54205667c1f65a320f667d73c354ed8e86f1b9d9"]},{"id": "054bbbab7d8324d381903b2f33d4e8a17b54eff0", "title": "Double Sparsity: Learning Sparse Dictionaries for Sparse Signal Approximation", "authors": ["Ron Rubinstein", "Michael Zibulevsky", "Michael Elad"], "date": "2010", "abstract": "An efficient and flexible dictionary structure is proposed for sparse and redundant signal representation.", "references": ["08253ca281c3603e0eaf3a4955fa468d42d165b6", "ffd2b18ddbb8e08b7e21a91e723d07589ea60b6f", "1d9ef403969035e022b1b61c7dc513ffe189f031", "17e7cca7e795d8ba1fa9d2c88bf2675c2d6ddfe8", "372d470555112ca147a83a125aba7431bb53ecaa", "7f98bd398a6b422e140b9cd83f0d64444f8dbca5", "ffd2b18ddbb8e08b7e21a91e723d07589ea60b6f", "1d9ef403969035e022b1b61c7dc513ffe189f031", "fb2d7003623a31ffabba86841cf05b8b57030796", "fb2d7003623a31ffabba86841cf05b8b57030796"]},{"id": "019de3bca6f53fddb12a52b626b428e18de854f0", "title": "Proximal Methods for Sparse Hierarchical Dictionary Learning", "authors": ["Rodolphe Jenatton", "Julien Mairal", "Francis R. Bach"], "date": "ICML", "abstract": "We propose to combine two approaches for modeling data admitting sparse representations: on the one hand, dictionary learning has proven effective for various signal processing tasks.", "references": ["ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e", "9d65ba8bb20ae6dd001b9833c525c279dfe18916", "f1ef9f3123fd8119df3e16e7d2abaa27ebf07214", "ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e", "4f5e2d78128805249cff3075dc7e8e526f0e4fb1", "910f8df95db7849036910e5773dcc9b09e70f03f", "4f5e2d78128805249cff3075dc7e8e526f0e4fb1", "9d65ba8bb20ae6dd001b9833c525c279dfe18916", "ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e", "e07416eabd4ba6c69fa473756bb04ae7161177be"]},{"id": "17e7cca7e795d8ba1fa9d2c88bf2675c2d6ddfe8", "title": "Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 minimization", "authors": ["David L. Donoho", "Michael Elad"], "date": "2003", "abstract": "Given a dictionary D = {d(k)} of vectors d(k), we seek to represent a signal S as a linear combination S = summation operator(k) gamma(k)d(k), with scalar coefficients gamma(k.", "references": ["b4b70a1c7e13c95bfcde27a5d7705b919882900b", "64a4094ccbbb7f00491b25ac9089b7b6a58be721", "e9f195e1d5304eeb1f6a654513ec476b2b6e0445", "b4b70a1c7e13c95bfcde27a5d7705b919882900b", "32f6583fee27f9825fe6a0e86ff9d4de100db974", "f690671cc46c6ab1ff2d6e285fbe16c402fcc6c0", "6302c0103e1fe99b3160220e8019680ceed37253", "f690671cc46c6ab1ff2d6e285fbe16c402fcc6c0", "f690671cc46c6ab1ff2d6e285fbe16c402fcc6c0", "f690671cc46c6ab1ff2d6e285fbe16c402fcc6c0"]},{"id": "306de9c553695822ae9e6de044b6856baf0cce7d", "title": "Dictionary Learning Algorithms for Sparse Representation", "authors": ["Kenneth Kreutz-Delgado", "Joseph F. Murray", "Terrence J. Sejnowski"], "date": "2003", "abstract": "Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or… ", "references": ["2805537bec87a6177037b18f9a3a9d3f1038867b", "6bfbf789df18c8a6f825ef4f8777a0f145bf3b0f", "2210a7157565422261b03cf2cdf4e91b583df5a0", "4ed0700119ed281d210897117863fa290d383cd0", "09af20623f098c86ffa33361b78f34b7393ca540", "6bfbf789df18c8a6f825ef4f8777a0f145bf3b0f", "2805537bec87a6177037b18f9a3a9d3f1038867b", "42d906c733f273109c0ed716a5ef6e2a379beb26", "2210a7157565422261b03cf2cdf4e91b583df5a0", "f44e85948ef6bdc9645581173b1349d5dc569c9c"]},{"id": "49feb3e86487595adcdb6365c29788a18db04042", "title": "ERGODIC UNITARILY INVARIANT MEASURES ON THE SPACE OF INFINITE HERMITIAN MATRICES", "authors": ["Grigori Olshanski", "A. Vershik"], "date": "1996", "abstract": "Let $H$ be the space of all Hermitian matrices of infinite order and $U(\\infty)$ be the inductive limit of the chain $U(1)\\subset U(2)\\subset...$ of compact unitary groups. The group $U(\\infty)$ operates on the space $H$ by conjugations, and our aim is to classify the ergodic $U(\\infty)$-invariant probability measures on $H$ by making use of a general asymptotic approach proposed in Vershik's note \\cite{V}. The problem is reduced to studying the limit behavior of orbital integrals of the form… ", "references": ["1cc7a838a91625e16546c670bcfc88db9e9bded6", "1ae694e3bebdad7dadb425ccbea09ef305f6787f", "b0342138ae6b8d786c0fa1a4442ea6832e1d0e79", "06131de77b4268cc9d4334a661846c42873cb8e4", "263152ab369757a3e0bf5141bca9d1b405adf36a", "ab57a092f510c17d2203a24f5a6dac3cf95f3891", "3a486b8b8c60204b9fb41158b9b30e583dd29e87", "ea4bd47301c029f2dc118c94dfb3bb2f684e0129", "bab7a53795e22ad70cfcf632335a577d7c515a8a", "bab7a53795e22ad70cfcf632335a577d7c515a8a"]},{"id": "d5715f6b7f2bb482ae45b50899128590afb8ec41", "title": "Supervised Learning of Quantizer Codebooks by Information Loss Minimization", "authors": ["Svetlana Lazebnik", "Maxim Raginsky"], "date": "2009", "abstract": "This paper proposes a technique for jointly quantizing continuous features and the posterior distributions of their class labels based on minimizing empirical information loss such that the quantizer index of a given feature vector approximates a sufficient statistic for its class label. Informally, the quantized representation retains as much information as possible for classifying the feature vector correctly. We derive an alternating minimization procedure for simultaneously learning… ", "references": ["b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "9a9049a50dfe94fa4473880a9b60c99333ade685", "bde43668a483bb270e79dcfe6e57e62d8b9fc3cd", "ed79d892ed25123cb74d14a7a7134386ad1c882e", "b91180d8853d00e8f2df7ee3532e07d3d0cce2af", "ed79d892ed25123cb74d14a7a7134386ad1c882e", "6bc13ee021b70833795c145da10dc809ae198ff2", "9a9049a50dfe94fa4473880a9b60c99333ade685", "74b1e6c4b39682cf855622989e71ef0159b9db75", "4ce8bc485df9ac987f18d99c7af1d95f9cbea6b2"]},{"id": "03a073589eaf8ce3440464d020e0d0b26df5869b", "title": "Object categorization by learned universal visual dictionary", "authors": ["John M. Winn", "Antonio Criminisi", "Tom Minka"], "date": "2005", "abstract": "This paper presents a new algorithm for the automatic recognition of object classes from images (categorization). Compact and yet discriminative appearance-based object class models are automatically learned from a set of training images. The method is simple and extremely fast, making it suitable for many applications such as semantic image retrieval, Web search, and interactive image editing. It classifies a region according to the proportions of different visual words (clusters in feature… ", "references": ["191c3c15cc4c957ee3437fc27ba3178bae292e7f", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "2809b20aea8d2a15ac655e17b47fd5dfb304aa4c", "d044d7d92dd1fb80275d04d035aed71bcd3374e5", "7e3c3fee11758b15b56d719cca819303eca9b54b", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "7e3c3fee11758b15b56d719cca819303eca9b54b", "d044d7d92dd1fb80275d04d035aed71bcd3374e5", "70284b4fe852f472d4576c30f97a6fddbfef2aee", "25051e2b14a3e9e6ea5d77a9ef6c84ea700d1aac"]},{"id": "ba0548583a5ab3dca551f60e30f85ea42b2a4873", "title": "On feature combination for multiclass object classification", "authors": ["Peter V. Gehler", "Sebastian Nowozin"], "date": "2009", "abstract": "A key ingredient in the design of visual object classification systems is the identification of relevant class specific aspects while being robust to intra-class variations. While this is a necessity in order to generalize beyond a given set of training images, it is also a very difficult problem due to the high variability of visual appearance within each class. In the last years substantial performance gains on challenging benchmark datasets have been reported in the literature. This progress… ", "references": ["3edd9e15976ca3aa2d627d41384e1f0908a91632", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "b98c68f01d84ac07dc7fc51af782018070da748f", "b3f49fe647a84b7a560de2d15ce30f064a9ff5e3", "e9dd235240904627b12782653b66318712780703", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "0ced4cb1abacd567eaa08b282b90d804971dd94d", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "e9dd235240904627b12782653b66318712780703", "0ced4cb1abacd567eaa08b282b90d804971dd94d"]},{"id": "a8b515f2e5d065ed9e8c25710356014262dc0c6e", "title": "Learning to Sense Sparse Signals: Simultaneous Sensing Matrix and Sparsifying Dictionary Optimization", "authors": ["Julio Martin Duarte-Carvajalino", "Guillermo Sapiro"], "date": "2009", "abstract": "Sparse signal representation, analysis, and sensing have received a lot of attention in recent years from the signal processing, optimization, and learning communities. On one hand, learning overcomplete dictionaries that facilitate a sparse representation of the data as a liner combination of a few atoms from such dictionary leads to state-of-the-art results in image and video restoration and classification. On the other hand, the framework of compressed sensing (CS) has shown that sparse… ", "references": ["04b2734b2b3d9983bbdbe2afbcd59fd65677a693", "9245cce6b24ab671c7649cbd3789e68536dbf2ad", "e2cfa3d7898d6eeac142ab9613f9bb2ea20683c3", "6f14a338e8837fae059cab41064155cd84cb9cd5", "d0682e1dbdec523567c1167036aa8d46c5a8ab3e", "6f14a338e8837fae059cab41064155cd84cb9cd5", "9245cce6b24ab671c7649cbd3789e68536dbf2ad", "d0682e1dbdec523567c1167036aa8d46c5a8ab3e", "83b522f4bfa5db7f7d34f839475af7d078107634", "4d21babefb0711de139fc0d7c7990a18e9ce99c4"]},{"id": "d48f35d1d37c58b79901c82c5d905be1cdb14842", "title": "Discrete orthogonal polynomial ensembles and the Plancherel measure", "authors": ["Kurt Johansson"], "date": "1999", "abstract": "We consider discrete orthogonal polynomial ensembles which are discrete analogues of the orthogonal polynomial ensembles in random matrix theory. These ensembles occur in certain problems in combinatorial probability and can be thought of as probability measures on partitions. The Meixner ensemble is related to a two-dimensional directed growth model, and the Charlier ensemble is related to the lengths of weakly increasing subsequences in random words. The Krawtchouk ensemble occurs in… ", "references": ["bef4d324c284aac446369a86e94a8ba14f5bef8a", "bef4d324c284aac446369a86e94a8ba14f5bef8a", "ae4024002dfaa2381530f7894df258593d90b91f", "ae4024002dfaa2381530f7894df258593d90b91f", "72f42eeff9a4fee8d550e57768558bd27f59cc87", "ab64a4d4d67578cf5a2783f11b12216a709374ee", "f469009af94cd97f1caa15f7a34bf1a8697f5c85", "f469009af94cd97f1caa15f7a34bf1a8697f5c85", "72f42eeff9a4fee8d550e57768558bd27f59cc87", "ae4024002dfaa2381530f7894df258593d90b91f"]},{"id": "c79d0e8193fdc26a5920c8128db7dd9b1f17b62e", "title": "POINT PROCESSES AND THE INFINITE SYMMETRIC GROUP", "authors": ["Alexei Borodin", "Grigori Olshanski"], "date": "1998", "abstract": "The central theme of noncommutative harmonic analysis is decomposition of natural unitary representations T into elementary ones (i.e., into irreducible or factor representations). When T is endowed with a distinguished cyclic vector, its decomposition is governed by a measure, called the spectral or Plancherel measure. For instance, one of the achievements of the classical representation theory is the explicit calculation by Gindikin and Karpelevich of the spectral measure for the natural… ", "references": ["baaf53d0ba6977e209134b63c2ee9f78eef3f461", "c4addaa0a95e9b4bda542e88cc9d30ef5765accb", "2fd8bc26b60ae2681125a238f5ea7320aca66c8c", "ec650d75f2394d93af9470e592caf786fa013097", "2fd8bc26b60ae2681125a238f5ea7320aca66c8c", "8e085ee85ae2f0264cec37295fdd1edfd194d830", "8e085ee85ae2f0264cec37295fdd1edfd194d830", "06131de77b4268cc9d4334a661846c42873cb8e4", "06131de77b4268cc9d4334a661846c42873cb8e4", "1315593b964b0ed451e8939bdc9715a4403c6991"]},{"id": "65cccb5b4aec5dd9a7161ddebcb0213b39f53439", "title": "Learning Local Image Descriptors", "authors": ["Simon A. J. Winder", "Matthew A. Brown"], "date": "2007", "abstract": "In this paper we study interest point descriptors for image matching and 3D reconstruction. We examine the building blocks of descriptor algorithms and evaluate numerous combinations of components. Various published descriptors such as SIFT, GLOH, and Spin images can be cast into our framework. For each candidate algorithm we learn good choices for parameters using a training set consisting of patches from a multi-image 3D reconstruction where accurate ground-truth matches are known. The best… ", "references": ["dc4452325732150c7baaa9dbe6df478d54d27696", "12c7fc38debaf3589e712973642246bd54fe63b3", "4cab9c4b571761203ed4c3a4c5a07dd615f57a91", "4cab9c4b571761203ed4c3a4c5a07dd615f57a91", "4cab9c4b571761203ed4c3a4c5a07dd615f57a91", "f9f836d28f52ad260213d32224a6d227f8e8849a", "f9f836d28f52ad260213d32224a6d227f8e8849a", "f9f836d28f52ad260213d32224a6d227f8e8849a", "128477ed907857a8ae96cfd25ea6b2bef74cf827", "12c7fc38debaf3589e712973642246bd54fe63b3"]},{"id": "e13109dbb390e815a2e7ac328fbe4b6cb1113946", "title": "Classical Orthogonal Polynomials of a Discrete Variable", "authors": ["Arnold F. Nikiforov", "V. B. Uvarov", "Sergei K. Suslov"], "date": "1991", "abstract": "The basic properties of the polynomials p n (x) that satisfy the orthogonality relations \n \n$$ \\int_a^b {{p_n}(x)} {p_m}(x)\\rho (x)dx = 0\\quad (m \\ne n) $$ \n \n(2.0.1) \n \nhold also for the polynomials that satisfy the orthogonality relations of a more general form, which can be expressed in terms of Stielties integrals \n \n$$ \\int_a^b {{p_n}(x)} {p_m}(x)dw(x) = 0\\quad (m \\ne n), $$ \n \n(2.0.2) \n \nwhere w(x) is a monotonic nondecreasing function (usually called the distribution function). The… ", "references": []},{"id": "00b0e2735ddac0c8e4911d40a72fbb7b631385b4", "title": "An introduction to complex analysis in several variables", "authors": ["Lars Hörmander"], "date": "1990", "abstract": "I. Analytic Functions of One Complex Variable. II. Elementary Properties of Functions of Several Complex Variables. III. Applications to Commutative Banach Algebras. IV. L2 Estimates and Existence Theorems for the Operator. V. Stein Manifolds. VI. Local Properties of Analytic Functions. VII. Coherent Analytic Sheaves on Stein Manifolds. Bibliography. Index. ", "references": []},{"id": "0da5140de01361460b8d72d4ecbb4a964c07ea8b", "title": "Statistical theory of the energy levels of complex systems. I", "authors": ["Freeman J. Dyson"], "date": "1962", "abstract": "In previous papers of this series, a theory was constructed for the description of the statistical properties of the eigenvalues of a random matrix of high order. This paper deduces from the same theoretical model the statistical behavior to be expected for a finite stretch of observed eigenvalues chosen out of a much longer stretch of unobserved ones. In comparing the model with experimental data, we have always to deal with such a finite stretch of observed levels. Three ``statistics'' are… ", "references": []},{"id": "fef27316c2a2aab78ef8bc7e9368e3b124ef10e2", "title": "States of classical statistical mechanical systems of infinitely many particles. II. Characterization of correlation measures", "authors": ["Andrew Lenard"], "date": "1975", "abstract": "This paper is devoted to the solution of the problem of characterizing correlation measures arising naturally in classical statistical mechanics of point particles. A correlation measure ρ must be related to a (not necessarily unique) probability measure μ over an infinite particle configuration space X by the formula μ(H)=∝NH(ξ)dμ where {NH} is a certain family of integer valued random variables. We prove that there are three conditions, namely (S) symmetry, (P) positivity, and (N… ", "references": []},{"id": "c79e704512c235cca5bd7ce60efe9b4bca1c3c4a", "title": "The Riemann Zeros and Eigenvalue Asymptotics", "authors": ["Michael V. Berry", "Jonathan P. Keating"], "date": "1999", "abstract": "Comparison between formulae for the counting functions of the heights tn of the Riemann zeros and of semiclassical quantum eigenvalues En suggests that the tn are eigenvalues of an (unknown) hermitean operator H, obtained by quantizing a classical dynamical system with hamiltonian Hcl. Many features of Hcl are provided by the analogy; for example, the \"Riemann dynamics\" should be chaotic and have periodic orbits whose periods are multiples of logarithms of prime numbers. Statistics of the tn… ", "references": ["ec41f6fe90aa09dbd662c7ba6502dbed6ca101a6", "85bf74cdad3d5b7acbc86bb5acc8b6935b7d1308", "749a59bd2e6ea4110511b3368ea3951d22b99438", "85bf74cdad3d5b7acbc86bb5acc8b6935b7d1308", "e007eff528137346a0d212601579978e4a57fb9c", "85bf74cdad3d5b7acbc86bb5acc8b6935b7d1308", "f25ba5438ec9d043d813a4e86ffceba75cf5b8e0", "ec41f6fe90aa09dbd662c7ba6502dbed6ca101a6", "e007eff528137346a0d212601579978e4a57fb9c", "5c69707c1f9b5e6235583fbf9fa44ce83c300332"]},{"id": "8b9c4015bba287acd0db6c9239a18e6edfca816f", "title": "Brownian motion in a Weyl chamber, non-colliding particles, and random matrices", "authors": ["David J. Grabiner"], "date": "1997", "abstract": "Abstract Let n particles move in standard Brownian motion in one dimension, with the process terminating if two particles collide. This is a specific case of Brownian motion constrained to stay inside a Weyl chamber; the Weyl group for this chamber is An−1, the symmetric group. For any starting positions, we compute a determinant formula for the density function for the particles to be at specified positions at time t without having collided by time t. We show that the probability that there… ", "references": ["8ba0b30aa3cf6b55acd14068b486aa0d70784001", "14cb56405346306f6aaacb28cfcdd9dcbf030daf", "009acdc4da6a67185cab82dcad4fdbc3e9becbf9", "5a235e6ee96356ed7db4f3d202f6b4c5d9f9a336", "689ba933fcee050f97f0c2aaecc1c4175396ef3c", "95590b90372713b61a2056bb488f795d77deb764", "9aab47430ae46bfddc807f60a097c19b4d0e3485", "363de182369cb5883dc6a8899dc714467dbecfb1", "5a235e6ee96356ed7db4f3d202f6b4c5d9f9a336", "8ba0b30aa3cf6b55acd14068b486aa0d70784001"]},{"id": "689ba933fcee050f97f0c2aaecc1c4175396ef3c", "title": "A Brownian‐Motion Model for the Eigenvalues of a Random Matrix", "authors": ["Freeman J. Dyson"], "date": "1962", "abstract": "A new type of Coulomb gas is defined, consisting of n point charges executing Brownian motions under the influence of their mutual electrostatic repulsions. It is proved that this gas gives an exact mathematical description of the behavior of the eigenvalues of an (n × n) Hermitian matrix, when the elements of the matrix execute independent Brownian motions without mutual interaction. By a suitable choice of initial conditions, the Brownian motion leads to an ensemble of random matrices which… ", "references": []},{"id": "ddfc25e3a2f53eea1086a070f97d557a830fea1e", "title": "Biorthogonal ensembles", "authors": ["Alexei Borodin"], "date": "1998", "abstract": "Abstract. One object of interest in random matrix theory is a family of point ensembles (random point configurations) related to various systems of classical orthogonal polynomials. The paper deals with a one–parametric deformation of these ensembles, which is defined in terms of the biorthogonal polynomials of Jacobi, Laguerre and Hermite type. Our main result is a series of explicit expressions for the correlation functions in the scaling limit (as the number of points goes to infinity). As… ", "references": ["598b1a0a6451f58298c38955ca2acebf84437211", "7422b726a35551eff938d0b4a4188f859c081181", "c79d0e8193fdc26a5920c8128db7dd9b1f17b62e", "08df2cf605d6d1930285b48295961ee55f93b746", "b9a59476cf9780cdc0b758e24ffeb397069032a5", "fad2e88ba0a3b198f617b1332dc5a72f8239b618", "598b1a0a6451f58298c38955ca2acebf84437211", "b9a59476cf9780cdc0b758e24ffeb397069032a5"]},{"id": "85bf74cdad3d5b7acbc86bb5acc8b6935b7d1308", "title": "Semiclassical formula for the number variance of the Riemann zeros", "authors": ["Michael V. Berry"], "date": "1988", "abstract": "By pretending that the imaginery parts Em of the Riemann zeros are eigenvalues of a quantum Hamiltonian whose corresponding classical trajectories are chaotic and without time-reversal symmetry, it is possible to obtain by asymptotic arguments a formula for the mean square difference V(L;x) between the actual and average number of zeros near the xth zero in an interval where the expected number is L. This predicts that when L >Lmax, V will have quasirandom oscillations about the mean value pi… ", "references": ["ec41f6fe90aa09dbd662c7ba6502dbed6ca101a6"]},{"id": "a97581760cd0f6bbab5218dcd2ed5893712f9650", "title": "Universality and scaling of correlations between zeros on complex manifolds", "authors": ["Pavel Bleher", "Bernard Shiffman", "Steve Zelditch"], "date": "2000", "abstract": "Abstract.We study the limit as N→∞ of the correlations between simultaneous zeros of random sections of the powers LN of a positive holomorphic line bundle L over a compact complex manifold M, when distances are rescaled so that the average density of zeros is independent of N. We show that the limit correlation is independent of the line bundle and depends only on the dimension of M and the codimension of the zero sets. We also provide some explicit formulas for pair correlations. In… ", "references": ["b09b2fbd46217c2dcfe97e0885f067ef06099cda", "4a314e5f6da3f1215db575bea9caa98652aaa7f4", "6062589dc4d36d64caf9c4081c9f0e7dc62eca97", "fe6f57217c11f675991095349ae6b50cbc7ec347", "9593aeee5f771e36099171ee7b224ec70dab6aa3", "fe6f57217c11f675991095349ae6b50cbc7ec347", "4a314e5f6da3f1215db575bea9caa98652aaa7f4", "8d03cde4ca95dc901582ec2351ef0550e73eb08d", "507fb17c4271aec03be1ec9467fd7ad88c3bc5d7", "8d03cde4ca95dc901582ec2351ef0550e73eb08d"]},{"id": "18c760a9012e33b61c62b5709f00b7b0fb7e4f6b", "title": "EQUILIBRIUM DISTRIBUTION OF ZEROS OF RANDOM POLYNOMIALS", "authors": ["Bernard Shiffman", "Steve Zelditch"], "date": "2002", "abstract": "We consider ensembles of random polynomials of the form p(z) = P N=1 ajPj where {aj} are independent complex normal random variables and where {Pj} are the orthonormal polynomials on the boundary of a bounded simply connected analytic plane domain ⊂ C relative to an analytic weight �(z)|dz| . In the simplest case where is the unit disk and � = 1, so that Pj(z) = z j , it is known that the average distribution of zeros is the uniform measure on S 1 . We show that for any analytic (,�), the zeros… ", "references": []},{"id": "a7f63c00882b7c5462005d63f57069579c5d41d1", "title": "Random Matrix Theory and ζ(1/2+it)", "authors": ["Jon P. Keating", "Nina C. Snaith"], "date": "2000", "abstract": "Abstract: We study the characteristic polynomials Z(U, θ) of matrices U in the Circular Unitary Ensemble (CUE) of Random Matrix Theory. Exact expressions for any matrix size N are derived for the moments of |Z| and Z/Z*, and from these we obtain the asymptotics of the value distributions and cumulants of the real and imaginary parts of log Z as N→∞. In the limit, we show that these two distributions are independent and Gaussian. Costin and Lebowitz [15] previously found the Gaussian limit… ", "references": ["c79e704512c235cca5bd7ce60efe9b4bca1c3c4a", "85bf74cdad3d5b7acbc86bb5acc8b6935b7d1308", "3181ffefe4f563c16e21333cc23965a248b1cbc2", "fdbc29001694ea9651a037d7444c318867e5881e", "c3dcaea33e86aa241ea4b000e73f6e0ff38b23f4", "fdbc29001694ea9651a037d7444c318867e5881e", "c36c4c3da0d28881493a36df3fdee99809ba3bda", "d758fc73bab53354266d176c0b4585c9a6ef044f", "c36c4c3da0d28881493a36df3fdee99809ba3bda", "0727eb1ab032c46a9d186c887db9cc5f1e22c910"]},{"id": "c4eb502ceedce63371ffb828608c989e84866f9b", "title": "SU(1, 1) Random Polynomials", "authors": ["Pavel Bleher", "Denis Ridzal"], "date": "2001", "abstract": "We study statistical properties of zeros of random polynomials and random analytic functions associated with the pseudoeuclidean group of symmetries SU(1, 1), by utilizing both analytical and numerical techniques. We first show that zeros of the SU(1, 1) random polynomial of degree N are concentrated in a narrow annulus of the order of N−1 around the unit circle on the complex plane, and we find an explicit formula for the scaled density of the zeros distribution along the radius in the limit N… ", "references": ["f1949263a6e2484c4b1366d283364814ee3f2954", "507fb17c4271aec03be1ec9467fd7ad88c3bc5d7", "848e5efdb3578a785856ecad0c55416baa24c0d9", "fe6f57217c11f675991095349ae6b50cbc7ec347", "b2df1e21ab0d11a199c86964ae5eb2942f079fb9", "cac8a59faaf24073423e36619626354e399c52bc", "6b34de0ccd655d92f0314132c85488ba3b77721b", "cac8a59faaf24073423e36619626354e399c52bc", "cac8a59faaf24073423e36619626354e399c52bc", "cac8a59faaf24073423e36619626354e399c52bc"]},{"id": "507fb17c4271aec03be1ec9467fd7ad88c3bc5d7", "title": "Chaotic analytic zero points: exact statistics for those of a random spin state", "authors": ["J. H. Hannay"], "date": "1996", "abstract": "A natural statistical ensemble of 2J points on the unit sphere can be associated, via the Majorana representation, with a random quantum state of spin J, and an exact expression is obtained here for the general k point correlation function in this ensemble. The pair correlation in the large-J limit takes the relatively simple form where and is the angular separation of the pair of points on the sphere. It appears (from the numerical work of others) that, in this limit, these statistics are… ", "references": []},{"id": "4a314e5f6da3f1215db575bea9caa98652aaa7f4", "title": "How many zeros of a random polynomial are real", "authors": ["Alan Edelman", "Eric Kostlan"], "date": "1995", "abstract": "We provide an elementary geometric derivation of the Kac integral formula for the expected number of real zeros of a random polynomial with independent standard normally distributed coefficients. We show that the expected number of real zeros is simply the length of the moment curve (1, t, . . . , tn) projected onto the surface of the unit sphere, divided by π. The probability density of the real zeros is proportional to how fast this curve is traced out. We then relax Kac’s assumptions by… ", "references": []},{"id": "46274063ceac6f65a6625753e5def7209a145b41", "title": "Determinantal random point fields", "authors": ["Alexander Soshnikov"], "date": "2000", "abstract": "This paper contains an exposition of both recent and rather old results on determinantal random point fields. We begin with some general theorems including proofs of necessary and sufficient conditions for the existence of a determinantal random point field with Hermitian kernel and of a criterion for weak convergence of its distribution. In the second section we proceed with examples of determinantal random fields in quantum mechanics, statistical mechanics, random matrix theory, probability… ", "references": ["a1aa596471518d8d0efffdb9a769d31fffc1655e", "6d4da741bd60bf2d13411e2cd6912aa31466985f", "b78c7da19209176c7877245d7c6afa85488fd554", "d48f35d1d37c58b79901c82c5d905be1cdb14842", "d3a4bdea291a79280d1e1d1c075a27feff417e64", "6d4da741bd60bf2d13411e2cd6912aa31466985f", "6d4da741bd60bf2d13411e2cd6912aa31466985f", "6d4da741bd60bf2d13411e2cd6912aa31466985f", "d48f35d1d37c58b79901c82c5d905be1cdb14842", "a1aa596471518d8d0efffdb9a769d31fffc1655e"]},{"id": "fc9135a1e14823a2733d2f341b0722819ad10c00", "title": "The distribution and moments of the smallest eigenvalue of a random matrix of Wishart type", "authors": ["Alan Edelman"], "date": "1991", "abstract": "Abstract Given a random rectangular m × n matrix with elements from a normal distribution, what is the distribution of the smallest singular value? To pose an equivalent question in the language of multivariate statistics, what is the distribution of the smallest eigenvalue of a matrix from the central Wishart distribution in the null case? We give new results giving the distribution as a simple recursion. This includes the more difficult case when n – m is an even integer, without resorting to… ", "references": []},{"id": "d3a4bdea291a79280d1e1d1c075a27feff417e64", "title": "Exponential Ensemble for Random Matrices", "authors": ["Burt V. Bronk"], "date": "1965", "abstract": "Using the ideas of information theory, it is pointed out that the Gaussian ensemble for random Hermitian matrices can be characterized as the ``most random'' ensemble of these matrices. Pursuing the same characterization for positive matrices, we are led in a natural way to the definition of the exponential ensemble. Transforming to a representation of eigenvectors and eigenvalues, the joint distribution function for the eigenvalues of positive Hermitian matrices is found for this ensemble. The… ", "references": []},{"id": "ddcc048fbf590853aeec150932993435f96d6ce9", "title": "On the Distribution of Roots of Random Polynomials", "authors": ["Eric Kostlan"], "date": "1993", "abstract": "In the study of algebraic and numerical properties of polynomials one occasionally introduces the notion of a random polynomial. For example, this chapter was originally motivated by investigations, including [Ki, Re, SS, Sml, Sm2], into the complexity of root-finding algorithms for polynomials. We restrict our attention to central normal measures on spaces of polynomials, i.e., we assume that the coefficients of the polynomials have a multivariate normal distribution with mean zero and known… ", "references": ["7d18514fe8b06bc8900187da37ecf75d56787a77", "7d18514fe8b06bc8900187da37ecf75d56787a77", "8c058ed75c03fcb1b31899bc52bc01925c521daf", "8c058ed75c03fcb1b31899bc52bc01925c521daf", "7f26baa2f3c2995ec06a78db98e483f9e81b51e4", "762cbbbeb46f0b1873cfe1dccb57a37da3a5af3e", "8c058ed75c03fcb1b31899bc52bc01925c521daf", "d99972c370547e08fed2f8dacaae425a9fcdb073", "7f26baa2f3c2995ec06a78db98e483f9e81b51e4", "a930cb21c9ed9c7feb235b2f388acec8dc9b482c"]},{"id": "2f6fc2f2ef282f6a710b517ee701c3341e2c6e82", "title": "The Asymptotics of a Continuous Analogue of Orthogonal Polynomials", "authors": ["Harold Widom"], "date": "1994", "abstract": "Szeg� polynomials are associated with weight functions on the unit circle. M. G. Krein introduced a continuous analogue of these, a family of entire functions of exponential type associated with a weight function on the real line. An investigation of the asymptotics of the resolvent kernel of sin(x � y)/�(x � y) on 0, s] leads to questions of the asymptotics of the Krein functions associated with the characteristic function of the complement of the interval �1, 1]. Such asymptotics are… ", "references": []},{"id": "8cc47caf8eaf223d1628ce161a6f823054b62e8c", "title": "On the two-dimensional Coulomb gas", "authors": ["Françoise Cornu", "Bernard Jancovici"], "date": "1987", "abstract": "This is a sequel to a recent work of Gaudin, who studied the classical equilibrium statistical mechanics of the two-dimensional Coulomb gas on a lattice at a special value of the coupling constantГ such that the model is exactly solvable. This model is briefly reviewed, and it is shown that the correlation functions obey the sum rules that characterize a conductive phase. A related model in which the particles are constrained to move on an array of equidistant parallel lines has simpler… ", "references": []},{"id": "36597c0d5103c95d2436dab19621b9613fb2db92", "title": "Introduction to random matrices", "authors": ["Craig A. Tracy", "Harold Widom"], "date": "1993", "abstract": "Here I = S j (a2j 1,a2j) andI(y) is the characteristic function of the set I. In the Gaussian Unitary Ensemble (GUE) the probability that no eigenvalues lie in I is equal to �(a). Also �(a) is a tau-function and we present a new simplified derivation of the system of nonlinear completely integrable equations (the aj's are the independent variables) that were first derived by Jimbo, Miwa, Mori, and Sato in 1980. In the case of a single interval these equations are reducible to a Painleve V… ", "references": ["8e557bf1e14abd4e2d28970fa9154ed940221845", "d0f9e4dbbcd9ffa92e5de1c86db6dd779f41f295", "8e557bf1e14abd4e2d28970fa9154ed940221845", "cf49d57962bb2b67c001258e5f96f2b624cd169f", "bd18c4559c1e3f4b349ab294932b61924fd653fc", "96b34f8c2983b6dd3f0d96e3b15a9a5f520a5df1", "f981a45e1057c81cbf71a17e5a20fce8c7f9eb2f", "bd18c4559c1e3f4b349ab294932b61924fd653fc", "f981a45e1057c81cbf71a17e5a20fce8c7f9eb2f", "66d13d93eaee4b37ebae46566bf605cffd70ea14"]},{"id": "598b1a0a6451f58298c38955ca2acebf84437211", "title": "The spectrum edge of random matrix ensembles", "authors": ["Peter J. Forrester"], "date": "1993", "abstract": "Abstract The scaled n -level distribution and scaled level spacing distribution for the small and large eigenvalues of various ensembles of random matrices are considered. Exact results for both these quantities are obtained for various special values of the parameters in the gaussian and Laguerre ensembles. On the basis of a Coulomb gas analogy, an asymptotic formula for the distribution of the smallest and largest eigenvalue is given in terms of the eigenvalue density at the spectrum edge. ", "references": []},{"id": "cf49d57962bb2b67c001258e5f96f2b624cd169f", "title": "Universal scaling of the tail of the density of eigenvalues in random matrix models", "authors": ["Mark J. Bowick", "Edouard Brézin"], "date": "1991", "abstract": "Abstract Large random matrices have eigenvalue density distributions limited to a finite support. Near the endpoint of the support, when the size N of the matrices is large, one can study a scaling region of size N−μ in which the cross-over from a non-zero density to a vanishing density takes place. This cross-over is shown to be universal (for random hermitian matrices with a unitary-invariant probability distribution), in the sense that it depends only on the order of multicriticality of the… ", "references": []},{"id": "9ea50374077a506b86dce4796c683abcd98e18d7", "title": "A theoretical basis for the use of co-occurence data in information retrieval", "authors": ["Van Rijsbergen"], "date": "1977", "abstract": "This paper provides a foundation for a practical way of improving the effectiveness of an automatic retrieval system. Its main concern is with the weighting of index terms as a device for increasing retrieval effectiveness. Previously index terms have been assumed to be independent for the good reason that then a very simple weighting scheme can be used. In reality index terms are most unlikely to be independent. This paper explores one way of removing the independence assumption. Instead the… ", "references": []},{"id": "3add8c6c36af4755cecaf994bda0e5a202035811", "title": "Discriminative models for information retrieval", "authors": ["Ramesh Nallapati"], "date": "SIGIR '04", "abstract": "Discriminative models have been preferred over generative models in many machine learning problems in the recent past owing to some of their attractive theoretical properties. In this paper, we explore the applicability of discriminative classifiers for IR. We have compared the performance of two popular discriminative models, namely the maximum entropy model and support vector machines with that of language modeling, the state-of-the-art generative model for IR. Our experiments on ad-hoc… ", "references": ["c6d61f2f6804507917f95df1144668a0e3d0df59", "eb82d3035849cd23578096462ba419b53198a556", "6adb7a21e59d962c6420471d151602cdfbec48a8", "99109a88930c49bda594463a706730b66a6a02e2", "f6e3e57567e9803718623ec088cd7fea65cfbc9d", "f6e3e57567e9803718623ec088cd7fea65cfbc9d", "385197d4c02593e2823c71e4f90a0993b703620e", "2471f7b9ec544554415dcc374049a507e460a415", "ba5bc6ff22d8f0613589251e1a0de670b91be35e", "2471f7b9ec544554415dcc374049a507e460a415"]},{"id": "337effdabae73d7964179b4ab7c704b9c4a6293c", "title": "Eigenvalues and condition numbers of random matrices", "authors": ["Alan Edelman"], "date": "1988", "abstract": "Given a random matrix, what condition number should be expected? This paper presents a proof that for real or complex $n \\times n$ matrices with elements from a standard normal distribution, the expected value of the log of the 2-norm condition number is asymptotic to $\\log n$ as $n \\to \\infty$. In fact, it is roughly $\\log n + 1.537$ for real matrices and $\\log n + 0.982$ for complex matrices as $n \\to \\infty$. The paper discusses how the distributions of the condition numbers behave for large… ", "references": []},{"id": "abba12ca7c82e73cee7204e0f694cb0072d25cc8", "title": "Minimax lower bounds for the two-armed bandit problem", "authors": ["Sanjeev R. Kulkarni", "Gábor Lugosi"], "date": "1997", "abstract": "We obtain minimax lower bounds on the regret for the classical two-armed bandit problem. We provide a finite-sample minimax version of the well-known log n asymptotic lower bound of Lai and Robbins (1985). Also, in contrast to the log n asymptotic results on the regret, we show that the minimax regret is achieved by mere random guessing under fairly mild conditions on the set of allowable configurations of the two arms. That is, we show that for every allocation rule and for every n, there is a… ", "references": ["b77aa53b45f6aa1873780d0fa7aad50efb422458", "2fa7c63ee5484c688e8ec62846ed7137c4924607", "bdc3d618db015b2f17cd76224a942bfdfc36dc73", "bdc3d618db015b2f17cd76224a942bfdfc36dc73", "ee2cd1d17f833d3c157a1016a778c7c22af555a2", "2fa7c63ee5484c688e8ec62846ed7137c4924607", "a12e0bc04ac79b6b0ae90a3f67b55634228ac48a", "a12e0bc04ac79b6b0ae90a3f67b55634228ac48a", "2fa7c63ee5484c688e8ec62846ed7137c4924607", "a2d5e774e878dc36141e8b78cdebdf712bd0a321"]},{"id": "dd90df6350c6245434655636f6e809e7b7a721d2", "title": "Automatic Phrase Indexing for Document Retrieval: An Examination of Syntactic and Non-Syntactic Methods", "authors": ["Joel L. Fagan"], "date": "SIGIR", "abstract": "An automatic phrase indexing method based on the term discrimination model is described, and the results of retrieval experiments on five document collections are presented. Problems related to this non-syntactic phrase construction method are discussed, and some possible solutions are proposed that make use of information about the syntactic structure of document and query texts. ", "references": []},{"id": "936fcbdbf013b0308a68d0c35c6f1a89d461184b", "title": "Q-Learning for Bandit Problems", "authors": ["Michael O. Duff"], "date": "ICML", "abstract": "Multi-armed bandits may be viewed as decompositionally-structured Markov decision processes (MDP''s) with potentially very large state sets. A particularly elegant methodology for computing optimal policies was developed over twenty ago by Gittins [Gittins \\& Jones, 1974]. Gittins'' approach reduces the problem of finding optimal policies for the original MDP to a sequence of low-dimensional stopping problems whose solutions determine the optimal policy through the so-called ``Gittins indices… ", "references": ["a03990c6a0526264153db624d9c1d871c0d0fa1f", "2c3337861b56120ff0b5b956f5c4e1084973bb45", "2c3337861b56120ff0b5b956f5c4e1084973bb45", "2c3337861b56120ff0b5b956f5c4e1084973bb45", "2c3337861b56120ff0b5b956f5c4e1084973bb45", "e4fe28113fed71999a0db30a930e0b42d3ce55f1", "2cca494cd58f483547a4bd059b319a915e5751bc", "b77aa53b45f6aa1873780d0fa7aad50efb422458", "9d79ec872a954e4c4aa2c93883207f506110f908", "2c3337861b56120ff0b5b956f5c4e1084973bb45"]},{"id": "eaec01700f5ea63af311cfd7a70a3869460ce080", "title": "Learning to Act Using Real-Time Dynamic Programming", "authors": ["Andrew G. Barto", "Steven J. Bradtke", "Satinder P. Singh"], "date": "1995", "abstract": "Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence.", "references": ["d8bab102150805b0181830d4fc03f28b45ba18bb", "60944c5243db70a687a320a2622d3bd1610802a8", "60944c5243db70a687a320a2622d3bd1610802a8", "7fa02258b3703ab98cfc57f0885de9e43de06e23", "7fa02258b3703ab98cfc57f0885de9e43de06e23", "7fa02258b3703ab98cfc57f0885de9e43de06e23", "954c5ffcdd876e6646e78c89be4ae5e4113a8728", "7fa02258b3703ab98cfc57f0885de9e43de06e23", "3abac8d1bf1a6c69805e8aa6f0335b66f39ca999", "290d1e46b71673b35231cb3bcd848912782af6f1"]},{"id": "305d689afb6574ffec7b01e24431d541d0ce6f5d", "title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "authors": ["Peter Auer", "Nicolò Cesa-Bianchi", "Robert E. Schapire"], "date": "1995", "abstract": "In the multi-armed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the… ", "references": ["ccf5208521cb8c35f50ee8873df89294b8ed7292", "a2d5e774e878dc36141e8b78cdebdf712bd0a321"]},{"id": "97b5d2379c86d1d58ba53d7e670c2f570fb3d2c3", "title": "Multi-Armed bandit problem revisited", "authors": ["Takashi Ishikida", "Pravin Varaiya"], "date": "1994", "abstract": "In this paper, we revisit aspects of the multi-armed bandit problem in the earlier work (Ref. 1). An alternative proof of the optimality of the Gittins index rule is derived under the discounted reward criterion. The proof does not involve an explicit use of the interchange argument. The ideas of the proof are extended to derive the asymptotic optimality of the index rule under the average reward criterion. Problems involving superprocesses and arm-acquiring bandits are also reexamined. The… ", "references": []},{"id": "8597ed8596c08ba2ba7bc8da3e9546749d6f4f7b", "title": "SAMPLE MEAN BASED INDEX POLICIES WITH O(logn) REGRET FOR THE MULTI-ARMED BANDIT PROBLEM", "authors": ["Rajeev Agrawal"], "date": "1995", "abstract": "We consider a non-Bayesian infinite horizon version of the multi-armed bandit problem with the objective of designing simple policies whose regret increases sldwly with time. In their seminal work on this problem, Lai and Robbins had obtained a O(logn) lower bound on the regret with a constant that depends on the KullbackLeibler number. They also constructed policies for some specific families of probability distributions (including exponential families) that achieved the lower bound. In this… ", "references": []},{"id": "883b8a189fe1bb97b9ad2a382e057ba7e2a2e56f", "title": "The Convergence of Contrastive Divergences", "authors": ["Alan L. Yuille"], "date": "NIPS", "abstract": "The Convergence of Contrastive Divergences Alan Yuille Department of Statistics University of California at Los Angeles Los Angeles, CA 90095 yuille@stat.ucla.edu Abstract This paper analyses the Contrastive Divergence algorithm for learning statistical parameters. We relate the algorithm to the stochastic approxi- mation literature. This enables us to specify conditions under which the algorithm is guaranteed to converge to the optimal solution (with proba- bility 1). This includes necessary… ", "references": ["9360e5ce9c98166bb179ad479a9d2919ff13d022", "f5081943e0bdc65dc4af7da55a073a5118afe77f", "b182834ae4c900460de72e5a9b932ddfaf4da7b8", "b182834ae4c900460de72e5a9b932ddfaf4da7b8", "6069ec0d0387b3f3516ae83ff108a581f10a76e4", "9360e5ce9c98166bb179ad479a9d2919ff13d022", "ca9b21e84ffc7e193d1b3bb45fb7c4e48226b59e", "b95799a25def71b100bd12e7ebb32cbcee6590bf", "ce6ff7eb1a5b88988c33470f5c2050355458e3ec", "b182834ae4c900460de72e5a9b932ddfaf4da7b8"]},{"id": "df511a5d9d12bff681438e2dbe2ecef70268c9c9", "title": "Cooling Schedules for Optimal Annealing", "authors": ["Bruce E. Hajek"], "date": "1988", "abstract": "A Monte Carlo optimization technique called “simulated annealing” is a descent algorithm modified by random ascent moves in order to escape local minima which are not global minima. The level of randomization is determined by a control parameter T, called temperature, which tends to zero according to a deterministic “cooling schedule.” We give a simple necessary and sufficient condition on the cooling schedule for the algorithm state to converge in probability to the set of globally minimum… ", "references": ["07ca0e6baf84ee67213f650be9821f02fcf15b80", "07ca0e6baf84ee67213f650be9821f02fcf15b80", "07ca0e6baf84ee67213f650be9821f02fcf15b80", "904cdb58826f614827aa54fd4d6df3e585da9aff", "e86cdbc5c54dc7cda0ea03db67155770357a578e"]},{"id": "9e7c4c8fc3bb8cf5353f2c3cac6a1c5fa95bae65", "title": "The SST method: a tool for analysing Web information search processes", "authors": ["Nils Pharo", "Kalervo Järvelin"], "date": "2004", "abstract": "The article presents the search situation transition (SST) method for analysing Web information search (WIS) processes. The idea of the method is to analyse searching behaviour, the process, in detail and connect both the searchers' actions (captured in a log) and his/her intentions and goals, which log analysis never captures. On the other hand, ex post factor surveys, while popular in WIS research, cannot capture the actual search processes. The method is presented through three facets: its… ", "references": ["e74c250e227cf48d14403ed5c49fdf7707e45988", "ea48a56aa7567b7b361613de2a3eb29e08c896eb", "b2d62bae36f1d98c3cbfd105352a1d78988456ca", "db9b38147ef65c063f95b4b92e2ad4cf825a4dc5", "616dfbd74b799af75f3a585c72f143b139933925", "e74c250e227cf48d14403ed5c49fdf7707e45988", "ea48a56aa7567b7b361613de2a3eb29e08c896eb", "b2d62bae36f1d98c3cbfd105352a1d78988456ca", "dc6428d7165520a379006f9e340e7b0dbd47214c", "7d8eb7a0a96649da90e0b80169459216f6f5deb4"]},{"id": "12d1d070a53d4084d88a77b8b143bad51c40c38f", "title": "Reinforcement Learning: A Survey", "authors": ["Leslie Pack Kaelbling", "Michael L. Littman", "Andrew W. Moore"], "date": "1996", "abstract": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in… ", "references": ["54c4cf3a8168c1b70f91cf78a3dc98b671935492", "563cf7359970015db2f8a05c41e9efef7025dda2", "f2eb733470921af04df5c611a6a3c76c281ce498", "45d2bdf5e7072c1d5a91a38efa365715def2f45d", "54c4cf3a8168c1b70f91cf78a3dc98b671935492", "45d2bdf5e7072c1d5a91a38efa365715def2f45d", "563cf7359970015db2f8a05c41e9efef7025dda2", "563cf7359970015db2f8a05c41e9efef7025dda2", "7a09464f26e18a25a948baaa736270bfb84b5e12", "f2eb733470921af04df5c611a6a3c76c281ce498"]},{"id": "4f7476037408ac3d993f5088544aab427bc319c1", "title": "Information processing in dynamical systems: foundations of harmony theory", "authors": ["Paul Smolensky"], "date": "1986", "abstract": "Abstract : At this early stage in the development of cognitive science, methodological issues are both open and central. There may have been times when developments in neuroscience, artificial intelligence, or cognitive psychology seduced researchers into believing that their discipline was on the verge of discovering the secret of intelligence. But a humbling history of hopes disappointed has produced the realization that understanding the mind will challenge the power of all these… ", "references": []},{"id": "81b8bb14524665e3dbaa772f297cd5e5e79ba0d6", "title": "Large margin rank boundaries for ordinal regression", "authors": ["Ralf Herbrich"], "date": "2000", "abstract": null, "references": []},{"id": "958f001b6f348f7c353260b289bed185fffac847", "title": "Large Margin Rank Boundaries for Ordinal Regression", "authors": ["Darren Gehring", "Thore Graepel"], "date": "2000", "abstract": null, "references": []},{"id": "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f", "title": "An Efficient Boosting Algorithm for Combining Preferences", "authors": ["Yoav Freund", "Raj D. Iyer", "Yoram Singer"], "date": "1998", "abstract": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and… ", "references": ["64a717dc148b76070bbb6a3190a7e05bb9734400", "37bad2daf9b5d26a2d4c0e99c412751e95d76c38", "0025b963134b1c0b64c1389af19610d038ab7072", "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d", "ccf5208521cb8c35f50ee8873df89294b8ed7292", "64a717dc148b76070bbb6a3190a7e05bb9734400", "d8ab36551e25f158a1f2b9d2596929d1cd3d643a", "64a717dc148b76070bbb6a3190a7e05bb9734400", "37bad2daf9b5d26a2d4c0e99c412751e95d76c38", "72e4197cef4c355a1892fa45b0f2f5c69fac75d0"]},{"id": "df648ec7958e04f93835df0808734e065f9a46e9", "title": "Advances in Neural Information Processing Systems 8", "authors": ["Marian Stewart Bartlett", "Paul A. Viola", "Paul Ekman"], "date": "1996", "abstract": null, "references": []},{"id": "624d8241c597ee12477053cf1970d017a7651417", "title": "IR evaluation methods for retrieving highly relevant documents", "authors": ["JärvelinKalervo", "KekäläinenJaana"], "date": "2017", "abstract": "This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to... ", "references": []},{"id": "b1a5961609c623fc816aaa77565ba38b25531a8e", "title": "Neural Networks: Tricks of the Trade", "authors": ["Juris Hartmanis", "Jan van Leeuwen"], "date": "Lecture Notes in Computer…", "abstract": "Many algorithms are available to learn deep hierarchies of features from unlabeled data, especially images. In many cases, these algorithms involve multi-layered networks of features (eg, neural networks) that are sometimes tricky to train and tune and are difficult. Abstract A commonly encountered problem in MLP (multi-layer perceptron) classification problems is related to the prior probabilities of the individual classes-if the number of training examples that correspond to each class varies… ", "references": []},{"id": "646d4888871aca2a25111eb2520e4c47e253b014", "title": "The TREC-8 Question Answering Track Report", "authors": ["Ellen M. Voorhees"], "date": "TREC", "abstract": "The TREC-8 Question Answering track was the first large-scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for finding answers when responses could be as long as a paragraph (250 bytes), but more… ", "references": ["0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001", "0ff65ac698013cdd9d61326cab49a1d75404e001"]},{"id": "1250f6d90ba07b242f8548e39bb30164b4b8b3aa", "title": "Overview of the trec 2001 question answering", "authors": ["Ellen M. Voorhees"], "date": "2001", "abstract": "A screening assembly comprising a plurality of screening elements, with each element including at least one hollow tubular protrusion extending through an aperture formed in a supporting grid. The protrusion is formed with a smaller diameter than the aperture and is expanded into clamping contact with the aperture by insertion of a securing pin through the protrusion. ", "references": []},{"id": "520028c1b1b6d47093d3e74ce35cd9c4fb4234d8", "title": "Overview of the TREC-9 Question Answering Track", "authors": ["Ellen M. Voorhees"], "date": "TREC", "abstract": "lnmGoqpqr sutIp v6wyxzv6{P| }U~^yxqIDby6xyQ6b*xbEy=wyxYxq=xy QQIx;qbxxqbbu I x.  b wyx ~^=xb%Q=(x=¡ ¢Iyx£ ¤4v6wyxQ¥w=bPy ; x ¦§Q. ̈© w=xx4wyx4bRbaQw £*xqP6b xxRx £Dba= y xb x«¬wuQqI^b%wyx QuQ(Q=6x\\¡¡~^=xb­ w=xw=b®w… ", "references": []},{"id": "56f4906d8fcc4097299d9644ee54fa4534b8e594", "title": "An integrated network for invariant visual detection and recognition", "authors": ["Yali Amit", "Massimo Mascaro"], "date": "2003", "abstract": "We describe an architecture for invariant visual detection and recognition. Learning is performed in a single central module. The architecture makes use of a replica module consisting of copies of retinotopic layers of local features, with a particular design of inputs and outputs, that allows them to be primed either to attend to a particular location, or to attend to a particular object representation. In the former case the data at a selected location can be classified in the central module… ", "references": ["86876c68d05af50330d07ee10790ddcf6578c7d9", "5d2ef51c912df93d31314d6827a98bc474374105", "9a4880278882962ccecd03d70d478507b1f2b851", "700bbcd3518ca8cb3dac50a89fc69cad3dc1a579", "700bbcd3518ca8cb3dac50a89fc69cad3dc1a579", "af4ae6e58b658ea5ca51780aabd2c31af7f9b860", "61b2d0383b186c4d634c5f51421cab67c16d90a1", "9a4880278882962ccecd03d70d478507b1f2b851", "9a4880278882962ccecd03d70d478507b1f2b851", "5d2ef51c912df93d31314d6827a98bc474374105"]},{"id": "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "title": "Visual features of intermediate complexity and their use in classification", "authors": ["Shimon Ullman", "Michel Vidal-Naquet", "Erez Sali"], "date": "2002", "abstract": "The human visual system analyzes shapes and objects in a series of stages in which stimulus features of increasing complexity are extracted and analyzed. The first stages use simple local features, and the image is subsequently represented in terms of larger and more complex features. These include features of intermediate complexity and partial object views. The nature and use of these higher-order representations remains an open question in the study of visual processing by the primate cortex… ", "references": ["083630744dbf60994867cbd776bfe601b4d0dbe6", "ae16712906bdb66907950fbb8c5436e54b281812", "ca1d23be869380ac9e900578c601c2d1febcc0c9", "ca1d23be869380ac9e900578c601c2d1febcc0c9", "ae16712906bdb66907950fbb8c5436e54b281812", "ae16712906bdb66907950fbb8c5436e54b281812", "083630744dbf60994867cbd776bfe601b4d0dbe6", "083630744dbf60994867cbd776bfe601b4d0dbe6", "33b98b5dc150680fc02a14c0cf629168dd0af08b", "1525a40e0947889dcd5083d277109f61452c09dc"]},{"id": "7aa7c0df2c2e154d7bddb873168ebc8446472425", "title": "On the Role of Object-Specific Features for Real World Object Recognition in Biological Vision", "authors": ["Thomas Serre", "Maximilian Riesenhuber", "Tomaso A. Poggio"], "date": "Biologically Motivated…", "abstract": "Models of object recognition in cortex have so far been mostly applied to tasks involving the recognition of isolated objects presented on blank backgrounds. However, ultimately models of the visual system have to prove themselves in real world object recognition tasks. Here we took a first step in this direction: We investigated the performance of the HMAX model of object recognition in cortex recently presented by Riesenhuber & Poggio [1,2] on the task of face detection using natural images… ", "references": ["b44a278ceccabc60548f2959d43a1f258d38f55d", "015a3cdf3c335fa29dd8bb0b82b01a09ed2eb5a0", "015a3cdf3c335fa29dd8bb0b82b01a09ed2eb5a0", "85abadb689897997f1e37baa7b5fc6f7d497518b", "85abadb689897997f1e37baa7b5fc6f7d497518b", "85abadb689897997f1e37baa7b5fc6f7d497518b", "d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7", "5451278e1a11cf3f1be28a05f38d36c8641e68f7", "015a3cdf3c335fa29dd8bb0b82b01a09ed2eb5a0"]},{"id": "0b8af3a7db6ff26f73f69c15cfa6635d607d58ec", "title": "A Unified System For Object Detection, Texture Recognition, and Context Analysis Based on the Standard Model Feature Set", "authors": ["Stanley M. Bileschi", "Lior Wolf"], "date": "BMVC", "abstract": "Recently, a neuroscience inspired set of visual features was introduced. It was shown that this representation facilitates better performance than stateof-the-art vision systems for object recognition in cluttered and unsegmented images. In this paper, we investigate the utility of these features in other common scene-understanding tasks. We show that this outstanding performance extends to shape-based object detection in the usual windowing framework, to amorphous object detection as a texture… ", "references": ["1cf1527807ebb16020b04d4166e7ba8d27652302", "3565c5a65842f26091578b9d71d496cc1561239d", "8fdc86b3b648449b89940f57f55ff23328c18b89", "4f948352d3dd8e53a83315359cf5797084a480d2", "4f948352d3dd8e53a83315359cf5797084a480d2", "fb659afbd80521206717b03ffa68b8a85e2434aa", "fb659afbd80521206717b03ffa68b8a85e2434aa", "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "3565c5a65842f26091578b9d71d496cc1561239d", "3565c5a65842f26091578b9d71d496cc1561239d"]},{"id": "7883dd07bc13a024f018ef50e9f4313218cf1718", "title": "Risk Minimization and Language Modeling in Text Retrieval – Thesis Summary", "authors": ["ChengXiang Zhai"], "date": "2002", "abstract": "This thesis presents a new general probabilistic framework for text retrieval based on Bayesian decision theory. In this framework, queries and documents are modeled using statistical language models, user preferences are modeled through loss functions, and retrieval is cast as a risk minimization problem. This risk minimization framework not only unifies several existing retrieval models within one general probabilistic framework, but also facilitates the development of new principled… ", "references": ["c669c8cb28c6d219419e0e904a795164e8c6be05", "c2872fb23b02597034a179f4adb82a00d6ffda8d", "f126ca6ac5455bdec0c26c2f89e36064c0b72f23", "73a76dd71abfbd29dbba4ea034ab52284626aa71", "73a76dd71abfbd29dbba4ea034ab52284626aa71", "305ea2c0992052c4c9574cf1873a855fbb88b957", "73a76dd71abfbd29dbba4ea034ab52284626aa71", "c669c8cb28c6d219419e0e904a795164e8c6be05", "5e52c6ff85ac6d53cb0dac8581c0a76edc6fade7", "decf799682a402b9ee50b9d6c0267f6c545a7031"]},{"id": "e03bda45248b4169e2a20cb9124ae60440cad2de", "title": "Learning a dictionary of shape-components in visual cortex: comparison with neurons, humans and machines", "authors": ["Tomaso A. Poggio", "Thomas Serre"], "date": "2006", "abstract": "In this thesis, I describe a quantitative model that accounts for the circuits and computations of the feedforward path of the ventral stream of visual cortex.", "references": ["3c523f408efb0b32eccdbc12e9e57fb9b3b19c46", "56f4906d8fcc4097299d9644ee54fa4534b8e594", "3e175842665dffdedf9b6a634d16f8265d31d70d", "56f4906d8fcc4097299d9644ee54fa4534b8e594", "8f17ee1661c1641a34e2ec02348ce5b539660e54", "56f4906d8fcc4097299d9644ee54fa4534b8e594", "bb373d57b2dd98c97cc7e30d29bd050de12e54d4", "8f17ee1661c1641a34e2ec02348ce5b539660e54", "682bed63aa347fa601f5c85c452332e646ac9849", "8f17ee1661c1641a34e2ec02348ce5b539660e54"]},{"id": "68c9e02b77a5f46677d87f6efe86aafa8fb1772c", "title": "Bayesian Network Models for Information Retrieval", "authors": ["Berthier A. Ribeiro-Neto", "Ilmério Reis da Silva", "Richard R. Muntz"], "date": "2000", "abstract": "In this chapter, we apply Bayesian networks to the problem of retrieving information about a subject or topic and show that Bayesian networks provide an effective and flexible framework for dealing with information retrieval (IR) in general. Our discussion focus on two Bayesian networks models proposed in the literature namely, the inference network and the belief network models. We compare the expressiveness of these two models and show that the belief network model is more general. We also… ", "references": []},{"id": "03a8cb23b78ae1e8662b226d96e4a0ac2bf5d3fd", "title": "Document language models, query models, and risk minimization for information retrieval", "authors": ["John D. Lafferty", "ChengXiang Zhai"], "date": "SIGIR '01", "abstract": "We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model… ", "references": []},{"id": "836bfe1994a174b41c46e7244845b44f8702eeab", "title": "Similarity Searching in Large Image Databases", "authors": ["Euripides G. M. Petrakis", "Christos Faloutsos"], "date": "1994", "abstract": "A weakened underwater section of deteriorating pile is provided with a concrete splint. One of two companion seal members is first installed below the weakened section. A split tubular form is then installed about the section. The lower end of the form provides the second of the companion seal members that engages beneath the first seal member. The form is supported from above as water is pumped from the form. The buoyant upthrust moves the companion sealing members into firm sealing engagement… ", "references": []},{"id": "6bb4de9c81b221f97d541b81134aab9029c42b91", "title": "Generic object recognition with boosting", "authors": ["Andreas Opelt", "Axel Pinz", "Peter Auer"], "date": "2006", "abstract": "This paper explores the power and the limitations of weakly supervised categorization. We present a complete framework that starts with the extraction of various local regions of either discontinuity or homogeneity. A variety of local descriptors can be applied to form a set of feature vectors for each local region. Boosting is used to learn a subset of such feature vectors (weak hypotheses) and to combine them into one final hypothesis for each visual category. This combination of individual… ", "references": ["f354310098e09c1e1dc88758fca36767fd9d084d", "bb4e87caeff86e407cebef544d26924ec3673f0e", "96d9ab468299fe51a4e14d86d8ea953ccf62b900", "36cd88ed2c17a596001e9c7d89533ac46c28dec0", "96d9ab468299fe51a4e14d86d8ea953ccf62b900", "0c91808994a250d7be332400a534a9291ca3b60e", "38101fac622a70b78f13625fc6502000b8756d3a", "0c91808994a250d7be332400a534a9291ca3b60e", "36cd88ed2c17a596001e9c7d89533ac46c28dec0", "ac3645f03959b9e06c0a437e318a00278b4a20da"]},{"id": "f33ab691655bbefea841eabaac34903ae9755ab7", "title": "Retrieval of similar pictures on pictorial databases", "authors": ["Chin-Chen Chang", "Suh-Yin Lee"], "date": "1991", "abstract": "Abstract In this paper, we suggest a method of retrieving iconic pictures in a pictorial database based upon the spatial relationship among the objects in the picture. This special kind of database query problem is generally known as a spatial match retrieval problem. We transform each picture or query into a set of ordered triples ( O i , O j , r ij )s, where O i and O j are two symbolic objects and r ij is the spatial relationship between O i and O j . Then we construct a hashing table for… ", "references": []},{"id": "5ce8078bce4dea312a2c28d34af4dd82667bbffe", "title": "On modeling information retrieval with probabilistic inference", "authors": ["S. K. Michael Wong", "Yiyu Yao"], "date": "1995", "abstract": "This article examines and extends the logical models of information retrieval in the context of probability theory. The fundamental notions of term weights and relevance are given probabilistic interpretations. A unified framework is developed for modeling the retrieval process with probabilistic inference. This new approach provides a common conceptual and mathematical basis for many retrieval models, such as the Boolean, fuzzy set, vector space, and conventional probabilistic models. Within… ", "references": ["6cba4ace41cc20de16438c2e538f3f40e476e6af", "514bd50d54908ff49cc6ec41386e4b7a5d606e0f", "df6c5734f166b405ccce36df210e121a7481305a"]},{"id": "b8bede7bcffa3358a0bd76c63137230fa856e0cc", "title": "The role of parts and spatial relations in object identification.", "authors": ["Carolyn Backer Cave", "Stephen M. Kosslyn"], "date": "1993", "abstract": "An investigation of the role of parts and their spatial relations in object identification is reported. At the most general level, two important results were obtained. First, proper spatial relations among components of an object are critical for easy identification. When parts were scrambled on the page, naming times and error rates increased. And, second, the way an object is divided into parts (parsed) affects identification only under the most impoverished viewing conditions. When subjects… ", "references": []},{"id": "293f77164e068cb62ca5bf76ff4116333df7de4b", "title": "Perceptual categories for spatial layout.", "authors": ["Daniel J. Kersten"], "date": "1997", "abstract": "The central problems of vision are often divided into object identification and localization. Object identification, at least at fine levels of discrimination, may require the application of top-down knowledge to resolve ambiguous image information. Utilizing top-down knowledge, however, may require the initial rapid access of abstract object categories based on low-level image cues. Does object localization require a different set of operating principles than object identification or is… ", "references": ["06d7c8dbd23aaf6ac8c758d43705657c289a756e", "8b37258659bcdbc380b1e6c4e22cce9ea06397a1", "b95bb08703c92d3429865cdc4b8a0910e4ed7059", "f2abdf94c4810678e1d517556361c47ff5b0d846", "21a01fb6b2d9fdeeca02b5fe4a44c1db13f7ee8a", "06d7c8dbd23aaf6ac8c758d43705657c289a756e", "1c565c0f06ea2d9b7f4b37e6e95c9b193dee47db", "21a01fb6b2d9fdeeca02b5fe4a44c1db13f7ee8a", "8f5bad4cab5a0a0309b4efa8eaeb37e659a339b9", "c4f6954c6080774b8f7c47c705fa3c4463546a6c"]},{"id": "87cc226aa060db976fbf6ac3a07969b33b544b96", "title": "Virage image search engine: an open framework for image management", "authors": ["Jeffrey R. Bach", "Charles Edward Fuller", "Chiao-Fe Shu"], "date": "Electronic Imaging", "abstract": "Until recently, the management of large image databases has relied exclusively on manually entered alphanumeric annotations. Systems are beginning to emerge in both the research and commercial sectors based on 'content-based' image retrieval, a technique which explicitly manages image assets by directly representing their visual attributes. The Virage image search engine provides an open framework for building such systems. The Virage engine expresses visual features as image 'primitives… ", "references": ["d0c8b0540ad4ab007d1d99d5056cf71d7bfced7b", "c47f7fcda4071bce2e161beb66c278352a8e7daf", "c183048cd41e47ffee77e19e0d198db5748b7d25", "00a6ff44da7b64ac866c5c8f7feb350c06637815", "119bb9ad8cc4cc71c22dfa1b3cc88d3bfffa69bf", "764e18101bdbb67dcd75d12d90ede920098ffa13", "d0c8b0540ad4ab007d1d99d5056cf71d7bfced7b", "4fa5856ce0aa7331c7a9ee70a225b0a7d5aa1d37", "764e18101bdbb67dcd75d12d90ede920098ffa13", "d0c8b0540ad4ab007d1d99d5056cf71d7bfced7b"]},{"id": "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "title": "From Blobs to Boundary Edges: Evidence for Time- and Spatial-Scale-Dependent Scene Recognition", "authors": ["Philippe G. Schyns", "Aude Oliva"], "date": "1994", "abstract": "In very fast recognition tasks, scenes are identified as fast as isolated objects How can this efficiency be achieved, considering the large number of component objects and interfering factors, such as cast shadows and occlusions? Scene categories tend to have distinct and typical spatial organizations of their major components If human perceptual structures were tuned to extract this information early in processing, a coarse-to-fine process could account for efficient scene recognition A… ", "references": []},{"id": "aff99b6538c2a6b89478a7e7901fe31878e63f03", "title": "Semantic versus perceptual influences of color in object recognition.", "authors": ["John E. Joseph", "Dennis R. Proffitt"], "date": "1996", "abstract": "The influence of color as a surface feature versus its influence as stored knowledge in object recognition was assessed. Participants decided whether a briefly presented and masked picture matched a test name. For pictures and words referring to similarly shaped objects, semantic color similarity (SCS) was present when picture and word shared the same prototypical color (e.g., purple apple followed by cherry). Perceptual color similarity (PCS) was present when the surface color of the picture… ", "references": []},{"id": "91f2cb5a7ba9da57c140e6f2303cdd16a4c77ecb", "title": "Surface versus edge-based determinants of visual recognition", "authors": ["Irving Biederman", "Ginny Ju"], "date": "1988", "abstract": "Two roles hypothesized for surface characteristics, such as color, brightness, and texture, in object recognition are that such information can (a) define the gradients needed for a 212-D sketch so that a 3-D representation can be derived (e.g., Marr & Nishihara, 1978) and (b) provide additional distinctive features for accessing memory. In a series of five experiments, subjects either named or verified (against a target name) brief (50–100 ms) presentations of slides of common objects. Each… ", "references": ["70bbfe1357047bbba560239002c4d2a173f10eae", "bdf9b8f4de001f5f37bc844efbce1210d581d599", "d55f01fd8d6ce029a44ce25692f1928a3e16572d", "d55f01fd8d6ce029a44ce25692f1928a3e16572d", "8b37258659bcdbc380b1e6c4e22cce9ea06397a1", "76361a44e145732a39dbc68d9418871038c83be2", "d55f01fd8d6ce029a44ce25692f1928a3e16572d", "2c05d131576d01668ef01156ac73e52c3152384a", "8999355e47248bc60f5768fc2168fd28295b5f27", "8b37258659bcdbc380b1e6c4e22cce9ea06397a1"]},{"id": "f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "title": "Statistical Context Priming for Object Detection", "authors": ["Antonio Torralba", "Pawan Sinha"], "date": "ICCV", "abstract": "There is general consensus that context can be a rich source of information about an object’s identity, location and scale. However, the issue of how to formalize contextual influences is still largely open. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties. We represent global context information in terms of the spatial layout of spectral components. The resulting scheme serves as an effective procedure for context driven… ", "references": ["913beb8d70383bacaf7f0133d9a88ca592542af7", "a8cf3f0ea76961eca50bf26ab31e677037cab622", "bd232cf2ab28cc0ba06942875f14206f04ebbae0", "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a", "74b312560b79929540734067e58de46966b96130", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "913beb8d70383bacaf7f0133d9a88ca592542af7", "a5b309957c0113d45458268f2324b36c52ae3f73", "c411f93539714f512e437c45a7a9d0a6d5a7675e", "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a"]},{"id": "f05f91c03fc7497162f754fdbfcddc5fcf4179fb", "title": "Contextual Modulation of Target Saliency", "authors": ["Antonio Torralba"], "date": "NIPS", "abstract": "The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. In this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. ", "references": ["0f45a46dedadf599c12874b22645d596205ed8d5", "2e1cdf906b1588b40734e12fa3183cc22f0ecd74", "06c908a5dea82fdbffa8285beae1c3d60b028902", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "0f45a46dedadf599c12874b22645d596205ed8d5", "76361a44e145732a39dbc68d9418871038c83be2", "913beb8d70383bacaf7f0133d9a88ca592542af7", "a52b81b322087ce82e4ec4124b3a47cbb391a0eb", "2e1cdf906b1588b40734e12fa3183cc22f0ecd74", "2e1cdf906b1588b40734e12fa3183cc22f0ecd74"]},{"id": "acf0b6745708457a53d5327eea345c0bcf466e97", "title": "Depth Estimation from Image Structure", "authors": ["Antonio Torralba", "Aude Oliva"], "date": "2002", "abstract": "In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges, and junctions may provide a 3D model of the scene but it will not provide information about the actual \"scale\" of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, object recognition, under unconstrained conditions… ", "references": ["a1c45a16a17a94fbb1fb794f07237eabaddbe5e6", "c21738e116aeabca1e523f612610605718ae00ff", "ca317a064d508b7dceae7a7a8d1334c97f94985e", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "ca317a064d508b7dceae7a7a8d1334c97f94985e", "0f45a46dedadf599c12874b22645d596205ed8d5", "d0690e1129f652a836bd45c418b9335a38b4c841", "ca317a064d508b7dceae7a7a8d1334c97f94985e", "c21738e116aeabca1e523f612610605718ae00ff", "bd232cf2ab28cc0ba06942875f14206f04ebbae0"]},{"id": "2196b6fcf0a65872c70cf9852af29a6a839a55fa", "title": "Sensory and cognitive contributions of color to the recognition of natural scenes", "authors": ["Karl R. Gegenfurtner", "Jochem Rieger"], "date": "2000", "abstract": "Although color plays a prominent part in our subjective experience of the visual world, the evolutionary advantage of color vision is still unclear [1] [2], with most current answers pointing towards specialized uses, for example to detect ripe fruit amongst foliage [3] [4] [5] [6]. We investigated whether color has a more general role in visual recognition by looking at the contribution of color to the encoding and retrieval processes involved in pattern recognition [7] [8] [9]. Recognition… ", "references": ["101402476205f7f17f3d47204d2dcca6c2bc7492", "101402476205f7f17f3d47204d2dcca6c2bc7492", "b1ab08d6d368512c549ddb0826e58ccc23f138ae", "101402476205f7f17f3d47204d2dcca6c2bc7492", "b1ab08d6d368512c549ddb0826e58ccc23f138ae", "ba3cccafa80e389666debec8600f317b24e1c7af", "0e151e0bf74a2fd129083cea277f08ea190ac691", "b1ab08d6d368512c549ddb0826e58ccc23f138ae", "d5a76e4903f3d9e2ef0c68723357c901a70a2a0d", "101402476205f7f17f3d47204d2dcca6c2bc7492"]},{"id": "48f0d6211dc70a56202c1a0c95124c65a51f721f", "title": "Coarse Blobs or Fine Edges? Evidence That Information Diagnosticity Changes the Perception of Complex Visual Stimuli", "authors": ["Aude Oliva", "Philippe G. Schyns"], "date": "1997", "abstract": "Efficient categorizations of complex visual stimuli require effective encodings of their distinctive properties. However, the question remains of how processes of object and scene categorization use the information associated with different perceptual spatial scales. The psychophysics of scale perception suggests that recognition uses coarse blobs before fine scale edges, because the former is perceptually available before the latter. Although possible, this perceptually determined scenario… ", "references": ["dd174f66cab413ef5a904c9fb8a428c8fcbc1130", "1cd678493090fd0709c2c45ddd81d0e9f7723fd9", "1cd678493090fd0709c2c45ddd81d0e9f7723fd9", "5188b8d12e1011e67c3b63eabca2df20c78b1c7b", "7e6fcd89eaa3a4d610887969b445fb109e153827", "bdc836b9b9842cfd9a60fbe366384e450449f255", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "4999a6f43a987c835516ff87533c29459f4374bf", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "8a07f8d2eafb2921a92cb90dc5a859925858b759"]},{"id": "3cc6c4b2881c5607df9d3d6bb25ed94fd0add236", "title": "Top-down control of visual attention in object detection", "authors": ["Aude Oliva", "Antonio Torralba", "John M. Henderson"], "date": "2003", "abstract": "Current computational models of visual attention focus on bottom-up information and ignore scene context. However, studies in visual cognition show that humans use context to facilitate object detection in natural scenes by directing their attention or eyes to diagnostic regions. Here we propose a model of attention guidance based on global scene configuration. We show that the statistics of low-level features across the scene image determine where a specific object (e.g. a person) should be… ", "references": ["f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "7fd53a3ac2bfbef24883876cd469611ae1a1193b", "06c908a5dea82fdbffa8285beae1c3d60b028902", "f6a48c0dbff6e7fa752fdcf8ef34f8cba8202b41", "d223589f8f170363d71e70d7aac18020814bd464", "d223589f8f170363d71e70d7aac18020814bd464", "89b6a5a136bc0a938b792df6bdde134def28335e"]},{"id": "913beb8d70383bacaf7f0133d9a88ca592542af7", "title": "Region-based image querying", "authors": ["Chad Carson", "Serge J. Belongie", "J. Malík"], "date": "1997", "abstract": "Retrieving images from large and varied collections using image content as a key is a challenging and important problem. In this paper, we present a new image representation which provides a transformation from the raw pixel data to a small set of localized coherent regions in color and texture space. This so-called &ldquo;blobworld&rdquo; representation is based on segmentation using the expectation-maximization algorithm on combined color and texture features. The texture features we use for… ", "references": ["abe29e529e17b7015b5cbfaa2abfc8f6d6f3269a", "071fc55dff9486dcee4ecafb50ae12b70bd0a1d1", "60da0f08f353d8a738e0c9b93e20f7ab53ff539c", "09ba48b1bcd1e98da8a3dfc28cce7be6659a690e", "3ce69aa63d6d2de24eef8819ece23cca2bc41cbd", "c9e58a5b063d4252a15fd94c0613f83a4fdb780e", "abe29e529e17b7015b5cbfaa2abfc8f6d6f3269a", "3ce69aa63d6d2de24eef8819ece23cca2bc41cbd", "3ce69aa63d6d2de24eef8819ece23cca2bc41cbd", "60da0f08f353d8a738e0c9b93e20f7ab53ff539c"]},{"id": "364c60c92a77e2eca1b7faeea763aea3a7d2070a", "title": "Global Semantic Classi cation of Scenes using Power Spectrum Templates 3 Power Spectrum Families", "authors": ["Aude Oliva", "Antonio Torralba", "Jeanny Hérault"], "date": "1999", "abstract": "Scene recognition and content-based procedures are of great interest for image indexing applications processing very large databases. Knowing the context of a scene, a retrieval system may compute its semantic category in advance and lter out scenes belonging to irrelevant classes. In this paper, we introduce a computational approach which classi es and organises real-world scenes along broad semantic axes. Fundamental to our approach is the computation of global spectral templates providing a… ", "references": ["48f0d6211dc70a56202c1a0c95124c65a51f721f", "5e28e81e757009d2f76b8674e0da431f5845884a", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "c5505f4c0fa9689cd2598231538bb08e9b06851f", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "a8cf3f0ea76961eca50bf26ab31e677037cab622", "0f45a46dedadf599c12874b22645d596205ed8d5", "48f0d6211dc70a56202c1a0c95124c65a51f721f", "48f0d6211dc70a56202c1a0c95124c65a51f721f", "c5505f4c0fa9689cd2598231538bb08e9b06851f"]},{"id": "9191de6f4059469b54cb576322e331c51312dc6f", "title": "HYPER: A New Approach for the Recognition and Positioning of Two-Dimensional Objects", "authors": ["Nicholas Ayache", "Olivier D. Faugeras"], "date": "1986", "abstract": "A new method has been designed to identify and locate objects lying on a flat surface. The merit of the approach is to provide strong robustness to partial occlusions (due for instance to uneven lighting conditions, shadows, highlights, touching and overlapping objects) thanks to a local and compact description of the objects boundaries and to a new fast recognition method involving generation and recursive evaluation of hypotheses named HYPER (HY potheses Predicted and Evaluated Recursively… ", "references": ["8734b7c9d55b8383a32262de6d32d771496b31c0", "f46165aee5006d1cd19e44a2481fbfc889f845c0", "440dfcbdd54f4ad376795a8af4454fa84fcaad3b", "39d8df286932d5a328a462db25d634ee3c872669", "ff55cdeb15c3bd9424cfeb01cf0ec7822a07a0a2", "39d8df286932d5a328a462db25d634ee3c872669", "49222b111513ae06bacfec095d7322e0c5886785", "db393e1758be80deeb0e099389adfe1de80bfe24", "db393e1758be80deeb0e099389adfe1de80bfe24", "3988f50436d5c357a0ada5f76ff10a10f3acbc62"]},{"id": "bd580fad7a14f93d6d59765a5fe91974e2653281", "title": "RECOVERING INTRINSIC SCENE CHARACTERISTICS FROM IMAGES", "authors": ["Harry G. Barrow", "Joan M. Tenenbaum"], "date": "1978", "abstract": "We suggest that an appropriate role of early visual processing is to describe a scene in terms of intrinsic (vertical) characteristics -- such as range, orientation, reflectance, and incident illumination -- of the surface element visible at each point in the image. Support for this idea comes from three sources: the obvious utility of intrinsic characteristics for higher-level scene analysis; the apparent ability of humans to determine these characteristics, regardless of viewing conditions or… ", "references": ["aa4b60b5847999c2f778e3e67ca1f2201e396abb", "aa4b60b5847999c2f778e3e67ca1f2201e396abb", "aa4b60b5847999c2f778e3e67ca1f2201e396abb", "9aef4c7405f4ea1b3c75e6efce4ac5976aa8ad92", "23384ac18d7edea7a6c6145fd605912e506dd671", "9aef4c7405f4ea1b3c75e6efce4ac5976aa8ad92", "23384ac18d7edea7a6c6145fd605912e506dd671", "1af25c587e3d7f664c9f14ed8494315418cae7f1", "23384ac18d7edea7a6c6145fd605912e506dd671", "aa4b60b5847999c2f778e3e67ca1f2201e396abb"]},{"id": "50899b2355d6908a304bacb5e406f800f3dde558", "title": "A network that learns to recognize three-dimensional objects", "authors": ["Tomaso Poggio", "Shimon Edelman"], "date": "1990", "abstract": "THE visual recognition of three-dimensional (3-D) objects on the basis of their shape poses at least two difficult problems. First, there is the problem of variable illumination, which can be addressed by working with relatively stable features such as intensity edges rather than the raw intensity images1,2. Second, there is the problem of the initially unknown pose of the object relative to the viewer. In one approach to this problem, a hypothesis is first made about the viewpoint, then the… ", "references": []},{"id": "33f65bf55cf25bb474aa0e593b7cc4a128c7f1bf", "title": "Global Depth Perception from Familiar Scene Structure", "authors": ["Antonio Torralba", "Aude Oliva"], "date": "2001", "abstract": "In the absence of cues for absolute depth measurements as binocular disparity, motion, or defocus, the absolute distance between the observer and a scene cannot be measured. The interpretation of shading, edges and junctions may provide a 3D model of the scene but it will not inform about the actual 'size' of the space. One possible source of information for absolute depth estimation is the image size of known objects. However, this is computationally complex due to the diÆculty of the object… ", "references": ["bf85123feb2cf0ec42930ce324501fd2cd9c049c", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "a1c45a16a17a94fbb1fb794f07237eabaddbe5e6", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "869171b2f56cfeaa9b81b2626cb4956fea590a57", "83f4aa8845b3cc37ad02d7878171f4e8cfaba02d", "c21738e116aeabca1e523f612610605718ae00ff", "a1c45a16a17a94fbb1fb794f07237eabaddbe5e6", "bf85123feb2cf0ec42930ce324501fd2cd9c049c", "c21738e116aeabca1e523f612610605718ae00ff"]},{"id": "3b900851aca0872dd0d87c22ed32335eb38f5e36", "title": "Recognizing three-dimensional objects by comparing two-dimensional images", "authors": ["Daniel P. Huttenlocher", "Liana M. Lorigo"], "date": "1996", "abstract": "In this paper we address the problem of recognizing an object from a novel viewpoint, given a single \"model\" view of that object. As is common in model-based recognition, objects and images are represented as sets of feature points. We present an efficient algorithm for determining whether two sets of image points (in the plane) could be projections of a common object (a three-dimensional point set). The method relies on the fact that two sets of points in the plane are orthographic projections… ", "references": ["9191de6f4059469b54cb576322e331c51312dc6f", "c5648d1f511a5180cc0bf7af80a42d3dea3a4680", "d781d5e651e12bf666cf993ae307db785113b9ae", "1f314b8066e4b8248172e0b9c1e73a31588e73f6", "cc72c84f1933de920c2f7a9cb08969b50fe4b7f0", "d781d5e651e12bf666cf993ae307db785113b9ae", "ef768a5c9bd0aaeddafea1d56b08b0c8180760c0", "c5648d1f511a5180cc0bf7af80a42d3dea3a4680", "d781d5e651e12bf666cf993ae307db785113b9ae", "6578b296f61c8f352ef36b7828477d6c90595fde"]},{"id": "27ce5a120a86632dd56f869ee65656b7d7312a3a", "title": "Perceptual Organization and Visual Recognition", "authors": ["David G. Lowe"], "date": "1985", "abstract": "A computational model is presented for the visual recognition of three-dimensional objects based upon their spatial correspondence with two-dimensional features in an image. A number of components of this model are developed in further detail and implemented as computer algorithms. At the highest level, a verification process has been developed which can determine exact values of viewpoint and object parameters from hypothesized matches between three-dimensional object features and two… ", "references": []},{"id": "c5648d1f511a5180cc0bf7af80a42d3dea3a4680", "title": "Learning and recognition of 3D objects from appearance", "authors": ["Hiroshi Murase", "Shree K. Nayar"], "date": "1993", "abstract": "The authors address the problem of automatically learning object models for recognition and pose estimation. In contrast to the traditional approach, they formulate the recognition problem as one of matching visual appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illumination conditions. While shape and reflectance are intrinsic properties of an object and are constant, pose and… ", "references": ["a56ba22d1e14b6ff00081dabaab3cd5cced49a01", "2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1", "a56ba22d1e14b6ff00081dabaab3cd5cced49a01", "5e0ce7719dcb315145284fea50fd7c96df3599ab", "5e0ce7719dcb315145284fea50fd7c96df3599ab", "2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1", "089a76dbc62a06ad30ae1925530e8733e850268e", "2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1", "da0a8bdd9b6a99e86bd68fe96d70925db0de6750", "da0a8bdd9b6a99e86bd68fe96d70925db0de6750"]},{"id": "b61f99b452b9d213f0d850b85287f2a31ceb5676", "title": "The drawing of objects by a visual form agnosic: Contribution of surface properties and memorial representations", "authors": ["Philip Servos", "Melvyn A. Goodale", "G. Keith Humphrey"], "date": "1993", "abstract": "Although edge-based representations of objects are thought to play a central role in object identification, it is clear that real objects convey more information about their form than line drawings. Patients with visual form agnosia, for example, are able to identify real objects more easily than the corresponding line drawings of those objects, even if exactly the same projection planes are used [Goodale et al. Object versus picture identification in a patient with visual form agnosia. Paper… ", "references": []},{"id": "d06fc7272b8baf936a6468bd8b8a14771c3f853f", "title": "Perception and action in 'visual form agnosia'.", "authors": ["Anthony David Milner", "David I. Perrett", "E Terazzi"], "date": "1991", "abstract": "A single case study of a patient with 'visual form agnosia' is presented. A severe visual recognition deficit was accompanied by impairments in discriminating shape, reflectance, and orientation, although visual acuity and colour vision, along with tactile recognition and intelligence, were largely preserved. Neuropsychological and behavioural investigations have indicated that the patient is able to utilize visual pattern information surprisingly well for the control of hand movements during… ", "references": ["6b482ecbca85e9131b8ec0912a8ce379049f9348", "b575f118a8f6cd9a17aad9d24c64bebeacf6f29e", "54aa4619251cf8e1559ae3066b4cb702b7514f0b", "ef91f5adb5c0bce89307bbe26c332ba949ff000c", "f7accdf4efb122ae50f784e663c218fcbdb6b0da", "cce9ff2a9df6d171e35c3532cae28f7e230e95bd", "e619f4e5b46c7cb680b3646d1e7d0cf4c3444654", "3776c1e9dacbe4f76cfc29ffefeace29c442b24e", "54aa4619251cf8e1559ae3066b4cb702b7514f0b", "f7accdf4efb122ae50f784e663c218fcbdb6b0da"]},{"id": "e22e79780108f6562a62d6c74036b325a771fedb", "title": "Ventral occipital lesions impair object recognition but not object-directed grasping: an fMRI study.", "authors": ["Thomas W. James", "Jody C. Culham", "Melvyn A. Goodale"], "date": "2003", "abstract": "D.F., a patient with severe visual form agnosia, has been the subject of extensive research during the past decade. The fact that she could process visual input accurately for the purposes of guiding action despite being unable to perform visual discriminations on the same visual input inspired a novel interpretation of the functions of the two main cortical visual pathways or 'streams'. Within this theoretical context, the authors proposed that D.F. had suffered severe bilateral damage to her… ", "references": ["96a0bd3baa22fa3330b87943fed790160b0af7fe", "00cc179e50fee1f722ccbec4077c475799050ebf", "8df9c26b853a9c9908d4624150d6e407d535dea8", "a664c2aabc03f71f7835a9311a8e69d3d46fa1b6", "bceddbc2aa8e746d25a7be6d5561af83c60130a2", "bceddbc2aa8e746d25a7be6d5561af83c60130a2", "1525a40e0947889dcd5083d277109f61452c09dc", "96a0bd3baa22fa3330b87943fed790160b0af7fe", "ac559da719558f016511cb26b474c39d383a98a1", "bceddbc2aa8e746d25a7be6d5561af83c60130a2"]},{"id": "110c6d754d5767eb534151052f990a200da34f53", "title": "Localized principal components of natural images-an analytic solution", "authors": ["Yong Liu", "Harel Z. Shouval"], "date": "1994", "abstract": "The structure of receptive fields in the visual cortex is believed to be shaped by unsupervised learning. A simple variant of unsupervised learning is the extraction of principal components. In this paper, we derive analytically the form of the principal components of natural images. An assumption is made that only small circular regions of the images are being used as training patterns. The derivation relies on results about the correlation function of natural images. Our results predict both… ", "references": ["3e00dd12caea7c4dab1633a35d1da3cb2e76b420", "3e00dd12caea7c4dab1633a35d1da3cb2e76b420"]},{"id": "2740b47a162ea190d3faf80bdb9a2f6916791a19", "title": "Neuropsychological evidence for a topographical learning mechanism in parahippocampal cortex", "authors": ["Russell Epstein", "Edgar A. DeYoe", "Nancy Kanwisher"], "date": "2001", "abstract": "The Parahippocampal Place Area (PPA; Epstein & Kanwisher, 1998) is a region within posterior parahippocampal cortex that responds selectively to visual stimuli that convey information about the layout of local space. Here we describe two patients who suffered damage to the PPA after vascular incidents. Both subsequently exhibited memory problems for topographical materials and were unable to navigate unassisted in unfamiliar environments. Performance on a continuous n-back visual memory test… ", "references": ["9e344d8be72cef5ac9b982887e43f34eaf9dedd1", "8975ba9ea0e340b16003edd6608ea90972605e5d", "25c9e0267eceb6c0fd3f5d25f9d439ddc03f10e3", "7ebb7f861d5c07320a3688842284a34361539ff6", "9e344d8be72cef5ac9b982887e43f34eaf9dedd1", "537ef7bbac959c3db361bdcc3e86625e79a6a801", "693f050e0fa2bb3955844cd26064874335a0838e", "d010340b1dba87dc05cea7a5fe02020466df44f5", "537ef7bbac959c3db361bdcc3e86625e79a6a801", "25c9e0267eceb6c0fd3f5d25f9d439ddc03f10e3"]},{"id": "0e9a0da864277b24c635f4bea918fffe2bc3f865", "title": "Classification of scene photographs from local orientations features", "authors": ["Anne Guérin-Dugué", "Aude Oliva"], "date": "2000", "abstract": "Abstract Natural image understanding is a very active and promising research domain both in psychology for visual perception modelling and in computer science for image retrieval. In this study, we investigate the efficiency of orientation distributions over the whole image in the scale space. The global distribution of the local dominant orientations (LDO) appears to be a powerful feature for discriminating between four semantic categories of real world scenes (indoor scenes, urban scenes… ", "references": ["0f45a46dedadf599c12874b22645d596205ed8d5", "3cb6369c5564d2d4244b17b02a92913d741ef736", "a8cf3f0ea76961eca50bf26ab31e677037cab622", "b26c24a81ad3780b91eab45d3892b30e061a1c03", "e901a4e8acddf3514d40c6013514575b64e5f012", "7eb1ebe839501e2e7371ff4b2a8552941fa9c091", "3cb6369c5564d2d4244b17b02a92913d741ef736", "b26c24a81ad3780b91eab45d3892b30e061a1c03", "d01805e377a90563292322cdce31828ccde40ba5", "3cb6369c5564d2d4244b17b02a92913d741ef736"]},{"id": "aeb37769d72999bcbfb0582b73607fd8d23f4545", "title": "Relations between the statistics of natural images and the response properties of cortical cells.", "authors": ["David J. Field"], "date": "1987", "abstract": "The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images… ", "references": ["85af5a0b7102919e96eebe9143c082fe9e453e46", "e26aaa33c6615d90a9b02ecbe5d776f5e712c582", "ded1e8ac4ed8150337c1c33eb2823924bce65285", "d315e72c4226997edeb5940255f2996e483e1356", "709e9e78094d3f077c2c9156bb3792bbb2dd9f3a", "6d346fa98748dd1ee5435e8ee46698356d3c6596", "1e1f6f39b97b1ed460dd6e1a8819903ea8966cd8", "709e9e78094d3f077c2c9156bb3792bbb2dd9f3a", "85af5a0b7102919e96eebe9143c082fe9e453e46", "1e1f6f39b97b1ed460dd6e1a8819903ea8966cd8"]},{"id": "dd35d4ed68f2bc7a29b78adac63d4ce9139fdda7", "title": "To what extent can matching algorithms based on direct outputs of spatial filters account for human object recognition?", "authors": ["Jaromír Fiser", "Irving Biederman", "Eric E. Cooper"], "date": "1996", "abstract": "A number of recent successful models of face recognition posit only two layers, an input layer consisting of a lattice of spatial filters and a single subsequent stage by which those descriptor values are mapped directly onto an object representation layer by standard matching methods such as stochastic optimization. Is this approach sufficient for modeling human object recognition? We tested whether a highly efficient version of such a two-layer model would manifest effects similar to those… ", "references": []},{"id": "5a30589d274c2867425ead17780a0d22c69fc672", "title": "Object Recognition Using Multidimensional Receptive Field Histograms", "authors": ["Bernt Schiele", "James L. Crowley"], "date": "ECCV", "abstract": "This paper presents a technique to determine the identity of objects in a scene using histograms of the responses of a vector of local linear neighborhood operators (receptive fields). This technique can be used to determine the most probable objects in a scene, independent of the object's position, image-plane orientation and scale. In this paper we describe the mathematical foundations of the technique and present the results of experiments which compare robustness and recognition rates for… ", "references": ["fc482ed9a9ad837337d219637108d2474e0e627c", "5b1e1696564e5a3021ac3a501c9deeb6c0fbc637", "993b1083455b5c4d631eaf44f230b061994e75c3", "fc482ed9a9ad837337d219637108d2474e0e627c", "7f1acd0f6c9068d74252869eb1d74bd9a22ec015", "993b1083455b5c4d631eaf44f230b061994e75c3", "07f55d5e1bda539e689f051de61292ab2c91d26e", "07f55d5e1bda539e689f051de61292ab2c91d26e", "7f1acd0f6c9068d74252869eb1d74bd9a22ec015", "c7cb3b96aba81b123278627d03c1f60eafa0bed0"]},{"id": "7fa7606947a9b2f4ff7efb39d3c04c29fc4a7a17", "title": "3D Object Recognition: A Model of View-Tuned Neurons", "authors": ["Emanuela Bricolo", "Tomaso A. Poggio", "Nikos K. Logothetis"], "date": "NIPS", "abstract": "In 1990 Poggio and Edelman proposed a view-based model of object recognition that accounts for several psychophysical properties of certain recognition tasks. The model predicted the existence of view-tuned and view-invariant units, that were later found by Logothetis et al. (Logothetis et al., 1995) in IT cortex of monkeys trained with views of specific paperclip objects. The model, however, does not specify the inputs to the view-tuned units and their internal organization. In this paper we… ", "references": ["82962da5c273a9e6627a040d56c8a7973fe22440", "7e7bc6278d4bc7a5e1cdca46ef4230246e2d3bde", "be89df7b320640f5bcb5706c381087af57597073", "82962da5c273a9e6627a040d56c8a7973fe22440", "7d7721e2c556e02f35654428953ed83cfa8adff8", "be89df7b320640f5bcb5706c381087af57597073", "3781ae2fd845dc912aeda34ffcf9e94cba3b5c5e", "1a04baa4207d7c9da4dd71179b03307ae37ee142", "50899b2355d6908a304bacb5e406f800f3dde558", "50899b2355d6908a304bacb5e406f800f3dde558"]},{"id": "5ff14f5eddeaa425ab15504a1bc45f328dc9221e", "title": "Illumination planning for object recognition in structured environments", "authors": ["Hiroshi Murase", "Shree K. Nayar"], "date": "1994", "abstract": "This paper addresses the problem of illumination planning for robust object recognition in structured environments. Given a set of objects, the goal is to determine the illumination for which the objects are most distinguishable in appearance from each other. For each object, a large number of images is automatically obtained by varying pose and illumination. Images of all objects, together, constitute the planning image set. The planning set is compressed using the Karhunen-Loeve transform to… ", "references": ["bf21af9a70aafa135942c5eee5b9fc59706f0005"]},{"id": "8b37258659bcdbc380b1e6c4e22cce9ea06397a1", "title": "Recognition-by-components: a theory of human image understanding.", "authors": ["Irving Biederman"], "date": "1987", "abstract": "The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N £ 36), can be derived from contrasts of five readily detectable properties of edges in a two… ", "references": ["1af25c587e3d7f664c9f14ed8494315418cae7f1", "29144feb3fa93dd54f5759c9f4ccb962eecea9ad", "57a2991f56fa4972ff8fe2a8236001ff5d0e47af", "150662e38fc67ca81ac93faf981cf2151fbf6a9a", "afc51a561ae5a6bcdf615435ce88f6d3c16fec77", "afc51a561ae5a6bcdf615435ce88f6d3c16fec77", "e2dc6b0d1f0454e990778b412c92b9048c2115cd", "afc51a561ae5a6bcdf615435ce88f6d3c16fec77", "afc51a561ae5a6bcdf615435ce88f6d3c16fec77", "29144feb3fa93dd54f5759c9f4ccb962eecea9ad"]},{"id": "aa1eea18e569a13bb262e3e6b266509b36bf6bb4", "title": "Receptive field spaces and class-based generalization from a single view in face recognition", "authors": ["Maria Lando", "Shimon Edelman"], "date": "1995", "abstract": "Abtraet. We describe a computational model of face recognition, which generalizes from single views of faces by taking advantage of prior experience with other faces. seen under a wider range of viewing conditions. The model represents face images by veclo~s of activities of graded overlapping receptive fields (m). It relies on high-spatial-frequency information to estimate the~viewing conditions, which are then used to normalize (via a h’ansfonnation specific for faces), and identify, the low… ", "references": ["7e80f9b9d97845728dbd52e4265520e5878bd202", "7382437ce90d46a19cb4c07defdddacac07b52c6", "82962da5c273a9e6627a040d56c8a7973fe22440", "7382437ce90d46a19cb4c07defdddacac07b52c6", "82962da5c273a9e6627a040d56c8a7973fe22440", "7382437ce90d46a19cb4c07defdddacac07b52c6", "8abe2824f9851d3c465b1aa11849661430d60ca0", "db186e92e0083dd44d6abdc84b7cb0368e00faac", "7382437ce90d46a19cb4c07defdddacac07b52c6"]},{"id": "90c6694f16f4104f1bc261f2485f5c4ddc05eeaa", "title": "Recognizing solid objects by alignment with an image", "authors": ["Daniel P. Huttenlocher", "Shimon Ullman"], "date": "1990", "abstract": "In this paper we consider the problem of recognizing solid objects from a single two-dimensional image of a three-dimensional scene. We develop a new method for computing a transformation from a three-dimensional model coordinate frame to the two-dimensional image coordinate frame, using three pairs of model and image points. We show that this transformation always exists for three noncollinear points, and is unique up to a reflective ambiguity. The solution method is closed-form and only… ", "references": ["de2fa641d79397c3b383aea77776d8ebe49de08a", "a9ef54f467438cb7024ec591da67b51110054d5e", "a9ef54f467438cb7024ec591da67b51110054d5e", "3b33b0cc54c4d65baba6911c280049d532173bfc", "8735690a9e8f8884bf27717877ddf7f9071472e5", "e88df129649b24e3f782deba6d64aed592995c3b", "a9ef54f467438cb7024ec591da67b51110054d5e", "de2fa641d79397c3b383aea77776d8ebe49de08a", "25da182b67ecff73e33d962ca88ce8157e499140", "754b3ce07da5ea86193c4f4733be80fa72de5858"]},{"id": "6c47e23b8ad2833df6346afab4b3151af3ae2399", "title": "Recognizing 3-D Objects Using Surface Descriptions", "authors": ["Ting-Jun Fan", "Gérard G. Medioni", "Ramakant Nevatia"], "date": "1989", "abstract": "The authors provide a complete method for describing and recognizing 3-D objects, using surface information. Their system takes as input dense range date and automatically produces a symbolic description of the objects in the scene in terms of their visible surface patches. This segmented representation may be viewed as a graph whose nodes capture information about the individual surface patches and whose links represent the relationships between them, such as occlusion and connectivity. On the… ", "references": []},{"id": "a9561071b5902c0d3df0dc6eb23f819f4abd8cac", "title": "Efficient Calculation of Primary Images from a Set of Images", "authors": ["Hiroyasu Murakami", "B. V. K. Vijaya Kumar"], "date": "1982", "abstract": "A set of images is modeled as a stochastic process and Karhunen-Loeve expansion is applied to extract the feature images. Although the size of the correlation matrix for such a stochastic process is very large, we show the way to calculate the eigenvectors when the rank of the correlation matrix is not large. We also propose an iterative algorithm to calculate the eigenvectors which save computation time andc omputer storage requirements. This iterative algorithm gains its efficiency from the… ", "references": ["fbf75104b26aec301837ffbaf29859f6cc842aaa", "fbf75104b26aec301837ffbaf29859f6cc842aaa", "9d254e143b73580300014345ac1d6cef6035275a", "735bcbba68b1f7ec40c14a49d83bf4bc7c3c8736", "569ebb736a8d3659fdd7b196f9c220f4f895d07f", "9d254e143b73580300014345ac1d6cef6035275a"]},{"id": "5e0ce7719dcb315145284fea50fd7c96df3599ab", "title": "Model-based recognition in robot vision", "authors": ["Roland T. Chin", "Charles R. Dyer"], "date": "1986", "abstract": "This paper presents a comparative study and survey of model-based object-recognition algorithms for robot vision.", "references": []},{"id": "7d8bb8044d9a41fa45c50e5e3fee21cda8ba9cec", "title": "Multidimensional indexing for recognizing visual shapes", "authors": ["Andrea Califano", "Rakesh Mohan"], "date": "1991", "abstract": "A homogeneous approach for acquisition, storage, and recognition of nonparametric shapes from images, using a novel shape representation based on shape autocorrelation operators is presented. A theoretical and experimental analysis of the computational complexity, recognition performance with increasing database size, and fault tolerance of the approach is presented. The system has been tested extensively with more than 300 arbitrary shapes in the database. Using a set of complex shapes, the… ", "references": []},{"id": "5eb5243a0de130c96abeb33032e406c195dc103a", "title": "Mental rotation and orientation-dependence in shape recognition", "authors": ["Michael J. Tarr", "Steven Pinker"], "date": "1989", "abstract": "How do we recognize objects despite differences in their retinal projections when they are seen at different orientations? Marr and Nishihara (1978) proposed that shapes are represented in memory as structural descriptions in objectcentered coordinate systems, so that an object is represented identically regardless of its orientation. An alternative hypothesis is that an object is represented in memory in a single representation corresponding to a canonical orientation, and a mental rotation… ", "references": ["47c24cf9495f6b2a8fcedd4527abcd79b6bc0a9e", "cc70cc57914aaf7d93ac7463a58251a766de7cf6", "b5c3ac68515a41ab85b26804396a77be0ac1b121", "2771cdc0e7add58ebc4998b7d275547fcc04c571", "cc70cc57914aaf7d93ac7463a58251a766de7cf6", "2771cdc0e7add58ebc4998b7d275547fcc04c571", "47c24cf9495f6b2a8fcedd4527abcd79b6bc0a9e", "cc70cc57914aaf7d93ac7463a58251a766de7cf6", "cc70cc57914aaf7d93ac7463a58251a766de7cf6", "936e7e1fe87510c5d66d3d38021e1534c453273e"]},{"id": "d781d5e651e12bf666cf993ae307db785113b9ae", "title": "Recognition by Linear Combinations of Models", "authors": ["Shimon Ullman", "Ronen Basri"], "date": "1991", "abstract": "An approach to visual object recognition in which a 3D object is represented by the linear combination of 2D images of the object is proposed. It is shown that for objects with sharp edges as well as with smooth bounding contours, the set of possible images of a given object is embedded in a linear space spanned by a small number of views. For objects with sharp edges, the linear combination representation is exact. For objects with smooth boundaries, it is an approximation that often holds… ", "references": ["be7d48e11700ebcf2d2eacbec9de608e5a7a8121", "587f126726b89ee92d6c692d9c89e5adb5f46dd3", "2fb9aeba42264f0b4277e05730af59364d82936d", "be7d48e11700ebcf2d2eacbec9de608e5a7a8121", "1c5ca852ba75e3614c9f55fbe441c40b74258292", "1c5ca852ba75e3614c9f55fbe441c40b74258292", "939a51497e3c2d7806cc2c1a8ee22fe3fd518d96", "2fb9aeba42264f0b4277e05730af59364d82936d", "1c5ca852ba75e3614c9f55fbe441c40b74258292", "140b1d27f53bd809ba55a47f55f1abe2347e742f"]},{"id": "8ccc8ab68720396b6646e649d6d1d12ebcce9a7a", "title": "The Illumination-Invariant Recognition of 3D Objects Using Local Color Invariants", "authors": ["David Slater", "Glenn Healey"], "date": "1996", "abstract": "Traditional approaches to three dimensional object recognition exploit the relationship between three dimensional object geometry and two dimensional image geometry. The capability of object recognition systems can be improved by also incorporating information about the color of object surfaces. Using physical models for image formation, the authors derive invariants of local color pixel distributions that are independent of viewpoint and the configuration, intensity, and spectral content of… ", "references": []},{"id": "6818668fb895d95861a2eb9673ddc3a41e27b3b3", "title": "A Combined Corner and Edge Detector", "authors": ["Christopher G. Harris", "Mike Stephens"], "date": "Alvey Vision Conference", "abstract": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work.", "references": ["7954a6ad9fdd35a6374ab8f38b27f41244265f47", "b80deba9cce6ad3bc8f5624c4a151a64ee226f14", "f4c83b78c787e01d3d4e58ae18c8f3ab5933a02b", "93b376bd451db8ed94a18c556da16f25a3e7961b", "93b376bd451db8ed94a18c556da16f25a3e7961b", "7954a6ad9fdd35a6374ab8f38b27f41244265f47", "93b376bd451db8ed94a18c556da16f25a3e7961b", "7954a6ad9fdd35a6374ab8f38b27f41244265f47", "7954a6ad9fdd35a6374ab8f38b27f41244265f47", "93b376bd451db8ed94a18c556da16f25a3e7961b"]},{"id": "8560e9c39c50ea928eef7c115d99fdae5acfdfa3", "title": "Three-dimensional object recognition", "authors": ["Paul J. Besl", "Ramesh C. Jain"], "date": "1985", "abstract": "A general-purpose computer vision system must be capable of recognizing three-dimensional (3-D) objects. This paper proposes a precise definition of the 3-D object recognition problem, discusses basic concepts associated with this problem, and reviews the relevant literature. Because range images (or depth maps) are often used as sensor input instead of intensity images, techniques for obtaining, processing, and characterizing range data are also surveyed. ", "references": ["4b48e19d9cf64accaa3db9332e48d91f0b421e7c", "4b48e19d9cf64accaa3db9332e48d91f0b421e7c", "4b48e19d9cf64accaa3db9332e48d91f0b421e7c", "e848aef6d94bf824baf0f91215659f3387786ce9", "17b252c4b28079fcf652712fe25e643893901361", "4b48e19d9cf64accaa3db9332e48d91f0b421e7c", "4b48e19d9cf64accaa3db9332e48d91f0b421e7c", "17b252c4b28079fcf652712fe25e643893901361", "e848aef6d94bf824baf0f91215659f3387786ce9", "17b252c4b28079fcf652712fe25e643893901361"]}]